<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-16T15:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mrwt6z</id>
    <title>so whats the easiest way to get started ?</title>
    <updated>2025-08-16T14:28:04+00:00</updated>
    <author>
      <name>/u/Environmental-Elk959</name>
      <uri>https://old.reddit.com/user/Environmental-Elk959</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey guys,&lt;/p&gt; &lt;p&gt;first of all a desclaimer that when it comes to local LLMs i am completely a noob.&lt;/p&gt; &lt;p&gt;i have an old mining rig with 4 RTX 3060 and 4 RTX 3070, all on risers and connected to a windows machine with an i7 8th gen, 16GB RAM, all GPUs are properly installed and windows see all of them.&lt;/p&gt; &lt;p&gt;so i was told the easiest way to get started is LM Studio (yes i know ubuntu is more effienent but i just want to see what kind of t/p i can get) but i tried loading the 20B varient (15gb size) of qwen coder and the latest chatgpt oss (20B varient)(11 gb in size) and non worked. one issue with volkan allocation thingy and the other another memory issue.&lt;/p&gt; &lt;p&gt;so i need some basic guidance here:&lt;/p&gt; &lt;p&gt;- is the hardware good enough or rubbish ?&lt;/p&gt; &lt;p&gt;- is the cpu/ram config fine ? or do i need to upgrade them to use local llm ?&lt;/p&gt; &lt;p&gt;- is 20B parameter too much ? how can i estimate the right parameter size i can handle ?&lt;/p&gt; &lt;p&gt;- i can get 3 more semilar rigs, is there a way to run them as a big cluster ?&lt;/p&gt; &lt;p&gt;- whats the story with SLM (if i only need a conversational chatbot, how do i idintify the right llm to use)&lt;/p&gt; &lt;p&gt;- what is quantization ? as a user should i know that or its a the trainers/creators worrysome&lt;/p&gt; &lt;p&gt;sorry for the noobish questions again&lt;/p&gt; &lt;p&gt;update: deepseek R1 8b did run (5gb model) but at 2,7 t/s, i am sure something wrong&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Environmental-Elk959"&gt; /u/Environmental-Elk959 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrwt6z/so_whats_the_easiest_way_to_get_started/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrwt6z/so_whats_the_easiest_way_to_get_started/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrwt6z/so_whats_the_easiest_way_to_get_started/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T14:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr4fdk</id>
    <title>Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released</title>
    <updated>2025-08-15T17:27:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt; &lt;img alt="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" src="https://preview.redd.it/3beo5klvv7jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dd6143a597d6b0048011fe35125ed52dd343f90" title="Qwen 2.5 (7B/14B/32B) Finetunes Outperforming Opus 4 &amp;amp; Sonnet 4/3.5 on Out-of-Distribution Tasks with RL --- Code, Weights, Data, and Paper Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3beo5klvv7jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr4fdk/qwen_25_7b14b32b_finetunes_outperforming_opus_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T17:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mruird</id>
    <title>Beginner Question: Am I running LLMs unsafely?</title>
    <updated>2025-08-16T12:57:10+00:00</updated>
    <author>
      <name>/u/Saruphon</name>
      <uri>https://old.reddit.com/user/Saruphon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m very new to LLMs and only have minimal programming knowledge. My background is in data analytics and data science, but I don’t have any formal programming training. I only know Python and SQL from on-the-job experience. Honestly, I’m also the kind of person who might run sudo rm -rf --no-preserve-root / if someone explained it convincingly enough, so I’m trying to be extra careful about safety here.&lt;/p&gt; &lt;p&gt;Right now, I’ve been running .safetensors for SDXL (via StableDiffusionXLPipeline) and .guff files for LLMs like Gemma and Qwen (via LlamaCpp library) directly in my Python IDE (Spyder) and communicate with them via the Spyder console. I prefer working in a Python IDE rather than the terminal if possible, but if it’s truly necessary for safety, I’ll put in the effort to learn how to use the terminal properly. I will likely get a new expensive PC soon and do not want to accidentally destroy it due to unsafe practices I could avoid (my hardware-related skills aren’t great as well as I’ve killed 2 PCs in the past).&lt;/p&gt; &lt;p&gt;I’m mostly experimenting with LLMs and RAG at the moment to improve my skills. My main goal is to use LLMs purely for data analytics, RAG projects, and maybe coding once I get a more powerful PC that can run larger models. For context, my data analysis workflow would mostly involve running loops of prompts, performing classification tasks, or having the LLM process data and then save results to CSV or JSON files. For now, I only plan to run everything locally, with no online access or API exposure.&lt;/p&gt; &lt;p&gt;Recently I came across &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mrrvxb/if_i_selfhost_an_llm_like_deepseek_how_do_i/"&gt;this Reddit post&lt;/a&gt; which suggests that the way I’m doing things might actually be unsafe. In particular, one of the comments &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mrrvxb/comment/n8zo8ag/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here&lt;/a&gt; talks about using containerized or sandboxed environments (like Docker or Firecracker) instead.&lt;/p&gt; &lt;p&gt;So my questions are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is my current approach (running model files directly in Spyder) actually unsafe? If so, what are the main risks? (I’m especially worried about the idea of an LLM somehow running code behind my back, rather than just suggesting bad code for me to run — is that even possible?)&lt;/li&gt; &lt;li&gt;Should I immediately switch to Docker, a virtual machine, or some other isolated runtime?&lt;/li&gt; &lt;li&gt;For someone like me (data background, beginner at devops/programming tools, prefers IDE over terminal) who wants to use LLMs for local analytics projects and eventual RAG systems, what’s the simplest safe setup you’d recommend?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance for helping a beginner stay safe while learning! Hopefully I don’t sound too clueless here…&lt;/p&gt; &lt;p&gt;EDIT:&lt;br /&gt; Also if possible can you help me with additional PC Build question:&lt;br /&gt; | plan to get a PC with RTX 5090 (I dont have easy access to dual 3090 and other set up)&lt;br /&gt; 1) Is there advantage to getting Intel 285k over 265k or is it the advantage minimal.&lt;br /&gt; 2) Is 128 GB ram for offloading enough, or should i just go for 256 GB Ram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saruphon"&gt; /u/Saruphon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mruird/beginner_question_am_i_running_llms_unsafely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mruird/beginner_question_am_i_running_llms_unsafely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mruird/beginner_question_am_i_running_llms_unsafely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T12:57:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrr72l</id>
    <title>Are there lightweight LLM vscode plugin for local models?</title>
    <updated>2025-08-16T10:18:37+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, so roocode, cline, etc see to be very fancy and have large structured contexts that can overwhelm local models (and require a lot of prompt processing). I have a 24gb MacBook and run a 3 bit version of qwen3 30b coder, I might buy a new 64 or 96fb MacBook Pro. I figure that lets me run like oss-120b or glm4.5 air. Still those can get confused by the huge contexts cline and roocode gov eto the LLM. Are there alternative coding tools optimized to have a lean/modest structured prompt, designed to work very well with mid size local models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrr72l/are_there_lightweight_llm_vscode_plugin_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrr72l/are_there_lightweight_llm_vscode_plugin_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrr72l/are_there_lightweight_llm_vscode_plugin_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T10:18:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mryqbh</id>
    <title>First NPU-only Vision Model on AMD Ryzen AI</title>
    <updated>2025-08-16T15:38:47+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team working on &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a lightweight runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek, and now Gemma (Vision)&lt;/strong&gt; exclusively on the AMD Ryzen™ AI NPU.&lt;/p&gt; &lt;p&gt;⚡ Runs &lt;strong&gt;entirely on the NPU&lt;/strong&gt; — no CPU or iGPU fallback.&lt;br /&gt; 👉 Think Ollama, but &lt;strong&gt;purpose-built for AMD NPUs&lt;/strong&gt;, with both CLI and REST API modes.&lt;/p&gt; &lt;h1&gt;🔑 Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Supports: &lt;strong&gt;LLaMA3.1/3.2, Qwen3, DeepSeek-R1, Gemma3:4B (Vision)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;First &lt;strong&gt;NPU-only VLM&lt;/strong&gt; shipped&lt;/li&gt; &lt;li&gt;Up to &lt;strong&gt;128K context&lt;/strong&gt; (LLaMA3.1/3.2, Gemma3:4B)&lt;/li&gt; &lt;li&gt;~11× power efficiency vs CPU/iGPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👉 Repo here: &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;GitHub – FastFlowLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love to hear your feedback if you give it a spin — what works, what breaks, and what you’d like to see next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryqbh/first_npuonly_vision_model_on_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryqbh/first_npuonly_vision_model_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mryqbh/first_npuonly_vision_model_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6929</id>
    <title>Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI</title>
    <updated>2025-08-15T18:33:10+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt; &lt;img alt="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" src="https://external-preview.redd.it/YJQ41TIHjSIHRPnnPYpoNGj-_TlQpYrRQFRV8JkhNlo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc9954302a8136bd445b302a8ce0f2c5e742b5e" title="Intel adds Shared GPU Memory Override feature for Core Ultra systems, enables larger VRAM for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-adds-shared-gpu-memory-override-feature-for-core-ultra-systems-enables-larger-vram-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6929/intel_adds_shared_gpu_memory_override_feature_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr8rfh</id>
    <title>Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation</title>
    <updated>2025-08-15T20:06:07+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt; &lt;img alt="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" src="https://preview.redd.it/30drwal3p8jf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=587da208e46dce9fd645a884879872736fb35f43" title="Analysis on hyped Hierarchical Reasoning Model (HRM) by ARC-AGI foundation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arcprize.org/blog/hrm-analysis"&gt;ARC AGI analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/30drwal3p8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr8rfh/analysis_on_hyped_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T20:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrm7oz</id>
    <title>LMArena’s leaderboard can be misleading</title>
    <updated>2025-08-16T05:47:59+00:00</updated>
    <author>
      <name>/u/Beneficial_Tough_367</name>
      <uri>https://old.reddit.com/user/Beneficial_Tough_367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LMArena’s leaderboard can be misleading: new models with fewer votes (e.g. GPT-5) can top the chart before scores stabilize, while older models (e.g. Gemini) are based on much larger and more robust sample sizes.&lt;/p&gt; &lt;p&gt;I think we need a “matched sample” ranking, only compare models based on their last N votes, to get a fair picture. Otherwise, the leaderboard is systematically biased.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial_Tough_367"&gt; /u/Beneficial_Tough_367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrm7oz/lmarenas_leaderboard_can_be_misleading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrpyln</id>
    <title>How to use GLM 4.5 as my coding agent in vs code?</title>
    <updated>2025-08-16T09:10:43+00:00</updated>
    <author>
      <name>/u/Asta-12</name>
      <uri>https://old.reddit.com/user/Asta-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to use GLM 4.5 as my coding agent in vs code?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asta-12"&gt; /u/Asta-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrpyln/how_to_use_glm_45_as_my_coding_agent_in_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrpyln/how_to_use_glm_45_as_my_coding_agent_in_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrpyln/how_to_use_glm_45_as_my_coding_agent_in_vs_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrqr5z</id>
    <title>Qwen 3 Coder 30b + Cline = kokoro powered API! :)</title>
    <updated>2025-08-16T09:55:07+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I needed a replacement for AWS Polly that offered multiple voices so I can have different characters use different voices in my game: &lt;a href="https://foreverfantasy.org"&gt;https://foreverfantasy.org&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I gave Qwen 3 coder the hello world example from the kokoro README and it nailed it in one shot!&lt;/p&gt; &lt;p&gt;Full details and code on the blog (no ads)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://convergence.ninja/post/blogs/000017-Qwen3Coder30bRules.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqr5z/qwen_3_coder_30b_cline_kokoro_powered_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqr5z/qwen_3_coder_30b_cline_kokoro_powered_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrurtu</id>
    <title>How big a dataset do you need to finetune a model? Gemma3 270M, Qwen30B A3B, Gpt-OSS20B, etc.?</title>
    <updated>2025-08-16T13:07:37+00:00</updated>
    <author>
      <name>/u/zekuden</name>
      <uri>https://old.reddit.com/user/zekuden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How big a dataset do you need to finetune a model? Gemma3 270M, Qwen30B A3B, Gpt-OSS20B, etc.?&lt;/p&gt; &lt;p&gt;other model information are welcome, these are just some examples of models to finetune.&lt;/p&gt; &lt;p&gt;and get consistent results. And as i understand it, for finetuning a dataset should look like:&lt;br /&gt; prompt:&lt;br /&gt; &amp;lt;dataset here&amp;gt;&lt;/p&gt; &lt;p&gt;output:&lt;br /&gt; &amp;lt;dataset here&amp;gt;&lt;/p&gt; &lt;p&gt;is that correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekuden"&gt; /u/zekuden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T13:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mryhac</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-16T15:29:24+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/OWwyYmp4YjFoZWpmMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdb2cc2e27aa842ee354eb0dee4dd5ee4b0cd840" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are bringing Computer Use to the web, you can now control cloud desktops from JavaScript right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer use was Python only shutting out web devs. Now you can automate real UIs without servers, VMs, or any weird work arounds.&lt;/p&gt; &lt;p&gt;What you can now build : Pixel-perfect UI tests,Live AI demos,In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Read more here : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x4psh9j1hejf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrcgcr</id>
    <title>Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479.</title>
    <updated>2025-08-15T22:25:13+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt; &lt;img alt="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." src="https://external-preview.redd.it/1uympFuPK52czHQ4IvmWhNt0vnP2FK278N3meDCoUq4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d72cc8f3a3871e1aa7795e415e94129d9d9c61" title="Rival Ryzen AI Max+ 395 Mini PC 96GB for $1479." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is yet another AMD Max+ 395 machine. This is unusual in that it's 96GB instead of 64GB or 128GB. At $1479 though, it's the same price as other's 64GB machines but gives you 96GB instead.&lt;/p&gt; &lt;p&gt;It looks to use the same Sixunited MB as other Max+ machines like the GMK X2 right down to the red color of the MB.&lt;/p&gt; &lt;p&gt;Update: I ran across a video of this machine being built.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/3esEHgoymCY"&gt;https://youtu.be/3esEHgoymCY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x-plus.store/products/xrival"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrcgcr/rival_ryzen_ai_max_395_mini_pc_96gb_for_1479/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr7m2r</id>
    <title>LM Studio now supports llama.cpp CPU offload for MoE which is awesome</title>
    <updated>2025-08-15T19:23:30+00:00</updated>
    <author>
      <name>/u/carlosedp</name>
      <uri>https://old.reddit.com/user/carlosedp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt; &lt;img alt="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" src="https://b.thumbs.redditmedia.com/8_UCLmbk5AUNXfDHLBVN5lWhMbgV6ZR8UC3ks8DeLuE.jpg" title="LM Studio now supports llama.cpp CPU offload for MoE which is awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now LM Studio (from 0.3.23 build 3) supports llama.cpp &lt;code&gt;--cpu-moe&lt;/code&gt; which allows offloading the MoE weights to the CPU leaving the GPU VRAM for layer offload.&lt;/p&gt; &lt;p&gt;Using Qwen3 30B (both thinking and instruct) on a 64GB Ryzen 7 and a RTX3070 with 8GB VRAM I've been able to use 16k context and fully offload the model's layers to GPU and got about 15 tok/s which is amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlosedp"&gt; /u/carlosedp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mr7m2r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr7m2r/lm_studio_now_supports_llamacpp_cpu_offload_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T19:23:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrvuab</id>
    <title>I Want Everything Local — Building My Offline AI Workspace</title>
    <updated>2025-08-16T13:50:50+00:00</updated>
    <author>
      <name>/u/badhiyahai</name>
      <uri>https://old.reddit.com/user/badhiyahai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt; &lt;img alt="I Want Everything Local — Building My Offline AI Workspace" src="https://external-preview.redd.it/QmTTXNVh4bThHR_hqXouTK6BMQOfpcZ731LR1bUqCFY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c04a6c25796a326d0d7b9bc93e22700684157322" title="I Want Everything Local — Building My Offline AI Workspace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;I want everything local — no cloud, no remote code execution.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;That’s what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.&lt;/p&gt; &lt;p&gt;What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ability to use chat with a cloud hosted LLM,&lt;/li&gt; &lt;li&gt;Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,&lt;/li&gt; &lt;li&gt;Ability to access the internet for new content or services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.&lt;/p&gt; &lt;p&gt;So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.&lt;/p&gt; &lt;h1&gt;🧠 The Idea&lt;/h1&gt; &lt;p&gt;We wanted a system where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLMs run completely &lt;strong&gt;locally&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code executes inside a lightweight VM&lt;/strong&gt;, not on the host machine&lt;/li&gt; &lt;li&gt;Bonus: &lt;strong&gt;headless browser&lt;/strong&gt; for automation and internet access&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktxl2zdsydjf1.png?width=2668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0db9f76936f89c242c0368fdff070deb6960af4"&gt;https://preview.redd.it/ktxl2zdsydjf1.png?width=2668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0db9f76936f89c242c0368fdff070deb6960af4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were &lt;a href="https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles"&gt;accessible&lt;/a&gt; from another's account!&lt;/p&gt; &lt;h1&gt;The Stack We Used&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLMs&lt;/strong&gt;: &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for local models (also private models for now)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend UI&lt;/strong&gt;: &lt;a href="https://github.com/assistant-ui/assistant-ui"&gt;&lt;code&gt;assistant-ui&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sandboxed VM Runtime&lt;/strong&gt;: &lt;a href="https://github.com/apple/container"&gt;&lt;code&gt;container&lt;/code&gt;&lt;/a&gt; by Apple&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;a href="https://github.com/instavm/coderunner"&gt;&lt;code&gt;coderunner&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: &lt;a href="https://playwright.dev"&gt;Playwright&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;💡 We ran this entirely on Apple Silicon, using &lt;code&gt;container&lt;/code&gt; for isolation.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;🛠️ Our Attempt at a Mac App&lt;/h1&gt; &lt;p&gt;We started with zealous ambition: make it feel native. We tried using &lt;a href="http://a0.dev"&gt;&lt;code&gt;a0.dev&lt;/code&gt;&lt;/a&gt;, hoping it could help generate a Mac app. But it appears to be meant more for iOS app development — and getting it to work for MacOS was painful, to say the least.&lt;/p&gt; &lt;p&gt;Even with help from the &amp;quot;world's best&amp;quot; LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.&lt;/p&gt; &lt;p&gt;Then we tried wrapping a &lt;code&gt;NextJS&lt;/code&gt; app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.&lt;/p&gt; &lt;p&gt;So, we gave up on the Mac app. The local web version of &lt;code&gt;assistant-ui&lt;/code&gt; was good enough — simple, configurable, and didn't fight back.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qxtnf0dwydjf1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65bd766164eaf8219879cce826ea5fcb84a2e190"&gt;https://preview.redd.it/qxtnf0dwydjf1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65bd766164eaf8219879cce826ea5fcb84a2e190&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Assistant UI&lt;/h1&gt; &lt;p&gt;We thought &lt;code&gt;Assistant-UI&lt;/code&gt; provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no. So, we had to look for examples on how to go about it, and &lt;code&gt;ai-sdk&lt;/code&gt; appeared to be the popular choice. Finally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m8bdeb00zdjf1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eca9d8df54641293ff17b98ade701fa544baa8d"&gt;https://preview.redd.it/m8bdeb00zdjf1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eca9d8df54641293ff17b98ade701fa544baa8d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Tool-calling&lt;/h1&gt; &lt;p&gt;Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;responseBody: '{&amp;quot;error&amp;quot;:&amp;quot;registry.ollama.ai/library/deepseek-r1:8b does not support tools&amp;quot;}', &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.&lt;/p&gt; &lt;p&gt;At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.&lt;/p&gt; &lt;h1&gt;Containerized execution&lt;/h1&gt; &lt;p&gt;After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code. So, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at &lt;code&gt;http://coderunner.local:8222/mcp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;mcpServers&amp;quot;: { &amp;quot;coderunner&amp;quot;: { &amp;quot;httpUrl&amp;quot;: &amp;quot;http://coderunner.local:8222/mcp&amp;quot; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see below, Claude figured out it should use the tool &lt;code&gt;execute_python_code&lt;/code&gt; exposed from our isolated VM via the MCP endpoint. Aside - if you want to just use the &lt;code&gt;coderunner&lt;/code&gt; bit as an MCP to execute code with your existing tools, the code for &lt;code&gt;coderunner&lt;/code&gt; is &lt;a href="https://github.com/instavm/coderunner"&gt;public&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l1xlqa83zdjf1.png?width=1224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb7be9c8899eb659a67274ffec83b809ba8f9452"&gt;https://preview.redd.it/l1xlqa83zdjf1.png?width=1224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb7be9c8899eb659a67274ffec83b809ba8f9452&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A tangent - if you're planning to work with Apple &lt;code&gt;container&lt;/code&gt; and building VM images using it, have an abundance of patience. The build keeps failing with &lt;code&gt;Trap&lt;/code&gt; error or just hangs without any output. To continue, you should &lt;code&gt;pkill&lt;/code&gt; all container processes and restart the &lt;code&gt;container&lt;/code&gt; tool. Then remove the &lt;code&gt;buildkit&lt;/code&gt; image so that the next &lt;code&gt;build&lt;/code&gt; process fetches a fresh one. And repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Back to our app, we tested the &lt;code&gt;UI + LLMs + CodeRunner&lt;/code&gt; on a task to &lt;code&gt;edit a video&lt;/code&gt; and it worked!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b14cq1b7zdjf1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9131c4007e1d8ff6c1a60d8b4f2362902ce9c2ec"&gt;https://preview.redd.it/b14cq1b7zdjf1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9131c4007e1d8ff6c1a60d8b4f2362902ce9c2ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I asked it to address me as Lord Voldemort as a sanity check for system instructions&lt;/em&gt;&lt;/p&gt; &lt;p&gt;After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for &lt;code&gt;research&lt;/code&gt;. We chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ow78fju9zdjf1.jpg?width=1428&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=363cf853179b4c6bd1173dca1753414dd573fb61"&gt;https://preview.redd.it/ow78fju9zdjf1.jpg?width=1428&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=363cf853179b4c6bd1173dca1753414dd573fb61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this our basic set up was ready: &lt;strong&gt;Local LLM + Sandboxed arbitrary code execution + Headless browser&lt;/strong&gt; for latest information.&lt;/p&gt; &lt;h1&gt;What It Can Do (Examples)&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Do research on a topic&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate and render charts&lt;/strong&gt; from CSV using plain English&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edit videos&lt;/strong&gt; (via &lt;code&gt;ffmpeg&lt;/code&gt;) — e.g., “cut between 0:10 and 1:00”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edit images&lt;/strong&gt; — resize, crop, convert formats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install tools from GitHub&lt;/strong&gt; in a containerized space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use a headless browser&lt;/strong&gt; to fetch pages and summarize content etc.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Volumes and Isolation&lt;/h1&gt; &lt;p&gt;We mapped a volume from &lt;code&gt;~/.coderunner/assets&lt;/code&gt; (host) to &lt;code&gt;/app/uploads&lt;/code&gt; (container)&lt;/p&gt; &lt;p&gt;So files edited/generated stay in a safe shared space, &lt;strong&gt;but code never touches the host system&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Limitations &amp;amp; Next Steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently &lt;strong&gt;only works on Apple Silicon&lt;/strong&gt; (macOS 26 is optional)&lt;/li&gt; &lt;li&gt;Needs better UI for managing tools and output streaming&lt;/li&gt; &lt;li&gt;Headless browser is classified as bot by various sites&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;This is more than a just an experiment. It's a philosophy shift &lt;strong&gt;bringing compute and agency back to your machine&lt;/strong&gt;. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We didn't just imagine it. We built it. And now, you can use it too.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;🔗 Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/assistant-ui/assistant-ui"&gt;assistant-ui&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/instavm/coderunner"&gt;instavm/coderunner&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/apple/container"&gt;Apple/container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/instavm/coderunner-ui"&gt;instavm/coderunner-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badhiyahai"&gt; /u/badhiyahai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T13:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrxuwd</id>
    <title>Best Opensource LM Studio alternative</title>
    <updated>2025-08-16T15:06:22+00:00</updated>
    <author>
      <name>/u/haterloco</name>
      <uri>https://old.reddit.com/user/haterloco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best app to use llama.cpp or Ollama with a GUI on Linux.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haterloco"&gt; /u/haterloco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrp54l</id>
    <title>How do you all discover new models?</title>
    <updated>2025-08-16T08:24:24+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently trying to search Huggingface to find a model that is around 70B, has thinking built in, and is a mixture of experts. I am surprised that I can't easily select these features during the search. All that is available is the parameter count.&lt;/p&gt; &lt;p&gt;I'm feeling a bit baffled that I can't seem to figure out a way to easily search for models using a series of filters like this. &lt;/p&gt; &lt;p&gt;Am I just blind and missing something obvious? Is there a much better method for shopping for new models? Another service perhaps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrp54l/how_do_you_all_discover_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T08:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mr6sdc</id>
    <title>Jedi code Gemma 27v vs 270m</title>
    <updated>2025-08-15T18:52:56+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt; &lt;img alt="Jedi code Gemma 27v vs 270m" src="https://preview.redd.it/4icjlje4c8jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecea0a1e7e78f6cb9509e2f3f9a7433184fb5a5" title="Jedi code Gemma 27v vs 270m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 270m coding a jedi in existence&lt;/p&gt; &lt;p&gt;Quite interesting how bad the small model is to following instructions, this is the first semblence to doing what i said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4icjlje4c8jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mr6sdc/jedi_code_gemma_27v_vs_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T18:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsfcc</id>
    <title>OpenAI Cookbook - Verifying gpt-oss implementations</title>
    <updated>2025-08-16T11:21:32+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"&gt; &lt;img alt="OpenAI Cookbook - Verifying gpt-oss implementations" src="https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11ab391878f109e16178aaa55bd6d3f3b344fed6" title="OpenAI Cookbook - Verifying gpt-oss implementations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cookbook.openai.com/articles/gpt-oss/verifying-implementations"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T11:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mroal8</id>
    <title>Huihui-gpt-oss-120b-BF16-abliterated</title>
    <updated>2025-08-16T07:36:52+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt; &lt;img alt="Huihui-gpt-oss-120b-BF16-abliterated" src="https://external-preview.redd.it/Whbl3EQ8tzvwyKl63iWfJrIBTWW6XBRLW7AQQgHk37I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5c4cc8d89ef3ca3df1e29ec46225752e44231a" title="Huihui-gpt-oss-120b-BF16-abliterated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T07:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrbtqt</id>
    <title>DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM</title>
    <updated>2025-08-15T22:00:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt; &lt;img alt="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" src="https://external-preview.redd.it/dm1scXBiZnU4OWpmMbd7l6YK9EDz0b8q8nzrd_PHLYbyTzK6nb4d-_lrl57d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a680cf7593e65adcab4110d0090bab480e862303" title="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DINOv3 released yesterday, a new state-of-the-art vision backbone trained to produce rich, dense image features. I loved their demo video so much that I decided to re-create their visualization tool. &lt;/p&gt; &lt;p&gt;Everything runs locally in your browser with Transformers.js, using WebGPU if available and falling back to WASM if not. Hope you like it! &lt;/p&gt; &lt;p&gt;Link to demo + source code: &lt;a href="https://huggingface.co/spaces/webml-community/dinov3-web"&gt;https://huggingface.co/spaces/webml-community/dinov3-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yhe3jbfu89jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsoug</id>
    <title>GPT-OSS-20B is in the sweet spot for building Agents</title>
    <updated>2025-08-16T11:34:29+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest updates to llama.cpp greatly improve tool calling and stability with the OSS models. I have found that they are now quite reliable for my Agent Network, which runs a number of tools, ie; MCPs, RAG, and SQL answering, etc. The MoE and Quant enables me to run this quite easily on a 32Gb developer MacBook at ~40tks without breaking a sweat, I t’s almost game-changing! How has everyone else faired with these models??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T11:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrlpxd</id>
    <title>My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)</title>
    <updated>2025-08-16T05:21:33+00:00</updated>
    <author>
      <name>/u/FunConversation7257</name>
      <uri>https://old.reddit.com/user/FunConversation7257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt; &lt;img alt="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" src="https://external-preview.redd.it/Os4oYZsYLVlsXnga3hPOUAlxvPVzcyCPA6N9lZAIVyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7ea802471412bf40b6e93f29c186991e9a7c4e2" title="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174"&gt;https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, Codex, OpenAI's coding CLI released a way to authenticate with your ChatGPT account, and use that for usage instead of api keys. I dug through the code and saw that by using Codex CLI, you can login with your account and send requests right to OpenAI, albeit restricted by slightly tougher rate limits than on the ChatGPT app.&lt;/p&gt; &lt;p&gt;However, still was decent enough for my use case, so I made a python script which allows one to login with their ChatGPT account, and then serve a OpenAI compatible endpoint you can use programmatically or via a chat app of your choice.&lt;br /&gt; Might be useful for you too for data analysis, or just chatting in a better app than the ChatGPT desktop app. It's also customisable with thinking effort, and even sends back thinking summaries, and can use tools.&lt;/p&gt; &lt;p&gt;Not strictly &amp;quot;local&amp;quot;, but brought that 2023 vibe back, and thought it was kinda cool.&lt;/p&gt; &lt;p&gt;Will try to make it a better package soon than just python files.&lt;br /&gt; Github link: &lt;a href="https://github.com/RayBytes/ChatMock"&gt;https://github.com/RayBytes/ChatMock&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: have now also released a macos gui version, should be easier to use than simply running the flask server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FunConversation7257"&gt; /u/FunConversation7257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrqj6y</id>
    <title>Moxie goes local</title>
    <updated>2025-08-16T09:42:55+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt; &lt;img alt="Moxie goes local" src="https://external-preview.redd.it/NjRrNWZhaTZyY2pmMSz-4GeMjZaaPuK_BtqJdauJLy8SeG31djvp2OceGUPi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f56d4e2d6d85d38d0a6fee04a3f5cd06f2d2d7df" title="Moxie goes local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a localllama version of the OpenMoxie&lt;/p&gt; &lt;p&gt;It uses faster-whisper on the local for STT or the OpenAi whisper api (when selected in setup)&lt;/p&gt; &lt;p&gt;Supports LocalLLaMA, or OpenAi for conversations.&lt;/p&gt; &lt;p&gt;I also added support for XAI (Grok3 et al ) using the XAI API.&lt;/p&gt; &lt;p&gt;allows you to select what AI model you want to run for the local service.. right now 3:2b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiwf36o6rcjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfqsd</id>
    <title>Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months</title>
    <updated>2025-08-16T00:40:29+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt; &lt;img alt="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" src="https://preview.redd.it/kbdu3pyq1ajf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6766455308e18a9b20204df7a38e2406f44eff0" title="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kbdu3pyq1ajf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
