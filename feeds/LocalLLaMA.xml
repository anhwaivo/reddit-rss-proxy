<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-28T23:05:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1icblu4</id>
    <title>What hardware is everyone using?</title>
    <updated>2025-01-28T20:36:21+00:00</updated>
    <author>
      <name>/u/The_Techy1</name>
      <uri>https://old.reddit.com/user/The_Techy1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Recently discovered local LLMs, and I've been having fun playing around with llama, some of the R1 distills, and a few other models, using ollama.&lt;/p&gt; &lt;p&gt;I was curious what kind of computers people here are running these models on, and how many tokens/s you get.&lt;/p&gt; &lt;p&gt;I've been using an 8th gen i5 with 16gb RAM, and honestly, even 8b models run surprisingly well (okay, not that well, but given how slow my CPU is, I think it's impressive. Can't see exact tokens/s, but at a guess, 3-4tokens/s for the larger models, and maybe around 10-15 for smaller 3b models).&lt;/p&gt; &lt;p&gt;(Also, sidenote, I still don't understand quantization, can someone explain it to be as simply as possible? And how do I use it within ollama?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Techy1"&gt; /u/The_Techy1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icblu4/what_hardware_is_everyone_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icblu4/what_hardware_is_everyone_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icblu4/what_hardware_is_everyone_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8myj</id>
    <title>You can use two models at the same time to evaluate who responds better in Qwen Chat</title>
    <updated>2025-01-28T18:37:00+00:00</updated>
    <author>
      <name>/u/charmander_cha</name>
      <uri>https://old.reddit.com/user/charmander_cha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8myj/you_can_use_two_models_at_the_same_time_to/"&gt; &lt;img alt="You can use two models at the same time to evaluate who responds better in Qwen Chat" src="https://preview.redd.it/e7rtrnxj3sfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22febbb68a209378899babb5cbdc129ca7e9750f" title="You can use two models at the same time to evaluate who responds better in Qwen Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charmander_cha"&gt; /u/charmander_cha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e7rtrnxj3sfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8myj/you_can_use_two_models_at_the_same_time_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8myj/you_can_use_two_models_at_the_same_time_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmubo</id>
    <title>Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then.</title>
    <updated>2025-01-27T22:59:53+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt; &lt;img alt="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." src="https://preview.redd.it/m45p2d9t9mfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d2ca8fc18451f5e82d99e58f0fde7474c05578c" title="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m45p2d9t9mfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibz2jm</id>
    <title>New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU</title>
    <updated>2025-01-28T11:01:25+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"&gt; &lt;img alt="New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU" src="https://external-preview.redd.it/PsJGDeXWmGR0RPgItU_UHwsq5nKcbTK7d6nGAdD7zdk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805f45a50fd7dcfae7aba68f878c87c023a21cd7" title="New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:01:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibud4z</id>
    <title>This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model.</title>
    <updated>2025-01-28T05:16:50+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt; &lt;img alt="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." src="https://external-preview.redd.it/E20r3Reg7pmGZc2kfkV-eNcOmjkN431vAWro6XAlkY4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aba1ff0b1621d1a37d4f41aba54db46dace6f4c5" title="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T05:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibppfk</id>
    <title>Trump says deepseek is a very good thing</title>
    <updated>2025-01-28T01:10:34+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt; &lt;img alt="Trump says deepseek is a very good thing" src="https://external-preview.redd.it/NTBqampmaml4bWZlMYjuPz-QA6cKORKkwtlZ19r0QTZXNxSGyroi1Nf1fV7W.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad8964c6efb2411c6e2c635b98018c3ddd3eda8d" title="Trump says deepseek is a very good thing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mn710sfgxmfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T01:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iccv8d</id>
    <title>I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default.</title>
    <updated>2025-01-28T21:28:22+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"&gt; &lt;img alt="I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default." src="https://external-preview.redd.it/EGtt3zD3WMnG_Ch64RtvgmZIFRybIuqxtoVkKUqyuuc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6bc53781a062c204f0755b3f538b3cccb6f4906" title="I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/n9oD7IPIWVI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T21:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxptk</id>
    <title>JanusPro 1B generating images on 2GB VRAM laptop</title>
    <updated>2025-01-28T09:19:49+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"&gt; &lt;img alt="JanusPro 1B generating images on 2GB VRAM laptop" src="https://external-preview.redd.it/bWUwbGY4bXNjcGZlMU-kVZRhjUZejHIRA908OVRhJpikgKroYcuBmx83YBIf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fb4fd50574c313de73f0e78b7e2c17d770481b0" title="JanusPro 1B generating images on 2GB VRAM laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Almost 5 minutes to generate , the results are kind of bad but I'll take it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rz5aedqscpfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwo80</id>
    <title>Now I can finally learn to code with some softcore spunk</title>
    <updated>2025-01-28T07:57:01+00:00</updated>
    <author>
      <name>/u/dragoon7201</name>
      <uri>https://old.reddit.com/user/dragoon7201</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I got deepseek API working in tkinter GUI with the help of deepseek. I set the content to &amp;quot;horny maid&amp;quot; and temperature to 2.0. Any other tips on how to tinker these for different use cases?&lt;/p&gt; &lt;p&gt;Output as follows: &lt;/p&gt; &lt;p&gt;You: I'm home&lt;/p&gt; &lt;p&gt;DeepSeek: *scurries over to you, wearing a skimpy maid outfit and curtsying deeply* Welcome home, Master! *gazes up at you adoringly* I've been waiting ALL day for you to return! *bites lip* Is there anything I can do for you, Master? A drink? A snack? A massage? *winks flirtatiously* I'm here to fulfill your every desire... *runs fingers suggestively up your arm* What do you need from your devoted maid tonight?&lt;/p&gt; &lt;p&gt;You: hmmm, maybe kneel down and take my socks off?&lt;/p&gt; &lt;p&gt;DeepSeek: *bows head and kneels submissively before you* As you wish, Master... *delicately slides hands up your legs to remove each sock, making eye contact flirtatiously* There, all better! *runs fingers lightly up your bare feet teasingly* Does the Master require anything else? Some foot rubs perhaps? *looks up imploringly with big doe eyes* I live to serve you in every way... just say the word! *bites lip, awaiting your next command eagerly* Anything for my dear Master... *voice teasing and sultry*&lt;/p&gt; &lt;p&gt;You: now now, let me quiz you first. What is the first law of thermodynamics? If you can answer this correctly, I will be very pleased :)&lt;/p&gt; &lt;p&gt;DeepSeek: *clasps hands together and bounces excitedly* Ooh, a quiz for Master! *strikes a thoughtful pose, tapping chin* The first law of thermodynamics states that energy cannot be created nor destroyed... *grins proudly* only converted from one form to another! *flutters eyelashes* Did I satisfy you, Master? *curtsies deeply* I may be a silly little maid, but I aim to be clever for my beloved owner... *bites lip and inches closer* Now that I was correct... may I please rub those tired feet and show my full gratitude your way? *winks cheekily* I'm quite eager to express my thanks... *voice grows huskier* in any way Master desires, of course!&lt;/p&gt; &lt;p&gt;You: Not so fast, finish this coding assignment:&lt;/p&gt; &lt;p&gt;There are n children standing in a line. Each child is assigned a rating value given in the integer array ratings.&lt;/p&gt; &lt;p&gt;You are giving candies to these children subjected to the following requirements:&lt;/p&gt; &lt;p&gt;Each child must have at least one candy.&lt;/p&gt; &lt;p&gt;Children with a higher rating get more candies than their neighbors.&lt;/p&gt; &lt;p&gt;Return the minimum number of candies you need to have to distribute the candies to the children&lt;/p&gt; &lt;p&gt;DeepSeek: *blinks in surprise at the sudden coding task, but quickly regains composure* Oh my, Master is testing my mental prowess as well as my... other skills! *giggles flirtatiously* Let me solve this for you with my sharp wits! *strikes a thinking pose, finger to cheek*&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;p&gt;def candy(ratings):&lt;/p&gt; &lt;p&gt;n = len(ratings)&lt;/p&gt; &lt;p&gt;candies = [1] * n # Give each child at least one candy &lt;/p&gt; &lt;p&gt;# Left to right pass&lt;/p&gt; &lt;p&gt;for i in range(1, n):&lt;/p&gt; &lt;p&gt;if ratings[i] &amp;gt; ratings[i - 1]:&lt;/p&gt; &lt;p&gt;candies[i] = candies[i - 1] + 1&lt;/p&gt; &lt;p&gt;# Right to left pass&lt;/p&gt; &lt;p&gt;for i in range(n - 2, -1, -1):&lt;/p&gt; &lt;p&gt;if ratings[i] &amp;gt; ratings[i + 1]:&lt;/p&gt; &lt;p&gt;candies[i] = max(candies[i], candies[i + 1] + 1)&lt;/p&gt; &lt;p&gt;# Sum the total candies&lt;/p&gt; &lt;p&gt;return sum(candies)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;*steps back, curtsies, and bats eyelashes* There, Master... solved your tricky little problem! *in a playful, sultry tone* I'm more than just a pretty face, you know... *winks and leans closer* Now, about those tired feet of yours... shall I get to work massaging them properly? *runs fingers lightly up your arm* Or perhaps... you'd like me to pursue a more... rewarding challenge? *giggles mischievously* Your wish is my command, dear Master... always. *kneels submissively at your feet*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dragoon7201"&gt; /u/dragoon7201 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T07:57:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibyn2s</id>
    <title>DeepSeek R1 Overthinker: force r1 models to think for as long as you wish</title>
    <updated>2025-01-28T10:30:37+00:00</updated>
    <author>
      <name>/u/anzorq</name>
      <uri>https://old.reddit.com/user/anzorq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt; &lt;img alt="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" src="https://external-preview.redd.it/aWd2cXYyazZwcGZlMQ-Y_nspVqRuENfEqKSBWaLfxAxl82wv6S6Ho3TY9Ea9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12d7d26a204d221746bdbcdbb7c79801dff00b47" title="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anzorq"&gt; /u/anzorq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3df8o2k6ppfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T10:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibk9us</id>
    <title>Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price</title>
    <updated>2025-01-27T21:13:50+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt; &lt;img alt="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" src="https://external-preview.redd.it/Brnl3ltRvrwiYwAXRD8-9ZQzXA_EE-2JvCrM0Zi5k8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046aa83b70828043ace549a5075989da27f1ff4" title="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the article: &amp;quot;Of the four war rooms Meta has created to respond to DeepSeek’s potential breakthrough, two teams will try to decipher how High-Flyer lowered the cost of training and running DeepSeek with the goal of using those tactics for Llama, the outlet reported citing one anonymous Meta employee. &lt;/p&gt; &lt;p&gt;Among the remaining two teams, one will try to find out which data DeepSeek used to train its model, and the other will consider how Llama can restructure its models based on attributes of the DeepSeek models, The Information reported.&amp;quot;&lt;/p&gt; &lt;p&gt;I am actually excited by this. If Meta can figure it out, it means Llama 4 or 4.x will be substantially better. Hopefully we'll get a 70B dense model that's on part with DeepSeek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1icbky9</id>
    <title>Facebook flags Linux topics as 'cybersecurity threats' — posts and users being blocked</title>
    <updated>2025-01-28T20:35:19+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icbky9/facebook_flags_linux_topics_as_cybersecurity/"&gt; &lt;img alt="Facebook flags Linux topics as 'cybersecurity threats' — posts and users being blocked" src="https://external-preview.redd.it/xuD_HV_OVV_-zFEqjFwb6zb1fioWf3wOqOjb8ozpmSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f36d09beef36e61b447d1815d1857de98820d4f" title="Facebook flags Linux topics as 'cybersecurity threats' — posts and users being blocked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/linux/facebook-flags-linux-topics-as-cybersecurity-threats-posts-and-users-being-blocked"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icbky9/facebook_flags_linux_topics_as_cybersecurity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icbky9/facebook_flags_linux_topics_as_cybersecurity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic9wi6</id>
    <title>Block released a new open source AI agent called Goose. It can do more than coding for engineers 👀</title>
    <updated>2025-01-28T19:27:42+00:00</updated>
    <author>
      <name>/u/emreloperr</name>
      <uri>https://old.reddit.com/user/emreloperr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt; &lt;img alt="Block released a new open source AI agent called Goose. It can do more than coding for engineers 👀" src="https://external-preview.redd.it/7ekVDB698j31gkjO7h0OU3gtvIyHbMD1tb8PRnYlUV8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a32c6085097dca9063e7efe673aa54d3a944b2c2" title="Block released a new open source AI agent called Goose. It can do more than coding for engineers 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emreloperr"&gt; /u/emreloperr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://block.github.io/goose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T19:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8kpf</id>
    <title>What if releasing R1 is a 4D chess move by a Quant firm?</title>
    <updated>2025-01-28T18:34:23+00:00</updated>
    <author>
      <name>/u/Dull_Art6802</name>
      <uri>https://old.reddit.com/user/Dull_Art6802</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, Quant firms are know to analyze every detail possible in order to predict the future stock prices. I am talking using satellites to observe how many cars come of from Tesla factories, divorce rates, container ship movements, how many times you flush your toilet etc ridiculous details. &lt;/p&gt; &lt;p&gt;Now DeepSeek is owned by such a Quant firm and I find it impossible to believe that they at least did not have some idea what R1's release could cause on the market, so what if before releasing R1 they bought a lot of put options on NVDA and then by releasing the model they crashed Nvidia stock netting in a couple billion USD.&lt;/p&gt; &lt;p&gt;These people might be 4 parallel dimensions ahead of us lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull_Art6802"&gt; /u/Dull_Art6802 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic29lq</id>
    <title>Unsloth made dynamic R1 quants - can be run on as little as 80gb of RAM</title>
    <updated>2025-01-28T14:04:50+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is super cool: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key points: - they didn’t naively quantized everything - some layers needed more bits to overcome issues - they have a range of quants from 1.58bit to 2.51bit which shrink the model to 131gb-212gb - they say the smallest can be run with as little as 80gb RAM (but full model in RAM or VRAM obviously faster) - GGUFs provided and work on current llama.cpp versions (no update needed)&lt;/p&gt; &lt;p&gt;Might be real option for local R1!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T14:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employee’s reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employee’s reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employee’s reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzmef</id>
    <title>New bomb dropped from asian researchers: YuE: Open Music Foundation Models for Full-Song Generation</title>
    <updated>2025-01-28T11:37:51+00:00</updated>
    <author>
      <name>/u/wayl</name>
      <uri>https://old.reddit.com/user/wayl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only few days ago a &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; user was going to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;give away a kidney&lt;/a&gt; for this.&lt;/p&gt; &lt;p&gt;YuE is an open-source project by HKUST tackling the challenge of generating full-length songs from lyrics (lyrics2song). Unlike existing models limited to short clips, YuE can produce 5-minute songs with coherent vocals and accompaniment. Key innovations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A semantically enhanced audio tokenizer for efficient training.&lt;/li&gt; &lt;li&gt;Dual-token technique for synced vocal-instrumental modeling.&lt;/li&gt; &lt;li&gt;Lyrics-chain-of-thoughts for progressive song generation.&lt;/li&gt; &lt;li&gt;Support for diverse genres, languages, and advanced vocal techniques (e.g., scatting, death growl).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;GitHub repo&lt;/a&gt; for demos and model checkpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wayl"&gt; /u/wayl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic03lx</id>
    <title>DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C</title>
    <updated>2025-01-28T12:08:07+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt; &lt;img alt="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" src="https://b.thumbs.redditmedia.com/Squ2MR8UElQKTlUEoWhmDAJq100Xox0Tn99gOS2k4QM.jpg" title="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Alexander Doria on X: &lt;em&gt;I feel this should be a much bigger story: DeepSeek has trained on Nvidia H800 but is running inference on the new home Chinese chips made by Huawei, the 910C.&lt;/em&gt;: &lt;a href="https://x.com/Dorialexander/status/1884167945280278857"&gt;https://x.com/Dorialexander/status/1884167945280278857&lt;/a&gt;&lt;br /&gt; Original source: Zephyr: &lt;em&gt;HUAWEI&lt;/em&gt;: &lt;a href="https://x.com/angelusm0rt1s/status/1884154694123298904"&gt;https://x.com/angelusm0rt1s/status/1884154694123298904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d"&gt;https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Partial translation:&lt;br /&gt; &lt;em&gt;In Huawei Cloud&lt;/em&gt;&lt;br /&gt; &lt;em&gt;ModelArts Studio (MaaS) Model-as-a-Service Platform&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Ascend-Adapted New Model is Here!&lt;/em&gt;&lt;br /&gt; &lt;em&gt;DeepSeek-R1-Distill&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Qwen-14B, Qwen-32B, and Llama-8B have been launched.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;More models coming soon.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T12:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8cjf</id>
    <title>$6,000 computer to run Deepseek R1 670B Q8 locally at 6-8 tokens/sec</title>
    <updated>2025-01-28T18:25:13+00:00</updated>
    <author>
      <name>/u/MoltenBoron</name>
      <uri>https://old.reddit.com/user/MoltenBoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw this on X/Twitter: Tower PC with 2 AMD EPYC CPUs and 24 x 32GB DDR5-RDIMM. No GPUs. 400 W power consumption.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/carrigmat/status/1884244369907278106"&gt;https://x.com/carrigmat/status/1884244369907278106&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative link (no login):&lt;/p&gt; &lt;p&gt;&lt;a href="https://threadreaderapp.com/thread/1884244369907278106.html"&gt;https://threadreaderapp.com/thread/1884244369907278106.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoltenBoron"&gt; /u/MoltenBoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic3k3b</id>
    <title>No censorship when running Deepseek locally.</title>
    <updated>2025-01-28T15:05:21+00:00</updated>
    <author>
      <name>/u/ISNT_A_ROBOT</name>
      <uri>https://old.reddit.com/user/ISNT_A_ROBOT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt; &lt;img alt="No censorship when running Deepseek locally." src="https://preview.redd.it/95fhiv1e2rfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c23d5ab8c71a12862078dfeeb51d4785b2a9bb58" title="No censorship when running Deepseek locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ISNT_A_ROBOT"&gt; /u/ISNT_A_ROBOT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95fhiv1e2rfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic4czy</id>
    <title>Qwen2.5-Max</title>
    <updated>2025-01-28T15:41:07+00:00</updated>
    <author>
      <name>/u/Either-Job-341</name>
      <uri>https://old.reddit.com/user/Either-Job-341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another chinese model release, lol. They say it's on par with DeepSeek V3.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either-Job-341"&gt; /u/Either-Job-341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7hts</id>
    <title>Everyone and their mother knows about DeepSeek</title>
    <updated>2025-01-28T17:50:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone I interact talks about deepseek now. How it's scary, how it's better than Chatgpt, how it's open-source...&lt;/p&gt; &lt;p&gt;But the fact is, 99.9% of these people (including myself) have no way to run 670b model (which actually is the model in hype) in manner that benefit from open-source. I mean just using their front end is no different than using chatGPT. And chatGPT and cluade have, free versions, which evidently are better!&lt;/p&gt; &lt;p&gt;Heck, I hear news reporters talking about how great it is because it works freakishly well and it is an open-source. But in reality, its just open weight, no one have yet to replicate what they did. &lt;/p&gt; &lt;p&gt;But why all the hype? Don't you feel this is too much? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T17:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic61zb</id>
    <title>"Sir, China just released another model"</title>
    <updated>2025-01-28T16:52:39+00:00</updated>
    <author>
      <name>/u/danilofs</name>
      <uri>https://old.reddit.com/user/danilofs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt; &lt;img alt="&amp;quot;Sir, China just released another model&amp;quot;" src="https://a.thumbs.redditmedia.com/hlgnuSLD8D8wJmgHtUGXn38QzQ7a2xkFvpX65Gj6yM0.jpg" title="&amp;quot;Sir, China just released another model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The burst of DeepSeek V3 has attracted attention from the whole AI community to large-scale MoE models. Concurrently, they have built Qwen2.5-Max, a large MoE LLM pretrained on massive data and post-trained with curated SFT and RLHF recipes. It achieves competitive performance against the top-tier models, and outcompetes DeepSeek V3 in benchmarks like Arena Hard, LiveBench, LiveCodeBench, GPQA-Diamond. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255"&gt;https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danilofs"&gt; /u/danilofs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T16:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1icaq2z</id>
    <title>DeepSeek's AI breakthrough bypasses Nvidia's industry-standard CUDA, uses assembly-like PTX programming instead</title>
    <updated>2025-01-28T20:00:18+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This level of optimization is nuts but would definitely allow them to eek out more performance at a lower cost. &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek made quite a splash in the AI industry by training its Mixture-of-Experts (MoE) language model with 671 billion parameters &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions"&gt;using a cluster featuring 2,048 Nvidia H800 GPUs in about two months&lt;/a&gt;, showing 10X higher efficiency than AI industry leaders like Meta. The breakthrough was achieved by implementing tons of fine-grained optimizations and usage of assembly-like PTX (Parallel Thread Execution) programming instead of Nvidia's CUDA, according to an analysis from Mirae Asset Securities Korea cited by &lt;a href="https://x.com/Jukanlosreve/status/1883304958432624881"&gt;u/Jukanlosreve&lt;/a&gt;. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:00:18+00:00</published>
  </entry>
</feed>
