<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-12T16:07:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j9hqni</id>
    <title>GRPO on a diffusion model - Unsloth?</title>
    <updated>2025-03-12T11:19:28+00:00</updated>
    <author>
      <name>/u/heisenbork4</name>
      <uri>https://old.reddit.com/user/heisenbork4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know if unsloth can load diffusion LLMs? I don't think I see any in the list of supported models...&lt;/p&gt; &lt;p&gt;I wondered if it might be possible to try training a reasoning model following their GRPO tutorial (&lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo"&gt;https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo&lt;/a&gt;), but using the dLLM because it generates faster. I have a very cool application in mind, and maybe even some half decent training data I can line up for it.&lt;/p&gt; &lt;p&gt;There's probably more to it, like getting LoRA support working for dLLMs, but I'd love to give this a go if anyone has any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/heisenbork4"&gt; /u/heisenbork4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hqni/grpo_on_a_diffusion_model_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:19:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9erfw</id>
    <title>Try Gemma 3 with our new Gemma Python library!</title>
    <updated>2025-03-12T07:43:09+00:00</updated>
    <author>
      <name>/u/ResponsibleSolid8404</name>
      <uri>https://old.reddit.com/user/ResponsibleSolid8404</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleSolid8404"&gt; /u/ResponsibleSolid8404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gemma-llm.readthedocs.io/en/latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T07:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8tfh5</id>
    <title>Reka Flash 3, New Open Source 21B Model</title>
    <updated>2025-03-11T15:29:02+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tweet: &lt;a href="https://x.com/RekaAILabs/status/1899481289495031825"&gt;https://x.com/RekaAILabs/status/1899481289495031825&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;https://huggingface.co/RekaAI/reka-flash-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.reka.ai/news/introducing-reka-flash"&gt;https://www.reka.ai/news/introducing-reka-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T15:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gfki</id>
    <title>Manus is IMPRESSIVE But</title>
    <updated>2025-03-12T09:51:32+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In just 3 hours after its release, the open-source community responded with:&lt;/p&gt; &lt;p&gt;🦉 Owl by CAMEL-AI - 10.2K Stars -&amp;gt; &lt;a href="http://github.com/camel-ai/owl"&gt;github.com/camel-ai/owl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open Manus 30K Stars -&amp;gt; &lt;a href="http://github.com/mannaandpoem/O%E2%80%A6"&gt;github.com/mannaandpoem/O…&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The community moves really FAST.⚡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gfki/manus_is_impressive_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gfki/manus_is_impressive_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gfki/manus_is_impressive_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T09:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9fgyd</id>
    <title>smOllama – A tiny, no-Bloat chat interface for Ollama</title>
    <updated>2025-03-12T08:35:46+00:00</updated>
    <author>
      <name>/u/GUNNM_VR</name>
      <uri>https://old.reddit.com/user/GUNNM_VR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt; &lt;img alt="smOllama – A tiny, no-Bloat chat interface for Ollama" src="https://a.thumbs.redditmedia.com/HgzECGrbkfEPXlBWKXdlsBaI_EFQkyJkyVUIPQYlew0.jpg" title="smOllama – A tiny, no-Bloat chat interface for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I created &lt;strong&gt;smOllama&lt;/strong&gt;, a lightweight web interface for Ollama models. It’s just &lt;strong&gt;24KB&lt;/strong&gt;, a single &lt;strong&gt;HTML file&lt;/strong&gt;, and runs with &lt;strong&gt;zero dependencies&lt;/strong&gt; - pure HTML, CSS, and JavaScript.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use it?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No setup - just open in a browser&lt;/li&gt; &lt;li&gt;Fast and minimalist&lt;/li&gt; &lt;li&gt;Markdown &amp;amp; LaTeX support&lt;/li&gt; &lt;li&gt;Works on any device&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s simple but does the job. If you’re interested, check it out: &lt;a href="https://github.com/GUNNM-VR/smOllama"&gt;GitHub&lt;/a&gt;. Feedback is welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b5uddu0xk8oe1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e11d6eb48ffa1ab71e1774355131a168e712771"&gt;https://preview.redd.it/b5uddu0xk8oe1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e11d6eb48ffa1ab71e1774355131a168e712771&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GUNNM_VR"&gt; /u/GUNNM_VR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9fgyd/smollama_a_tiny_nobloat_chat_interface_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T08:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r2nr</id>
    <title>M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)</title>
    <updated>2025-03-11T13:44:15+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt; &lt;img alt="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" src="https://external-preview.redd.it/Z3KKrFryWMuFPZGHYHDmgzf48KaEB5A-Ze6pFibC3lk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992e2d56bcc2473a9ea6913ceadc30c7eb46bb1f" title="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=J4qwuCXyAcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9iq4w</id>
    <title>Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?</title>
    <updated>2025-03-12T12:19:07+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt; &lt;img alt="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" src="https://b.thumbs.redditmedia.com/4ay_KKrgtBYQvDUAcyo7ae-1n_dMK9Xm5bS78dAQ9jY.jpg" title="Gemma3-12b-Q4 seems a lot slower on Ollama than Deepseek-R1-14b-q8? Did I mess something up?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j9iq4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iq4w/gemma312bq4_seems_a_lot_slower_on_ollama_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9ih6e</id>
    <title>English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance</title>
    <updated>2025-03-12T12:04:59+00:00</updated>
    <author>
      <name>/u/FrostAutomaton</name>
      <uri>https://old.reddit.com/user/FrostAutomaton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt; &lt;img alt="English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance" src="https://b.thumbs.redditmedia.com/8rQh_ppCr3h41WHkafDa1NsZ4BhdFCIkML4NhFbcs2w.jpg" title="English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should be more open to making negative (positive?) results publicly available so here they are.&lt;/p&gt; &lt;p&gt;TLDR: Quantization on the .gguf format is generally done with an importance matrix which calculates how important each weight is to an LLM. I had a thought that quantizing a model based on different language importance matrices (unsurprisingly, the quants we find online are practically always made with an English importance matrix) might be less destructive to multi-lingual performance, but the results do not back this up. In fact, quanting based on these alternate importance matrices might &lt;em&gt;slightly&lt;/em&gt; harm it, though these results are not statistically significant.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hm3cfw728oe1.png?width=4764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20e10986170889985682e41d01851e7acaee1e27"&gt;Results on MixEval multiple choice questions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4325127c28oe1.png?width=4764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dc1301bbfa782ec473d162c65af547d27516182"&gt;Results on MixEval Free-form questions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experiments were performed by quanting Llama 3.3 70B based on English, Norwegian, and Malayalam importance matrices and evaluating them on MixEval in English and translated to Norwegian. I've published a write-up on Arxiv here: &lt;a href="https://arxiv.org/abs/2503.03592"&gt;https://arxiv.org/abs/2503.03592&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to improve my paper-writing skills, so critiques and suggestions for it are appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrostAutomaton"&gt; /u/FrostAutomaton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ih6e/english_k_quantization_of_llms_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j95fjo</id>
    <title>Gemma 3 is confirmed to be coming soon</title>
    <updated>2025-03-11T23:49:44+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt; &lt;img alt="Gemma 3 is confirmed to be coming soon" src="https://preview.redd.it/0iudkfrrd5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5436382ec43a49f4f586d49c5ecdf024a9a21612" title="Gemma 3 is confirmed to be coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0iudkfrrd5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T23:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gz1i</id>
    <title>I call it Daddy LLM</title>
    <updated>2025-03-12T10:29:03+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt; &lt;img alt="I call it Daddy LLM" src="https://preview.redd.it/0kbolzlck8oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0acfd5a5ea43b09dd3077e24c0ebd0fa9b2f4238" title="I call it Daddy LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4x 3090 on an Asus rampage V extreme motherboard. Using LM studio it can do 15 tokens/s on 70b models, but I think 2 3090 are enough for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0kbolzlck8oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gz1i/i_call_it_daddy_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T10:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8u90g</id>
    <title>New Gemma models on 12th of March</title>
    <updated>2025-03-11T16:03:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt; &lt;img alt="New Gemma models on 12th of March" src="https://preview.redd.it/8qfnwj7433oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4ac1bb57e9292b5685c7637a5bd9e4ac889d7c" title="New Gemma models on 12th of March" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X pos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qfnwj7433oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9drfk</id>
    <title>Gemma 3: Technical Report</title>
    <updated>2025-03-12T06:49:36+00:00</updated>
    <author>
      <name>/u/David-Kunz</name>
      <uri>https://old.reddit.com/user/David-Kunz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/David-Kunz"&gt; /u/David-Kunz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j90u4u</id>
    <title>What happened to the promised open source o3-mini ?</title>
    <updated>2025-03-11T20:32:37+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does everybody forget that this was once promised ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dt8l</id>
    <title>Gemma 3 on Huggingface</title>
    <updated>2025-03-12T06:52:16+00:00</updated>
    <author>
      <name>/u/DataCraftsman</name>
      <uri>https://old.reddit.com/user/DataCraftsman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Gemma 3! Comes in 1B, 4B, 12B, 27B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-1b-it"&gt;https://huggingface.co/google/gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;https://huggingface.co/google/gemma-3-12b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-it"&gt;https://huggingface.co/google/gemma-3-27b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text string, such as a question, a prompt, or a document to be summarized&lt;/li&gt; &lt;li&gt;Images, normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context of 8192 tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update: They have added it to Ollama already!&lt;/p&gt; &lt;p&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apparently it has an ELO of 1338 on Chatbot Arena, better than DeepSeek V3 671B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataCraftsman"&gt; /u/DataCraftsman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9iazd</id>
    <title>Gemma3 technical report detailed analysis 💎</title>
    <updated>2025-03-12T11:54:57+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt; &lt;img alt="Gemma3 technical report detailed analysis 💎" src="https://preview.redd.it/a5jgz1vlz8oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd67d4f1cca341b88b273756d22a450cb91848ec" title="Gemma3 technical report detailed analysis 💎" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a5jgz1vlz8oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9iazd/gemma3_technical_report_detailed_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9lwlw</id>
    <title>QwQ on high thinking effort setup one-shotting the bouncing balls example</title>
    <updated>2025-03-12T14:56:52+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt; &lt;img alt="QwQ on high thinking effort setup one-shotting the bouncing balls example" src="https://external-preview.redd.it/YXRidHp4czB3OW9lMV0giusrq7hVuZKSGoynYltxxXlZH0h5sQXtJgMJk00r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=015cf534b9b44ba5711148e6aabed4d1a9d18009" title="QwQ on high thinking effort setup one-shotting the bouncing balls example" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrf0zws0w9oe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9lwlw/qwq_on_high_thinking_effort_setup_oneshotting_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dmny</id>
    <title>Gemma 3 27B</title>
    <updated>2025-03-12T06:42:38+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt; &lt;img alt="Gemma 3 27B" src="https://preview.redd.it/foonq7ewf7oe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30b8fa38fe5d18d26cf0aca72f47b346cd9ad56" title="Gemma 3 27B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/foonq7ewf7oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j981ci</id>
    <title>This is the first response from an LLM that has made me cry laughing</title>
    <updated>2025-03-12T01:50:28+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt; &lt;img alt="This is the first response from an LLM that has made me cry laughing" src="https://preview.redd.it/kw96telpz5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7223e09ee41672180f06db34a031ef87fae195a" title="This is the first response from an LLM that has made me cry laughing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kw96telpz5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T01:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9gafp</id>
    <title>EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s</title>
    <updated>2025-03-12T09:40:39+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt; &lt;img alt="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" src="https://external-preview.redd.it/TzhJISgYeqmJ66gJa8ISrsZbvEzELCLxSu1XvFxOBXk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c3d163b23ac0c0c9e4c6af40dd5ca4af8494305" title="EXO Labs ran full 8-bit DeepSeek R1 distributed across 2 M3 Ultra 512GB Mac Studios - 11 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1899735281781411907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9gafp/exo_labs_ran_full_8bit_deepseek_r1_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T09:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96j3g</id>
    <title>I hacked Unsloth's GRPO code to support agentic tool use. In 1 hour of training on my RTX 4090, Llama-8B taught itself to take baby steps towards deep research! (23%→53% accuracy)</title>
    <updated>2025-03-12T00:40:21+00:00</updated>
    <author>
      <name>/u/diegocaples</name>
      <uri>https://old.reddit.com/user/diegocaples</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I've been experimenting with getting &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;Llama-8B to bootstrap its own research skills through self-play.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified Unsloth's GRPO implementation (❤️ Unsloth!) to support function calling and agentic feedback loops.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama generates its own questions about documents (you can have it learn from any documents, but I chose the Apollo 13 mission report)&lt;/li&gt; &lt;li&gt;It learns to search for answers in the corpus using a search tool&lt;/li&gt; &lt;li&gt;It evaluates its own success/failure using llama-as-a-judge&lt;/li&gt; &lt;li&gt;Finally, it trains itself through RL to get better at research&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The model starts out hallucinating and making all kinds of mistakes, but after an hour of training on my 4090, it quickly improves. It goes from getting 23% of answers correct to 53%!&lt;/p&gt; &lt;p&gt;Here is the full &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;code and instructions&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diegocaples"&gt; /u/diegocaples &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T00:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9bvll</id>
    <title>Gemma 3 27b now available on Google AI Studio</title>
    <updated>2025-03-12T05:13:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt; &lt;img alt="Gemma 3 27b now available on Google AI Studio" src="https://external-preview.redd.it/4sjcMoBy8c8hywZZD7DFEQHtY85E3eDlhYRBqIdn2eQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f55bb78cef85467f757df883df24bca99ee8925" title="Gemma 3 27b now available on Google AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://aistudio.google.com/"&gt;https://aistudio.google.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context length 128k&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output length 8k&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/2WvMTPS"&gt;&lt;strong&gt;https://imgur.com/a/2WvMTPS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd"&gt;https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9hsfc</id>
    <title>Gemma 3 - GGUFs + recommended settings</title>
    <updated>2025-03-12T11:22:36+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We uploaded GGUFs and 16-bit versions of Gemma 3 to Hugging Face! Gemma 3 is Google's new multimodal models that come in 1B, 4B, 12B and 27B sizes. We also made a step-by-step guide on How to run Gemma 3 correctly: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training Gemma 3 with Unsloth does work (yet), but there's currently bugs with training in 4-bit QLoRA (not on Unsloth's side) so 4-bit dynamic and QLoRA training with our notebooks will be released tomorrow!&lt;/p&gt; &lt;p&gt;Gemma 3 GGUF uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-GGUF"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-GGUF"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-GGUF"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Gemma 3 Instruct 16-bit uploads:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;See the rest of our models &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;in our docs&lt;/a&gt;. Remember to pull the &lt;strong&gt;LATEST llama.cpp&lt;/strong&gt; for stuff to work!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update: Confirmed with the Gemma + Hugging Face team&lt;/strong&gt;, that the recommended settings for inference are (I auto made a params file for example in &lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params"&gt;https://huggingface.co/unsloth/gemma-3-27b-it-GGUF/blob/main/params&lt;/a&gt; which can help if you use Ollama ie like &lt;code&gt;ollama run&lt;/code&gt; &lt;a href="http://hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M"&gt;&lt;code&gt;hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_M&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 1.0 top_k = 64 top_p = 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the chat template is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user\nHello!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\nHey there!&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;user\nWhat is 1+1?&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;WARNING: Do not add a &amp;lt;bos&amp;gt; to llama.cpp&lt;/strong&gt; or other inference engines, or else you will get &lt;strong&gt;DOUBLE &amp;lt;BOS&amp;gt; tokens&lt;/strong&gt;! llama.cpp auto adds the token for you!&lt;/p&gt; &lt;p&gt;More spaced out chat template (newlines rendered):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;bos&amp;gt;&amp;lt;start_of_turn&amp;gt;user Hello!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model Hey there!&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;user What is 1+1?&amp;lt;end_of_turn&amp;gt; &amp;lt;start_of_turn&amp;gt;model\n &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Read more in our docs on how to run Gemma 3 effectively: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9hsfc/gemma_3_ggufs_recommended_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T11:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9kxqq</id>
    <title>Gemma 3 - Open source efforts - llama.cpp - MLX community</title>
    <updated>2025-03-12T14:09:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt; &lt;img alt="Gemma 3 - Open source efforts - llama.cpp - MLX community" src="https://preview.redd.it/x3jb302hn9oe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57903a9d653b1c8233ccf0bdddef2bb91bebc72a" title="Gemma 3 - Open source efforts - llama.cpp - MLX community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x3jb302hn9oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9kxqq/gemma_3_open_source_efforts_llamacpp_mlx_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T14:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9jfbt</id>
    <title>M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup</title>
    <updated>2025-03-12T12:56:28+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt; &lt;img alt="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" src="https://external-preview.redd.it/H9R-bqnloX40RFggVkXhbjJRVXE72K4CKNfmbfAALSA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e94f5a141e87c847ee3d0f8fbf75c728e3ce893" title="M3 Ultra Runs DeepSeek R1 With 671 Billion Parameters Using 448GB Of Unified Memory, Delivering High Bandwidth Performance At Under 200W Power Consumption, With No Need For A Multi-GPU Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/m3-ultra-chip-handles-deepseek-r1-model-with-671-billion-parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9jfbt/m3_ultra_runs_deepseek_r1_with_671_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T12:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dkvh</id>
    <title>Gemma 3 Release - a google Collection</title>
    <updated>2025-03-12T06:39:59+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt; &lt;img alt="Gemma 3 Release - a google Collection" src="https://external-preview.redd.it/XbF6RBBvzvCU6XDYyRoYk_HGSNjj77rcnuXfCRK9sgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d72653de324cc030e9dad7f7ea4df6ef94e0688" title="Gemma 3 Release - a google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:39:59+00:00</published>
  </entry>
</feed>
