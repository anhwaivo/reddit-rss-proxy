<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-23T06:57:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lgsykj</id>
    <title>AbsenceBench: LLMs can't tell what's missing</title>
    <updated>2025-06-21T09:58:18+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt; &lt;img alt="AbsenceBench: LLMs can't tell what's missing" src="https://b.thumbs.redditmedia.com/PjMEdZcgsAhmRkC952iAQojRviDlyPY_z4tXX2TYqCE.jpg" title="AbsenceBench: LLMs can't tell what's missing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://arxiv.org/pdf/2506.11440"&gt;AbsenceBench paper&lt;/a&gt; establishes a test that's basically Needle In A Haystack (NIAH) in reverse. &lt;a href="https://github.com/harvey-fin/absence-bench"&gt;Code here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The idea is that models score 100% on NIAH tests, thus perfectly identify added tokens that stand out - which is not equal to perfectly reasoning over longer context though - and try that in reverse, with added hints.&lt;/p&gt; &lt;p&gt;They gave the model poetry, number sequences and GitHub PRs, &lt;em&gt;together with&lt;/em&gt; a modified version with removed words or lines, and then asked the model to identify what's missing. A simple program can figure this out with 100% accurracy. The LLMs can't.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285"&gt;https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using around 8k thinking tokens improved the score by 8% on average. Those 8k thinking tokens are quite longer than the average input - just 5k, with almost all tests being shorter than 12k. Thus, this isn't an issue of long context handling, although results get worse with longer context. For some reason the results also got worse when testing with shorter omissions.&lt;/p&gt; &lt;p&gt;The hypothesis is that the attention mechanism can only attend to tokens that exist. Omissions have no tokens, thus there are no tokens to put attention on. They tested this by adding placeholders, which boosted the scores by 20% to 50%.&lt;/p&gt; &lt;p&gt;The NIAH test just tested finding literal matches. Models that didn't score close to 100% were also bad at long context understanding. Yet as we've seen with NoLiMa and fiction.liveBench, getting 100% NIAH score doesn't equal good long context &lt;em&gt;understanding&lt;/em&gt;. This paper only tests literal omissions and not semantic omissions, like incomplete evidence for a conclusion. Thus, like NIAH a model scoring 100% here won't automatically guarantee good long context understanding.&lt;/p&gt; &lt;p&gt;Bonus: They also shared the average reasoning tokens per model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874"&gt;https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lharbh</id>
    <title>Embedding With LM Studio - what am i doing wrong</title>
    <updated>2025-06-22T00:15:04+00:00</updated>
    <author>
      <name>/u/uber-linny</name>
      <uri>https://old.reddit.com/user/uber-linny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've updated LM Studio to 0.3.17 (build 7) and trying to run embedding models in the developer tab so that i can push it to AnythingLLM where my work is. &lt;/p&gt; &lt;p&gt;funny thing is , the original &amp;quot;text-embedding-nomic-embed-text-v1.5&amp;quot; loads fine and works with Anything.&lt;/p&gt; &lt;p&gt;but text-embedding-qwen3-embedding-0.6b &amp;amp; 8B and any other Embed model i use i get the below error:&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Failed to load the model&lt;/p&gt; &lt;p&gt;Failed to load embedding model&lt;/p&gt; &lt;h1&gt;Failed to load model into embedding engine. Message: Embedding engine exception: Failed to load model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers &lt;/h1&gt; &lt;p&gt;I'm just trying to understand and improve what i currently have working. The original idea was since im using Qwen3 for my work, why not try and use the Qwen3 embedding models as its probably designed to work with it. &lt;/p&gt; &lt;p&gt;Alot of the work i am currently doing is calling RAG from within documents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber-linny"&gt; /u/uber-linny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T00:15:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgwcfb</id>
    <title>Semantically search and ask your Gmail using local LLaMA</title>
    <updated>2025-06-21T13:16:44+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt; &lt;img alt="Semantically search and ask your Gmail using local LLaMA" src="https://external-preview.redd.it/u5mOM2CU1WqdBPMvqlJEeyyQe4ITGIoML7OHXi1ZUCs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4550f23557f54ee6f8ab8804140ab8bb8007bf02" title="Semantically search and ask your Gmail using local LLaMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got fed up with Apple Mail’s clunky search and built my own tool: a lightweight, local-LLM-first CLI that lets you semantically search and ask questions about your Gmail inbox:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/vs2cz0f66a8f1.gif"&gt;https://i.redd.it/vs2cz0f66a8f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Grab it here: &lt;a href="https://github.com/yahorbarkouski/semantic-mail"&gt;https://github.com/yahorbarkouski/semantic-mail&lt;/a&gt;&lt;/p&gt; &lt;p&gt;any feedback/contributions are very much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0noy</id>
    <title>Autopaste MFAs from Gmail using LLaMA</title>
    <updated>2025-06-21T16:32:58+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Apple's &amp;quot;insert code from SMS&amp;quot; feature, made a tool to speed up the process of inserting incoming email MFAs: &lt;a href="https://github.com/yahorbarkouski/auto-mfa"&gt;https://github.com/yahorbarkouski/auto-mfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T16:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgy12q</id>
    <title>moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face</title>
    <updated>2025-06-21T14:36:31+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"&gt; &lt;img alt="moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face" src="https://external-preview.redd.it/nn6Om0LrvY9dh6qkhvLPezIS-aJdRaC0O6BpYJYgA5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c180a6be6bee520890e9e2bcc9303148c925f48" title="moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lglhll</id>
    <title>Mistral's "minor update"</title>
    <updated>2025-06-21T02:12:10+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt; &lt;img alt="Mistral's &amp;quot;minor update&amp;quot;" src="https://preview.redd.it/rb70qb16v68f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7248b214307a876a51003f595eeeb9564be8245" title="Mistral's &amp;quot;minor update&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rb70qb16v68f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T02:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgyv8a</id>
    <title>Steering LLM outputs</title>
    <updated>2025-06-21T15:14:23+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"&gt; &lt;img alt="Steering LLM outputs" src="https://external-preview.redd.it/NmN0cDU5bnZwYThmMcWg2kr7Oe9IfY8fGfsf43KXN8n2ZXafTDS0jzzrXQ6i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7b47ea3b5a867f0a76afffe31ce20c58bbf78a4" title="Steering LLM outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Optimising LLM proxy runs workflow that mixes instructions from multiple anchor prompts based on their weights&lt;/li&gt; &lt;li&gt;Weights are controlled via specially crafted artifact. The artifact connects back to the workflow over websockets and is able of sending/receiving data.&lt;/li&gt; &lt;li&gt;The artifact can pause or slow down the generation as well for better control.&lt;/li&gt; &lt;li&gt;Runs completely outside the inference engine, at OpenAI-compatible API level&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/promx.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run it?&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standalone - &lt;code&gt;docker pull&lt;/code&gt; &lt;a href="http://ghcr.io/av/harbor-boost:latest"&gt;&lt;code&gt;ghcr.io/av/harbor-boost:latest&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/av/harbor/wiki/5.2.-Harbor-Boost#standalone-usage"&gt;configuration reference&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Also see &lt;a href="https://github.com/av/boost-starter"&gt;example starter repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;with &lt;a href="https://github.com/av/harbor"&gt;Harbor&lt;/a&gt; - &lt;code&gt;harbor up boost&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0351w9ovpa8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T15:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsxyw</id>
    <title>Unsloth Dynamic GGUF Quants For Mistral 3.2</title>
    <updated>2025-06-21T09:57:06+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt; &lt;img alt="Unsloth Dynamic GGUF Quants For Mistral 3.2" src="https://external-preview.redd.it/CrtSkHQg7FYlqUCKyAhEr6h8Hgeh7uXu4dg2iLzQFtI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=206fbbf02fe74bed130c7c80f847013da0053f61" title="Unsloth Dynamic GGUF Quants For Mistral 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh5gwl</id>
    <title>Built a LiteLLM adapter for locally hosted HuggingFace models on your machine because local transformers deserved the OpenAI API treatment</title>
    <updated>2025-06-21T20:03:10+00:00</updated>
    <author>
      <name>/u/arkbhatta</name>
      <uri>https://old.reddit.com/user/arkbhatta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Made local HuggingFace transformers work through LiteLLM's OpenAI-compatible interface. No more API inconsistencies between local and cloud models. Feel free to use it or help me enriching and making it more mature&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;So here's the thing: &lt;a href="https://docs.litellm.ai/docs/"&gt;LiteLLM&lt;/a&gt; is AMAZING for calling 100+ LLM providers through a unified OpenAI-like interface. It supports HuggingFace models too... but only through their cloud inference providers (Serverless, Dedicated Endpoints, etc.).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The missing piece?&lt;/strong&gt; Using your local HuggingFace models (the ones you run with &lt;code&gt;transformers&lt;/code&gt;) through the same clean OpenAI API interface.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/arkaprovob/litellm-hf-local"&gt;What I built&lt;/a&gt;:&lt;/h1&gt; &lt;p&gt;A &lt;strong&gt;custom LiteLLM provider&lt;/strong&gt; that bridges this gap, giving you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenAI API compatibility&lt;/strong&gt; for your local HF models no more switching between different interfaces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless integration&lt;/strong&gt; with any LiteLLM-compatible framework (CrewAI, LangChain, AutoGen, Google-ADK, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;4-bit/8-bit quantization&lt;/strong&gt; OOTB support for bitsandbytes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming support&lt;/strong&gt; that actually works properly with LiteLLM's chunk formatting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto chat templates&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-GPU support&lt;/strong&gt; and memory monitoring&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Option 1: Direct integration import litellm litellm.custom_provider_map = [ {&amp;quot;provider&amp;quot;: &amp;quot;huggingface-local&amp;quot;, &amp;quot;custom_handler&amp;quot;: adapter} ] response = litellm.completion( model=&amp;quot;huggingface-local/Phi-4-reasoning&amp;quot;, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello!&amp;quot;}] ) # Option 2: Proxy server (OpenAI-compatible API) # Start: litellm --config litellm_config.yaml # Then use in the following way: curl --location 'http://0.0.0.0:4000/v1/chat/completions' \ --header 'Content-Type: application/json' \ --data '{ &amp;quot;model&amp;quot;: &amp;quot;qwen-local&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a helpful assistant.&amp;quot; }, { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;what is LLM?&amp;quot; } ], &amp;quot;stream&amp;quot;: false }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The real value&lt;/strong&gt;: Your local models get OpenAI API compatibility + work with existing LiteLLM-based tools + serve via REST API and may more.&lt;/p&gt; &lt;h1&gt;Current status:&lt;/h1&gt; &lt;p&gt;✅ Working with Qwen, Phi-4, Gemma 3 models and technically should work with other Text generation models.&lt;br /&gt; ✅ Streaming, quantization, memory monitoring&lt;br /&gt; ✅ LiteLLM proxy server integration&lt;br /&gt; ✅ Clean, modular codebase&lt;/p&gt; &lt;h1&gt;Further improvement scope:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Testing more models&lt;/strong&gt; - especially newer architectures&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation/examples&lt;/strong&gt; - because good docs matter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This fills a real gap in the ecosystem. LiteLLM is fantastic for cloud providers, but local HF models deserved the same love. Now they have it!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The bottom line:&lt;/strong&gt; Your local HuggingFace models can now speak fluent OpenAI API, making them first-class citizens in the LiteLLM ecosystem.&lt;/p&gt; &lt;p&gt;Happy to get contribution or new feature requests if you have any, will be really glad if you find it useful or it helps you in any of your quest, and if you have any feedback I am all ears!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/arkaprovob/litellm-hf-local"&gt;https://github.com/arkaprovob/litellm-hf-local&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arkbhatta"&gt; /u/arkbhatta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh5gwl/built_a_litellm_adapter_for_locally_hosted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh5gwl/built_a_litellm_adapter_for_locally_hosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh5gwl/built_a_litellm_adapter_for_locally_hosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T20:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgy4wa</id>
    <title>Build Qwen3 from Scratch</title>
    <updated>2025-06-21T14:41:27+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a big fan of Sebastian Raschka's earlier work on LLMs from scratch. He recently switched from Llama to Qwen (a switch I recently made too thanks to someone in this subreddit) and wrote a Jupyter notebook implementing Qwen3 from scratch.&lt;/p&gt; &lt;p&gt;Highly recommend this resource as a learning project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhd69y</id>
    <title>ChatGPT alike local web ui for apple silicon?</title>
    <updated>2025-06-22T02:25:04+00:00</updated>
    <author>
      <name>/u/IntrigueMe_1337</name>
      <uri>https://old.reddit.com/user/IntrigueMe_1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for a specific local AI software that I can run on my Mac that lets me have a web ui with ChatGPT alike functions: uploading files, web search and possibly even deep research? Is there anything out there like this I can run locally and free?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntrigueMe_1337"&gt; /u/IntrigueMe_1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd69y/chatgpt_alike_local_web_ui_for_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd69y/chatgpt_alike_local_web_ui_for_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd69y/chatgpt_alike_local_web_ui_for_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T02:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgx222</id>
    <title>Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension</title>
    <updated>2025-06-21T13:51:53+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"&gt; &lt;img alt="Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension" src="https://preview.redd.it/o9sgqppkca8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3738a800b971da66fb76ce1bfd12e92a05565ae" title="Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o9sgqppkca8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhi8p8</id>
    <title>How much performance am I losing using chipset vs CPU lanes on 3080ti?</title>
    <updated>2025-06-22T07:34:33+00:00</updated>
    <author>
      <name>/u/FactoryReboot</name>
      <uri>https://old.reddit.com/user/FactoryReboot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3080ti and an MSI Z790 gaming plus wifi. For some reason my pcie slot with the cpu lanes isn’t working. The chipset one works fine. &lt;/p&gt; &lt;p&gt;How much performance should I expect to lose with local llama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FactoryReboot"&gt; /u/FactoryReboot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhi8p8/how_much_performance_am_i_losing_using_chipset_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T07:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgvl40</id>
    <title>After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch</title>
    <updated>2025-06-21T12:38:20+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"&gt; &lt;img alt="After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch" src="https://external-preview.redd.it/1Tc0yB3lwHCV4Qo8QzQZtbMZw5Hyi2St2rr1CDzMJaE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72c893ef4fea62f81300f8447709b6c1be403c" title="After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What hapening to zuck? after scale ai , now Safe Superintelligence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/06/20/after-trying-to-buy-ilya-sutskevers-32b-ai-startup-meta-looks-to-hire-its-ceo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T12:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgxjw2</id>
    <title>Self Adapting LLMs - legit?</title>
    <updated>2025-06-21T14:14:34+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"&gt; &lt;img alt="Self Adapting LLMs - legit?" src="https://preview.redd.it/rlhp01gfca8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad7294f3dda4aacfc84c69129907508eae493c63" title="Self Adapting LLMs - legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across the new MIT paper &lt;em&gt;Self-Adapting Language Models&lt;/em&gt; (Zweiger et al., June 2025).&lt;br /&gt; The core idea is wild:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM produces a &lt;strong&gt;self-edit&lt;/strong&gt;—a chunk of text that can (a) rewrite / augment the input data, (b) pick hyper-parameters, or (c) call external tools for data augmentation or gradient updates.&lt;/li&gt; &lt;li&gt;Those self-edits are fed straight back into supervised finetuning (or RL), so the model &lt;em&gt;persistently&lt;/em&gt; updates its own weights.&lt;/li&gt; &lt;li&gt;They train the model to &lt;em&gt;judge its own edits&lt;/em&gt; with a downstream reward signal, so it keeps iterating until performance improves.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially the model becomes both &lt;strong&gt;student and curriculum designer&lt;/strong&gt;, continuously generating the exactly-what-it-needs data to get better.&lt;/p&gt; &lt;p&gt;My (much humbler) attempt &amp;amp; pain points&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For a tweet-classification project I had GPT-4 &lt;strong&gt;select&lt;/strong&gt; real tweets and &lt;strong&gt;synthesize&lt;/strong&gt; new ones to expand the finetuning set.&lt;/li&gt; &lt;li&gt;Quality was decent, but (1) &lt;strong&gt;insanely expensive&lt;/strong&gt;, and (2) performance &lt;strong&gt;regressed&lt;/strong&gt; vs. a baseline where I manually hand-picked examples.&lt;/li&gt; &lt;li&gt;I only did straight SFT; didn’t try RL-style feedback (wasn’t aware of anything cleaner than full-blown PPO/DPO at the time).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I wrong to think that this will not hold in main use cases? Why not just try GRPO RL for the use cases that the user wants? I am honestly a bit confused, can someone explain or discuss on what am I missing here? How can a model know what it needs other than a much bigger model giving it feedback on every iteration? Has RL worked on other stuff than text before in this context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rlhp01gfca8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:14:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhbgcn</id>
    <title>A Great Breakdown of the "Disney vs Midjourney" Lawsuit Case</title>
    <updated>2025-06-22T00:51:14+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you all know by now, Disney has sued Midjourney on the basis that the latter trained its AI image generating models on copyrighted materials. &lt;/p&gt; &lt;p&gt;This is a serious case that we all should follow up closely. LegalEagle broke down the case in their new YouTube video linked below:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=zpcWv1lHU6I"&gt;https://www.youtube.com/watch?v=zpcWv1lHU6I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I really hope Midjourney wins this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhbgcn/a_great_breakdown_of_the_disney_vs_midjourney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T00:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh359d</id>
    <title>From Arch-Function to Arch-Agent. Designed for fast multi-step, multi-turn workflow orchestration in agents.</title>
    <updated>2025-06-21T18:20:02+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh359d/from_archfunction_to_archagent_designed_for_fast/"&gt; &lt;img alt="From Arch-Function to Arch-Agent. Designed for fast multi-step, multi-turn workflow orchestration in agents." src="https://preview.redd.it/n7hvejg7kb8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52b7b89164644abd1aa046530243be33db354edd" title="From Arch-Function to Arch-Agent. Designed for fast multi-step, multi-turn workflow orchestration in agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello - in the past i've shared my work around &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hr9ll1/i_built_a_small_function_calling_llm_that_packs_a/"&gt;function-calling&lt;/a&gt; on this sub. The encouraging feedback and usage (over 100k downloads 🤯) has gotten me and my team cranking away. Six months from our initial launch, I am excited to share our agent models: Arch-Agent. &lt;/p&gt; &lt;p&gt;Full details in the model card: &lt;a href="https://huggingface.co/katanemo/Arch-Agent-7B"&gt;https://huggingface.co/katanemo/Arch-Agent-7B&lt;/a&gt; - but quickly, Arch-Agent offers state-of-the-art performance for advanced function calling scenarios, and sophisticated multi-step/multi-turn agent workflows. Performance was measured on BFCL, although we'll also soon publish results on the Tau-Bench as well. &lt;/p&gt; &lt;p&gt;These models will power &lt;a href="https://github.com/katanemo/archgw/"&gt;Arch&lt;/a&gt; (the universal data plane for AI) - the open source project where some of our science work is vertically integrated. &lt;/p&gt; &lt;p&gt;Hope like last time - you all enjoy these new models and our open source work 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n7hvejg7kb8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh359d/from_archfunction_to_archagent_designed_for_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh359d/from_archfunction_to_archagent_designed_for_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T18:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0qb9</id>
    <title>how many people will tolerate slow speed for running LLM locally?</title>
    <updated>2025-06-21T16:36:11+00:00</updated>
    <author>
      <name>/u/OwnSoup8888</name>
      <uri>https://old.reddit.com/user/OwnSoup8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just want to check how many people will tolerate speed for privacy? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnSoup8888"&gt; /u/OwnSoup8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0qb9/how_many_people_will_tolerate_slow_speed_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0qb9/how_many_people_will_tolerate_slow_speed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0qb9/how_many_people_will_tolerate_slow_speed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhgvq4</id>
    <title>[OpenSource]Multi-LLM client - LLM Bridge</title>
    <updated>2025-06-22T06:05:04+00:00</updated>
    <author>
      <name>/u/billythepark</name>
      <uri>https://old.reddit.com/user/billythepark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt; &lt;img alt="[OpenSource]Multi-LLM client - LLM Bridge" src="https://external-preview.redd.it/10mCBOjQL0RLrB--BfVKVkZcSDhwfEFJ4fJJfr9rSTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d26661e920b09c71c0e0d22c6b28b034db44cd7f" title="[OpenSource]Multi-LLM client - LLM Bridge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, I created a separate LLM client for Ollama for iOS and MacOS and released it as open source,&lt;/p&gt; &lt;p&gt;but I recreated it by integrating iOS and MacOS codes and adding APIs that support them based on Swift/SwiftUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/00dq12p66f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b97237c3558709596ef0396b5f5d197add9f794"&gt;https://preview.redd.it/00dq12p66f8f1.jpg?width=2880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5b97237c3558709596ef0396b5f5d197add9f794&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Supports Ollama and LMStudio as local LLMs.&lt;/p&gt; &lt;p&gt;* If you open a port externally on the computer where LLM is installed on Ollama, you can use free LLM remotely.&lt;/p&gt; &lt;p&gt;* MLStudio is a local LLM management program with its own UI, and you can search and install models from HuggingFace, so you can experiment with various models.&lt;/p&gt; &lt;p&gt;* You can set the IP and port in LLM Bridge and receive responses to queries using the installed model.&lt;/p&gt; &lt;p&gt;* Supports OpenAI&lt;/p&gt; &lt;p&gt;* You can receive an API key, enter it in the app, and use ChatGtp through API calls.&lt;/p&gt; &lt;p&gt;* Using the API is cheaper than paying a monthly membership fee. * Claude support&lt;/p&gt; &lt;p&gt;* Use API Key&lt;/p&gt; &lt;p&gt;* Image transfer possible for image support models&lt;/p&gt; &lt;p&gt;* PDF, TXT file support&lt;/p&gt; &lt;p&gt;* Extract text using PDFKit and transfer it&lt;/p&gt; &lt;p&gt;* Text file support&lt;/p&gt; &lt;p&gt;* Open source&lt;/p&gt; &lt;p&gt;* Swift/SwiftUI&lt;/p&gt; &lt;p&gt;* Source link&lt;/p&gt; &lt;p&gt;* &lt;a href="https://github.com/bipark/swift_llm_bridge"&gt;https://github.com/bipark/swift_llm_bridge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/billythepark"&gt; /u/billythepark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhgvq4/opensourcemultillm_client_llm_bridge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T06:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh2ffp</id>
    <title>CEO Bench: Can AI Replace the C-Suite?</title>
    <updated>2025-06-21T17:49:08+00:00</updated>
    <author>
      <name>/u/dave1010</name>
      <uri>https://old.reddit.com/user/dave1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put together a (slightly tongue in cheek) benchmark to test some LLMs. All open source and all the data is in the repo.&lt;/p&gt; &lt;p&gt;It makes use of the excellent &lt;code&gt;llm&lt;/code&gt; Python package from Simon Willison.&lt;/p&gt; &lt;p&gt;I've only benchmarked a couple of local models but want to see what the smallest LLM is that will score above the estimated &amp;quot;human CEO&amp;quot; performance. How long before a sub-1B parameter model performs better than a tech giant CEO?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dave1010"&gt; /u/dave1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ceo-bench.dave.engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh2ffp/ceo_bench_can_ai_replace_the_csuite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh2ffp/ceo_bench_can_ai_replace_the_csuite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T17:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgwsdr</id>
    <title>DeepSeek Guys Open-Source nano-vLLM</title>
    <updated>2025-06-21T13:38:49+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek guys just open-sourced &lt;a href="https://github.com/GeeeekExplorer/nano-vllm"&gt;nano-vLLM&lt;/a&gt;. It’s a lightweight vLLM implementation built from scratch.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🚀 &lt;strong&gt;Fast offline inference&lt;/strong&gt; - Comparable inference speeds to vLLM&lt;/li&gt; &lt;li&gt;📖 &lt;strong&gt;Readable codebase&lt;/strong&gt; - Clean implementation in ~ 1,200 lines of Python code&lt;/li&gt; &lt;li&gt;⚡ &lt;strong&gt;Optimization Suite&lt;/strong&gt; - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhhs1r</id>
    <title>Best open agentic coding assistants that don’t need an OpenAI key?</title>
    <updated>2025-06-22T07:03:21+00:00</updated>
    <author>
      <name>/u/Fabulous_Bluebird931</name>
      <uri>https://old.reddit.com/user/Fabulous_Bluebird931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for ai dev tools that actually let you use your own models, something agent-style that can analyse multiple files, track goals, and suggest edits/refactors, ideally all within vscode or terminal.&lt;/p&gt; &lt;p&gt;I’ve used Copilot’s agent mode, but it’s obviously tied to OpenAI. I’m more interested in&lt;/p&gt; &lt;p&gt;Tools that work with local models (via Ollama or similar)&lt;/p&gt; &lt;p&gt;API-pluggable setups (Gemini 1.5, deepseek, Qwen3, etc)&lt;/p&gt; &lt;p&gt;Agents that can track tasks, not just generate single responses&lt;/p&gt; &lt;p&gt;I’ve been trying Blackbox’s vscode integration, which has some agentic behaviour now. Also tried cline and roo, which are promising for CLI work.&lt;/p&gt; &lt;p&gt;But most tools either&lt;/p&gt; &lt;p&gt;Require a paid key to do anything useful Aren’t flexible with models&lt;/p&gt; &lt;p&gt;Or don’t handle full-project context&lt;/p&gt; &lt;p&gt;anyone found a combo that works well with open models and integrates tightly with your coding environment? Not looking for prompt uis, looking for workflow tools please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Bluebird931"&gt; /u/Fabulous_Bluebird931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T07:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhd1j0</id>
    <title>Some Observations using the RTX 6000 PRO Blackwell.</title>
    <updated>2025-06-22T02:17:39+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I would share some thoughts playing around with the RTX 6000 Pro 96GB Blackwell Workstation edition.&lt;/p&gt; &lt;p&gt;Using the card inside a Razer Core X GPU enclosure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I bought this bracket (&lt;a href="https://www.etsy.com/listing/1293010019/razer-core-x-bracket-for-corsair-power?ref=cart"&gt;link&lt;/a&gt;) and replaced the Razer Core X power supply with an SFX-L 1000W. Worked beautifully.&lt;/li&gt; &lt;li&gt;Razer Core X cannot handle a 600W card, the outside case gets very HOT with the RTX 6000 Blackwell 600 Watt workstation edition working.&lt;/li&gt; &lt;li&gt;I think this is a perfect use case for the 300W Max-Q edition.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Using the RTX 6000 96GB:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The RTX 6000 96GB Blackwell is bleeding edge. I had to build all libraries with the latest CUDA driver to get it to be usable. For Llama.cpp I had to build it and specifically set the flag to the CUDA architecture (the documents are misleading , need to set the min compute capability 90 not 120.)&lt;/li&gt; &lt;li&gt;When I built all the frame works the RTX 6000 allowed me to run bigger models but I noticed they ran kind of slow. At least with Llama I noticed it's not taking advantage of the architecture. I verified with Nvidia-smi that it was running on the card. The coding agent (llama-vscode, open-ai api) was dumber.&lt;/li&gt; &lt;li&gt;The dumber behavior was similar with freshly built VLLM and Open-Webui. Took so long to build PyTorch with the latest CUDA library to get it to work.&lt;/li&gt; &lt;li&gt;Switch back to the 3090 inside the Razer Core X and everything just works beautifully. The Qwen2.5 Coder 14B Instruct picked up on me converting c-style enums to C++ and it automatically suggested the next whole enum class vs Qwen 2.5 32B coder instruct FP16 and Q8.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I wasted way too much time (2 days?) rebuilding a bunch of libraries for Llama, VLM, etc.. to take advantage of RTX 6000 96GB. This includes time spent going the git issues with the RTX 6000. Don't get me started on some of these buggy/incorrect docker containers I tried to save build time. Props to LM studio for making using of the card though it felt dumber still.&lt;/p&gt; &lt;p&gt;Wish the A6000 and the 6000 ADA 48GB cards were cheaper though. I say if your time is a lot of money it's worth it for something that's stable, proven, and will work with all frameworks right out of the box.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lhd1j0/comment/mz3668v/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Proof&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: fixed typos. I suck at posting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhd1j0/some_observations_using_the_rtx_6000_pro_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T02:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhdu5q</id>
    <title>The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers</title>
    <updated>2025-06-22T03:01:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt; &lt;img alt="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" src="https://external-preview.redd.it/sIlsOyewqWKbkaq9LXBmI2vpBNvSB1xv0YAMiyBxo9s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a462876ceca8aa7da6001fdcb8936398a0cfa6d5" title="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was interested in merging &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;DeepSeek-R1-0528-Qwen3-8B&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen3-8B&lt;/a&gt; as they were both my two favorite under 10b~ models, and finding the Deepseek distill especially impressive. Noted in their model card was the following:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which made me realize, they were both good merge candidates for each other, both being not finetunes, but fully trained models off the Qwen3-8B-Base, and even sharing the same favored sampler settings. The only real difference were the tokenizers. This took me to a crossroads, which tokenizer should my merge inherit? Asking around, I was told there shouldn't be much difference, but I ended up finding out very differently once I did some actual testing. The TL;DR is, the Qwen tokenizer seems to perform better &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; use far less tokens for it's thinking. It is a larger tokenizer I noted, and was told that means the tokenizer is more optimized, but I was skeptical about this and decided to test it.&lt;/p&gt; &lt;p&gt;This turned out not to be a not so easy endeavor, since the benchmark I decided on (LocalAIME by &lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; which I thank for making and sharing this tool), takes rather long to complete when you use a thinking model, since they require quite a few tokens to get to their answer with any amount of accuracy. I first tested with 4k context, then 8k, then briefly even 16k before realizing the LLM responses were still getting cut off, resulting in poor accuracy. GLM 9B did not have this issue, and used very few tokens in comparison even with context set to 30k. Testing took very long, but with the help of others from the KoboldAI server (shout out to everyone there willing to help, a lot of people volunteered their help, who I will accredit below), we were able to eventually get it done.&lt;/p&gt; &lt;p&gt;This is the most useful graph that came of this, you can see below models using the Qwen tokenizer used less tokens than any of the models using the Deepseek tokenizer, and had higher accuracy. Both merges also performed better than their same tokenizer parent model counterparts. I was actually surprised since I quite preferred the R1 Distill to the Qwen3 instruct model, and had thought it was better before this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lbpldqh57e8f1.png?width=2969&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41dd5f79caaa5a59c3e89cf26accf2b4fc062693"&gt;Model Performance VS Tokens Generated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would have liked to have tested at a higher precision, like Q8_0, and on more problem attempts (like 3-5) for better quality data but didn't have the means to. If anyone with the means to do so is interested in giving it a try, please feel free to reach out to me for help, or if anyone wants to loan me their hardware I would be more than happy to run the tests again under better settings.&lt;/p&gt; &lt;p&gt;For anyone interested, more information is available in the model cards of the merges I made, which I will link below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;w/ Qwen3 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;w/ Deepseek R1 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently only my own static GGUF quants are available (in Q4_K_S and Q8_0) but hopefully others will provide more soon enough.&lt;/p&gt; &lt;p&gt;I've stored all my raw data, and test results in a repository here: &lt;a href="https://github.com/lemon07r/LocalAIME_results"&gt;https://github.com/lemon07r/LocalAIME_results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Special Thanks to The Following People&lt;/strong&gt; (for making this possible)&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eisenstein for their modified fork of LocalAIME to work better with KoboldCPP and modified sampler settings for Qwen/Deepseek models, and doing half of my testing for me on his machine. Also helping me with a lot of my troubleshooting.&lt;/li&gt; &lt;li&gt;Twistedshadows for loaning me some of their runpod hours to do my testing.&lt;/li&gt; &lt;li&gt;Henky as well, for also loaning me some of their runpod hours, and helping me troubleshoot some issues with getting KCPP to work with LocalAIME&lt;/li&gt; &lt;li&gt;Everyone else on the KoboldAI discord server, there were more than a few willing to help me out in the way of advice, troubleshooting, or offering me their machines or runpod hours to help with testing if the above didn't get to it first.&lt;/li&gt; &lt;li&gt;&lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; for making and sharing his LocalAIME tool&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For full transparency, I do want to disclaim that this method isn't really an amazing way to test tokenizers against each other, since the deepseek part of the two merges are still trained using the deepseek tokenizer, and the qwen part with it's own tokenizer* (see below, turns out, this doesn't really apply here). You would have to train two different versions from the ground up using the different tokenizers on the same exact data to get a completely fair assessment. I still think this testing and further testing is worth doing to see how these merges perform in comparison to their parents, and under which tokenizer they perform better.&lt;/p&gt; &lt;p&gt;*EDIT - Under further investigation I've found the Deepseek tokenizer and qwen tokenizer have virtually a 100% vocab overlap, making them pretty much interchangeable, and using models trained using either the perfect candidates for testing both tokenizers against each other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhed49</id>
    <title>50 days building a tiny language model from scratch, what I’ve learned so far</title>
    <updated>2025-06-22T03:31:14+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m starting a new weekday series on June 23 at 9:00 AM PDT where I’ll spend 50 days coding a two LLM (15–30M parameters) from the ground up: no massive GPU cluster, just a regular laptop or modest GPU.&lt;/p&gt; &lt;p&gt;Each post will cover one topic:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data collection and subword tokenization&lt;/li&gt; &lt;li&gt;Embeddings and positional encodings&lt;/li&gt; &lt;li&gt;Attention heads and feed-forward layers&lt;/li&gt; &lt;li&gt;Training loops, loss functions, optimizers&lt;/li&gt; &lt;li&gt;Evaluation metrics and sample generation&lt;/li&gt; &lt;li&gt;Bonus deep dives: MoE, multi-token prediction,etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why bother with tiny models?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They run on the CPU.&lt;/li&gt; &lt;li&gt;You get daily feedback loops.&lt;/li&gt; &lt;li&gt;Building every component yourself cements your understanding.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ve already tried:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A 30 M-parameter GPT variant for children’s stories&lt;/li&gt; &lt;li&gt;A 15 M-parameter DeepSeek model with Mixture-of-Experts&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ll drop links to the code in the first comment.&lt;/p&gt; &lt;p&gt;Looking forward to the discussion and to learning together. See you on Day 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:31:14+00:00</published>
  </entry>
</feed>
