<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-26T10:05:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k7t089</id>
    <title>Any possibility for Small size models of Llama 3.3 &amp; 4 in future?</title>
    <updated>2025-04-25T19:03:18+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm part of No/Poor GPU club. My old laptop doesn't have GPU at all. Friend's laptop has 8GB VRAM. Time to time I use his laptop only for LLM stuff.&lt;/p&gt; &lt;p&gt;I use small size models till 3.2 version. Then both later versions came with large models. (Frankly expected 10-15B models from 3.3 or 4 Versions).&lt;/p&gt; &lt;p&gt;I know Meta won't touch 3.3 version anymore &amp;amp; hereafter won't release small model for 4 version. I don't think in future we'll get small models from Meta.&lt;/p&gt; &lt;p&gt;So any possibility of small size models from 3.3 or 4 versions models by some other way? Hope someday some legends do this &amp;amp; uploads small models to HuggingFace for same.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Llama&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt; 70.6B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt; 70.6B 405B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama 3.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1B 3B 11B&lt;/strong&gt; 90B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4&lt;/td&gt; &lt;td align="left"&gt;109B 400B 2T&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t089/any_possibility_for_small_size_models_of_llama_33/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7kv9a</id>
    <title>Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations</title>
    <updated>2025-04-25T13:25:15+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt; &lt;img alt="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" src="https://external-preview.redd.it/yTiUURrBkqcGYJGBhqzC01YOstzVvXfVd3FxAo3YWYU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73fdf48b98f8c4bbf03db30badce672add745943" title="Intel Updates Its PyTorch Extension With DeepSeek-R1 Support, New Optimizations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Intel-PyTorch-Extension-2.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7kv9a/intel_updates_its_pytorch_extension_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7zmfm</id>
    <title>Effects of quantisation of task-specific downstream tasks</title>
    <updated>2025-04-25T23:58:18+00:00</updated>
    <author>
      <name>/u/mayodoctur</name>
      <uri>https://old.reddit.com/user/mayodoctur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7zmfm/effects_of_quantisation_of_taskspecific/"&gt; &lt;img alt="Effects of quantisation of task-specific downstream tasks" src="https://b.thumbs.redditmedia.com/I6CWfR9hJISNkybN7IBoBP4KGTxcexP-YXzV4RPc4no.jpg" title="Effects of quantisation of task-specific downstream tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some experimentation for a project where Im doing on quantisation and fine-tuning. I wanted a way of doing news significance scoring similar to what &lt;a href="http://newsminimalist.com"&gt;newsminimalist.com&lt;/a&gt; did in his work. So I fine-tuned the Llama 3.2 1B parameter model using PEFT to score significance on news articles and Quantised the model to 4-bit and 8-bit to see how comptuationally efficient I could make it. The prompt is some guidelines on how to score significance, some examples, then an injected full news article. You could do this for any article or piece of text. I tested the model performance and memory usage across &lt;code&gt;BF16, INT8, INT4&lt;/code&gt; .&lt;/p&gt; &lt;p&gt;I wanted to share my findings with people here&lt;/p&gt; &lt;p&gt;Notably, the performance of the INT4 model on scoring compared to BF16 were very similar on my validation sets. It failed to produce a structure output once but every other time, the results were the exact same.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lkkxy4zu33xe1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41e6751fa512fff3591f2901b06cf39f84202daf"&gt;https://preview.redd.it/lkkxy4zu33xe1.png?width=529&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41e6751fa512fff3591f2901b06cf39f84202daf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l5fjsvamk2xe1.png?width=375&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=892f403287a7c19e1fff700c17182280bc8f84d3"&gt;https://preview.redd.it/l5fjsvamk2xe1.png?width=375&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=892f403287a7c19e1fff700c17182280bc8f84d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GT being the ground truth.&lt;/p&gt; &lt;p&gt;Let me know what you guys think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayodoctur"&gt; /u/mayodoctur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7zmfm/effects_of_quantisation_of_taskspecific/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7zmfm/effects_of_quantisation_of_taskspecific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7zmfm/effects_of_quantisation_of_taskspecific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T23:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k88k0h</id>
    <title>System Prompt vs. User Prompt</title>
    <updated>2025-04-26T08:53:10+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. What difference does it make, if I split my instructions into a system and user prompt, compared to just writing everything in the user prompt and keeping the system prompt empty or the generic &amp;quot;You are a helpful assistant&amp;quot;? &lt;/p&gt; &lt;p&gt;Assume the instruction is composed of an almost constant part (e.g. here is the data), and a more variable part (the question about the data). Is there any tangible difference in correctness, consistency etc?&lt;/p&gt; &lt;p&gt;And given that OpenAI API allows multiple user messages in the same request (does it?), will it have any benefit to separate a message into multiple user messages?&lt;/p&gt; &lt;p&gt;It's not an interactive scenario, so jailbreaking is not an issue. And for paid models, the tokens are anyways counted for the whole payload at the same rate, right?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k88k0h/system_prompt_vs_user_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T08:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7r8qu</id>
    <title>SOTA Spatial Reasoning in 2025</title>
    <updated>2025-04-25T17:51:12+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"&gt; &lt;img alt="SOTA Spatial Reasoning in 2025" src="https://b.thumbs.redditmedia.com/AasNY8iky2mDi_qpsR-CfGk7uE_qPRprH6Ci0ul1Isg.jpg" title="SOTA Spatial Reasoning in 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ability to accurately estimate distances from RGB image input is just at the 𝗳𝗿𝗼𝗻𝘁𝗶𝗲𝗿 𝗼𝗳 𝗰𝘂𝗿𝗿𝗲𝗻𝘁 𝗔𝗜 𝗺𝗼𝗱𝗲𝗹 𝗰𝗮𝗽𝗮𝗯𝗶𝗹𝗶𝘁𝗶𝗲𝘀.&lt;/p&gt; &lt;p&gt;Nonetheless, distance estimation is a 𝗰𝗿𝗶𝘁𝗶𝗰𝗮𝗹 𝗳𝗼𝗿 𝗽𝗲𝗿𝗰𝗲𝗽𝘁𝗶𝗼𝗻 𝗮𝗻𝗱 𝗽𝗹𝗮𝗻𝗻𝗶𝗻𝗴 𝗶𝗻 𝗲𝗺𝗯𝗼𝗱𝗶𝗲𝗱 𝗔𝗜 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀 𝗹𝗶𝗸𝗲 𝗿𝗼𝗯𝗼𝘁𝗶𝗰𝘀 which must navigate around our 3D world.&lt;/p&gt; &lt;p&gt;Making a 𝗼𝗽𝗲𝗻-𝘄𝗲𝗶𝗴𝗵𝘁 model 𝘀𝗺𝗮𝗹𝗹 and 𝗳𝗮𝘀𝘁 enough to run 𝗼𝗻-𝗱𝗲𝘃𝗶𝗰𝗲, using 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗰𝗼𝗱𝗲 and 𝗱𝗮𝘁𝗮, we aim to democratize embodied AI.&lt;/p&gt; &lt;p&gt;I've updated the comparison among closed APIs with SOTA performance in &lt;strong&gt;quantitative spatial reasoning&lt;/strong&gt; tasks like distance/size estimation from RGB inputs and our 3B open-weight model: SpaceThinker&lt;/p&gt; &lt;p&gt;The performance for the the 3B SpaceThinker lies between gpt-4o and gemini-2.5-pro in estimating distances using the QSpatial++ split of Q-Spatial-Bench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluation Results:&lt;/strong&gt; &lt;a href="https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#qspatial-comparison-table-42525"&gt;https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B#qspatial-comparison-table-42525&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interesting finding:&lt;/strong&gt; By switching model name in &lt;a href="https://colab.research.google.com/drive/1buEe2QC4_pnrJwQ9XyRAH7RfaIa6pbex?usp=sharing"&gt;this colab&lt;/a&gt;, using the non-reasoning variant &lt;a href="https://huggingface.co/remyxai/SpaceQwen2.5-VL-3B-Instruct"&gt;SpaceQwen&lt;/a&gt;, you'll find using the &lt;a href="https://github.com/andrewliao11/Q-Spatial-Bench-code/blob/main/prompt_templates/spatial_prompt_steps.txt"&gt;step-by-step reasoning prompt&lt;/a&gt; actually hurts performance, challenging the convention that reasoning models &lt;a href="https://huggingface.co/blog/NormalUhr/deepseek-r1-explained#74-prompt-engineering-sensitivities"&gt;don't benefit&lt;/a&gt; from complex instructions the way non-reasoning models do.&lt;/p&gt; &lt;p&gt;Modifying the above colab, you can also compare SpaceThinker to it's base model to assess the performance impact due to SFT by LoRA using the SpaceThinker dataset: &lt;a href="https://huggingface.co/datasets/remyxai/SpaceThinker"&gt;https://huggingface.co/datasets/remyxai/SpaceThinker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k7r8qu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7r8qu/sota_spatial_reasoning_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T17:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k82h2k</id>
    <title>Quantization + Distillation Best Practices?</title>
    <updated>2025-04-26T02:28:16+00:00</updated>
    <author>
      <name>/u/charlesrwest0</name>
      <uri>https://old.reddit.com/user/charlesrwest0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking into integrating LLMs with video games, but there's some real practical problems: 1. I found that using a 5 bit quant of llama 3.2 3B worked decently for most used cases (even without a Lora), but it ate roughly 3 gigs of vram. That's a lot for a game subsystem and lower quants didn't seem to do well. 2. Generation speed is a major issue if you use it for anything besides chat. The vulkan backend to llama.cpp doesn't handle multiple execution threads and was the only portable one. The newish dynamic backend might help (support cuda and AMD) but usually the AMD one has to target a specific chipset...&lt;/p&gt; &lt;p&gt;I keep seeing awesome reports about super high quality quants, some of which require post quant training and some of which are supposed to support ludicrous inference speeds on cpu (bitnets, anyone?). I mostly care about performance on a narrow subset of tasks (sometimes dynamically switching LORAs).&lt;/p&gt; &lt;p&gt;Does anyone know of some decent guides on using these more advanced quant methods (with or without post quant training) and make a gguf that's llama.cpp compatible at the end?&lt;/p&gt; &lt;p&gt;On a related note, are there any good guides/toolkits for distilling a bigger model into a smaller one? Is &amp;quot;make a text dataset and train on it&amp;quot; the only mainstream supported mode? I would think that training on the entire token output distribution would be a much richer gradient signal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charlesrwest0"&gt; /u/charlesrwest0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k82h2k/quantization_distillation_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k82h2k/quantization_distillation_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k82h2k/quantization_distillation_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T02:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rnu9</id>
    <title>How far can we take quantization aware training (QAT)?</title>
    <updated>2025-04-25T18:07:46+00:00</updated>
    <author>
      <name>/u/gofiend</name>
      <uri>https://old.reddit.com/user/gofiend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;TLDR: Why can't we train quantization aware models to optimally use the lowest bit quantization it can for every layer / block of parameters?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;There was a recent post here on a very clever new 11 bit float &amp;quot;format&amp;quot; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DF11&lt;/a&gt; that has interesting inferencing time vs. memory tradeoffs compared to BF16. It got me thinking further along a fun topic - what does (smallish) model training look like in ~2 years?&lt;/p&gt; &lt;p&gt;We already have frontier (for their size 😅) quantization-aware trained models from &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;Google&lt;/a&gt;, and I suspect most labs will release something similar. But I think we're going to go further:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's obvious that there is value from BF16/INT8 parameters in some blocks and not in others, and a lot of value in clustering parameters that need dynamic range together&lt;/li&gt; &lt;li&gt;A smaller model (all else being equal) is better for inferencing because memory bandwidth (not compute) is the speed contraint&lt;/li&gt; &lt;li&gt;Model parameters almost seem like a legacy concept at this point. We would all prefer to spend 17GB of VRAM on &lt;a href="https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf"&gt;gemma-3-27b-it-qat-q4_0-gguf&lt;/a&gt; vs. ~24GB of VRAM on &lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;gemma-3-12b-it&lt;/a&gt; at BF16&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So: can we train models with their memory footprint and estimated token generation rate (targeting a reference architecture) as part of the objective function?&lt;/p&gt; &lt;p&gt;My &lt;em&gt;naive&lt;/em&gt; proposal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add memory footprint and a function that approximates token generation rate to the training loss function&lt;/li&gt; &lt;li&gt;Add a differentiable &amp;quot;quantization&amp;quot; parameter for every ~4K of parameters (activation, weights etc.)&lt;/li&gt; &lt;li&gt;During each batch of the forward pass, use the quantization parameter to drop the block of parameters from BF16 to DF11 to INT8 to INT4 probabilistically based on value i.e. &lt;ul&gt; &lt;li&gt;A high value would mostly do the forward pass in BF16, a little in DF11 and very little in INT8/4&lt;/li&gt; &lt;li&gt;A middle value would be mostly INT8 with a little DF11 and INT4&lt;/li&gt; &lt;li&gt;A low value would be mostly INT4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Calculate the average memory footprint and tokens/second rate (again an approximate reference model is fine) and incorporate into the loss, then run the backward pass &lt;ul&gt; &lt;li&gt;This should make the quantization parameter nicely differentiable and trainable (?)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;At the end of training freeze blocks of parameters at the quantization level that reflects the final values of the quantization parameter (i.e. a mid value would freeze at INT8) &lt;ul&gt; &lt;li&gt;In theory the model would have learnt to cluster its use of high dynamic range parameters to minimize the use of BF16 and maximize the use of INT8/4&lt;/li&gt; &lt;li&gt;You can imagine training multiple sizes of the same model almost in parallel by varying the cost function&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll poke at the literature, but I'd appreciate pointers to anything similar that folks have done already (and of course your thoughts on why this naive approach is ... naive).&lt;/p&gt; &lt;p&gt;A really simple first step might be running an optimization exercise like this on an existing model ... but &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; might just be all over &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k71mab/unsloth_dynamic_v20_ggufs_llama_4_bug_fixes_kl/"&gt;that already&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gofiend"&gt; /u/gofiend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rnu9/how_far_can_we_take_quantization_aware_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7k1ck</id>
    <title>No thinking, is the right way to think?</title>
    <updated>2025-04-25T12:45:32+00:00</updated>
    <author>
      <name>/u/Eralyon</name>
      <uri>https://old.reddit.com/user/Eralyon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2504.09858"&gt;https://arxiv.org/abs/2504.09858&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR:&lt;br /&gt; Bypassing the thinking process, forcing the beginning of the answer by &amp;quot;Thinking: Okay, I think I have finished thinking&amp;quot; (lol), they get similar/better inference results !!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eralyon"&gt; /u/Eralyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7k1ck/no_thinking_is_the_right_way_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7seqn</id>
    <title>What’s Meta hinting at with this cryptic post? We need Bindy to decode this for us:</title>
    <updated>2025-04-25T18:38:23+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"&gt; &lt;img alt="What’s Meta hinting at with this cryptic post? We need Bindy to decode this for us:" src="https://preview.redd.it/w1t0tdarz0xe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c52aab51f1bacd76acfaa9ceb42eb78619be9fdf" title="What’s Meta hinting at with this cryptic post? We need Bindy to decode this for us:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w1t0tdarz0xe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7seqn/whats_meta_hinting_at_with_this_cryptic_post_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k87gc4</id>
    <title>How are people converting Gemma 3 loras / models to gguf? Both latest transformers and unsloth seem to be broken for them atm.</title>
    <updated>2025-04-26T07:33:39+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/35887"&gt;https://github.com/huggingface/transformers/pull/35887&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k87gc4/how_are_people_converting_gemma_3_loras_models_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k87gc4/how_are_people_converting_gemma_3_loras_models_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k87gc4/how_are_people_converting_gemma_3_loras_models_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T07:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k83moy</id>
    <title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
    <updated>2025-04-26T03:31:47+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://arxiv.org/abs/2504.13837"&gt;https://arxiv.org/abs/2504.13837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;video&lt;/p&gt; &lt;p&gt;Recent breakthroughs in reasoning-focused large language models (LLMs) like OpenAI-o1, DeepSeek-R1, and Kimi-1.5 have largely relied on &lt;em&gt;Reinforcement Learning with Verifiable Rewards&lt;/em&gt; (RLVR), which replaces human annotations with automated rewards (e.g., verified math solutions or passing code tests) to scale self-improvement. While RLVR enhances reasoning behaviors such as self-reflection and iterative refinement, we challenge a core assumption:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Does RLVR actually expand LLMs' reasoning capabilities, or does it merely optimize existing ones?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;By evaluating models via &lt;em&gt;pass@k&lt;/em&gt;, where success requires just one correct solution among &lt;em&gt;k&lt;/em&gt; attempts, we uncover that RL-trained models excel at low &lt;em&gt;k&lt;/em&gt; (e.g., pass@1) but are consistently &lt;em&gt;outperformed by base models&lt;/em&gt; at high &lt;em&gt;k&lt;/em&gt; (e.g., pass@256). This demonstrates that RLVR &lt;em&gt;narrows the model's exploration&lt;/em&gt;, favoring known high-reward paths instead of discovering new reasoning strategies. Crucially, all correct solutions from RL-trained models already exist in the base model's distribution, proving RLVR enhances &lt;em&gt;sampling efficiency&lt;/em&gt;, not reasoning capacity, while inadvertently shrinking the solution space.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k83moy/video/sb8m5ckim3xe1/player"&gt;The effect of RLVR on LLM's reasoning ability. Search trees are generated by repeated sampling from the base and RLVR-trained models for a given problem. Grey indicates paths that are unlikely to be sampled by the model, while black indicates paths that are likely to be sampled. Green indicates correct paths, which has positive rewards. Our key finding is that all reasoning paths in the RLVR model are already present in the base model. For certain problems like Problem A, RLVR training biases the distribution toward rewarded paths, improving sampling efficiency. However, this comes at the cost of reduced scope of reasoning capacity: For other problems like Problem B, the base model contains the correct path, whereas that of the RLVR model does not.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;**RL-trained models perform worse than base models in pass@**&lt;strong&gt;&lt;em&gt;k&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;at large k values.&lt;/strong&gt; While RL-trained models outperform base models at low sampling sizes (small &lt;em&gt;k&lt;/em&gt;), base models consistently surpass them at larger &lt;em&gt;k&lt;/em&gt; across all benchmarks, even achieving higher pass@&lt;em&gt;k&lt;/em&gt; scores. Manual inspection reveals that base models can solve problems thought to require RL training by generating diverse reasoning paths, with at least one correct solution per problem. This indicates that RL training does not enhance—and may even limit—the full reasoning potential of LLMs compared to aggressive sampling in the base model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL boosts sampling efficiency but reduces the reasoning capacity boundary.&lt;/strong&gt; The analysis reveals that RLVR-trained models generate reasoning paths already within the base model's output distribution, meaning RLVR biases the model toward higher-rewarded solutions rather than creating entirely new reasoning abilities. However, this focus on rewarded paths reduces the model's exploration capacity, limiting its coverage of solvable problems at larger sampling sizes. These findings suggest that RLVR does not fundamentally transcend the base model's reasoning capabilities but instead optimizes existing pathways at the cost of broader problem-solving diversity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RLVR algorithms perform similarly and remain far from optimal.&lt;/strong&gt; The study compares various RL algorithms (PPO, GRPO, Reinforce++) and finds their performance differences minor, as measured by the sampling efficiency gap (∆SE), which assesses how close they get to optimal sampling efficiency. Despite slight variations in ∆SE among algorithms, the gap remains large across all methods. This indicates that current RL approaches, focused on improving sampling efficiency, still fall far short of optimal performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RLVR and distillation are fundamentally different.&lt;/strong&gt; While RL improves sampling efficiency, distillation can genuinely introduce new knowledge into the model. As a result, distilled models often exhibit an expanded scope of reasoning capability beyond that of the base model by learning from distilled models, in contrast to RLVR-trained models whose capacity remains bounded by the base.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;**RL-trained models perform worse than base models in pass@**&lt;strong&gt;&lt;em&gt;k&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;at large k values.&lt;/strong&gt; While RL-trained models outperform base models at low sampling sizes (small &lt;em&gt;k&lt;/em&gt;), base models consistently surpass them at larger &lt;em&gt;k&lt;/em&gt; across all benchmarks, even achieving higher pass@&lt;em&gt;k&lt;/em&gt; scores. Manual inspection reveals that base models can solve problems thought to require RL training by generating diverse reasoning paths, with at least one correct solution per problem. This indicates that RL training does not enhance—and may even limit—the full reasoning potential of LLMs compared to aggressive sampling in the base model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL boosts sampling efficiency but reduces the reasoning capacity boundary.&lt;/strong&gt; The analysis reveals that RLVR-trained models generate reasoning paths already within the base model's output distribution, meaning RLVR biases the model toward higher-rewarded solutions rather than creating entirely new reasoning abilities. However, this focus on rewarded paths reduces the model's exploration capacity, limiting its coverage of solvable problems at larger sampling sizes. These findings suggest that RLVR does not fundamentally transcend the base model's reasoning capabilities but instead optimizes existing pathways at the cost of broader problem-solving diversity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RLVR algorithms perform similarly and remain far from optimal.&lt;/strong&gt; The study compares various RL algorithms (PPO, GRPO, Reinforce++) and finds their performance differences minor, as measured by the sampling efficiency gap (∆SE), which assesses how close they get to optimal sampling efficiency. Despite slight variations in ∆SE among algorithms, the gap remains large across all methods. This indicates that current RL approaches, focused on improving sampling efficiency, still fall far short of optimal performance.&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;RLVR and distillation are fundamentally different.&lt;/strong&gt; While RL improves sampling efficiency, distillation can genuinely introduce new knowledge into the model. As a result, distilled models often exhibit an expanded scope of reasoning capability beyond that of the base model by learning from distilled models, in contrast to RLVR-trained models whose capacity remains bounded by the base.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/article"&gt;u/article&lt;/a&gt;{yue2025limit-of-rlvr, title={Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?}, author={Yue, Yang and Chen, Zhiqi and Lu, Rui and Zhao, Andrew and Wang, Zhaokai and Yue, Yang and Song, Shiji and Huang, Gao}, journal={arXiv preprint arXiv:2504.13837}, year={2025} }&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k83moy/does_reinforcement_learning_really_incentivize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k83moy/does_reinforcement_learning_really_incentivize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k83moy/does_reinforcement_learning_really_incentivize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T03:31:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7ue47</id>
    <title>Trained the tiny stories dataset on a 12M parameter model.</title>
    <updated>2025-04-25T20:02:05+00:00</updated>
    <author>
      <name>/u/Slaghton</name>
      <uri>https://old.reddit.com/user/Slaghton</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"&gt; &lt;img alt="Trained the tiny stories dataset on a 12M parameter model." src="https://preview.redd.it/qnx9gqc671xe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdcb4160b1d2416d1a24263a7cd97dc785946e9f" title="Trained the tiny stories dataset on a 12M parameter model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trained a 12M Parameter model on the tiny stories dataset. &lt;/p&gt; &lt;p&gt;**GPU used is an Nvidia 4080** &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/roneneldan/TinyStories"&gt;https://huggingface.co/datasets/roneneldan/TinyStories&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I played some video games while it was running off and on so it probably would've finished a bit earlier around 45 hours or so. &lt;/p&gt; &lt;p&gt;I think for smaller models, if you go past the Chinchilla Scaling Law of using 20 tokens per parameter, you can see improvements. This becomes less and less as the model is scaled up though I believe. &lt;/p&gt; &lt;p&gt;(Though maybe bigger models would actually benefit to but the compute becomes ridiculous and gains might be much lower than smaller models)&lt;/p&gt; &lt;p&gt;P.S. The stories aren't the best (lol), but they are pretty coherent. &lt;/p&gt; &lt;p&gt;Configuration info below.&lt;/p&gt; &lt;p&gt;config = LlamaConfig(&lt;/p&gt; &lt;p&gt;vocab_size=vocab_size,&lt;/p&gt; &lt;p&gt;hidden_size=384,&lt;/p&gt; &lt;p&gt;intermediate_size=768, &lt;/p&gt; &lt;p&gt;num_hidden_layers=8, &lt;/p&gt; &lt;p&gt;num_attention_heads=8, &lt;/p&gt; &lt;p&gt;max_position_embeddings=6000,&lt;/p&gt; &lt;p&gt;rms_norm_eps=1e-5,&lt;/p&gt; &lt;p&gt;initializer_range=0.02,&lt;/p&gt; &lt;p&gt;use_cache=True,&lt;/p&gt; &lt;p&gt;tie_word_embeddings=False,&lt;/p&gt; &lt;p&gt;attention_dropout=0.1,&lt;/p&gt; &lt;p&gt;hidden_dropout=0.1,&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;training_args = TrainingArguments(&lt;/p&gt; &lt;p&gt;output_dir=output_dir,&lt;/p&gt; &lt;p&gt;overwrite_output_dir=False,&lt;/p&gt; &lt;p&gt;num_train_epochs=1, &lt;/p&gt; &lt;p&gt;per_device_train_batch_size=8,&lt;/p&gt; &lt;p&gt;gradient_accumulation_steps=1,&lt;/p&gt; &lt;p&gt;save_strategy=&amp;quot;steps&amp;quot;, # Use steps for saving&lt;/p&gt; &lt;p&gt;save_steps=5000,&lt;/p&gt; &lt;p&gt;logging_strategy=&amp;quot;steps&amp;quot;, # Use steps for logging&lt;/p&gt; &lt;p&gt;logging_steps=100, # Log training loss frequently for the scheduler&lt;/p&gt; &lt;p&gt;save_total_limit=10,&lt;/p&gt; &lt;p&gt;prediction_loss_only=True, # Often True for Causal LM if not evaluating metrics like perplexity&lt;/p&gt; &lt;p&gt;learning_rate=.0008, # Initial learning rate for AdamW&lt;/p&gt; &lt;p&gt;weight_decay=.05,&lt;/p&gt; &lt;p&gt;fp16=True,&lt;/p&gt; &lt;p&gt;gradient_checkpointing=True,&lt;/p&gt; &lt;p&gt;max_grad_norm=1.0,&lt;/p&gt; &lt;p&gt;# Evaluation settings (important if using eval_loss with scheduler later)&lt;/p&gt; &lt;p&gt;evaluation_strategy=&amp;quot;steps&amp;quot; if not disable_eval else &amp;quot;no&amp;quot;,&lt;/p&gt; &lt;p&gt;eval_steps=5000 if not disable_eval else None,&lt;/p&gt; &lt;p&gt;report_to=&amp;quot;wandb&amp;quot;, # Log to W&amp;amp;B&lt;/p&gt; &lt;p&gt;)&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Training stats below.&lt;/p&gt; &lt;p&gt;{'train_runtime': 180146.524, 'train_samples_per_second': 35.091, 'train_steps_per_second': 4.386, 'train_loss': 0.23441845736255604, 'epoch': 3.0}&lt;/p&gt; &lt;p&gt;100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 790191/790191 [50:02:26&amp;lt;00:00, 4.39it/s]&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:42,894 - INFO - Saving final model and training state...&lt;/p&gt; &lt;p&gt;***** train metrics *****&lt;/p&gt; &lt;p&gt;epoch = 3.0&lt;/p&gt; &lt;p&gt;total_flos = 711039651GF&lt;/p&gt; &lt;p&gt;train_loss = 0.2344&lt;/p&gt; &lt;p&gt;train_runtime = 2 days, 2:02:26.52&lt;/p&gt; &lt;p&gt;train_samples_per_second = 35.091&lt;/p&gt; &lt;p&gt;train_steps_per_second = 4.386&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:43,067 - INFO - Training completed successfully!&lt;/p&gt; &lt;p&gt;2025-04-25 13:32:43,068 - INFO - Final model saved to: ./llama_model_test\final&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;wandb: Run summary:&lt;/p&gt; &lt;p&gt;wandb: eval/loss 0.19124&lt;/p&gt; &lt;p&gt;wandb: eval/runtime 47.0576&lt;/p&gt; &lt;p&gt;wandb: eval/samples_per_second 225.022&lt;/p&gt; &lt;p&gt;wandb: eval/steps_per_second 28.136&lt;/p&gt; &lt;p&gt;wandb: lr 0.0&lt;/p&gt; &lt;p&gt;wandb: total_flos 7.634730128676549e+17&lt;/p&gt; &lt;p&gt;wandb: train/epoch 3&lt;/p&gt; &lt;p&gt;wandb: train/global_step 790191&lt;/p&gt; &lt;p&gt;wandb: train/grad_norm 0.22934&lt;/p&gt; &lt;p&gt;wandb: train/learning_rate 0.0&lt;/p&gt; &lt;p&gt;wandb: train/loss 0.1965&lt;/p&gt; &lt;p&gt;wandb: train_loss 0.23442&lt;/p&gt; &lt;p&gt;wandb: train_runtime 180146.524&lt;/p&gt; &lt;p&gt;wandb: train_samples_per_second 35.091&lt;/p&gt; &lt;p&gt;wandb: train_steps_per_second 4.386&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slaghton"&gt; /u/Slaghton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qnx9gqc671xe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7ue47/trained_the_tiny_stories_dataset_on_a_12m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7stfg</id>
    <title>I built a debugging MCP server that saves me ~2 programming hours a day</title>
    <updated>2025-04-25T18:55:29+00:00</updated>
    <author>
      <name>/u/klawisnotwashed</name>
      <uri>https://old.reddit.com/user/klawisnotwashed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"&gt; &lt;img alt="I built a debugging MCP server that saves me ~2 programming hours a day" src="https://external-preview.redd.it/O7iRdpSAOTXAp8ugLZRXApZZDYusuN_fEGs3eP-yzAo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7b677b4629df76504b2adac01e7283245681d54" title="I built a debugging MCP server that saves me ~2 programming hours a day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Deebo is an agentic debugging system wrapped in an MCP server, so it acts as a copilot for your coding agent.&lt;/p&gt; &lt;p&gt;Think of your main coding agent as a single threaded process. Deebo introduces multi threadedness to AI-assisted coding. You can have your agent delegate tricky bugs, context heavy tasks, validate theories, run simulations, etc.&lt;/p&gt; &lt;p&gt;The cool thing is the agents inside the deebo mcp server USE mcp themselves! They use git and file system MCP tools in order to actually read and edit code. They also do their work in separate git branches which provides natural process isolation. &lt;/p&gt; &lt;p&gt;Deebo scales to production codebases, too. I took on a tinygrad bug bounty with me + Cline + Deebo with no previous experience with the tinygrad codebase. Deebo spawned 17 scenario agents over multiple OODA loops, and synthesized 2 valid fixes! You can read the &lt;a href="https://github.com/snagasuri/deebo-prototype/tree/master/memory-bank/9bd38e9840d3/sessions/session-1744006973678"&gt;session logs here&lt;/a&gt; and see &lt;a href="https://github.com/snagasuri/deebo-prototype/blob/master/memory-bank/9bd38e9840d3/progress.md"&gt;the final fix here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you’ve ever gotten frustrated with your coding agent for looping endlessly on a seemingly simple task, you can install Deebo with a one line npx &lt;a href="mailto:deebo-setup@latest"&gt;deebo-setup@latest&lt;/a&gt;. The code is fully open source! Take a look at the code! &lt;a href="https://github.com/snagasuri/deebo-prototype"&gt;https://github.com/snagasuri/deebo-prototype&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I came up with all the system design, implementation, etc. myself so if anyone wants to chat about how Deebo works/has any questions I'd love to talk! Would highly appreciate your guys feedback! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klawisnotwashed"&gt; /u/klawisnotwashed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/snagasuri/deebo-prototype"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7stfg/i_built_a_debugging_mcp_server_that_saves_me_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k85izg</id>
    <title>5tps with Llama 4 Scout via Ollama and Unsloth dynamic quants, CPU only</title>
    <updated>2025-04-26T05:26:45+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that the llama 4 branch was just merged into ollama main, so I updated ollama and grabbed the 2.71 bit unsloth dynamic quant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ollama run --verbose hf.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF:Q2_K_XL&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It works!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;total duration: 2m7.090132071s load duration: 45.646389ms prompt eval count: 91 token(s) prompt eval duration: 4.847635243s prompt eval rate: 18.77 tokens/s eval count: 584 token(s) eval duration: 2m2.195920773s eval rate: 4.78 tokens/s&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;42GB is the size of the 2.71Q model on disk, and it is much faster (of course) than equivalent 70B Q4 (which is also 42GB on disc)&lt;/p&gt; &lt;p&gt;CPU is 64GB Ryzen 7&lt;/p&gt; &lt;p&gt;Feels lightning fast for CPU only compared to 70B and even 27-32B dense models. &lt;/p&gt; &lt;p&gt;First test questions worked great. &lt;/p&gt; &lt;p&gt;Looking forward to using this; I've been hoping for a large MoE with small experts for a while, very excited.&lt;/p&gt; &lt;p&gt;Next will be Maverick on the AI server (500GB RAM, 24GB VRAM)...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k85izg/5tps_with_llama_4_scout_via_ollama_and_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T05:26:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7t6dm</id>
    <title>Deepseek r2 when?</title>
    <updated>2025-04-25T19:10:33+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope it comes out this month, i saw a post that said it was gonna come out before May..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7t6dm/deepseek_r2_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7tg8n</id>
    <title>GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)</title>
    <updated>2025-04-25T19:22:16+00:00</updated>
    <author>
      <name>/u/danihend</name>
      <uri>https://old.reddit.com/user/danihend</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt; &lt;img alt="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" src="https://external-preview.redd.it/M3Z4eDhhdmU3MXhlMYg3hh2y6NN7WC_nAJWjhF3jltCetUE7ORI41iUNIAJC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45713b47f9ba53d37e0b87a41e271222a2364c40" title="GLM-4-9B(Q5_K_L) Heptagon Balls sim (multi-prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title pretty much says it but just to clarify - it wasn't one-shot. It was prompt-&amp;gt;response-&amp;gt;error, then this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here is an error after running the sim: &amp;lt;error&amp;gt; Exception in Tkinter callback Traceback (most recent call last): File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 1967, in call return self.func(*args) ^^^^^^^^^^^^^^^^ File &amp;quot;C:\Users\username\anaconda3\Lib\tkinter_init_.py&amp;quot;, line 861, in callit func(*args) File &amp;quot;c:\Users\username\VSCodeProjects\model_tests\balls\GLM49B_Q5KL_balls.py&amp;quot;, line 140, in update current_time_ms = float(current_time) ^^^^^^^^^^^^^^^^^^^ ValueError: could not convert string to float: 'after#2' &amp;lt;/error&amp;gt; Now think as hard as you can about why this is happening. Look at the entire script and consider how the parts work together. You are free to think as long as you need if you use thinking tags like this: &amp;lt;think&amp;gt;thoughts here&amp;lt;/think&amp;gt;. Once finished thinking, just provide the patch to the code. No need to rewrite it all. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then I applied the fix, got another error, replaced the original Assistant code block with the new code and presented the new error as if it were the 1st error by editing my message. I think that resulted in the working version.&lt;/p&gt; &lt;p&gt;So TL;DR - couple of prompts to get it working.&lt;/p&gt; &lt;p&gt;Simply pasting error after error did not work, but structured prompting with a bit of thinking seems to bring out some more potential.&lt;/p&gt; &lt;p&gt;Just thought I'd share in case it helps people with prompting it and just to show that it is not a bad model for it's size. The result is very similar to the 32B version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danihend"&gt; /u/danihend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zrjvo8ve71xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7tg8n/glm49bq5_k_l_heptagon_balls_sim_multiprompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T19:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7quqt</id>
    <title>Do people trying to squeeze every last GB out of their GPU use their IGPU to display to their monitor?</title>
    <updated>2025-04-25T17:34:59+00:00</updated>
    <author>
      <name>/u/Golfclubwar</name>
      <uri>https://old.reddit.com/user/Golfclubwar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By default, just for basic display, Linux can eat 500MB, windows can eat 1.1GB. I imagine for someone with like an 8-12GB card trying to barely squeeze the biggest model they can onto the gpu by tweaking context size and quant etc., this is a highly nontrivial cost. &lt;/p&gt; &lt;p&gt;Unless for some reason you needed the dgpu for something else, why wouldn’t they just display using their IGPU instead? Obviously there’s still a fixed driver overhead, but you’d save nearly a gigabyte, and in terms of simply using an IDE and a browser it’s hard to think of any drawbacks.&lt;/p&gt; &lt;p&gt;Am I stupid and this wouldn’t work the way I think it would or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Golfclubwar"&gt; /u/Golfclubwar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7quqt/do_people_trying_to_squeeze_every_last_gb_out_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T17:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uxxk</id>
    <title>LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released</title>
    <updated>2025-04-25T20:25:39+00:00</updated>
    <author>
      <name>/u/ispolin</name>
      <uri>https://old.reddit.com/user/ispolin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt; &lt;img alt="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" src="https://b.thumbs.redditmedia.com/zPNTVgAISYkoCxdv3xYABX-74BRXUsT6QERGjZPbVto.jpg" title="LM Studio 0.3.15 with support for GLM-4 models and NVIDIA RTX50-series just got released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1"&gt;https://preview.redd.it/mxja601ei1xe1.png?width=2102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31ca8d6f8f7b767e7379e5b00878cc43622b19c1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ispolin"&gt; /u/ispolin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uxxk/lm_studio_0315_with_support_for_glm4_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7rgyv</id>
    <title>Tiny Agents: a MCP-powered agent in 50 lines of code</title>
    <updated>2025-04-25T18:00:23+00:00</updated>
    <author>
      <name>/u/julien_c</name>
      <uri>https://old.reddit.com/user/julien_c</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt; &lt;img alt="Tiny Agents: a MCP-powered agent in 50 lines of code" src="https://external-preview.redd.it/fCTs8gI7KvvOKk5o8AQ0g6EQWi7h5KkDI0MBs8uNyiw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09d0dce63a6cd60077b6242bb1e5e6a8b6411b5f" title="Tiny Agents: a MCP-powered agent in 50 lines of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm a co-founder of HuggingFace and a big &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; fan.&lt;/p&gt; &lt;p&gt;Today I'm dropping Tiny Agents, a 50 lines-of-code Agent in Javascript 🔥&lt;/p&gt; &lt;p&gt;I spent the last few weeks diving into MCP (Model Context Protocol) to understand what the hype was about.&lt;/p&gt; &lt;p&gt;It is fairly simple, but still quite useful as a standard API to expose sets of Tools that can be hooked to LLMs.&lt;/p&gt; &lt;p&gt;But while implementing it I came to my second realization:&lt;/p&gt; &lt;p&gt;Once you have a MCP Client, an Agent is literally just a while loop on top of it. 🤯&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/tiny-agents"&gt;https://huggingface.co/blog/tiny-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5"&gt;https://preview.redd.it/v0acl2n6t0xe1.png?width=1846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cecc5f62c6e05855d5ea1b67cceb56e2ccddbf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/julien_c"&gt; /u/julien_c &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7rgyv/tiny_agents_a_mcppowered_agent_in_50_lines_of_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T18:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7krlm</id>
    <title>Gemma 3 fakes (and ignores) the system prompt</title>
    <updated>2025-04-25T13:20:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt; &lt;img alt="Gemma 3 fakes (and ignores) the system prompt" src="https://preview.redd.it/xuycbwnk4zwe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fba119d92fca9059223ac136a22602c0f3b43b8" title="Gemma 3 fakes (and ignores) the system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The screenshot shows what Gemma 3 said when I pointed out that it wasn't following its system prompt properly. &amp;quot;Who reads the fine print? 😉&amp;quot; - really, seriously, WTF?&lt;/p&gt; &lt;p&gt;At first I thought it may be an issue with the format/quant, an inference engine bug or just my settings or prompt. But digging deeper, I realized I had been fooled: While the [Gemma 3 chat template](&lt;a href="https://huggingface.co/google/gemma-3-27b-it/blob/main/chat%5C_template.json"&gt;https://huggingface.co/google/gemma-3-27b-it/blob/main/chat\_template.json&lt;/a&gt;) *does* support a system role, all it *really* does is dump the system prompt into the first user message. That's both ugly *and* unreliable - doesn't even use any special tokens, so there's no way for the model to differentiate between what the system (platform/dev) specified as general instructions and what the (possibly untrusted) user said. 🙈&lt;/p&gt; &lt;p&gt;Sure, the model still follows instructions like any other user input - but it never learned to treat them as higher-level system rules, so they're basically &amp;quot;optional&amp;quot;, which is why it ignored mine like &amp;quot;fine print&amp;quot;. That makes Gemma 3 utterly unreliable - so I'm switching to Mistral Small 3.1 24B Instruct 2503 which has proper system prompt support.&lt;/p&gt; &lt;p&gt;Hopefully Google will provide *real* system prompt support in Gemma 4 - or the community will deliver a better finetune in the meantime. For now, I'm hoping Mistral's vision capability gets wider support, since that's one feature I'll miss from Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xuycbwnk4zwe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T13:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7uvpm</id>
    <title>Qwen introduces their mobile app</title>
    <updated>2025-04-25T20:22:54+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt; &lt;img alt="Qwen introduces their mobile app" src="https://preview.redd.it/ewjq8s2ei1xe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe3b4f5cf5c69932dece355c02addb1e439cdd0" title="Qwen introduces their mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewjq8s2ei1xe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7uvpm/qwen_introduces_their_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T20:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k88v0p</id>
    <title>Newelle 0.9.5 Released: Internet Access, Improved Document Reading</title>
    <updated>2025-04-26T09:15:18+00:00</updated>
    <author>
      <name>/u/iTzSilver_YT</name>
      <uri>https://old.reddit.com/user/iTzSilver_YT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"&gt; &lt;img alt="Newelle 0.9.5 Released: Internet Access, Improved Document Reading" src="https://preview.redd.it/6n7tbbk5c5xe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=c94def82cef2aab0509b503092cef40a3a4c19f3" title="Newelle 0.9.5 Released: Internet Access, Improved Document Reading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newelle 0.9.5 Released! Newelle is an advanced AI assistant for Linux supporting any LLM (Local or Online), voice commands, extensions and much more!&lt;/p&gt; &lt;p&gt;🔎 Implemented Web Search with SearXNG, DuckDuckGo, and Tavily&lt;br /&gt; 🌐 Website Reading: ask questions about websites (Write #url to embed it)&lt;br /&gt; 🔢 Improved inline LaTeX support&lt;br /&gt; 🗣 New empty chat placeholder&lt;br /&gt; 📎 Improved Document reading: semantic search will only be done if the document is too long&lt;br /&gt; 💭 New thinking widget&lt;br /&gt; 🧠 Add vision support for llama4 on Groq and possibility to choose provider on OpenRouter&lt;br /&gt; 🌍 New translations (Traditional Chinese, Bengali, Hindi)&lt;br /&gt; 🐞 Various bug fixes&lt;/p&gt; &lt;p&gt;Source Code: &lt;a href="https://github.com/qwersyk/Newelle/"&gt;https://github.com/qwersyk/Newelle/&lt;/a&gt;&lt;br /&gt; Flathub: &lt;a href="https://flathub.org/apps/io.github.qwersyk.Newelle"&gt;https://flathub.org/apps/io.github.qwersyk.Newelle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iTzSilver_YT"&gt; /u/iTzSilver_YT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6n7tbbk5c5xe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k88v0p/newelle_095_released_internet_access_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T09:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k801ba</id>
    <title>It's been a while since we had new Qwen &amp; Qwen Coder models...</title>
    <updated>2025-04-26T00:18:43+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saying... 😉&lt;/p&gt; &lt;p&gt;In all seriousness if they need to cook further - let them cook.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k801ba/its_been_a_while_since_we_had_new_qwen_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T00:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k7o89n</id>
    <title>We compress any BF16 model to ~70% size during inference, while keeping the output LOSSLESS so that you can fit in more ERP context or run larger models.</title>
    <updated>2025-04-25T15:47:29+00:00</updated>
    <author>
      <name>/u/choHZ</name>
      <uri>https://old.reddit.com/user/choHZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glad to share another interesting piece of work from us: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;&lt;strong&gt;70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DF11)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tl;dr of this work is super simple. We — and several prior works — noticed that while &lt;strong&gt;BF16&lt;/strong&gt; is often promoted as a “more range, less precision” alternative to FP16 (especially to avoid value overflow/underflow during training), &lt;strong&gt;its range part (exponent bits) ends up being pretty redundant once the model is trained.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, although BF16 as a data format can represent a wide range of numbers, most trained models' exponents are plenty sparse. In practice, the exponent bits carry around 2.6 bits of actual information on average — far from the full 8 bits they're assigned.&lt;/p&gt; &lt;p&gt;This opens the door for classic Huffman coding — where shorter bit sequences are assigned to more frequent values — to &lt;strong&gt;compress the model weights&lt;/strong&gt; into a new data format we call &lt;strong&gt;DFloat11/DF11&lt;/strong&gt;, resulting in a &lt;strong&gt;LOSSLESS compression down to ~11 bits&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;But isn’t this just Zip?&lt;/h1&gt; &lt;p&gt;Not exactly. It is true that tools like Zip also leverage Huffman coding, but the tricky part here is &lt;strong&gt;making it memory efficient during inference&lt;/strong&gt;, as end users are probably not gonna be too trilled if it just makes model checkpoint downloads a bit faster (in all fairness, smaller chekpoints means a lot when training at scale, but that's not a problem for everyday users).&lt;/p&gt; &lt;p&gt;What does matter to everyday users is &lt;strong&gt;making the memory footprint smaller during GPU inference, which requires nontrivial efforts.&lt;/strong&gt; But we have figured it out, and we’ve open-sourced the code.&lt;/p&gt; &lt;p&gt;So now you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Run models that previously didn’t fit into your GPU memory.&lt;/li&gt; &lt;li&gt;Or run the same model with &lt;strong&gt;larger batch sizes and/or longer sequences&lt;/strong&gt; (very handy for those lengthy ERPs, or so I have heard).&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPU Type&lt;/th&gt; &lt;th align="left"&gt;Method&lt;/th&gt; &lt;th align="left"&gt;Successfully Run?&lt;/th&gt; &lt;th align="left"&gt;Required Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-405B-Instruct&lt;/td&gt; &lt;td align="left"&gt;8×H100-80G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;811.71 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;551.22 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×H200-141G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;141.11 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;96.14 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct&lt;/td&gt; &lt;td align="left"&gt;1×A6000-48G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;65.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;45.53 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-Distill-Llama-8B&lt;/td&gt; &lt;td align="left"&gt;1×RTX 5080-16G&lt;/td&gt; &lt;td align="left"&gt;BF16&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;16.06 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;DF11 (Ours)&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;11.23 GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some research promo posts try to surgercoat their weakness or tradeoff, thats not us. So here's are some honest FAQs:&lt;/p&gt; &lt;h1&gt;What’s the catch?&lt;/h1&gt; &lt;p&gt;Like all compression work, there’s a cost to decompressing. And here are some efficiency reports.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On an A100 with batch size 128, DF11 is &lt;strong&gt;basically just as fast&lt;/strong&gt; as BF16 (1.02x difference, assuming both version fits in the GPUs with the same batch size). See Figure 9.&lt;/li&gt; &lt;li&gt;It is up to &lt;strong&gt;38.8x faster&lt;/strong&gt; than CPU offloading, so if you have a model that can't be run on your GPU in BF16, but can in DF11, there are plenty sweet performance gains over CPU offloading — one of the other popular way to run larger-than-capacity models. See Figure 3.&lt;/li&gt; &lt;li&gt;With the model weight being compressed, you can use the saved real estate for larger batch size or longer context length. This is expecially significant if the model is already tightly fitted in GPU. See Figure 4.&lt;/li&gt; &lt;li&gt;What about batch size 1 latency when both versions (DF11 &amp;amp; BF16) can fit in a single GPU? This is where DF11 is the weakest — we observe &lt;strong&gt;~40% slower&lt;/strong&gt; (2k/100 tokens for in/out). So there is not much motivation in using DF11 if you are not trying to run larger model/bigger batch size/longer sequence length.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why not just (lossy) quantize to 8-bit?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The short answer is you should totally do that if you are satisfied with the output lossy 8-bit quantization with respect to your task. But how do you really know it is always good?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many benchmark literature suggest that compressing a model (weight-only or otherwise) to 8-bit-ish is typically a safe operation, even though it's technically lossy. What we found, however, is that while this claim is often made in quantization papers, their benchmarks tend to focus on general tasks like MMLU and Commonsense Reasoning; which do not present a comprehensive picture of model capability.&lt;/p&gt; &lt;p&gt;More challenging benchmarks — such as those involving complex reasoning — and real-world user preferences often reveal noticeable differences. One good example is Chatbot Arena indicates the 8-bit (though it is W8A8 where DF11 is weight only, so it is not 100% apple-to-apple) and 16-bit Llama 3.1 405b tend to behave quite differently on some categories of tasks (e.g., Math and Coding).&lt;/p&gt; &lt;p&gt;Although the broader question: &lt;em&gt;“Which specific task, on which model, using which quantization technique, under what conditions, will lead to a noticeable drop compared to FP16/BF16?”&lt;/em&gt; is likely to remain open-ended simply due to the sheer amount of potential combinations and definition of “noticable.” &lt;strong&gt;It is fair to say that lossy quantization introduces complexities that some end-users would prefer to avoid, since it creates uncontrolled variables that must be empirically stress-tested for each deployment scenario.&lt;/strong&gt; DF11 offeres an alternative that avoids this concern 100%.&lt;/p&gt; &lt;h1&gt;What about finetuning?&lt;/h1&gt; &lt;p&gt;Our method could potentially pair well with PEFT methods like LoRA, where the base weights are frozen. But since we compress block-wise, we can’t just apply it naively without breaking gradients. We're actively exploring this direction. If it works, if would potentially become a QLoRA alternative where you can lossly LoRA finetune a model with reduced memory footprint.&lt;/p&gt; &lt;p&gt;(As always, happy to answer questions or chat until my advisor notices I’m doomscrolling socials during work hours :&amp;gt; )&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;https://arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/LeanModels/DFloat11"&gt;https://github.com/LeanModels/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choHZ"&gt; /u/choHZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-25T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8601g</id>
    <title>Qwen AI - My most used LLM!</title>
    <updated>2025-04-26T05:56:59+00:00</updated>
    <author>
      <name>/u/Glittering-Cancel-25</name>
      <uri>https://old.reddit.com/user/Glittering-Cancel-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Qwen, DeepSeek, paid ChatGPT, and paid Claude. I must say, i find myself using Qwen the most often. It's great, especially for a free model! &lt;/p&gt; &lt;p&gt;I use all of the LLMs for general and professional work. E.g., writing, planning, management, self-help, idea generation, etc. For most of those things, i just find that Qwen produces the best results and requires the least rework, follow ups, etc. I've tested all of the LLMs by putting in the exact same prompt (i've probably done this a couple dozen times) and overall (but not always), Qwen produces the best result for me. I absolutely can't wait until they release Qwen3 Max! I also have a feeling DeepSeek is gonna go with with R2... &lt;/p&gt; &lt;p&gt;Id love to know what LLM you find yourself using the most, what you use them for (that makes a big difference), and why you think that one is the best. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Cancel-25"&gt; /u/Glittering-Cancel-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T05:56:59+00:00</published>
  </entry>
</feed>
