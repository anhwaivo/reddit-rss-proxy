<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-29T22:51:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mc5e54</id>
    <title>Single-File Qwen3 Inference in Pure CUDA C</title>
    <updated>2025-07-29T07:50:39+00:00</updated>
    <author>
      <name>/u/Awkward_Click6271</name>
      <uri>https://old.reddit.com/user/Awkward_Click6271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.&lt;/p&gt; &lt;p&gt;It works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. ~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to ~70.&lt;/p&gt; &lt;p&gt;The CUDA version is built upon my qwen.c repo. It's a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.&lt;/p&gt; &lt;p&gt;Both versions use the GGUF file directly, with no conversion to binary. The tokenizer‚Äôs vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.&lt;/p&gt; &lt;p&gt;These projects draw inspiration from Andrej Karpathy‚Äôs &lt;a href="https://github.com/karpathy/llama2.c"&gt;llama2.c&lt;/a&gt; and share the same commitment to minimalism. Both projects are MIT licensed. I‚Äôd love to hear your feedback!&lt;/p&gt; &lt;p&gt;qwen3.cu: &lt;a href="https://github.com/gigit0000/qwen3.cu"&gt;https://github.com/gigit0000/qwen3.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;qwen3.c: &lt;a href="https://github.com/gigit0000/qwen3.c"&gt;https://github.com/gigit0000/qwen3.c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Click6271"&gt; /u/Awkward_Click6271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T07:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mck6o7</id>
    <title>NVIDIA Llama Nemotron Super v1.5 is #1 on Artificial Analysis Intelligence Index for the 70B Open Model Category.</title>
    <updated>2025-07-29T18:58:44+00:00</updated>
    <author>
      <name>/u/PDXcoder2000</name>
      <uri>https://old.reddit.com/user/PDXcoder2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/"&gt; &lt;img alt="NVIDIA Llama Nemotron Super v1.5 is #1 on Artificial Analysis Intelligence Index for the 70B Open Model Category." src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="NVIDIA Llama Nemotron Super v1.5 is #1 on Artificial Analysis Intelligence Index for the 70B Open Model Category." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre excited to share that ü•áNVIDIA Llama Nemotron Super 49B v1.5 -- our just released open reasoning model -- is #1 on the &lt;a href="https://nvda.ws/44TJw4n"&gt;Artificial Analysis Intelligence Index&lt;/a&gt; - a leaderboard that spans advanced math, science, and agentic tasks, in the 70B open model category. &lt;/p&gt; &lt;p&gt;Super 49B v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. It delivers state-of-the-art accuracy and throughput, running on a single H100.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;üéØ Leading accuracy on multi-step reasoning, math, coding, and function-calling&lt;/p&gt; &lt;p&gt;üèóÔ∏è Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples&lt;/p&gt; &lt;p&gt;üìä Fully transparent training data and techniques&lt;/p&gt; &lt;p&gt;If you're building AI agents and want a high accuracy, fully-open, and transparent reasoning model that you can deploy anywhere, try Super v1.5 on &lt;a href="http://build.nvidia.com"&gt;build.nvidia.com&lt;/a&gt; or download from &lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;Hugging Face&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;Leaderboard ‚û°Ô∏è &lt;a href="https://nvda.ws/44TJw4n"&gt;https://nvda.ws/44TJw4n&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816"&gt;https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PDXcoder2000"&gt; /u/PDXcoder2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T18:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc687c</id>
    <title>New Benchmark - FamilyBench - Test models ability to understand complex tree type relationship and reason on massive context. Immune to contamination. GML 4.5 64.02%, Gemini 2.5 pro 81,48%.</title>
    <updated>2025-07-29T08:46:06+00:00</updated>
    <author>
      <name>/u/Orolol</name>
      <uri>https://old.reddit.com/user/Orolol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;This is a new &lt;strong&gt;opensource&lt;/strong&gt; project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context. &lt;/p&gt; &lt;p&gt;The idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs. &lt;/p&gt; &lt;p&gt;You can find the code here &lt;a href="https://github.com/Orolol/familyBench"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current leaderboard&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.&lt;/p&gt; &lt;p&gt;Example of family description : &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...&amp;quot;&lt;/p&gt; &lt;p&gt;Example of questions : &amp;quot;Which of Paula's grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot;&lt;/p&gt; &lt;p&gt;The no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Total tokens&lt;/th&gt; &lt;th&gt;No response rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td&gt;81.48%&lt;/td&gt; &lt;td&gt;271,500&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek R1 0528&lt;/td&gt; &lt;td&gt;75.66%&lt;/td&gt; &lt;td&gt;150,642&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sonnet 4&lt;/td&gt; &lt;td&gt;67.20%&lt;/td&gt; &lt;td&gt;575,624&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM 4.5&lt;/td&gt; &lt;td&gt;64.02%&lt;/td&gt; &lt;td&gt;216,281&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM 4.5 air&lt;/td&gt; &lt;td&gt;57.14%&lt;/td&gt; &lt;td&gt;909,228&lt;/td&gt; &lt;td&gt;26.46%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen-3.2-2507-thinking&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;743,131&lt;/td&gt; &lt;td&gt;20.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kimi K2&lt;/td&gt; &lt;td&gt;34.92%&lt;/td&gt; &lt;td&gt;67,071&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Hunyuan A13B&lt;/td&gt; &lt;td&gt;30.16%&lt;/td&gt; &lt;td&gt;121,150&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen-3.2-2507&lt;/td&gt; &lt;td&gt;28.04%&lt;/td&gt; &lt;td&gt;3,098&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral Small 3.2&lt;/td&gt; &lt;td&gt;22.22%&lt;/td&gt; &lt;td&gt;5,353&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 27B&lt;/td&gt; &lt;td&gt;17.99%&lt;/td&gt; &lt;td&gt;2,888&lt;/td&gt; &lt;td&gt;0.53%~~~~&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;EDIT : Added R1, Sonnet 4, Hunyuan A13b and Gemma 3 27b&lt;/p&gt; &lt;p&gt;Reasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orolol"&gt; /u/Orolol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mchsyd</id>
    <title>[tutorial] Use GLM 4.5 (or any LLM) with Claude Code</title>
    <updated>2025-07-29T17:30:36+00:00</updated>
    <author>
      <name>/u/shaman-warrior</name>
      <uri>https://old.reddit.com/user/shaman-warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Step 1. Get this &lt;a href="https://github.com/musistudio/claude-code-router"&gt;https://github.com/musistudio/claude-code-router&lt;/a&gt; you get it up with 2 npm installs&lt;br /&gt; Step 2. Create an openrouter account and top up 10 bucks or whatevs. Get API key.&lt;br /&gt; Step 3. Put this in the JSON (look at the instructions from that repo: ~/.claude-code-router/config.json )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;LOG&amp;quot;: true, &amp;quot;API_TIMEOUT_MS&amp;quot;: 600000, &amp;quot;Providers&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;openrouter&amp;quot;, &amp;quot;api_base_url&amp;quot;: &amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;, &amp;quot;api_key&amp;quot;: &amp;quot;sk-or-v1-XXX&amp;quot;, &amp;quot;models&amp;quot;: [&amp;quot;z-ai/glm-4.5&amp;quot;], &amp;quot;transformer&amp;quot;: { &amp;quot;use&amp;quot;: [&amp;quot;openrouter&amp;quot;] } }, ], &amp;quot;Router&amp;quot;: { &amp;quot;default&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;, &amp;quot;background&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;, &amp;quot;think&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;, &amp;quot;longContext&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;, &amp;quot;longContextThreshold&amp;quot;: 60000, &amp;quot;webSearch&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step 4. Ensure the 'server' restarts run 'ccr restart'&lt;br /&gt; Step 5. Write `ccr code` and just enjoy. &lt;/p&gt; &lt;p&gt;Careful I burned 3$ with just one agentic query that took 10 minutes and it was still thinking. I'm going to try more with Qwen3 235B and experiment. &lt;/p&gt; &lt;p&gt;GLM 4.5 is pretty smart. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shaman-warrior"&gt; /u/shaman-warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5oh2</id>
    <title>This year‚Äôs best open-source models and most cost-effective models</title>
    <updated>2025-07-29T08:09:33+00:00</updated>
    <author>
      <name>/u/Apart-River475</name>
      <uri>https://old.reddit.com/user/Apart-River475</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt; &lt;img alt="This year‚Äôs best open-source models and most cost-effective models" src="https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg" title="This year‚Äôs best open-source models and most cost-effective models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br /&gt; The &lt;strong&gt;GLM-4.5&lt;/strong&gt; series models are foundation models designed for intelligent agents. GLM-4.5 has &lt;strong&gt;355&lt;/strong&gt; billion total parameters with &lt;strong&gt;32&lt;/strong&gt; billion active parameters, while GLM-4.5-Air adopts a more compact design with &lt;strong&gt;106&lt;/strong&gt; billion total parameters and &lt;strong&gt;12&lt;/strong&gt; billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"&gt;Bench performance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://z.ai/blog/glm-4.5"&gt;blog&lt;/a&gt;ÔΩú&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;huggingface&lt;/a&gt;ÔΩú &lt;a href="https://github.com/zai-org/GLM-4.5"&gt;github&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-River475"&gt; /u/Apart-River475 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8evq</id>
    <title>Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ</title>
    <updated>2025-07-29T11:02:25+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt; &lt;img alt="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ" src="https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg" title="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! üòÖ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt; &lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt; &lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt; &lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)&lt;/li&gt; &lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt; &lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt; &lt;li&gt;~¬£30-50k needed for full training run of 1000 epochs (I could only afford testing üòÖ)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt: &lt;ul&gt; &lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt; &lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt; &lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚≠êÔ∏è &lt;a href="https://github.com/Danau5tin/terminal-bench-rl"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Danau5tin/tbench-agentic-data-pipeline"&gt;‚≠êÔ∏è Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href="https://github.com/rllm-org/rllm"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href="https://www.tbench.ai/"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mc8evq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc9sk0</id>
    <title>üåü Ming-lite-omni v1.5 is here! Our recent upgrade for omni-modal AI! üöÄ</title>
    <updated>2025-07-29T12:14:38+00:00</updated>
    <author>
      <name>/u/Dependent-Roll-8934</name>
      <uri>https://old.reddit.com/user/Dependent-Roll-8934</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ming-lite-omni v1.5 demonstrates highly competitive results compared to industry-leading models of similar scale.&lt;/p&gt; &lt;p&gt;ü§ñGithub: &lt;a href="https://github.com/inclusionAI/Ming"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü´ÇHugging Face: &lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üç≠ModelScope: &lt;a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5"&gt;https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ming-lite-omni v1.5 features three key improvements compared to Ming-lite-omni: &lt;/p&gt; &lt;p&gt;üß† Enhanced Multimodal Comprehension: Ming-lite-omni v1.5 now understands all data types‚Äîimages, text, video, and speech‚Äîsignificantly better, thanks to extensive data upgrades.&lt;/p&gt; &lt;p&gt;üé® Precise Visual Editing Control: Achieve superior image generation and editing with Ming-lite-omni v1.5, featuring advanced controls for consistent IDs and scenes, and enhanced support for visual tasks like detection and segmentation.&lt;/p&gt; &lt;p&gt;‚ú® Optimized User Experience: Expect a smoother, more accurate, and aesthetically pleasing interaction with Ming-lite-omni v1.5.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent-Roll-8934"&gt; /u/Dependent-Roll-8934 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T12:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcp7dp</id>
    <title>GLM-4.5 on fiction.livebench</title>
    <updated>2025-07-29T22:11:50+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"&gt; &lt;img alt="GLM-4.5 on fiction.livebench" src="https://preview.redd.it/aey1fr0e0wff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=164ac21dfb78027d97359b16dae6fc48436681ec" title="GLM-4.5 on fiction.livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aey1fr0e0wff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T22:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc9o4m</id>
    <title>Stuck on a problem? We're excited to share a glimpse of what's possible! üëã</title>
    <updated>2025-07-29T12:08:36+00:00</updated>
    <author>
      <name>/u/Dependent-Roll-8934</name>
      <uri>https://old.reddit.com/user/Dependent-Roll-8934</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"&gt; &lt;img alt="Stuck on a problem? We're excited to share a glimpse of what's possible! üëã" src="https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4fbfc313e05d906ffc795bed85b7f8f2c8e0e3c5" title="Stuck on a problem? We're excited to share a glimpse of what's possible! üëã" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our experimental Ming-lite-omni v1.5 (&lt;a href="https://github.com/inclusionAI/Ming"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;) leverages advanced audio-visual capabilities to explore new frontiers in interactive learning. This model, still under development, aims to understand your handwriting, interpret your thoughts, and guide you through solutions in real-time. We're eagerly continuing our research and look forward to sharing future advancements! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent-Roll-8934"&gt; /u/Dependent-Roll-8934 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sdqo34a90tff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T12:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcnq7r</id>
    <title>AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio</title>
    <updated>2025-07-29T21:12:54+00:00</updated>
    <author>
      <name>/u/ZZZCodeLyokoZZZ</name>
      <uri>https://old.reddit.com/user/ZZZCodeLyokoZZZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"&gt; &lt;img alt="AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio" src="https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1407a6de51b1efe682d3aec309cbbdadb1b1d910" title="AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too ~15 tk/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZZZCodeLyokoZZZ"&gt; /u/ZZZCodeLyokoZZZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcjz8j</id>
    <title>One year‚Äôs benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models</title>
    <updated>2025-07-29T18:50:58+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"&gt; &lt;img alt="One year‚Äôs benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models" src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="One year‚Äôs benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year‚Äôs hindsight. Of course benchmarks aren‚Äôt everything. They aren‚Äôt nothing either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T18:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbvf2z</id>
    <title>its getting comical</title>
    <updated>2025-07-28T23:09:30+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt; &lt;img alt="its getting comical" src="https://preview.redd.it/txsukljc5pff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d" title="its getting comical" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txsukljc5pff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T23:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcl15k</id>
    <title>Qwen 1.7B tool calling across Android on Pixel 9 and S22</title>
    <updated>2025-07-29T19:30:26+00:00</updated>
    <author>
      <name>/u/Economy-Mud-6626</name>
      <uri>https://old.reddit.com/user/Economy-Mud-6626</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"&gt; &lt;img alt="Qwen 1.7B tool calling across Android on Pixel 9 and S22" src="https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0090c885e3fd475871f7cc3149139b5e73e39a86" title="Qwen 1.7B tool calling across Android on Pixel 9 and S22" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How about running a local agent on a smartphone? Here's how I did it. &lt;/p&gt; &lt;p&gt;I stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via &lt;code&gt;uint16_t&lt;/code&gt;), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! &lt;/p&gt; &lt;h1&gt;Tool Calling Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-step conversation support&lt;/strong&gt; with automatic tool execution&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON-based tool calling&lt;/strong&gt; with &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; XML tags&lt;/li&gt; &lt;li&gt;&lt;strong&gt;test tools&lt;/strong&gt;: weather, math calculator, time, location&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Used &lt;a href="https://github.com/mlc-ai/tokenizers-cpp"&gt;tokenizer-cpp&lt;/a&gt; from MLC&lt;/h1&gt; &lt;p&gt;which binds rust &lt;a href="https://github.com/huggingface/tokenizers"&gt;huggingface/tokenizers&lt;/a&gt; giving full support for android/iOS.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// - dist/tokenizer.json void HuggingFaceTokenizerExample() { auto blob = LoadBytesFromFile(&amp;quot;dist/tokenizer.json&amp;quot;); auto tok = Tokenizer::FromBlobJSON(blob); std::string prompt = &amp;quot;What is the capital of Canada?&amp;quot;; std::vector&amp;lt;int&amp;gt; ids = tok-&amp;gt;Encode(prompt); std::string decoded_prompt = tok-&amp;gt;Decode(ids); } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Push LLM streams into Kotlin Flows&lt;/h1&gt; &lt;pre&gt;&lt;code&gt; suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&amp;gt;Unit) : String? { val res = NimbleNet.runMethod( &amp;quot;prompt_for_tool_calling&amp;quot;, inputs = hashMapOf( &amp;quot;prompt&amp;quot; to NimbleNetTensor(input, DATATYPE.STRING, null), &amp;quot;output_stream_callback&amp;quot; to createNimbleNetTensorFromForeignFunction(callback) ), ) assert(res.status) { &amp;quot;NimbleNet.runMethod('prompt_for_tool_calling') failed with status: ${res.status}&amp;quot; } return res.payload?.get(&amp;quot;results&amp;quot;)?.data as String? } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the code soon merging in Delite AI (&lt;a href="https://github.com/NimbleEdge/deliteAI/pull/165"&gt;https://github.com/NimbleEdge/deliteAI/pull/165&lt;/a&gt;)&lt;br /&gt; Or try in the assistant app (&lt;a href="https://github.com/NimbleEdge/assistant"&gt;https://github.com/NimbleEdge/assistant&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Mud-6626"&gt; /u/Economy-Mud-6626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3wcxuotf7vff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T19:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mce9tt</id>
    <title>zai-org/GLM-4.5 ¬∑ We Have Gemini At Home</title>
    <updated>2025-07-29T15:20:33+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"&gt; &lt;img alt="zai-org/GLM-4.5 ¬∑ We Have Gemini At Home" src="https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=403fb88f398e6c5fe36b4f1c95408e0675027e55" title="zai-org/GLM-4.5 ¬∑ We Have Gemini At Home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested for same, is it trained on gemini outputs ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5/discussions/1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T15:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc6fbp</id>
    <title>GLM 4.5 support is landing in llama.cpp</title>
    <updated>2025-07-29T08:59:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt; &lt;img alt="GLM 4.5 support is landing in llama.cpp" src="https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd" title="GLM 4.5 support is landing in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mchj7h</id>
    <title>AFM 4.5B</title>
    <updated>2025-07-29T17:20:54+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"&gt; &lt;img alt="AFM 4.5B" src="https://preview.redd.it/c7yvmvdgkuff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68967cc776ececd5151071c32eb068a2fd1ddad" title="AFM 4.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting small model, hadn't seen it before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-GGUF"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c7yvmvdgkuff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcfuka</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-29T16:19:15+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new qwen moe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8tks</id>
    <title>I just tried GLM 4.5</title>
    <updated>2025-07-29T11:24:54+00:00</updated>
    <author>
      <name>/u/AI-On-A-Dime</name>
      <uri>https://old.reddit.com/user/AI-On-A-Dime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt; &lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt; &lt;p&gt;Here‚Äôs the link to the results: &lt;a href="https://chat.z.ai/space/r05c76960ff0-ppt"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here‚Äôs the initial prompt:&lt;/p&gt; &lt;p&gt;‚ÄùCreate a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.‚Äù&lt;/p&gt; &lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt; &lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt; &lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-On-A-Dime"&gt; /u/AI-On-A-Dime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:24:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mco449</id>
    <title>Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT</title>
    <updated>2025-07-29T21:28:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt; &lt;img alt="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" src="https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d458366fefeec47c4d65d3844419bf9e79783f21" title="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF ¬∑ Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7xpye5hurvff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcee42</id>
    <title>My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX</title>
    <updated>2025-07-29T15:25:02+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"&gt; &lt;img alt="My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX" src="https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5a9e25e0e831120dffb4dbb77fc7392c4ccb49" title="My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simonwillison.net/2025/Jul/29/space-invaders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T15:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcoce7</id>
    <title>AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs</title>
    <updated>2025-07-29T21:37:02+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt; &lt;img alt="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" src="https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0" title="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcji8s</id>
    <title>Qwen3-30b-3ab-2507 is a beast for MCP usage!</title>
    <updated>2025-07-29T18:33:19+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt; &lt;img alt="Qwen3-30b-3ab-2507 is a beast for MCP usage!" src="https://b.thumbs.redditmedia.com/Ku7pPJKnjoNSXHTx41JonGncgMhMPCUf8ZnqoDSjoGY.jpg" title="Qwen3-30b-3ab-2507 is a beast for MCP usage!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;C'est la premi√®re fois qu'un mod√®le utilise intelligemment les serveurs MCP tout seul ! Ce n'est pas juste un ou deux serveurs et puis une r√©ponse compl√®tement √† c√¥t√© de la plaque !&lt;/p&gt; &lt;p&gt;For those who want my MCP flow, here‚Äôs the Pastebin:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/WNPrcjLS"&gt;https://pastebin.com/WNPrcjLS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268"&gt;https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T18:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcg4qt</id>
    <title>üöÄ Qwen3-30B-A3B Small Update</title>
    <updated>2025-07-29T16:29:59+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt; &lt;img alt="üöÄ Qwen3-30B-A3B Small Update" src="https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d" title="üöÄ Qwen3-30B-A3B Small Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt; &lt;p&gt;‚ú® Key Enhancements:&lt;/p&gt; &lt;p&gt;‚úÖ Enhanced reasoning, coding, and math skills&lt;/p&gt; &lt;p&gt;‚úÖ Broader multilingual knowledge&lt;/p&gt; &lt;p&gt;‚úÖ Improved long-context understanding (up to 256K tokens)&lt;/p&gt; &lt;p&gt;‚úÖ Better alignment with user intent and open-ended tasks&lt;/p&gt; &lt;p&gt;‚úÖ No more &amp;lt;think&amp;gt; blocks ‚Äî now operating exclusively in non-thinking mode&lt;/p&gt; &lt;p&gt;üîß With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Chat: &lt;a href="https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nd904g7gbuff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcfmd2</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face</title>
    <updated>2025-07-29T16:11:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mci7uu</id>
    <title>Newest Qwen made me cry. It's not perfect, but I still love it.</title>
    <updated>2025-07-29T17:45:49+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt; &lt;img alt="Newest Qwen made me cry. It's not perfect, but I still love it." src="https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=431c53f32897af3a4225062d97bdc95913f53ec0" title="Newest Qwen made me cry. It's not perfect, but I still love it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ‚ù§&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gnkbnxzlouff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:45:49+00:00</published>
  </entry>
</feed>
