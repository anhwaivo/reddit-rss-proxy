<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-20T04:13:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lfqsps</id>
    <title>Running DeepSeek locally using ONNX Runtime</title>
    <updated>2025-06-20T00:42:28+00:00</updated>
    <author>
      <name>/u/DangerousGood4561</name>
      <uri>https://old.reddit.com/user/DangerousGood4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to drop this here for anyone interested in running models locally using ONNX Runtime. The focus here is on using the NPU in Snapdragon X Elite, but can be extended to other systems as well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DangerousGood4561"&gt; /u/DangerousGood4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/live/VRDB_ob7ulA?si=sR3Pes-BGUlPPJxh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfqsps/running_deepseek_locally_using_onnx_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfqsps/running_deepseek_locally_using_onnx_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T00:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfd7e2</id>
    <title>Has anyone tried the new ICONN-1 (an Apache licensed model)</title>
    <updated>2025-06-19T15:09:13+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"&gt; &lt;img alt="Has anyone tried the new ICONN-1 (an Apache licensed model)" src="https://external-preview.redd.it/SPYrTwyJE3TQKvjnrmxAQjGjLKoUyWEDHwmv3_PzeoA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba21b522cf50bc1f2cabea8c087c1617d7ee467b" title="Has anyone tried the new ICONN-1 (an Apache licensed model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A post was made by the creators on the Huggingface subreddit. I haven‚Äôt had a chance to use it yet. Has anyone else?&lt;/p&gt; &lt;p&gt;It isn‚Äôt clear at a quick glance if this is a dense model or MoE. The description mentions MoE so I assume it is, but no discussion on the expert size.&lt;/p&gt; &lt;p&gt;Supposedly this is a new base model, but I wonder if it‚Äôs a ‚ÄòMoE‚Äô made of existing Mistral models. The creator mentioned spending 50k on training it in the huggingface subreddit post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ICONNAI/ICONN-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfd7e2/has_anyone_tried_the_new_iconn1_an_apache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfeein</id>
    <title>[Project] DeepSeek-Based 15M-Parameter Model for Children‚Äôs Stories (Open Source)</title>
    <updated>2025-06-19T15:58:06+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt; &lt;img alt="[Project] DeepSeek-Based 15M-Parameter Model for Children‚Äôs Stories (Open Source)" src="https://external-preview.redd.it/USEPksTbnhSpjNDP3AWTvRB_hIM8jFv6ba_v6qu8L9U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78593e09dc3ea14e60985fc4600b4c2f05f69690" title="[Project] DeepSeek-Based 15M-Parameter Model for Children‚Äôs Stories (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/49xxdwf6pw7f1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41f5e4e22b0b053f35293d4bf29db60bbbda8f7f"&gt;https://preview.redd.it/49xxdwf6pw7f1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41f5e4e22b0b053f35293d4bf29db60bbbda8f7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been exploring how far tiny language models can go when optimized for specific tasks.&lt;/p&gt; &lt;p&gt;Recently, I built a 15M-parameter model using DeepSeek‚Äôs architecture (MLA + MoE + Multi-token prediction), trained on a dataset of high-quality children‚Äôs stories.&lt;/p&gt; &lt;p&gt;Instead of fine-tuning GPT-2, this one was built from scratch using PyTorch 2.0. The goal: a resource-efficient storytelling model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multihead Latent Attention&lt;/li&gt; &lt;li&gt;Mixture of Experts (4 experts, top-2 routing)&lt;/li&gt; &lt;li&gt;Multi-token prediction&lt;/li&gt; &lt;li&gt;RoPE embeddings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code &amp;amp; Model:&lt;/strong&gt;&lt;br /&gt; &lt;a href="http://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear thoughts from others working on small models or DeepSeek-based setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfeein/project_deepseekbased_15mparameter_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfbqgw</id>
    <title>Local AI setup 1x5090, 5x3090</title>
    <updated>2025-06-19T14:08:38+00:00</updated>
    <author>
      <name>/u/Emergency_Fuel_2988</name>
      <uri>https://old.reddit.com/user/Emergency_Fuel_2988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt; &lt;img alt="Local AI setup 1x5090, 5x3090" src="https://a.thumbs.redditmedia.com/4j9dI7_6M7_J4HmChLe-I_nyPiLiYdWe2VXne3OTO88.jpg" title="Local AI setup 1x5090, 5x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/orhogvwgow7f1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6388beca42f8ede320b52252ebfc59fed1bcb171"&gt;https://preview.redd.it/orhogvwgow7f1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6388beca42f8ede320b52252ebfc59fed1bcb171&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/orbkwkvkow7f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1e41d0223bad473c21ff4618a8c15113d06bbf5"&gt;https://preview.redd.it/orbkwkvkow7f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1e41d0223bad473c21ff4618a8c15113d06bbf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x25gl8mrqw7f1.png?width=1150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35bf80c636b87db84ea3bf203be424c9c0350c81"&gt;https://preview.redd.it/x25gl8mrqw7f1.png?width=1150&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35bf80c636b87db84ea3bf203be424c9c0350c81&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rd8cy8mrqw7f1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8a641c5fe1582f4499ea1193d7267e805da6284"&gt;https://preview.redd.it/rd8cy8mrqw7f1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8a641c5fe1582f4499ea1193d7267e805da6284&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxun35mrqw7f1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4c8b5ec6b6685230f3aeda1c21fbe3ca7d5c7b6"&gt;https://preview.redd.it/gxun35mrqw7f1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4c8b5ec6b6685230f3aeda1c21fbe3ca7d5c7b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôve been building lately: a local multi-model AI stack that‚Äôs getting kind of wild (in a good way)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Been heads-down working on a local AI stack that‚Äôs all about fast iteration and strong reasoning, fully running on consumer GPUs. It‚Äôs still evolving, but here‚Äôs what the current setup looks like:&lt;/p&gt; &lt;h1&gt;üßë‚Äçüíª Coding Assistant&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Devstral Q6 on LMStudio&lt;br /&gt; &lt;strong&gt;Specs:&lt;/strong&gt; Q4 KV cache, 128K context, running on a 5090&lt;br /&gt; Getting ~72 tokens/sec and still have 4GB VRAM free. Might try upping the quant if quality holds, or keep it as-is to push for a 40K token context experiment later.&lt;/p&gt; &lt;h1&gt;üß† Reasoning Engine&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Magistral Q4 on LMStudio&lt;br /&gt; &lt;strong&gt;Specs:&lt;/strong&gt; Q8 KV cache, 128K context, running on a single 3090&lt;br /&gt; Tuned more for heavy-duty reasoning tasks. Performs effectively up to 40K context.&lt;/p&gt; &lt;h1&gt;üß™ Eval + Experimentation&lt;/h1&gt; &lt;p&gt;Using local Arize Phoenix for evals, tracing, and tweaking. Super useful to visualize what‚Äôs actually happening under the hood.&lt;/p&gt; &lt;h1&gt;üìÅ Codebase Indexing&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Using:&lt;/strong&gt; Roo Code&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3 8B embedding model, FP16, 40K context, 4096D embeddings&lt;/li&gt; &lt;li&gt;Running on a dedicated 3090&lt;/li&gt; &lt;li&gt;Talking to Qdrant (GPU mode), though having a minor issue where embedding vectors aren‚Äôt passing through cleanly‚Äîmight just need to dig into what‚Äôs getting sent/received.&lt;/li&gt; &lt;li&gt;Would love a way to dedicate &lt;em&gt;part&lt;/em&gt; of a GPU just to embedding workloads. Anyone done that? ‚úÖ Indexing status: green&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîú What‚Äôs next&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Testing &lt;strong&gt;Kimi-Dev 72B&lt;/strong&gt; (EXL3 quant @ 5bpw, layer split) across 3x3090s‚Äîtwo for layers, one for the context window‚Äîvia TextGenWebUI or vLLM on WSL2&lt;/li&gt; &lt;li&gt;Also experimenting with an &lt;strong&gt;8B reranker model&lt;/strong&gt; on a single 3090 to improve retrieval quality, still playing around with where it best fits in the workflow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This stack is definitely becoming a bit of a GPU jungle, but the speed and flexibility it gives are worth it.&lt;/p&gt; &lt;p&gt;If you're working on similar local inference workflows‚Äîor know a good way to do smart GPU assignment in multi-model setups‚ÄîI‚Äôm super interested in this one challenge:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When a smaller model fails (say, after 3 tries), auto-escalate to a larger model with the same context, and save the larger model‚Äôs response as a reference for the smaller one in the future.&lt;/strong&gt; Would be awesome to see something like that integrated into Roo Code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency_Fuel_2988"&gt; /u/Emergency_Fuel_2988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfbqgw/local_ai_setup_1x5090_5x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T14:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lftglj</id>
    <title>[DEAL] On-demand B200 GPUs for $1.49/hr at DeepInfra (promo ends June 30)</title>
    <updated>2025-06-20T03:00:40+00:00</updated>
    <author>
      <name>/u/temirulan</name>
      <uri>https://old.reddit.com/user/temirulan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;no commitments any configuration (1x, 2x and so on) minute level billing cheapest in the marketüëå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/temirulan"&gt; /u/temirulan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lftglj/deal_ondemand_b200_gpus_for_149hr_at_deepinfra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lftglj/deal_ondemand_b200_gpus_for_149hr_at_deepinfra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lftglj/deal_ondemand_b200_gpus_for_149hr_at_deepinfra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T03:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lei5mb</id>
    <title>Oops</title>
    <updated>2025-06-18T14:12:39+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt; &lt;img alt="Oops" src="https://preview.redd.it/iv35yrek1p7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a1be0e37ffab5a4926e5a5a7a869b2ee3a9c853" title="Oops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iv35yrek1p7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T14:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lewhla</id>
    <title>We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!</title>
    <updated>2025-06-18T23:55:55+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt; &lt;img alt="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" src="https://preview.redd.it/775o8e8hxr7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c12230c686bdb16949fed6cf8cf00afff6399ea3" title="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.&lt;/p&gt; &lt;p&gt;In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/LMCache/LMCache"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/775o8e8hxr7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T23:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfrmj6</id>
    <title>Performance scaling from 400W to 600W on 2 5090s (MSI, Inno) and 2 4090s (ASUS, Gigabyte) from compute-bound task (SDXL).</title>
    <updated>2025-06-20T01:23:51+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hoping you are having a good day/night!&lt;/p&gt; &lt;p&gt;Continuing a bit from this post &lt;a href="https://www.reddit.com/r/nvidia/comments/1ld3f9n/small_comparison_of_2_5090s_1_voltage_efficient_1/"&gt;https://www.reddit.com/r/nvidia/comments/1ld3f9n/small_comparison_of_2_5090s_1_voltage_efficient_1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now this this time, someone gave me the idea to compare how the power scales from each GPU itself as you give it more power.&lt;/p&gt; &lt;p&gt;From the past post,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My most efficient 5090: MSI Vanguard SOC&lt;/li&gt; &lt;li&gt;My least efficient 5090: Inno3D X3&lt;/li&gt; &lt;li&gt;My most efficient 4090: ASUS TUF&lt;/li&gt; &lt;li&gt;My least efficient 5090: Gigabyte Gaming OC&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;TL:DR: 5090 Inno has a worse bin than the 5090 MSI, needing a lot more power to reach the same performance (and it doesn't reaches it at 600W either). On 4090s the TUF performs better as the more efficient GPU vs the Gigabyte.&lt;/p&gt; &lt;p&gt;Then, doing a SDXL task, which had the settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Batch count 2&lt;/li&gt; &lt;li&gt;Batch size 2&lt;/li&gt; &lt;li&gt;896x1088&lt;/li&gt; &lt;li&gt;Hiresfix at 1.5x, to 1344x1632&lt;/li&gt; &lt;li&gt;4xBHI_realplksr_dysample_multi upscaler&lt;/li&gt; &lt;li&gt;25 normal steps with DPM++ SDE Sampler&lt;/li&gt; &lt;li&gt;10 hi-res steps with Restart Sampler&lt;/li&gt; &lt;li&gt;reForge webui (I may continue dev soon?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;SDXL is a txt2img generator, and at this low batch sizes, performance is limited by compute, rather by bandwidth.&lt;/p&gt; &lt;p&gt;Other hardware-software config:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Fedora 41 (Linux), Kernel 6.19&lt;/li&gt; &lt;li&gt;Torch 2.7.1+cu128&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, both 4090s have the GALAX 666W VBIOS (this VBIOS gives more performance per clock) and both 5090s have the Gigabyte Aorus Master VBIOS (same thing as the Galax one but at a big minor scale).&lt;/p&gt; &lt;p&gt;Now instead of comparing the 4090 TUF as baseline (as it is the most efficient baseline), I compare instead vs 400W of each GPU itself instead. With this, we can see how poorly the 4090 scales with power.&lt;/p&gt; &lt;p&gt;Here are the results!&lt;/p&gt; &lt;h1&gt;RTX 4090 TUF (non-OC)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Power&lt;/th&gt; &lt;th align="left"&gt;Time (s)&lt;/th&gt; &lt;th align="left"&gt;Performance&lt;/th&gt; &lt;th align="left"&gt;Power Increase&lt;/th&gt; &lt;th align="left"&gt;Performance Gain&lt;/th&gt; &lt;th align="left"&gt;Efficiency Ratio&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;400W&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;475W&lt;/td&gt; &lt;td align="left"&gt;44.8&lt;/td&gt; &lt;td align="left"&gt;101.3%&lt;/td&gt; &lt;td align="left"&gt;+18.8%&lt;/td&gt; &lt;td align="left"&gt;+1.3%&lt;/td&gt; &lt;td align="left"&gt;0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;530W&lt;/td&gt; &lt;td align="left"&gt;44.2&lt;/td&gt; &lt;td align="left"&gt;102.7%&lt;/td&gt; &lt;td align="left"&gt;+32.5%&lt;/td&gt; &lt;td align="left"&gt;+2.7%&lt;/td&gt; &lt;td align="left"&gt;0.08&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Spoiler but maybe not surprising: This is the worse scaling GPU, even if it's the more efficient. It hits a voltage limit very early so even if you give it more power, it is hard to make use of it (+32.5% power for only +2.7% performance). Basically I can't make it use more than 530W effectively (without touching voltage at least).&lt;/p&gt; &lt;h1&gt;RTX 4090 Gigabyte Gaming OC&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Power&lt;/th&gt; &lt;th align="left"&gt;Time (s)&lt;/th&gt; &lt;th align="left"&gt;Performance&lt;/th&gt; &lt;th align="left"&gt;Power Increase&lt;/th&gt; &lt;th align="left"&gt;Performance Gain&lt;/th&gt; &lt;th align="left"&gt;Efficiency Ratio&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;400W&lt;/td&gt; &lt;td align="left"&gt;46.0&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;475W&lt;/td&gt; &lt;td align="left"&gt;44.2&lt;/td&gt; &lt;td align="left"&gt;104.1%&lt;/td&gt; &lt;td align="left"&gt;+18.8%&lt;/td&gt; &lt;td align="left"&gt;+4.1%&lt;/td&gt; &lt;td align="left"&gt;0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;530W&lt;/td&gt; &lt;td align="left"&gt;43.3&lt;/td&gt; &lt;td align="left"&gt;106.2%&lt;/td&gt; &lt;td align="left"&gt;+32.5%&lt;/td&gt; &lt;td align="left"&gt;+6.2%&lt;/td&gt; &lt;td align="left"&gt;0.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;560W&lt;/td&gt; &lt;td align="left"&gt;42.9&lt;/td&gt; &lt;td align="left"&gt;107.2%&lt;/td&gt; &lt;td align="left"&gt;+40.0%&lt;/td&gt; &lt;td align="left"&gt;+7.2%&lt;/td&gt; &lt;td align="left"&gt;0.18&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This card scales a bit more with power. At 475W is already +19% power for 4% performance. Then at 600W, you get 7.2% more perf by using +40% power. I also have a hard time making it use more than 560W effectively (voltage limit before power limit).&lt;/p&gt; &lt;p&gt;So this is why the 4090s are so famous about being able to undervolt it heavily and/or power limit them and not lose much performance.&lt;/p&gt; &lt;h1&gt;RTX 5090 Inno3D X3 OC&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Power&lt;/th&gt; &lt;th align="left"&gt;Time (s)&lt;/th&gt; &lt;th align="left"&gt;Performance&lt;/th&gt; &lt;th align="left"&gt;Power Increase&lt;/th&gt; &lt;th align="left"&gt;Performance Gain&lt;/th&gt; &lt;th align="left"&gt;Efficiency Ratio&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;400W&lt;/td&gt; &lt;td align="left"&gt;42.0&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;475W&lt;/td&gt; &lt;td align="left"&gt;38.1&lt;/td&gt; &lt;td align="left"&gt;110.2%&lt;/td&gt; &lt;td align="left"&gt;+18.8%&lt;/td&gt; &lt;td align="left"&gt;+10.2%&lt;/td&gt; &lt;td align="left"&gt;0.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;600W&lt;/td&gt; &lt;td align="left"&gt;34.9&lt;/td&gt; &lt;td align="left"&gt;120.3%&lt;/td&gt; &lt;td align="left"&gt;+50.0%&lt;/td&gt; &lt;td align="left"&gt;+20.3%&lt;/td&gt; &lt;td align="left"&gt;0.41&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This GPU, and 5090 in general, have the opposite problem vs the 4090. It is really hard to make it reach voltage limit with 600W, so it is constantly power limited. Even at 600W clocks will drop as it reaches power limit and then in consequence, voltage drops.&lt;/p&gt; &lt;p&gt;It scales way better with power, but still less efficient than the baseline. At 600W it uses 50% more power for 20.3% more performance. Or you could say this card at 400W performs ~83% as fast vs 600W.&lt;/p&gt; &lt;p&gt;Despite being a worse bin vs the MSI, it scales better? with power, as we will see next.&lt;/p&gt; &lt;h1&gt;RTX 5090 MSI Vanguard SOC Launch Edition&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Power&lt;/th&gt; &lt;th align="left"&gt;Time (s)&lt;/th&gt; &lt;th align="left"&gt;Performance&lt;/th&gt; &lt;th align="left"&gt;Power Increase&lt;/th&gt; &lt;th align="left"&gt;Performance Gain&lt;/th&gt; &lt;th align="left"&gt;Efficiency Ratio&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;400W&lt;/td&gt; &lt;td align="left"&gt;39.4&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;475W&lt;/td&gt; &lt;td align="left"&gt;36.1&lt;/td&gt; &lt;td align="left"&gt;109.1%&lt;/td&gt; &lt;td align="left"&gt;+18.8%&lt;/td&gt; &lt;td align="left"&gt;+9.1%&lt;/td&gt; &lt;td align="left"&gt;0.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;545W&lt;/td&gt; &lt;td align="left"&gt;34.8&lt;/td&gt; &lt;td align="left"&gt;113.2%&lt;/td&gt; &lt;td align="left"&gt;+36.3%&lt;/td&gt; &lt;td align="left"&gt;+13.2%&lt;/td&gt; &lt;td align="left"&gt;0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;565W&lt;/td&gt; &lt;td align="left"&gt;34.4&lt;/td&gt; &lt;td align="left"&gt;114.5%&lt;/td&gt; &lt;td align="left"&gt;+41.3%&lt;/td&gt; &lt;td align="left"&gt;+14.5%&lt;/td&gt; &lt;td align="left"&gt;0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;600W&lt;/td&gt; &lt;td align="left"&gt;34.0&lt;/td&gt; &lt;td align="left"&gt;115.9%&lt;/td&gt; &lt;td align="left"&gt;+50%&lt;/td&gt; &lt;td align="left"&gt;15.9%&lt;/td&gt; &lt;td align="left"&gt;0.32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This card is the one that performs the best as any given power point, but at the same time, vs the Inno3D, it scales worse as power increases. But even it is ahead, so in theory, this is a better bin vs the Inno, as it needs less power for the same performance.&lt;/p&gt; &lt;p&gt;Just as reference, the RTX 5090 MSI scores about ~16500 on Steel Nomad at 600W (&lt;a href="https://www.3dmark.com/sn/5412987"&gt;https://www.3dmark.com/sn/5412987&lt;/a&gt;), while the Inno3D does about ~15700 (didn't save the score, sorry!). So these both at 600W in that particular case, the MSI is 5% faster.&lt;/p&gt; &lt;p&gt;As TL:DR: 4090s scare very poorly with more power as they reach the voltage limit earlier (that's why they're famous by keeping the performance when undervolting and/or power limiting), while the 5090s have the opposite problem: they are heavily power limited and then by that, voltage drops to keep being on the desired power limit.&lt;/p&gt; &lt;p&gt;How it applies for LLMs? Probably not very applicable except by using an smaller model or by prompt processing speed, as generation speed is limited by bandwidth instead (and I can do a benchmark of this if someone wants, as there the 5090 is a good amount faster vs the 4090)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfrmj6/performance_scaling_from_400w_to_600w_on_2_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfrmj6/performance_scaling_from_400w_to_600w_on_2_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfrmj6/performance_scaling_from_400w_to_600w_on_2_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T01:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfj8hf</id>
    <title>Any reason to go true local vs cloud?</title>
    <updated>2025-06-19T19:09:06+00:00</updated>
    <author>
      <name>/u/ghost202</name>
      <uri>https://old.reddit.com/user/ghost202</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Is there any value for investing in a GPU ‚Äî price for functionality?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My own use case and conundrum: I have access to some powerful enterprises level compute and environments at work (through Azure AI Foundry and enterprise Stack). I'm a hobbyist dev and tinkerer for LLMs, building a much needed upgrade to my personal setup. I don't game too muchnon PC, so really a GPU for my own tower would just be for local models (LLM and media generation). My current solution is paying for distributed platforms or even reserved hardware like RunPod.&lt;/p&gt; &lt;p&gt;I just can't make the math work for true local hardware. If it added value somehow, could justify it. But seems like I'm either dropping ~$2k for a 32GB ballpark that is going to have bandwidth issues, OR $8k or more for a workstation level card that will be outpaced in a couple of years anyway. Cost only starts to be justified when looking at 24/7 uptime, but then we're getting into API* and web service territory where cloud hosting is a much better fit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short of just the satisfaction of being in direct ownership of the machine, with the loose benefits of a totally local environment, is there a good reason to buy hardware solely to run truly locally in 2025?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Edit: * API &lt;em&gt;calling in&lt;/em&gt; and serving to web hosting. If I need 24/7 uptime for something that's not baking a larger project, I'm likely also not wanting it to be running on my home rig. ex. Toy web apps for niche users besides myself.&lt;/p&gt; &lt;p&gt;For clarity, I consider service API calls like OpenAI or Gemini to be a different use case. Not trying to solve that with this; I use a bunch of other platforms and like them (ex. Claude Code, Gemini w/ Google KG grounding, etc.)&lt;/p&gt; &lt;p&gt;This is just my use case of &amp;quot;local&amp;quot; models and tinkering.&lt;/p&gt; &lt;p&gt;Edit 2: appreciate the feedback! Still not convinced to drop the $ on local hardware yet, but this is good insight into what some personal use cases are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghost202"&gt; /u/ghost202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfj8hf/any_reason_to_go_true_local_vs_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfj8hf/any_reason_to_go_true_local_vs_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfj8hf/any_reason_to_go_true_local_vs_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T19:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfm412</id>
    <title>We just added LlamaIndex support to AG-UI ‚Äî bring a frontend to your agent</title>
    <updated>2025-06-19T21:08:06+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I'm on the team behind AG-UI, a lightweight standard that brings agents into the UI as dynamic, stateful, real-time collaborators.&lt;/p&gt; &lt;p&gt;I'm seriously excited to share that &lt;strong&gt;AG-UI now supports LlamaIndex&lt;/strong&gt; out of the box. You can wire up a LlamaIndex agent to a modern UI in seconds.&lt;/p&gt; &lt;h1&gt;AG-UI features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Real-time agentic chat with streaming&lt;/li&gt; &lt;li&gt;Bi-directional state synchronization&lt;/li&gt; &lt;li&gt;Generative UI and structured messages&lt;/li&gt; &lt;li&gt;Real-time context enrichment&lt;/li&gt; &lt;li&gt;Frontend tool integration&lt;/li&gt; &lt;li&gt;Human-in-the-loop collaboration&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Already adopted by:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;LangChain&lt;/li&gt; &lt;li&gt;CrewAI&lt;/li&gt; &lt;li&gt;Mastra&lt;/li&gt; &lt;li&gt;AG2&lt;/li&gt; &lt;li&gt;Agno&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt; (just landed!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And &lt;strong&gt;incoming&lt;/strong&gt;:&lt;br /&gt; AWS, A2A, AgentOps, AI-SDK, and more.&lt;/p&gt; &lt;p&gt;Build your first AG-UI app by following the guide below, written by the team at LlamaIndex&lt;/p&gt; &lt;p&gt;Would love feedback from anyone using LlamaIndex (or any other framework) with a frontend. If you've been hacking together agent interfaces, I'd love to chat with you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfm412/we_just_added_llamaindex_support_to_agui_bring_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfm412/we_just_added_llamaindex_support_to_agui_bring_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfm412/we_just_added_llamaindex_support_to_agui_bring_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T21:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfhjja</id>
    <title>New Finnish models (Poro 2) based on Llama 3.1 8B and 70B</title>
    <updated>2025-06-19T18:02:04+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Poro 2 models are based on Llama 3.1 for both 8B and 70B versions. They've been continually pre-trained on 165B tokens using a carefully balanced mix of Finnish, English, code, and math data.&lt;/p&gt; &lt;p&gt;In my opinion they perform better than Gemma 3 at least when it comes to Finnish. Gemma 3 is probably still smarter but won't work as well for Finnish. It's also much better at Finnish when comparing to Llama 3.1. Especially the 8B model is a huge difference. Other new models generally suck at Finnish besides DeepSeekV3/R1, so this is a pretty good release for GPU poor people.&lt;/p&gt; &lt;p&gt;Poro 2 Collection:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/LumiOpen/poro-2-6835bec8186e98712b061f02"&gt;https://huggingface.co/collections/LumiOpen/poro-2-6835bec8186e98712b061f02&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs (only for Instruct):&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/Llama-Poro-2-70B-Instruct-GGUF"&gt;https://huggingface.co/mradermacher/Llama-Poro-2-70B-Instruct-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/Llama-Poro-2-8B-Instruct-GGUF"&gt;https://huggingface.co/mradermacher/Llama-Poro-2-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfhjja/new_finnish_models_poro_2_based_on_llama_31_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:02:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfetix</id>
    <title>Computer-Use on Windows Sandbox</title>
    <updated>2025-06-19T16:14:44+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"&gt; &lt;img alt="Computer-Use on Windows Sandbox" src="https://external-preview.redd.it/MHY2YzU5dThzdzdmMUUIhfD3WmHuxYkgbFXnt7PvLDhATd-8_6cYVR-PGp7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7144e3f113d350586da9cabf1761e65562ea4e" title="Computer-Use on Windows Sandbox" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.&lt;/p&gt; &lt;p&gt;Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development.&lt;/p&gt; &lt;p&gt;Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing.&lt;/p&gt; &lt;p&gt;What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments.&lt;/p&gt; &lt;p&gt;Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and testing before deploying to Windows cloud instances (coming later this month).&lt;/p&gt; &lt;p&gt;Check out the github here : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/windows-sandbox"&gt;https://www.trycua.com/blog/windows-sandbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2xrdz059sw7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfetix/computeruse_on_windows_sandbox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T16:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgp3i</id>
    <title>Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon</title>
    <updated>2025-06-19T17:28:59+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"&gt; &lt;img alt="Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon" src="https://external-preview.redd.it/hH0pP3ONlv9RFU_tt26eUVTIN9Qz11vaCtIPHTz4lhc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aabe4d7154b0327e0fc0818f68c0eae9ed8d9581" title="Run Deepseek locally on a 24g GPU: Quantizing on our Giga Computing 6980P Xeon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=KQDpE2SLzbA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgp3i/run_deepseek_locally_on_a_24g_gpu_quantizing_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfnn7b</id>
    <title>Optimized Chatterbox TTS (Up to 2-4x non-batched speedup)</title>
    <updated>2025-06-19T22:14:13+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past few weeks I've been experimenting for speed, and finally it's stable - a version that easily triples the original inference speed on my Windows machine with Nvidia 3090. I've also streamlined the torch dtype mismatch, so it does not require torch.autocast and thus using half precision is faster, lowering the VRAM requirements (I &lt;em&gt;roughly&lt;/em&gt; see 2.5GB usage)&lt;/p&gt; &lt;p&gt;Here's the updated inference code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rsxdalv/chatterbox/tree/fast"&gt;https://github.com/rsxdalv/chatterbox/tree/fast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In order to unlock the speed you need to torch.compile the generation step like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; model.t3._step_compilation_target = torch.compile( model.t3._step_compilation_target, fullgraph=True, backend=&amp;quot;cudagraphs&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And use bfloat16 for t3 to reduce memory bandwidth bottleneck:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: &amp;quot;ChatterboxTTS&amp;quot;, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) return model &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even without that you should see faster speeds due to removal of CUDA synchronization and more aggressive caching, but in my case the CPU/Windows Python is too slow to fully saturate the GPU without compilation. I targetted cudagraphs to hopefully avoid all &lt;em&gt;painful requirements&lt;/em&gt; like triton and MSVC.&lt;/p&gt; &lt;p&gt;The UI code that incorporates the compilation, memory usage check, half/full precision selection and more is in TTS WebUI (as an extension):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/rsxdalv/TTS-WebUI"&gt;https://github.com/rsxdalv/TTS-WebUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The code of the extension: &lt;a href="https://github.com/rsxdalv/extension_chatterbox"&gt;https://github.com/rsxdalv/extension_chatterbox&lt;/a&gt; ) Note - in the UI, compilation can only be done at the start (as the first generation) due to multithreading vs PyTorch: &lt;a href="https://github.com/pytorch/pytorch/issues/123177"&gt;https://github.com/pytorch/pytorch/issues/123177&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Even more details:&lt;/p&gt; &lt;p&gt;After torch compilation is applied, the main bottleneck becomes memory speed. Thus, to further gain speed we can reduce the memory&lt;/p&gt; &lt;p&gt;Changes done:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prevent runtime checks in loops, cache all static embeddings, fix dtype mismatches preventing fp16, prevent cuda synchronizations, switch to StaticCache for compilation, use buffer for generated_ids in repetition_penalty_processor, check for EOS periodically, remove sliced streaming &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This also required copying the modeling_llama from Transformers to remove optimization roadblocks.&lt;/p&gt; &lt;p&gt;Numbers - these are system dependant! Thanks to user &amp;quot;a red pen&amp;quot; on TTS WebUI discord (with 5060 TI 16gb): Float32 Without Use Compilation: 57 it/s With Use Compilation: 46 it/s&lt;/p&gt; &lt;p&gt;Bfloat16: Without Use Compilation: 47 it/s With Use Compilation: &lt;strong&gt;81 it/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On my Windows PC with 3090: Float32:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:24, 38.26it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:23, 39.57it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:22, 40.80it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Float32 Compiled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:24, 37.87it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:22, 41.21it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:22, 41.07it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Float32 Compiled with Max_Cache_Len 600:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:01&amp;lt;00:07, 54.43it/s] Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:01&amp;lt;00:07, 59.87it/s] Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:01&amp;lt;00:07, 59.69it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:30, 30.56it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:25, 35.69it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:02&amp;lt;00:25, 36.31it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:13, 66.01it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:11, 78.61it/s] Estimated token count: 70 Sampling: 8%|‚ñä | 80/1000 [00:01&amp;lt;00:11, 78.64it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled with Max_Cache_Len 600:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:00&amp;lt;00:04, 84.08it/s] Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:00&amp;lt;00:04, 101.48it/s] Estimated token count: 70 Sampling: 16%|‚ñà‚ñå | 80/500 [00:00&amp;lt;00:04, 101.41it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bfloat16 Compiled with Max_Cache_Len 500:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 70 Sampling: 20%|‚ñà‚ñà | 80/400 [00:01&amp;lt;00:04, 78.85it/s] Estimated token count: 70 Sampling: 20%|‚ñà‚ñà | 80/400 [00:00&amp;lt;00:03, 104.57it/s] Estimated token count: 70 Sampling: 20%|‚ñà‚ñà | 80/400 [00:00&amp;lt;00:03, 104.84it/s] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My best result is when running via API, where it goes to 108it/s at 560 cache len:&lt;/p&gt; &lt;p&gt;``` Using chatterbox streaming with params: {'audio_prompt_path': 'voices/chatterbox/Infinity.wav', 'chunked': True, 'desired_length': 80, 'max_length': 200, 'halve_first_chunk': False, 'exaggeration': 0.8, 'cfg_weight': 0.6, 'temperature': 0.9, 'device': 'auto', 'dtype': 'bfloat16', 'cpu_offload': False, 'cache_voice': False, 'tokens_per_slice': None, 'remove_milliseconds': None, 'remove_milliseconds_start': None, 'chunk_overlap_method': 'undefined', 'seed': -1, 'use_compilation': True, 'max_new_tokens': 340, 'max_cache_len': 560}&lt;/p&gt; &lt;p&gt;Using device: cuda&lt;/p&gt; &lt;p&gt;Using cached model 'Chatterbox on cuda with torch.bfloat16' in namespace 'chatterbox'.&lt;/p&gt; &lt;p&gt;Generating chunk: Alright, imagine you have a plant that lives in the desert where there isn't a lot of water.&lt;/p&gt; &lt;p&gt;Estimated token count: 114&lt;/p&gt; &lt;p&gt;Sampling: 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 100/340 [00:00&amp;lt;00:02, 102.48it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: This plant, called a cactus, has a special body that can store water so it can survive without rain for a long time.&lt;/p&gt; &lt;p&gt;Estimated token count: 152&lt;/p&gt; &lt;p&gt;Sampling: 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 160/340 [00:01&amp;lt;00:01, 108.20it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: So while other plants might need watering every day, a cactus can go for weeks without any water.&lt;/p&gt; &lt;p&gt;Estimated token count: 118&lt;/p&gt; &lt;p&gt;Sampling: 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 140/340 [00:01&amp;lt;00:01, 108.76it/s]&lt;/p&gt; &lt;p&gt;Generating chunk: It's kind of like a squirrel storing nuts for winter, but the cactus stores water to survive hot, dry days.&lt;/p&gt; &lt;p&gt;Estimated token count: 152&lt;/p&gt; &lt;p&gt;Sampling: 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 140/340 [00:01&amp;lt;00:01, 108.89it/s]&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T22:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf9wof</id>
    <title>Explain AI and MCP to a 5 year old in the 90s</title>
    <updated>2025-06-19T12:45:32+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt; &lt;img alt="Explain AI and MCP to a 5 year old in the 90s" src="https://external-preview.redd.it/64oqjh3Mi1lX6NMRJ57nKz9L5oT26BTAsIGTdNtrvn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf2b1569a37ddebb64bb52fbe85643cb9a0b4ec" title="Explain AI and MCP to a 5 year old in the 90s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf9wof"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf9wof/explain_ai_and_mcp_to_a_5_year_old_in_the_90s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T12:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfe33m</id>
    <title>Skywork-SWE-32B</title>
    <updated>2025-06-19T15:45:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-SWE-32B"&gt;https://huggingface.co/Skywork/Skywork-SWE-32B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Skywork-SWE-32B&lt;/em&gt;&lt;/strong&gt; is a code agent model developed by &lt;a href="https://skywork.ai/home"&gt;Skywork AI&lt;/a&gt;, specifically designed for software engineering (SWE) tasks. It demonstrates strong performance across several key metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skywork-SWE-32B attains 38.0% pass@1 accuracy on the &lt;a href="https://www.swebench.com"&gt;SWE-bench Verified&lt;/a&gt; benchmark, outperforming previous open-source SoTA &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B"&gt;Qwen2.5-Coder-32B-based&lt;/a&gt; LLMs built on the &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; agent framework.&lt;/li&gt; &lt;li&gt;When incorporated with test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SoTA results for sub-32B parameter models.&lt;/li&gt; &lt;li&gt;We clearly demonstrate the data scaling law phenomenon for software engineering capabilities in LLMs, with no signs of saturation at 8209 collected training trajectories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF is progress &lt;a href="https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF"&gt;https://huggingface.co/mradermacher/Skywork-SWE-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfe33m/skyworkswe32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpewd</id>
    <title>Anyone else tracking datacenter GPU prices on eBay?</title>
    <updated>2025-06-19T23:35:16+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the habit of checking eBay for AMD Instinct prices for a few years now, and noticed just today that MI210 prices seem to be dropping pretty quickly (though still priced out of my budget!) and there is a used MI300X for sale there for the first time, for &lt;em&gt;only&lt;/em&gt; $35K /s&lt;/p&gt; &lt;p&gt;I watch MI60 and MI100 prices too, but MI210 is the most interesting to me for a few reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model to use a PCIe interface (later models use OAM or SH5), which I could conceivably use in servers I actually have,&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model that runs at an even halfway-sane power draw (300W),&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fabrication processes don't improve significantly in later models until the MI350.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my own mind, my MI60 is mostly for learning how to make these Instinct GPUs work and not burst into flame, and it has indeed been a learning experience. When I invest &amp;quot;seriously&amp;quot; in LLM hardware, it will probably be eBay MI210s, but not until they have come down in price quite a bit more, and not until I have well-functioning training/fine-tuning software based on llama.cpp which works on the MI60. None of that exists yet, though it's progressing.&lt;/p&gt; &lt;p&gt;Most people are probably more interested in Nvidia datacenter GPUs. I'm not in the habit of checking for that, but do see now that eBay has 40GB A100 for about $2500, and 80GB A100 for about $8800 (US dollars).&lt;/p&gt; &lt;p&gt;Am I the only one, or are other people waiting with bated breath for second-hand datacenter GPUs to become affordable too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfjmx4</id>
    <title>We Tested Apple's On-Device Model for RAG Task</title>
    <updated>2025-06-19T19:25:13+00:00</updated>
    <author>
      <name>/u/No_Salamander1882</name>
      <uri>https://old.reddit.com/user/No_Salamander1882</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We tested Apple‚Äôs on-device model (using &lt;a href="https://github.com/gety-ai/apple-on-device-openai"&gt;this project&lt;/a&gt; to turn the Apple foundation model framework into an OpenAI-compatible API) by applying our RAG evaluation framework to a set of 1000 questions.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The Good:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;8.5/10 factual accuracy&lt;/strong&gt; on questions it decides to answer (on par with best small models like Qwen3 4B and IBM Granite 3.3 2B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~30 tokens/second&lt;/strong&gt; on M3 MacBook Air (16GB)&lt;/li&gt; &lt;li&gt;Strong context adherence (doesn't hallucinate much)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Concerning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;45% incorrect rejection rate&lt;/strong&gt; (refuses to answer when it actually has the info)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;90% rejection rate&lt;/strong&gt; if you add &amp;quot;Answer the question based on search result&amp;quot; to system prompt&lt;/li&gt; &lt;li&gt;Won't elaborate or ask clarifying questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Weird:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Guardrails flag questions as &amp;quot;unsafe&amp;quot; (22/1000, mostly medical topics)&lt;/li&gt; &lt;li&gt;Adopts the vocabulary/tone from your query in its responses&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;We tested Apple's model as a &lt;strong&gt;summarizer in a RAG system&lt;/strong&gt;. The setup: model receives a user query plus 2-5 search result chunks (512 tokens max each) and must synthesize them into an accurate answer.&lt;/p&gt; &lt;p&gt;We used our &lt;a href="https://github.com/aizip/Rag-Eval-flow"&gt;RED-flow evaluation framework&lt;/a&gt; designed for testing small language models in RAG tasks. 1000 questions from policy documents, technical manuals, and other domains, testing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can it find and synthesize answers from the chunks?&lt;/li&gt; &lt;li&gt;Does it recognize when chunks lack sufficient info?&lt;/li&gt; &lt;li&gt;Will it ask helpful clarification questions?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The same evaluation runs in our &lt;a href="https://huggingface.co/spaces/aizip-dev/SLM-RAG-Arena"&gt;SLM RAG Arena&lt;/a&gt; where you can see how different models handle these challenges on the leaderboard.&lt;/p&gt; &lt;h1&gt;Detailed Observations&lt;/h1&gt; &lt;h1&gt;1. It's REALLY Sensitive to Instructions&lt;/h1&gt; &lt;p&gt;The system prompt dramatically affects performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No system prompt (default): 45% incorrect rejections&lt;/li&gt; &lt;li&gt;Add &amp;quot;Answer the question based on search result&amp;quot;: 90% incorrect rejections&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This suggests Apple's model is actually quite capable but becomes overly cautious with certain instructions.&lt;/p&gt; &lt;h1&gt;2. Guardrails Are... Interesting&lt;/h1&gt; &lt;p&gt;The model has guardrails that operate above the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;22/1000 queries flagged as &amp;quot;unsafe&amp;quot; with zero explanation&lt;/li&gt; &lt;li&gt;Most rejected topics are medical-related&lt;/li&gt; &lt;li&gt;Inconsistent between runs (same question might pass or fail)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Minimal Responses, No Discussion&lt;/h1&gt; &lt;p&gt;Unlike other SLMs or the reference model DeepSeek v3, this model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Won't elaborate even when prompted&lt;/li&gt; &lt;li&gt;Never asks clarifying questions&lt;/li&gt; &lt;li&gt;Just outputs &amp;quot;I cannot answer that&amp;quot; instead of explaining why&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with insufficient context:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: If I eat a rainbow and drink the sky, will my risks go bye-bye? [cancer risk context] Apple Model: I cannot answer that. Reference (DeepSeek v3): I understand you're asking about reducing health risks, but your question uses metaphorical language that I can't directly interpret from the provided context about cancer risk factors. Could you clarify what specific dietary or lifestyle changes you're asking about? For example, are you interested in colorful fruits and vegetables (&amp;quot;rainbow&amp;quot;) or hydration (&amp;quot;sky&amp;quot;) and their effects on cancer risk? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even when it has the information, responses are minimal:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: How is the urban core defined for statistical purposes? Apple Model: The urban core is defined as a geographic area with a population of 50,000 or more. Reference (DeepSeek v3): The urban core for statistical purposes is defined as the central urban area within a Metropolitan Statistical Area (MSA) that has a population of 50,000 or more. This core urban area is the basis for delineating the MSA, which includes the counties containing the core urban area and any adjacent counties with a high degree of social and economic integration (measured by commuting to work) with the urban core. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Correct but minimal.&lt;/p&gt; &lt;h1&gt;4. Surprisingly Good at Long Context&lt;/h1&gt; &lt;p&gt;No degradation with longer contexts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Hard&amp;quot; questions (requiring multiple chunks) score just as well&lt;/li&gt; &lt;li&gt;No &amp;quot;context spiral&amp;quot; or coherence loss&lt;/li&gt; &lt;li&gt;Suggests good attention mechanisms for a 3B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Personality Mirroring&lt;/h1&gt; &lt;p&gt;The model adopts your vocabulary and tone. If you use informal language in your query, it responds with the same style:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Query: Who do you tell if you see something sketchy with a work vehicle? Apple Model: If you see something sketchy with a work vehicle, you should notify the Office of Inspector General (OIG) and the DOT Fleet Manager. Reference (DeepSeek v3): If you observe apparent unofficial use or misuse of a Government motor vehicle, you should notify the Office of the Inspector General (OIG) and the DOT Fleet Manager. The OIG will then coordinate with the DOT Fleet Manager to conduct an internal investigation to determine whether a violation has occurred. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice how Apple's model picks up and reuses &amp;quot;sketchy&amp;quot; instead of using more formal language like &amp;quot;suspicious&amp;quot; or &amp;quot;apparent unofficial use&amp;quot;. This happens consistently across different types of informal or domain-specific vocabulary.&lt;/p&gt; &lt;h1&gt;What This Means&lt;/h1&gt; &lt;p&gt;Apple appears to be running a ~3B parameter model with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong factual accuracy when it works&lt;/li&gt; &lt;li&gt;Overly conservative rejection behavior&lt;/li&gt; &lt;li&gt;Hard guardrails that sometimes misfire&lt;/li&gt; &lt;li&gt;Design choices favoring brevity over helpfulness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a local, on-device model, it's impressively capable. But the high rejection rate and minimal responses might frustrate users expecting ChatGPT-style interactions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Theory:&lt;/strong&gt; Apple optimized for &amp;quot;never be wrong&amp;quot; over &amp;quot;always be helpful&amp;quot;.&lt;/p&gt; &lt;p&gt;Anyone else tested this? Curious if you're seeing similar patterns.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Salamander1882"&gt; /u/No_Salamander1882 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfjmx4/we_tested_apples_ondevice_model_for_rag_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T19:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfp66e</id>
    <title>Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery</title>
    <updated>2025-06-19T23:23:38+00:00</updated>
    <author>
      <name>/u/Thalesian</name>
      <uri>https://old.reddit.com/user/Thalesian</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt; &lt;img alt="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" src="https://external-preview.redd.it/dp9jZ9I5ulT5RZQDN9KwsbRB_C7Fi7IzWNUNN0l7OB8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69de3d9bf9c09f76fc51ddb8950c1601d76f26c9" title="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This rig is more for training than local inference (though there is a lot of the latter with Qwen) but I thought it might be helpful to see how the new Blackwell cards dissipate heat compared to the older blower style for Quadros prominent since Amphere.&lt;/p&gt; &lt;p&gt;There are two IR color ramps - a standard heat map and a rainbow palette that‚Äôs better at showing steep thresholds. You can see the majority of the heat is present at the two inner-facing triangles to the upper side center of the Blackwell card (84 C), with exhaust moving up and outward to the side. Underneath, you can see how effective the lower two fans are at moving heat in the flow through design, though the Ada Lovelace card‚Äôs fan input is a fair bit cooler. But the negative of the latter‚Äôs design is that the heat ramps up linearly through the card. The geometric heatmap of the Blackwell shows how superior its engineering is - it is overall comparatively cooler in surface area despite using double the wattage. &lt;/p&gt; &lt;p&gt;A note on the setup - I have all system fans with exhaust facing inward to push air out try open side of the case. It seems like this shouldn‚Äôt work, but the Blackwell seems to stay much cooler this way than with the standard front fans as intake and back fans as exhaust. Coolest part of the rig by feel is between the two cards. &lt;/p&gt; &lt;p&gt;CPU is liquid cooled, and completely unaffected by proximity to the Blackwell card.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thalesian"&gt; /u/Thalesian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfp66e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgfu5</id>
    <title>AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!</title>
    <updated>2025-06-19T17:18:57+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt; &lt;img alt="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" src="https://external-preview.redd.it/snRoVONGmevA0S70HKIe-_OVILdqukspGvuQ8vgG6Fg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=854af80e0d39d84623d14e840f4c075c5351100c" title="AMD Lemonade Server Update: Ubuntu, llama.cpp, Vulkan, webapp, and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;, it‚Äôs been a bit since my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;post&lt;/a&gt; introducing &lt;a href="https://lemonade-server.ai"&gt;Lemonade Server&lt;/a&gt;, AMD‚Äôs open-source local LLM server that prioritizes NPU and GPU acceleration.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I want to sincerely thank the community here for all the feedback on that post! It‚Äôs time for an update, and I hope you‚Äôll agree we took the feedback to heart and did our best to deliver.&lt;/p&gt; &lt;p&gt;The biggest changes since the last post are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;ü¶ôAdded llama.cpp, GGUF, and Vulkan support as an additional backend alongside ONNX. This adds support for: A) GPU acceleration on Ryzen‚Ñ¢ AI 7000/8000/300, Radeon‚Ñ¢ 7000/9000, and many other device families. B) Tons of new models, including VLMs.&lt;/li&gt; &lt;li&gt;üêßUbuntu is now a fully supported operating system for llama.cpp+GGUF+Vulkan (GPU)+CPU, as well as ONNX+CPU.&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;ONNX+NPU support in Linux, as well as NPU support in llama.cpp, are a work in progress.&lt;/p&gt; &lt;/blockquote&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;üíªAdded a web app for model management (list/install/delete models) and basic LLM chat. Open it by pointing your browser at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; while the server is running.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ü§ñAdded support for streaming tool calling (all backends) and demonstrated it in our &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/local-tiny-agents--mcp-agents-on-ryzen-ai-with-lemonade-server.html"&gt;MCP + tiny-agents blog post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚ú®Polished overall look and feel: new getting started website at &lt;a href="https://lemonade-server.ai"&gt;https://lemonade-server.ai&lt;/a&gt;, install in under 2 minutes, and server launches in under 2 seconds.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With the added support for Ubuntu and llama.cpp, Lemonade Server should give great performance on many more PCs than it did 2 months ago. The team here at AMD would be very grateful if y'all could try it out with your favorite apps (I like Open WebUI) and give us another round of feedback. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfgfu5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgfu5/amd_lemonade_server_update_ubuntu_llamacpp_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf5yog</id>
    <title>Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp; more stable now</title>
    <updated>2025-06-19T08:52:09+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt; &lt;img alt="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" src="https://external-preview.redd.it/9cdGcDSZyRjc_UX2wAVlzVVLsf-enKZlEOwOWD7xiXc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25d6c033d9aec8e5c2afd0f97b93b27a4a07b430" title="Jan got an upgrade: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster &amp;amp; more stable now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan v0.6.0 is out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully redesigned UI&lt;/li&gt; &lt;li&gt;Switched from Electron to Tauri for lighter and more efficient performance&lt;/li&gt; &lt;li&gt;You can create your own assistants with instructions &amp;amp; custom model settings&lt;/li&gt; &lt;li&gt;New themes &amp;amp; customization settings (e.g. font size, code block highlighting style)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Including improvements to thread handling and UI behavior to tweaking extension settings, cleanup, log improvements, and more.&lt;/p&gt; &lt;p&gt;Update your Jan or download the latest here: &lt;a href="https://jan.ai"&gt;https://jan.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes here: &lt;a href="https://github.com/menloresearch/jan/releases/tag/v0.6.0"&gt;https://github.com/menloresearch/jan/releases/tag/v0.6.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If you'd like to play with the new Jan but has not download a model via Jan, please import your GGUF models via Settings -&amp;gt; Model Providers -&amp;gt; llama.cpp -&amp;gt; Import. See the latest image in the post to do that.&lt;/li&gt; &lt;li&gt;Jan is going to get bigger update soon on MCP usage, we're testing MCP usage with our MCP-specific model, &lt;a href="https://huggingface.co/collections/Menlo/jan-nano-684f6ebfe9ed640fddc55be7"&gt;Jan Nano&lt;/a&gt;, that surpass DeepSeek V3 671B on agentic use cases. If you'd like to test it as well, feel free to join our Discord to see the build links.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lf5yog"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T08:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpkyv</id>
    <title>Qwen3 for Apple Neural Engine</title>
    <updated>2025-06-19T23:43:28+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just dropped ANEMLL 0.3.3 alpha with Qwen3 support for Apple's Neural Engine&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Anemll/Anemll"&gt;https://github.com/Anemll/Anemll&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Star ‚≠êÔ∏è and upvote to support open source! Cheers, Anemll ü§ñ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lficpj</id>
    <title>Kyutai's STT with semantic VAD now opensource</title>
    <updated>2025-06-19T18:33:58+00:00</updated>
    <author>
      <name>/u/phhusson</name>
      <uri>https://old.reddit.com/user/phhusson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai published their latest tech demo few weeks ago, unmute.sh. It is an impressive voice-to-voice assistant using a 3rd-party text-to-text LLM (gemma), while retaining the conversation low latency of Moshi.&lt;/p&gt; &lt;p&gt;They are currently opensourcing the various components for that.&lt;/p&gt; &lt;p&gt;The first component they opensourced is their STT, available at &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The best feature of that STT is Semantic VAD. In a local assistant, the VAD is a component that determines when to stop listening to a request. Most local VAD are sadly not very sophisticated, and won't allow you to pause or think in the middle of your sentence.&lt;/p&gt; &lt;p&gt;The Semantic VAD in Kyutai's STT will allow local assistant to be much more comfortable to use.&lt;/p&gt; &lt;p&gt;Hopefully we'll also get the streaming LLM integration and TTS from them soon, to be able to have our own low-latency local voice-to-voice assistant ü§û&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phhusson"&gt; /u/phhusson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpqs6</id>
    <title>Current best uncensored model?</title>
    <updated>2025-06-19T23:51:12+00:00</updated>
    <author>
      <name>/u/Accomplished-Feed568</name>
      <uri>https://old.reddit.com/user/Accomplished-Feed568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is probably one of the biggest advantages of local LLM's yet there is no universally accepted answer to what's the best model as of June 2025. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;So share your BEST uncensored model!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;by ''best uncensored model' i mean the least censored model (that helped you get a nuclear bomb in your kitched), but also the most intelligent one&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Feed568"&gt; /u/Accomplished-Feed568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgqkd</id>
    <title>Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts</title>
    <updated>2025-06-19T17:30:37+00:00</updated>
    <author>
      <name>/u/choose_a_guest</name>
      <uri>https://old.reddit.com/user/choose_a_guest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt; &lt;img alt="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" src="https://preview.redd.it/niqpo23p5x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22e9ad07139fcbaf4f1d83ce46f5c89ca3c94565" title="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Meta Platforms tried to poach OpenAI employees by offering signing bonuses as high as $100 million, with even larger annual compensation packages, OpenAI chief executive Sam Altman said.&amp;quot;&lt;br /&gt; &lt;a href="https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html"&gt;https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choose_a_guest"&gt; /u/choose_a_guest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niqpo23p5x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:30:37+00:00</published>
  </entry>
</feed>
