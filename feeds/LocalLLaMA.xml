<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-26T15:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iyoyx5</id>
    <title>H100 and A100 for rent</title>
    <updated>2025-02-26T14:38:35+00:00</updated>
    <author>
      <name>/u/Conscious_Class_9093</name>
      <uri>https://old.reddit.com/user/Conscious_Class_9093</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically my startup is not using the vms atm. Renting them out for very cheap. Also Tpus are available. Platform-GCp&lt;/p&gt; &lt;p&gt;.30$/hour for H100 Dms are open.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Class_9093"&gt; /u/Conscious_Class_9093 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyoyx5/h100_and_a100_for_rent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyoyx5/h100_and_a100_for_rent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyoyx5/h100_and_a100_for_rent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T14:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iymkw7</id>
    <title>Hi I am new, need advice - training a model on 50-100 research abstracts to search 10,000-100,000 abstracts for stuff/topics I need.</title>
    <updated>2025-02-26T12:39:23+00:00</updated>
    <author>
      <name>/u/ilikebig_icannotlie</name>
      <uri>https://old.reddit.com/user/ilikebig_icannotlie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to AI and large language models and been learning on my own reading reddit and using vairous AI/GPTs to do things. &lt;/p&gt; &lt;p&gt;I have an idea where conventional GPTs (free versions) have failed to produce results so far.&lt;/p&gt; &lt;p&gt;I do healthcare research, and I downloaded ~1200 PubMed abstracts as a text file, 500 pages, 97,000 words. I can manually read each title of the abstract and each abstract to say, yes or not whether it fits with my criteria&lt;/p&gt; &lt;p&gt;But I want to expand, and download 10,000 pubmed abstracts... I cannot manually review this much in a reasonable amount of time... &lt;/p&gt; &lt;p&gt;I was hoping to build a LLM model or use an LLM model to read the massive document and tag/pull-out any study that has keywords that I am interested in. Basically I will train it with papers that I already have, and want it to find similar papers and use key words.&lt;/p&gt; &lt;p&gt;So TLDR, I want to train it with 50-100 papers of the content I want and keywords and unleash it on a database of 10,000-100,000 abstracts to pull me any other papers that I missed in my manual review.&lt;/p&gt; &lt;p&gt;I know this is possible, but I am basic and fresh AF to this field... I would love to learn how to do this please. I have minimal coding experience...Also, i know this is possible. but where do i start?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilikebig_icannotlie"&gt; /u/ilikebig_icannotlie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymkw7/hi_i_am_new_need_advice_training_a_model_on_50100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymkw7/hi_i_am_new_need_advice_training_a_model_on_50100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iymkw7/hi_i_am_new_need_advice_training_a_model_on_50100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T12:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyi6ib</id>
    <title>Anyone Tested the new QWQ MAX model from Qwen ?</title>
    <updated>2025-02-26T07:35:00+00:00</updated>
    <author>
      <name>/u/bilalazhar72</name>
      <uri>https://old.reddit.com/user/bilalazhar72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was unable to find any official benchmarks&lt;br /&gt; in the intial testing is it any good ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bilalazhar72"&gt; /u/bilalazhar72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyi6ib/anyone_tested_the_new_qwq_max_model_from_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyi6ib/anyone_tested_the_new_qwq_max_model_from_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyi6ib/anyone_tested_the_new_qwq_max_model_from_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T07:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy88jt</id>
    <title>WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it</title>
    <updated>2025-02-25T22:46:34+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"&gt; &lt;img alt="WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it" src="https://external-preview.redd.it/c6siCekRyG6Ns9HYU0MVyr293slrzeLzV1u5DrO4Ww4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3ffa52940a2fd8ccec5c2a1e35ab247e4ce901e" title="WilmerAI: I just uploaded around 3 hours worth of video tutorials explaining the prompt routing, workflows, and walking through running it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/playlist?list=PLjIfeYFu5Pl7J7KGJqVmHM4HU56nByb4X"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy88jt/wilmerai_i_just_uploaded_around_3_hours_worth_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2qhm</id>
    <title>New form factor announced for AMD MAX cpu from Framework</title>
    <updated>2025-02-25T18:58:09+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Framework just announced a mini desktop version of the AMD MAX CPU chip featuring up to 128GB of unified memory with up to 96GB available for graphics.&lt;/p&gt; &lt;p&gt;Edit: So apparently, this new CPU Strix CPU from AMD requires a new motherboard and device redesign for laptops which makes the products more expensive.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This thing has a massive integrated GP that boasts performance that is similar to an RTX 4060 on integrated graphics and It even allows you to allocate up to 96 GB of its maximum 128 gigs of lpddr 5x to that GPU making it awesome for gamers creative professionals and AI developers no the disappointing thing was that this sick processor barely made it into any products all I saw at the show was one admittedly awesome laptop from HP and One gaming tablet from Asus &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Talking to those Brands they said the issue was that Strix Halo requires a complete motherboard and device redesign making its implementation in mobile devices really costly so I guess framework said screw it we're a small company and can't afford all that but what if we just made it into a desktop is that really how it went down that is literally how it went down&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://youtu.be/-lErGZZgUbY?t=158"&gt;https://youtu.be/-lErGZZgUbY?t=158&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2qhm/new_form_factor_announced_for_amd_max_cpu_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixzast</id>
    <title>olmOCR-7B by Ai2 - open-source model to extract clean plain text from PDFs.</title>
    <updated>2025-02-25T16:38:55+00:00</updated>
    <author>
      <name>/u/False_Care_2957</name>
      <uri>https://old.reddit.com/user/False_Care_2957</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview"&gt;https://huggingface.co/allenai/olmOCR-7B-0225-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Care_2957"&gt; /u/False_Care_2957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixzast/olmocr7b_by_ai2_opensource_model_to_extract_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T16:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyozra</id>
    <title>Running LLM on Macbook</title>
    <updated>2025-02-26T14:39:41+00:00</updated>
    <author>
      <name>/u/Chaotic_Evil_558</name>
      <uri>https://old.reddit.com/user/Chaotic_Evil_558</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to run Deephermes 3 8B locally or other similar smol models. Will be making a video tutorial on it with other hardware demonstrating installation and use and i want guidance on what minimum specs would be whilst still maintaining decent performance. (None of that &amp;quot;hi&amp;quot; ...-3mins later-&amp;quot;how are you?&amp;quot; ) I know MacBooks have their own unique gpu hardware which is why im asking about those specifically. If 8b is too big to run without butchering it too much with quantization lemme know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chaotic_Evil_558"&gt; /u/Chaotic_Evil_558 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyozra/running_llm_on_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyozra/running_llm_on_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyozra/running_llm_on_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T14:39:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7mco</id>
    <title>Magma: A Foundation Model for Multimodal AI Agents</title>
    <updated>2025-02-25T22:20:01+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"&gt; &lt;img alt="Magma: A Foundation Model for Multimodal AI Agents" src="https://external-preview.redd.it/3TcBPHiZT0ehLG7Chhf6Tcy2t-RW9H63I2r1U0-0qWg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73e9bb2225d928aa47a08c9bedee0f7751c754d0" title="Magma: A Foundation Model for Multimodal AI Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Magma-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7mco/magma_a_foundation_model_for_multimodal_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyli39</id>
    <title>Embedding models used today</title>
    <updated>2025-02-26T11:35:20+00:00</updated>
    <author>
      <name>/u/Born_Fox6153</name>
      <uri>https://old.reddit.com/user/Born_Fox6153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When we embed text today, we utilize methods such that the surrounding words/sentences are taken into context in some manner when generating vectors. As of today, is there a way to embed relevant concepts/related concepts and how they are linked together in a training data set and utilizing external knowledge as well (some sort of a knowledge graph construction by domain as an added input to embedding generation) to improve understanding concepts and reasoning representations for LLM training ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Fox6153"&gt; /u/Born_Fox6153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyli39/embedding_models_used_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyli39/embedding_models_used_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyli39/embedding_models_used_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T11:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iymmj7</id>
    <title>I built a Linux shell with Ollama integration and natural language commands in Rust</title>
    <updated>2025-02-26T12:42:02+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"&gt; &lt;img alt="I built a Linux shell with Ollama integration and natural language commands in Rust" src="https://external-preview.redd.it/okC7TfMdMSOgHLRE90xJF9n5RQbj7e5VdAzGJOsXCBQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eade031b76e801fa92e08d5614383b6f840d8032" title="I built a Linux shell with Ollama integration and natural language commands in Rust" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/phildougherty/llmsh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iymmj7/i_built_a_linux_shell_with_ollama_integration_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T12:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixubts</id>
    <title>🇨🇳 Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner.</title>
    <updated>2025-02-25T12:56:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt; &lt;img alt="🇨🇳 Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." src="https://preview.redd.it/z11vic3x8ale1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ec3818e2a144f0e19522fbf44016cae02b88dc1" title="🇨🇳 Sources: DeepSeek is speeding up the release of its R2 AI model, which was originally slated for May, but the company is now working to launch it sooner." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z11vic3x8ale1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixubts/sources_deepseek_is_speeding_up_the_release_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyen1x</id>
    <title>If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?</title>
    <updated>2025-02-26T03:55:56+00:00</updated>
    <author>
      <name>/u/Hv_V</name>
      <uri>https://old.reddit.com/user/Hv_V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt; &lt;img alt="If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?" src="https://b.thumbs.redditmedia.com/FveQ50mZ2Vdj8xoAq1CIe1I2fzt8nJFRmgyRdNll8Hs.jpg" title="If claude 3.7 is the best for coding then why is it ranked low on artificial analysis coding benchmarks?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0nan7xubpele1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e8f5ef5414b11c52f73974b2c15216220c1d0cf"&gt;https://preview.redd.it/0nan7xubpele1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e8f5ef5414b11c52f73974b2c15216220c1d0cf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gqpbsopcpele1.png?width=776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bc8235d5e2795b42b2a82d999ffd2ea8bd79"&gt;https://preview.redd.it/gqpbsopcpele1.png?width=776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04b1bc8235d5e2795b42b2a82d999ffd2ea8bd79&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aby185kdpele1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e833a4e7156d457f810a9f1b8541b2d4b0eff2c7"&gt;https://preview.redd.it/aby185kdpele1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e833a4e7156d457f810a9f1b8541b2d4b0eff2c7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7mk49xgepele1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=570b4ef625bcf82695b707c869471a4e337f8408"&gt;https://preview.redd.it/7mk49xgepele1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=570b4ef625bcf82695b707c869471a4e337f8408&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hv_V"&gt; /u/Hv_V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyen1x/if_claude_37_is_the_best_for_coding_then_why_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T03:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyn408</id>
    <title>Dual EPYC CPU build...avoiding the bottleneck</title>
    <updated>2025-02-26T13:08:11+00:00</updated>
    <author>
      <name>/u/Dry_Parfait2606</name>
      <uri>https://old.reddit.com/user/Dry_Parfait2606</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm figuring out if I can make a dual 7002 run without having a cpu-to-cpu bottleneck...&lt;/p&gt; &lt;p&gt;Its a 1-2TB ram build, so I'm just trying to get very cheap ram and being able to run the bigger models like 405b &amp;amp; 700B...at &amp;lt;1TB/s speeds of course.&lt;/p&gt; &lt;p&gt;I've read something about NUMA nodes but I have no idea where to begin with to actually resolve the bottleneck of a dual cpu.. Can someone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Parfait2606"&gt; /u/Dry_Parfait2606 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyn408/dual_epyc_cpu_buildavoiding_the_bottleneck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T13:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ixtxbw</id>
    <title>😂😂 someone made a "touch grass" app with a vLLM, you gotta go and actually touch grass to unlock your phone</title>
    <updated>2025-02-25T12:33:46+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt; &lt;img alt="😂😂 someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" src="https://b.thumbs.redditmedia.com/5hq40VPLgBMcOH3vwQ7e1MxMGeAfqIgssUMVtLMafsg.jpg" title="😂😂 someone made a &amp;quot;touch grass&amp;quot; app with a vLLM, you gotta go and actually touch grass to unlock your phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ixtxbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ixtxbw/someone_made_a_touch_grass_app_with_a_vllm_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T12:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybgj2</id>
    <title>TinyR1-32B-Preview (surpassing official R1 distill 32B performance)</title>
    <updated>2025-02-26T01:14:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt; &lt;img alt="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" src="https://external-preview.redd.it/n9Pibq5ap97rYKS_QfhVUwq5U1l5cN9jQ5aOHTyyDyg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659b372ed30298182f662bdd30b00d3b42381833" title="TinyR1-32B-Preview (surpassing official R1 distill 32B performance)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybgj2/tinyr132bpreview_surpassing_official_r1_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy22ux</id>
    <title>Gemma 3 27b just dropped (Gemini API models list)</title>
    <updated>2025-02-25T18:31:30+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt; &lt;img alt="Gemma 3 27b just dropped (Gemini API models list)" src="https://preview.redd.it/y2nlshypwble1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16e52e66ab49c258198ed2169eecedba2241176d" title="Gemma 3 27b just dropped (Gemini API models list)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2nlshypwble1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy22ux/gemma_3_27b_just_dropped_gemini_api_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T18:31:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7k6b</id>
    <title>Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider</title>
    <updated>2025-02-25T22:17:32+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt; &lt;img alt="Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" src="https://external-preview.redd.it/LHkWl_VkJgCRA11Syl07lcXlc5oC-0ZjNEgwTTmbGnM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=309552b833882287b79a1c8f0cb6385eb21a5228" title="Nvidia gaming GPUs modded with 2X VRAM for AI workloads — RTX 4090D 48GB and RTX 4080 Super 32GB go up for rent at Chinese cloud computing provider" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/nvidia-gaming-gpus-modded-with-2x-vram-for-ai-workloads"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7k6b/nvidia_gaming_gpus_modded_with_2x_vram_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyo8ul</id>
    <title>What's the best machine I can get for local LLM's with a $25k budget?</title>
    <updated>2025-02-26T14:04:35+00:00</updated>
    <author>
      <name>/u/NootropicDiary</name>
      <uri>https://old.reddit.com/user/NootropicDiary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This rig would be purely for running local LLM's and sending the data back and forth to my mac desktop (which I'll be upgrading to the new mac pro which should be dropping later this year and will be a beast in itself).&lt;/p&gt; &lt;p&gt;I do a lot of coding and I love the idea of a blistering fast reasoning model that doesn't require anything being sent over the external network + I reckon within the next year there's going to be some insane optimizations and distillations.&lt;/p&gt; &lt;p&gt;Budget can potentially take another $5/$10K on top if necessary.&lt;/p&gt; &lt;p&gt;Anyway, please advise!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NootropicDiary"&gt; /u/NootropicDiary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyo8ul/whats_the_best_machine_i_can_get_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T14:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy6rid</id>
    <title>Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together</title>
    <updated>2025-02-25T21:44:22+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt; &lt;img alt="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" src="https://b.thumbs.redditmedia.com/p-QWrsvBVwjKiJeQsp_Oo8L1SYP2Wqt_48i9b9Kgics.jpg" title="Framework Desktop 128gb Mainboard Only Costs $1,699 And Can Networked Together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy6rid"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy6rid/framework_desktop_128gb_mainboard_only_costs_1699/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T21:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy7e4x</id>
    <title>RTX 4090 48GB</title>
    <updated>2025-02-25T22:10:28+00:00</updated>
    <author>
      <name>/u/xg357</name>
      <uri>https://old.reddit.com/user/xg357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt; &lt;img alt="RTX 4090 48GB" src="https://b.thumbs.redditmedia.com/i7dCpIN6G-2UgPHgeJaiSWC47liBasG9PHIrLoSF1kw.jpg" title="RTX 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got one of these legendary 4090 with 48gb of ram from eBay. I am from Canada. &lt;/p&gt; &lt;p&gt;What do you want me to test? And any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xg357"&gt; /u/xg357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iy7e4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy7e4x/rtx_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T22:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iybcnl</id>
    <title>DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix</title>
    <updated>2025-02-26T01:09:10+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt; &lt;img alt="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" src="https://external-preview.redd.it/qnXEqoF7LuYpZmlhfqFWCAFt6nE0AU8_d2ok4KoHKZ0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abee123819338d8d068f68944ed6953760e40e9e" title="DeepSeek Realse 3th Bomb! DeepGEMM a library for efficient FP8 General Matrix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in DeepSeek-V3&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DeepGEMM"&gt;https://github.com/deepseek-ai/DeepGEMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9"&gt;https://preview.redd.it/616ztgnjvdle1.png?width=882&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fb2a1853f514cd4f0b57cd8861518cdcfe5a8f9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iybcnl/deepseek_realse_3th_bomb_deepgemm_a_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T01:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iy2t7c</id>
    <title>Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990</title>
    <updated>2025-02-25T19:01:07+00:00</updated>
    <author>
      <name>/u/sobe3249</name>
      <uri>https://old.reddit.com/user/sobe3249</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt; &lt;img alt="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" src="https://preview.redd.it/erki80wv1cle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ada9d2780ffd78b32f14450c762f69f014324845" title="Framework's new Ryzen Max desktop with 128gb 256gb/s memory is $1990" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sobe3249"&gt; /u/sobe3249 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erki80wv1cle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-25T19:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iym55d</id>
    <title>Is the Framework Desktop Overhyped for Running LLMs?</title>
    <updated>2025-02-26T12:14:12+00:00</updated>
    <author>
      <name>/u/roworu</name>
      <uri>https://old.reddit.com/user/roworu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I honestly don't understand hype about that new Framework Desktop. From what I saw, the bandwidth for them would become a bottleneck for all LLMs you could theoretically put in these 128GB. So what is the point then? Yes, the pricing per VRAM DB is better than Apple's, but the generation speed is like 6 t/s at absolute best? Why would anyone want these for running LLMs? Isn't M-based devices would be better for that purpose?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roworu"&gt; /u/roworu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iym55d/is_the_framework_desktop_overhyped_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T12:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iyfvhb</id>
    <title>Perplexity is forking Chrome</title>
    <updated>2025-02-26T05:05:09+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt; &lt;img alt="Perplexity is forking Chrome" src="https://preview.redd.it/ubxe59mr1fle1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f3bdf014e84bea19fddef06263361ac5e64ab3" title="Perplexity is forking Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ubxe59mr1fle1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iyfvhb/perplexity_is_forking_chrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T05:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iylebm</id>
    <title>Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily</title>
    <updated>2025-02-26T11:28:21+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"&gt; &lt;img alt="Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily" src="https://preview.redd.it/cgapkix5ygle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff465a64ccfdf7ec55be9d23c4cf95d51107da4a" title="Starting today, enjoy off-peak discounts on the DeepSeek API Platform from 16:30–00:30 UTC daily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgapkix5ygle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iylebm/starting_today_enjoy_offpeak_discounts_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T11:28:21+00:00</published>
  </entry>
</feed>
