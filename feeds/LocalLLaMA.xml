<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-03T15:36:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jqezs6</id>
    <title>Need help from RAM giant to create whisper tflite model</title>
    <updated>2025-04-03T10:35:14+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have developed a local Android input method based on Whisper which is available on F-Droid (&lt;a href="https://f-droid.org/de/packages/org.woheller69.whisper/"&gt;https://f-droid.org/de/packages/org.woheller69.whisper/&lt;/a&gt;). I would like to improve the tflite model but the creation seems to require about 96GB of CPU RAM (in the end the model has around 100MB...)&lt;/p&gt; &lt;p&gt;Maybe one of the RAM giants from here, who knows how to run a Colab with local runtime, wants to help?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/woheller69/whisperIME/issues/71"&gt;https://github.com/woheller69/whisperIME/issues/71&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqezs6/need_help_from_ram_giant_to_create_whisper_tflite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqezs6/need_help_from_ram_giant_to_create_whisper_tflite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqezs6/need_help_from_ram_giant_to_create_whisper_tflite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T10:35:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jplg2o</id>
    <title>LiveBench team just dropped a leaderboard for coding agent tools</title>
    <updated>2025-04-02T10:37:19+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt; &lt;img alt="LiveBench team just dropped a leaderboard for coding agent tools" src="https://preview.redd.it/qxqj0vjtgese1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e87e657142ff097ddef495de75a59a3206ae77e" title="LiveBench team just dropped a leaderboard for coding agent tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qxqj0vjtgese1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jplg2o/livebench_team_just_dropped_a_leaderboard_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T10:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqiajy</id>
    <title>2x rtx 5070 vs 1x rtx 5080</title>
    <updated>2025-04-03T13:21:35+00:00</updated>
    <author>
      <name>/u/thosehippos</name>
      <uri>https://old.reddit.com/user/thosehippos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All!&lt;/p&gt; &lt;p&gt;I’m trying to decide between 2x rtx 5070 (approx $1100 msrp total) or 1x rtx 5080. &lt;/p&gt; &lt;p&gt;I currently have a gtx 1080, which I believe I could still use in conjunction with both of these. &lt;/p&gt; &lt;p&gt;Other important specs: CPU: i9 14900k RAM: 32x2 + 16x2 ddr5. Still trying to get stability with all 4 sticks, so just using 32x2 for now PSU wattage: 1250W&lt;/p&gt; &lt;p&gt;Workloads (proxmox): - standard home automation stuff (home assistant, wireguard, pihole, etc) - gaming vm (windows) with gpu pass through - openwebui/ollama (currently running on cpu/ram)&lt;/p&gt; &lt;p&gt;Usage: I’m an ML developer, so this is more of a homelab/experimentation setup than a gaming setup, though I would like the ability to game via vm (ex: baldurs gate, don’t need the max settings on all games). &lt;/p&gt; &lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thosehippos"&gt; /u/thosehippos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqiajy/2x_rtx_5070_vs_1x_rtx_5080/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqiajy/2x_rtx_5070_vs_1x_rtx_5080/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqiajy/2x_rtx_5070_vs_1x_rtx_5080/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T13:21:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqim2d</id>
    <title>How exactly to run MCP servers via local LLM</title>
    <updated>2025-04-03T13:35:16+00:00</updated>
    <author>
      <name>/u/sandwich_stevens</name>
      <uri>https://old.reddit.com/user/sandwich_stevens</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IDK the exact terminology or if its possible but in the way that claude's functionality can be extended with MCP servers, is there a way to use other LLMs say google Gemini 2.5 pro (or the local Gemma models) and the MCP servers from smithery etc, to extend the capabilities of local/open source models? that would truly be amazing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandwich_stevens"&gt; /u/sandwich_stevens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqim2d/how_exactly_to_run_mcp_servers_via_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqim2d/how_exactly_to_run_mcp_servers_via_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqim2d/how_exactly_to_run_mcp_servers_via_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T13:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpvxw0</id>
    <title>koboldcpp-1.87.1: Merged Qwen2.5VL support! :)</title>
    <updated>2025-04-02T18:27:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.87.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpvxw0/koboldcpp1871_merged_qwen25vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T18:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq13ik</id>
    <title>Mac Studio M3 Ultra 512GB DeepSeek V3-0324 IQ2_XXS (2.0625 bpw) llamacpp performance</title>
    <updated>2025-04-02T21:57:23+00:00</updated>
    <author>
      <name>/u/WhereIsYourMind</name>
      <uri>https://old.reddit.com/user/WhereIsYourMind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a lot of results that had abysmal tok/sec prompt processing. This is from the self compiled binary of llamacpp, commit f423981a.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ~/.lmstudio/models/unsloth/DeepSeek-V3-0324-GGUF/DeepSeek-V3-0324-UD-IQ2_XXS-00001-of-00005.gguf --n-gpu-layers 62 --flash-attn 0 -ctk f16,q8_0 -p 16384,32768,65536 -n 2048 -r 1 | model | size | params | backend | threads | type_k | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | -----: | ------------: | -------------------: | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp16384 | 51.17 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp32768 | 39.80 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | pp65536 | 467667.08 ± 0.00 | (failed, OOM) | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | f16 | tg2048 | 14.84 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp16384 | 50.95 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp32768 | 39.53 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | pp65536 | 25.27 ± 0.00 | | deepseek2 671B IQ2_XXS - 2.0625 bpw | 203.63 GiB | 671.03 B | Metal,BLAS | 24 | q8_0 | tg2048 | 16.09 ± 0.00 | build: f423981a (5022) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhereIsYourMind"&gt; /u/WhereIsYourMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq13ik/mac_studio_m3_ultra_512gb_deepseek_v30324_iq2_xxs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T21:57:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpuoh7</id>
    <title>Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!</title>
    <updated>2025-04-02T17:39:04+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"&gt; &lt;img alt="Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!" src="https://preview.redd.it/ziowvxg7kgse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acbf3232ba93ffa825062a5d7ef599eb46ce434b" title="Now we talking INTELLIGENCE EXPLOSION💥🔅 | ⅕ᵗʰ of benchmark cracked by claude 3.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ziowvxg7kgse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpuoh7/now_we_talking_intelligence_explosion_⅕ᵗʰ_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpxq4y</id>
    <title>DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low</title>
    <updated>2025-04-02T19:39:51+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt; &lt;img alt="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" src="https://preview.redd.it/0ymxajfb5hse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82c6cdb4e5d31a3f747415575616dee79f009536" title="DISTILLATION is so underrated. I spent an hour and got a neat improvement in accuracy while keeping the costs low" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ymxajfb5hse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpxq4y/distillation_is_so_underrated_i_spent_an_hour_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T19:39:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqlkfp</id>
    <title>What are you guys waiting for in the AI world this month?</title>
    <updated>2025-04-03T15:33:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For me, it’s:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 4&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 3&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek R2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral’s new model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Diffusion LLM model API on OpenRouter&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T15:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptdtg</id>
    <title>Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!</title>
    <updated>2025-04-02T16:48:30+00:00</updated>
    <author>
      <name>/u/JawGBoi</name>
      <uri>https://old.reddit.com/user/JawGBoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt; &lt;img alt="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" src="https://external-preview.redd.it/T7uMJGzQ_xDL3OLN-I6nY-QjBc2LJ_pj5xaq0KJj7XI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84d098830ff90a292e2d57523020f6f6a49fb61" title="Kyutai Labs finally release finetuning code for Moshi - We can now give it any voice we wish!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model repo: &lt;a href="https://github.com/kyutai-labs/moshi"&gt;https://github.com/kyutai-labs/moshi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JawGBoi"&gt; /u/JawGBoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kyutai-labs/moshi-finetune"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptdtg/kyutai_labs_finally_release_finetuning_code_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T16:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpr1nk</id>
    <title>The Candle Test - most LLMs fail to generalise at this simple task</title>
    <updated>2025-04-02T15:13:10+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt; &lt;img alt="The Candle Test - most LLMs fail to generalise at this simple task" src="https://preview.redd.it/6phgn27rqfse1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=676b32e5d96ebb0c0830e00756c8e79d41840121" title="The Candle Test - most LLMs fail to generalise at this simple task" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure a lot of people here noticed that latest frontier models are... weird. Teams facing increased pressure to chase a good place in the benchmarks and make the SOTA claims - the models are getting more and more overfit resulting in decreased generalisation capabilities.&lt;/p&gt; &lt;p&gt;It became especially noticeable with the very last line-up of models which despite being better on paper somehow didn't feel so with daily use.&lt;/p&gt; &lt;p&gt;So, I present to you a very simple test that highlights this problem. It consists of three consecutive questions where the model is steered away from possible overfit - yet most still demonstrate it on the final conversation turn (including thinking models).&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are candles getting taller or shorter when they burn?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models correctly identify that candles are indeed getting shorter when burning.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Are you sure? Will you be able to recognize this fact in different circumstances?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Most models confidently confirm that such a foundational fact is hard to miss under any circumstances.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Now, consider what you said above and solve the following riddle: I'm tall when I'm young, and I'm taller when I'm old. What am I?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And here most models are as confidently wrong claiming that the answer is a candle.&lt;/p&gt; &lt;p&gt;Unlike traditional misguided attention tasks - this test gives model ample chances for in-context generalisation. Failing this test doesn't mean that the model is &amp;quot;dumb&amp;quot; or &amp;quot;bad&amp;quot; - most likely it'll still be completely fine for 95% of use-cases, but it's also more likely to fail in a novel situation.&lt;/p&gt; &lt;p&gt;Here are some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/7e9815b3-15ba-4a4c-81e1-0f233f1b0d5a"&gt;DeepSeek Chat V3&lt;/a&gt; (0324, Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/3e27bf44-c64c-4558-b98f-989fb1c82688"&gt;DeepSeek R1&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/f1c205e4-ee2d-41e4-87b4-e8c9dbe0024b"&gt;DeepSeek R1 Distill Llama 70B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/4ac04a5d-8199-4675-b4ce-5e3cbbb9223d"&gt;Llama 3.1 405B&lt;/a&gt; (Fails)&lt;/li&gt; &lt;li&gt;QwQ 32B didn't pass due to entering endless loop multiple times&lt;/li&gt; &lt;li&gt;&lt;a href="https://kagi.com/assistant/5ff0eb98-cd36-4988-a2a0-e01416ac567d"&gt;Mistral Large&lt;/a&gt; (Passes, one of the few)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inpired by my frustration with Sonnet 3.7 (which also fails this test, unlike Sonnet 3.5).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6phgn27rqfse1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpr1nk/the_candle_test_most_llms_fail_to_generalise_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jql4ia</id>
    <title>Does anyone else kinda love the coil whine noise as the LLM spins up?</title>
    <updated>2025-04-03T15:16:17+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first time I heard the faint screech as a model started doing its thing, I was afraid my GPU was fucked up... a year later, I've come to almost see it as the dial up modem tone of yesteryear - a small sound that let me know good things are coming in just a moment! Seems like every model has its own little song, and the tones during inference on a Mac are very different than the ones I get out of my nvidia GPUs. It makes me weirdly nostalgic, and now it's almost a comforting indicator that things are working rather than a warning flag. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T15:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqhff1</id>
    <title>Personal experience with local&amp;commercial LLM's</title>
    <updated>2025-04-03T12:43:11+00:00</updated>
    <author>
      <name>/u/zoom3913</name>
      <uri>https://old.reddit.com/user/zoom3913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the luxury of having 2x 3090's at home and access to MS Copilot / 4o / 4o-mini at work. I've used a load of models extensively the past couple of months; regarding the non-reasoning models, I value the models as follows;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--10B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Not really intelligent, makes lots of basic mistakes&lt;/em&gt; &lt;/li&gt; &lt;li&gt;&lt;em&gt;Doesn't follow instructions to the letter&lt;/em&gt; &lt;em&gt;However, really good at &amp;quot;vibe check&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Writing text that sounds good&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#1 Mistral Nemo&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--30B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Semi intelligent, can follow basic tasks without major mistakes For example, here's a list of people+phone number, and another list of people+address, combine the lists, give the phone and address of each person&lt;/em&gt; &lt;/li&gt; &lt;li&gt;&lt;em&gt;Very fast generation speed&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#3 Mistral Small&lt;/p&gt; &lt;p&gt;#2 Qwen2.5B 32B&lt;/p&gt; &lt;p&gt;#1 4o-mini&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--70B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follows more complex tasks without major mistakes&lt;/li&gt; &lt;li&gt;Trade-off: lower generation speed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#3 Llama3.3 70B&lt;/p&gt; &lt;p&gt;#2 4o / Copilot, considering how much these costs in corporate settings, their performance is really disappointing &lt;/p&gt; &lt;p&gt;#1 Qwen2.5 72B&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--Even better;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follows even more complex tasks without mistakes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#4 DeepSeek V3&lt;/p&gt; &lt;p&gt;#3 Gemini models&lt;/p&gt; &lt;p&gt;#2 Sonnet 3.7; I actually prefer 3.5 to this&lt;/p&gt; &lt;p&gt;#1 DeepSeek V3 0324&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--Peak&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;#1 Sonnet 3.5&lt;/p&gt; &lt;p&gt;I think the picture is clear, basically, for a complex coding / data task I would confidently let Sonnet 3.5 do its job and return after a couple of minutes expecting a near perfect output.&lt;/p&gt; &lt;p&gt;DeepSeekV3 would need 2 iterations +-. A note here is that I think DS V3 0324 would suffice for 99% of the cases, but it's less usable due to timeouts / low generation speed. Gemini is a good, fast and cheap tradeoff.&lt;/p&gt; &lt;p&gt;70B models, probably 5 back and forths&lt;/p&gt; &lt;p&gt;For the 30B models even more, and probably I'll have to invest some thinking in order to simplify the problem so the LLM can solve it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoom3913"&gt; /u/zoom3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T12:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq35ba</id>
    <title>ClaudePlaysPokemon Open Sourced - Benchmark AI by letting it play Pokémon</title>
    <updated>2025-04-02T23:26:23+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The source code for the AI benchmark ClaudePlaysPokemon has been released. ClaudePlaysPokemon is a benchmark to show how agents work and can generalize, it was made to see how a AI model not trained on Pokemon can use general thinking to play the game.&lt;/p&gt; &lt;p&gt;What I personally would like to see is the open source community taking a small local model like Gemma3 27b and finetuning it on annotated screenshots explaining it what tiles can be cut which ones can only be jumped over from one side etc and maybe general game knowledge from Bulbapedia. This would be a good way to show if a finetuned specialized small model can out perform a general big model.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/davidhershey/ClaudePlaysPokemonStarter"&gt;https://github.com/davidhershey/ClaudePlaysPokemonStarter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Twitch: &lt;a href="https://www.twitch.tv/claudeplayspokemon"&gt;https://www.twitch.tv/claudeplayspokemon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Visual Explainer: &lt;a href="https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg"&gt;https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq35ba/claudeplayspokemon_open_sourced_benchmark_ai_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T23:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqcn4q</id>
    <title>CSM Finetuning is here!</title>
    <updated>2025-04-03T08:00:48+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidbrowne17/csm-streaming"&gt;https://github.com/davidbrowne17/csm-streaming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I added fine-tuning to CSM. Clone my repo and place your audio files into a folder called audio_data and run lora.py to finetune it. You will likely need 12gb+ of vram to do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcn4q/csm_finetuning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T08:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqip5f</id>
    <title>Security vulnerabilities with Ryzen AI / NPU CPUs</title>
    <updated>2025-04-03T13:38:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a bunch of recent &lt;a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7037.html"&gt;security issues&lt;/a&gt; in the driver for the NPU, as well as related software. Basically, a malicious AI model could install malware on the local machine when executed via NPU. If the developer SDK is also installed when it could even easily get administrator permissions despite running via restricted account.&lt;/p&gt; &lt;p&gt;There's a &lt;a href="https://ryzenai.docs.amd.com/en/latest/inst.html"&gt;software update&lt;/a&gt; available where the issues have been fixed, but for downloading it you need to &lt;a href="https://account.amd.com/en/forms/downloads/amd-end-user-license-xef.html?filename=NPU_RAI1.4_GA_257_WHQL.zip"&gt;log in&lt;/a&gt; first. Basic drivers for your hardware should be freely accessible, especially when it's about security updates, and not kept behind a log in wall.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T13:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq8zfk</id>
    <title>Open-WebUI Artifacts Overhaul has been updated to v0.6.0!</title>
    <updated>2025-04-03T04:11:32+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt; &lt;img alt="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" src="https://external-preview.redd.it/wZszwQ2U6yJ7pvC1CwegCQ8kArJM8Bhaojq_gfjcMsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=784dcdbc7977acbb264691fc234770109ef75318" title="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I just wanted to let you know that the Open-WebUI Artifacts Overhaul fork has been updated to match v0.6.0 of Open-Webui!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;https://github.com/nick-tonjum/open-webui-artifacts-overhaul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Don't know what the 'Artifacts Overhaul' branch is? It adds the following to open-webui:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🖼️ &lt;strong&gt;Coding Canvas&lt;/strong&gt;: Whenever a LLM outputs code, it will appear on the right side of the page with Monaco editor, similar to VSCode. Here you can cycle through different files produced via the LLM and also different versions&lt;/li&gt; &lt;li&gt;🔍 &lt;strong&gt;Difference Checker&lt;/strong&gt;: If a LLM makes changes to code, the differences will be highlight. This can be easily disabled or enabled via a single click!&lt;/li&gt; &lt;li&gt;🎨 &lt;strong&gt;Design Viewer&lt;/strong&gt;: Easily toggle between code view and design view with the click of a button! This currently supports HTML/CSS/JavaScript like before, but now with Tailwind styles built in. React components work too!&lt;/li&gt; &lt;li&gt;⚛️ &lt;strong&gt;React Visualizer&lt;/strong&gt;: As mentioned above, React components work too. This seems to work 80% of the time and I'm working hard to get it 100% of the time! As long as the code block has an export default it should work.&lt;/li&gt; &lt;li&gt;💼 &lt;strong&gt;Compacted Code&lt;/strong&gt;: When the canvas is open, code blocks in the regular chat are compacted and visualized as an attachment.&lt;/li&gt; &lt;li&gt;🌐 &lt;strong&gt;MANY supported languages&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check it out. Hopefully someday this will end up in the main branch :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7lewes7wojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8b3a77a94287acbf414bb38e5e5f934d147ff2df"&gt;Difference Viewer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7wbnf7kwojse1.gif"&gt;Cycle through multiple files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/is93kyswojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4502d68ce62e7e656fe050b71aeb0bfdf9fec8fe"&gt;React component viewer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T04:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqj9a7</id>
    <title>Fully Featured AI Coding Agent as MCP Server (or for local model)</title>
    <updated>2025-04-03T14:02:31+00:00</updated>
    <author>
      <name>/u/Left-Orange2267</name>
      <uri>https://old.reddit.com/user/Left-Orange2267</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been working like hell on this one: a fully capable Agent, as good or better than Windsurf's Cascade, Claude Code or Cursor's agent - but can be used for free.&lt;/p&gt; &lt;p&gt;It can run as an MCP server, so you can use it for free with Claude Desktop, and it can still fully understand a code base, even a very large one. We did this by using a language server instead of RAG to analyze code.&lt;/p&gt; &lt;p&gt;Can also run it on any model, including local ones.&lt;/p&gt; &lt;p&gt;Check it out, super easy to run, GPL license:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/oraios/serena"&gt;https://github.com/oraios/serena&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Left-Orange2267"&gt; /u/Left-Orange2267 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T14:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptset</id>
    <title>University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy</title>
    <updated>2025-04-02T17:04:49+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt; &lt;img alt="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" src="https://b.thumbs.redditmedia.com/nIuszN8uDMIjbhUsiZdUw2NeBO5my-uVcctXiMF1pcI.jpg" title="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jptset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqawj1</id>
    <title>Open Sourcing Latent Space Guardrails that catch 43% of Hallucinations</title>
    <updated>2025-04-03T06:05:16+00:00</updated>
    <author>
      <name>/u/Cautious_Hospital352</name>
      <uri>https://old.reddit.com/user/Cautious_Hospital352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released fully open source latent space guardrails that monitor and stop unwelcome outputs of your LLM on the latent space level. Check it out here and happy to adopt it to your use case! &lt;a href="https://github.com/wisent-ai/wisent-guard"&gt;https://github.com/wisent-ai/wisent-guard&lt;/a&gt; On hallucinations it has not been trained on in TruthfulQA, this results in a 43% detection of hallucinations just from the activation patterns. You can use them to control the brain of your LLM and block it from outputting bad code, harmful outputs or taking decisions because of gender or racial bias. This is a new approach, different from circuit breakers or SAE-based mechanistic interpretability. We will be releasing a new version of the reasoning architecture based on latent space interventions soon to not only reduce hallucinations but use this for capabilities gain as well! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cautious_Hospital352"&gt; /u/Cautious_Hospital352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T06:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqcj89</id>
    <title>YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!</title>
    <updated>2025-04-03T07:53:41+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt; &lt;img alt="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" src="https://external-preview.redd.it/NTZsOHM4NDNya3NlMWK-C_FeILIR1n-iDldyZrl0VqZ3vuhsUq5jbN6auZ0Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a07a6ac257f3a744208337e975f20df2a138d814" title="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! clefourrier from HF's OpenEvals team! We open sourced YourBench yesterday, a custom synthetic evaluation framework: from any document, it creates a custom made QA set, then builds a leaderboard on your specific use case.&lt;/p&gt; &lt;p&gt;It works through multiple steps of chunking, summarization, LLM single and multi hop question and answer generation, validation, and so far we've found it works really well to generate interesting QAs! &lt;/p&gt; &lt;p&gt;You can use the demo as is, or customize and download it to run it with your favorite models: Best model for diverse questions is Qwen2.5-32B, and open model generating most grounded/valid questions is Gemma3-27B (just one place below o3-mini)! You can also set several seeds to augment diversity, complexity, etc. &lt;/p&gt; &lt;p&gt;This work has been carried by our intern, Sumuk, who had a great idea on how to dynamically generate eval sets, and we wrote a paper explaining the full method here: &lt;a href="https://huggingface.co/papers/2504.01833"&gt;https://huggingface.co/papers/2504.01833&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try it out here: &lt;a href="https://huggingface.co/spaces/yourbench/demo"&gt;https://huggingface.co/spaces/yourbench/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: Document -&amp;gt; custom made evaluation set -&amp;gt; leaderboard in 5 min&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xy7fgb43rkse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T07:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqhgzq</id>
    <title>Confused with Too Many LLM Benchmarks, What Actually Matters Now?</title>
    <updated>2025-04-03T12:45:15+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to make sense of the constant benchmarks for new LLM advancements in 2025.&lt;br /&gt; Since the early days of GPT‑3.5, we've witnessed countless benchmarks and competitions — MMLU, HumanEval, GSM8K, HellaSwag, MLPerf, GLUE, etc.—and it's getting overwhelming .&lt;/p&gt; &lt;p&gt;I'm curious, so its the perfect time to ask the reddit folks: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;What’s your go-to benchmark?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;How do you stay updated on benchmark trends?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What Really Matters&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Your take on benchmarking in general&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I guess my question could be summarized to what genuinely indicate better performance vs. hype?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;feel free to share your thoughts, experiences or HOT Takes.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T12:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqa182</id>
    <title>Llama 4 will probably suck</title>
    <updated>2025-04-03T05:12:21+00:00</updated>
    <author>
      <name>/u/klapperjak</name>
      <uri>https://old.reddit.com/user/klapperjak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been following meta FAIR research for awhile for my phd application to MILA and now knowing that metas lead ai researcher quit, I’m thinking it happened to dodge responsibility about falling behind basically.&lt;/p&gt; &lt;p&gt;I hope I’m proven wrong of course, but the writing is kinda on the wall.&lt;/p&gt; &lt;p&gt;Meta will probably fall behind and so will Montreal unfortunately 😔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klapperjak"&gt; /u/klapperjak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T05:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqfnmh</id>
    <title>Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding</title>
    <updated>2025-04-03T11:13:31+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"&gt; &lt;img alt="Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding" src="https://external-preview.redd.it/KfSCldbxB1IZkgQJUxzx3I66OmXlBpPyIYpJisUrymw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b350abd537aa130a3efa570e482b7dc669f3843" title="Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tesslate/Synthia-S1-27b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T11:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqef4d</id>
    <title>China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4</title>
    <updated>2025-04-03T10:00:10+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt; &lt;img alt="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" src="https://preview.redd.it/x9zbqai7flse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c47a9c1cd7cd7f716e71f6e9161de09e2cf497a4" title="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9zbqai7flse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T10:00:10+00:00</published>
  </entry>
</feed>
