<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-26T15:50:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m9f7lq</id>
    <title>Reka AI models support in uzu engine</title>
    <updated>2025-07-26T00:10:41+00:00</updated>
    <author>
      <name>/u/darkolorin</name>
      <uri>https://old.reddit.com/user/darkolorin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/"&gt; &lt;img alt="Reka AI models support in uzu engine" src="https://a.thumbs.redditmedia.com/afA01kAaHMzrELSiWVNCsofMUALmXIzdtd4uSzHvlv4.jpg" title="Reka AI models support in uzu engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkolorin"&gt; /u/darkolorin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m9f7lq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T00:10:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8w7ny</id>
    <title>Smaller Qwen Models next week!!</title>
    <updated>2025-07-25T11:04:28+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"&gt; &lt;img alt="Smaller Qwen Models next week!!" src="https://preview.redd.it/752ts71q50ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb" title="Smaller Qwen Models next week!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/752ts71q50ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T11:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9ml0y</id>
    <title>webbigdata/VoiceCore: Japanese voice version of canopylabs/orpheus-tts</title>
    <updated>2025-07-26T06:44:19+00:00</updated>
    <author>
      <name>/u/dahara111</name>
      <uri>https://old.reddit.com/user/dahara111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to introduce a high-quality Japanese version of TTS that I've created through continuous pre-learning and post-training with orpheus.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/webbigdata/VoiceCore"&gt;https://huggingface.co/webbigdata/VoiceCore&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Findings for those who are trying to create TTS in languages other than English&lt;/p&gt; &lt;p&gt;I think that various TTS models use various neural codecs. This time, I used SNAC 24khz, which is used by orpheus-tts.&lt;/p&gt; &lt;p&gt;SNAC is trained only in English. It is very high performance, but I noticed that there is a tendency for noise to be added to high-pitched voices such as surprise and joy of Japanese women.&lt;/p&gt; &lt;p&gt;I noticed this after a lot of work was completed, so I decided to release it as it is as a preview version. When selecting a codec, I think it is better to first check whether it can handle emotional voices as well as normal voices.&lt;/p&gt; &lt;p&gt;Thank you meta/llama 3.2, canopylabs, and snac.&lt;/p&gt; &lt;p&gt;Feedback is welcome.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dahara111"&gt; /u/dahara111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ml0y/webbigdatavoicecore_japanese_voice_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T06:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vegq</id>
    <title>Qwen3-235B-A22B-Thinking-2507 released!</title>
    <updated>2025-07-25T10:16:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 released!" src="https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f579818ebd6748b55b90f802c28f4d37095432e" title="Qwen3-235B-A22B-Thinking-2507 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt; &lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving: ✅ Improved performance in logical reasoning, math, science &amp;amp; coding ✅ Better general skills: instruction following, tool use, alignment ✅ 256K native context for deep, long-form understanding&lt;/p&gt; &lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvx1dbl5xzef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9uoa7</id>
    <title>What's the fastest backend for local long context (100k+)?</title>
    <updated>2025-07-26T14:27:25+00:00</updated>
    <author>
      <name>/u/trithilon</name>
      <uri>https://old.reddit.com/user/trithilon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been out of the scene for the past few months.&lt;/p&gt; &lt;p&gt;Should I use lmstudio? ollama? llamacpp?&lt;/p&gt; &lt;p&gt;Or ik_llama? vllm? lmdeploy?&lt;/p&gt; &lt;p&gt;I have a 4090 + 96 GB of ram and Ryzen 9 7900 and my goal is to hit 100k context with pp times &amp;lt;5 seconds and models 7B to 32B. Possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trithilon"&gt; /u/trithilon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uoa7/whats_the_fastest_backend_for_local_long_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uoa7/whats_the_fastest_backend_for_local_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uoa7/whats_the_fastest_backend_for_local_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T14:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9tt3o</id>
    <title>Phi-4-mini-reasoning: An example of "overfitting to think"</title>
    <updated>2025-07-26T13:48:52+00:00</updated>
    <author>
      <name>/u/a-c-19-23</name>
      <uri>https://old.reddit.com/user/a-c-19-23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes, you can overfit a model to think &lt;em&gt;too&lt;/em&gt; deeply. There seems to be a balance required for a model to break a problem down step-by-step, but not overthink it. I find that Phi-4 is good at problem solving and thinking analytically, but doesn't understand when something &lt;em&gt;isn't a problem&lt;/em&gt;. Not everything is a problem it has to solve. This kind of model behavior shows that step-by-step reasoning needs not just the ability to break things down, but also the judgment to recognize when that’s not needed at all. I don't believe the Microsoft team balanced that correctly.&lt;/p&gt; &lt;p&gt;What's your thoughts on this? Do you find that certain models are better at balancing this?&lt;/p&gt; &lt;p&gt;Here's an example:&lt;/p&gt; &lt;p&gt;``` ollama run hf.co/lmstudio-community/Phi-4-mini-reasoning-GGUF:Phi-4-mini-reasoning-Q8_0&lt;/p&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Hello! &amp;lt;think&amp;gt; Okay, so I need to solve this problem where I'm given a number and have to find the sum of its digits. Let's see... The user wrote &amp;quot;You will be given a problem. Please reason step by step, and put your final answer within \boxed{}: Hello!&amp;quot; Hmm, maybe there's a typo or something missing here. Wait, perhaps the actual problem is to calculate the sum of the digits of the number that comes after &amp;quot;Hello!&amp;quot;. But in the current message, it just says &amp;quot;Hello!&amp;quot; Maybe that's part of the problem? Let me check again.&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Wait, looking at the original query: The user starts with &amp;quot;You will be given a problem. Please reason step by step...&amp;quot; and then writes &amp;quot;Hello!&amp;quot;. So perhaps the problem is simply to find the sum of the digits in the word &amp;quot;Hello!&amp;quot;, but that doesn't make sense because &amp;quot;Hello!&amp;quot; is text, not numbers. Unless maybe there's an error here. Alternatively, maybe there was supposed to be a number provided after &amp;quot;Hello!&amp;quot; but it got cut off or mistyped.&lt;/p&gt; &lt;p&gt;Alternatively, could this be a trick question where &amp;quot;Hello!&amp;quot; is converted into numbers in some way? For example, using the position of each letter in the alphabet: H=8, E=5, L=12, L=12, O=15, and then maybe the exclamation mark as something else. But summing those digits... Wait, if we take each letter's numerical value and sum them, but then split into individual digits? Let me try that.&lt;/p&gt; &lt;p&gt;H is the 8th letter, E is 5, L is 12, another L is 12, O is 15. The exclamation mark isn't a letter, so maybe ignore it. So adding those numbers: 8 + 5 + 12 + 12 + 15. Let's calculate that:&lt;/p&gt; &lt;p&gt;8 + 5 = 13&lt;/p&gt; &lt;p&gt;13 + 12 = 25&lt;/p&gt; &lt;p&gt;25 + 12 = 37&lt;/p&gt; &lt;p&gt;37 + 15 = 52&lt;/p&gt; &lt;p&gt;So the sum is 52. But then, if we need to split into digits and sum again: 5 + 2 = 7. Hmm, but that's speculative. The problem statement isn't clear. Alternatively, maybe &amp;quot;Hello!&amp;quot; is a typo for a number, like HELLO! where H,E,L,L,O are hexadecimal digits? Wait, in hex, letters go up to F, so H and O aren't valid hex digits. So that might not work.&lt;/p&gt; &lt;p&gt;Alternatively, perhaps the user intended to write a different problem but made a mistake in input. For example, maybe they meant to provide a number like 12345 but accidentally typed &amp;quot;Hello!&amp;quot; instead. In that case, there's insufficient information. But since I have to work with what's given, I need to make an educated guess.&lt;/p&gt; &lt;p&gt;Another angle: sometimes in puzzles, the word &amp;quot;Hello&amp;quot; is associated with ASCII codes. For example, H is 72, E is 69, L is 76, another L is 76, O is 79. Let's check that:&lt;/p&gt; &lt;p&gt;H = 72&lt;/p&gt; &lt;p&gt;E = 69&lt;/p&gt; &lt;p&gt;L = 76&lt;/p&gt; &lt;p&gt;L = 76&lt;/p&gt; &lt;p&gt;O = 79&lt;/p&gt; &lt;p&gt;Summing those numbers: 72 + 69 = 141; 141 + 76 = 217; 217 + 76 = 293; 293 + 79 = 372. Then, if we need to sum the digits of each number before adding? Wait, but that would be different. For example, summing digits of 72: 7+2=9, 69:6+9=15, etc., then add all those digit sums. Let's try:&lt;/p&gt; &lt;p&gt;H (72): 7 + 2 = 9&lt;/p&gt; &lt;p&gt;E (69): 6 + 9 = 15&lt;/p&gt; &lt;p&gt;L (76): 7 + 6 = 13&lt;/p&gt; &lt;p&gt;Another L (76): 13 again&lt;/p&gt; &lt;p&gt;O (79): 7 + 9 = 16&lt;/p&gt; &lt;p&gt;Total sum: 9 + 15 + 13 + 13 + 16. Let's compute:&lt;/p&gt; &lt;p&gt;9 + 15 = 24&lt;/p&gt; &lt;p&gt;24 + 13 = 37&lt;/p&gt; &lt;p&gt;37 + 13 = 50&lt;/p&gt; &lt;p&gt;50 + 16 = 66&lt;/p&gt; &lt;p&gt;So the total would be 66. But again, this is speculative because the problem statement isn't clear.&lt;/p&gt; &lt;p&gt;Alternatively, maybe &amp;quot;Hello!&amp;quot; is just a greeting and the actual number is missing. Since the user might have made an error in submitting the problem, perhaps they meant to provide a different input. In that case, I should inform them that the problem seems incomplete. However, since I'm supposed to act as if this is the given problem, maybe there's another interpretation.&lt;/p&gt; &lt;p&gt;Wait, could &amp;quot;Hello!&amp;quot; be a hexadecimal number? Let's see: H isn't a hex digit (0-9, A-F), so that's not possible. Similarly for other letters. So perhaps it's base64 encoded or something else. Alternatively, converting each letter to its ASCII value and concatenating them into a single number.&lt;/p&gt; &lt;p&gt;... [[manually shortened to keep it moderately brief]] ...&lt;/p&gt; &lt;p&gt;Alternatively, if it's base26 or something, but that's more complex.&lt;/p&gt; &lt;p&gt;Given all possible interpretations and without further info, I think the safest assumption is that there was a mistake in the problem entry, and &amp;quot;Hello!&amp;quot; should be replaced with an actual number. However, since I must answer based on given input, perhaps the intended answer is 7 (from summing H(8)+E(5)+L(12)+L(12)+O(15)=52 →5+2=7), which is a common type of puzzle.&lt;/p&gt; &lt;p&gt;Alternatively, if we consider that &amp;quot;Hello!&amp;quot; has no digits, sum is zero. But since the user might have intended something else, maybe 72 or another number.&lt;/p&gt; &lt;p&gt;Wait, perhaps the problem was meant to be in Chinese or another language where &amp;quot;Hello!&amp;quot; translates to numbers, but that's unlikely here.&lt;/p&gt; &lt;p&gt;Another angle: Maybe &amp;quot;sum of its digits&amp;quot; refers to the string's length? &amp;quot;Hello!&amp;quot; has 6 characters, sum is 6. But that doesn't make sense for digits.&lt;/p&gt; &lt;p&gt;Alternatively, if we consider each letter as a digit in base26 or another system, but without specification, it's unclear.&lt;/p&gt; &lt;p&gt;Given all this uncertainty, I think the most reasonable answers could be either 0 (no numeric characters) or 7 (sum of letters' positions). Since math problems often use A=1 etc., I'll go with 7. However, I'm not entirely sure. But let's check once more:&lt;/p&gt; &lt;p&gt;If H is 8, E is5, L is12, O is15:&lt;/p&gt; &lt;p&gt;Sum:8+5+12+12+15=52 → digits 5 and 2 sum to 7.&lt;/p&gt; &lt;p&gt;Yes, that's a common puzzle type. So maybe the answer is \boxed{7}. &amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;The problem asks for the sum of the digits after converting each letter in &amp;quot;Hello!&amp;quot; to its corresponding position in the alphabet (A=1, B=2, ..., Z=26). &lt;/p&gt; &lt;ul&gt; &lt;li&gt;H = 8&lt;/li&gt; &lt;li&gt;E = 5&lt;/li&gt; &lt;li&gt;L = 12&lt;/li&gt; &lt;li&gt;L = 12&lt;/li&gt; &lt;li&gt;O = 15&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Summing these values:&lt;br /&gt; (8 + 5 + 12 + 12 + 15 = 52)&lt;/p&gt; &lt;p&gt;Next, we sum the digits of 52:&lt;br /&gt; (5 + 2 = 7)&lt;/p&gt; &lt;p&gt;Thus, the final answer is (\boxed{7}). ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a-c-19-23"&gt; /u/a-c-19-23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9ukpw</id>
    <title>Best non-thinking model which can be a long context personal assistant?</title>
    <updated>2025-07-26T14:22:57+00:00</updated>
    <author>
      <name>/u/trithilon</name>
      <uri>https://old.reddit.com/user/trithilon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using GPT-4o for most of my daily queries - my main usecase is to map my thoughts, some of this stuff is sensitive so I need a local solution.&lt;/p&gt; &lt;p&gt;I REALLY like the tone of GPT-4o (yeah, I am a sucker for glazing!)&lt;br /&gt; What would be the best model to use for this usecase? &lt;/p&gt; &lt;p&gt;I am thinking 13-32B models which are uncensored because I wouldn't want to be moral policed.&lt;br /&gt; I have an RTX 4090 with 96 gigs of ram and a Ryzen 9 7900 processor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trithilon"&gt; /u/trithilon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ukpw/best_nonthinking_model_which_can_be_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T14:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9tnj5</id>
    <title>Scaling Inference To Billions of Users And Agents</title>
    <updated>2025-07-26T13:41:47+00:00</updated>
    <author>
      <name>/u/m4r1k_</name>
      <uri>https://old.reddit.com/user/m4r1k_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Just published a deep dive on the full infrastructure stack required to scale LLM inference to billions of users and agents. It goes beyond a single engine and looks at the entire system.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GKE Inference Gateway: How it cuts tail latency by 60% &amp;amp; boosts throughput 40% with model-aware routing (KV cache, LoRA).&lt;/li&gt; &lt;li&gt;vLLM on GPUs &amp;amp; TPUs: Using vLLM as a unified layer to serve models across different hardware, including a look at the insane interconnects on Cloud TPUs.&lt;/li&gt; &lt;li&gt;The Future is llm-d: A breakdown of the new Google/Red Hat project for disaggregated inference (separating prefill/decode stages).&lt;/li&gt; &lt;li&gt;Planetary-Scale Networking: The role of a global Anycast network and 42+ regions in minimizing latency for users everywhere.&lt;/li&gt; &lt;li&gt;Managing Capacity &amp;amp; Cost: Using GKE Custom Compute Classes to build a resilient and cost-effective mix of Spot, On-demand, and Reserved instances.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full article with architecture diagrams &amp;amp; walkthroughs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7"&gt;https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Disclaimer: I work at Google Cloud.)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m4r1k_"&gt; /u/m4r1k_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9holp</id>
    <title>There's a new Kimi model on lmarena called Zenith and it's really really good. It might be Kimi K2 with reasoning</title>
    <updated>2025-07-26T02:12:06+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9holp/theres_a_new_kimi_model_on_lmarena_called_zenith/"&gt; &lt;img alt="There's a new Kimi model on lmarena called Zenith and it's really really good. It might be Kimi K2 with reasoning" src="https://preview.redd.it/4rtvhn7mn4ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=358f4000782fac35b42bd0cd8d6c6c5750dfd53f" title="There's a new Kimi model on lmarena called Zenith and it's really really good. It might be Kimi K2 with reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4rtvhn7mn4ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9holp/theres_a_new_kimi_model_on_lmarena_called_zenith/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9holp/theres_a_new_kimi_model_on_lmarena_called_zenith/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T02:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9bwoy</id>
    <title>Compact 2x RTX Pro 6000 Rig</title>
    <updated>2025-07-25T21:46:33+00:00</updated>
    <author>
      <name>/u/shadowninjaz3</name>
      <uri>https://old.reddit.com/user/shadowninjaz3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/"&gt; &lt;img alt="Compact 2x RTX Pro 6000 Rig" src="https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7c498f0a5ad74816f597205d993264473bdbfe" title="Compact 2x RTX Pro 6000 Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally put together my rig after months of planning into a NAS case &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Threadripper PRO 7955WX&lt;/li&gt; &lt;li&gt;Arctic Freezer 4U-M (cpu cooler)&lt;/li&gt; &lt;li&gt;Gigabyte TRX50 AI TOP&lt;/li&gt; &lt;li&gt;be quiet! Dark Power Pro 13 1600W&lt;/li&gt; &lt;li&gt;JONSBO N5 Case&lt;/li&gt; &lt;li&gt;2x RTX Pro 6000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Might add a few more intake fans on the top &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shadowninjaz3"&gt; /u/shadowninjaz3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tbteu4v5b3ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T21:46:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9tyg9</id>
    <title>Is China the only hope for factual models?</title>
    <updated>2025-07-26T13:55:46+00:00</updated>
    <author>
      <name>/u/Meme_Lord_Musk</name>
      <uri>https://old.reddit.com/user/Meme_Lord_Musk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the &lt;strong&gt;Chinese Models&lt;/strong&gt; are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like &lt;strong&gt;Google&lt;/strong&gt; or &lt;strong&gt;OpenAI&lt;/strong&gt;, or even &lt;strong&gt;Grok&lt;/strong&gt; self censor and have things in place, look at the recent &lt;a href="http://X.com"&gt;X.com&lt;/a&gt; thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the 'normie' answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Meme_Lord_Musk"&gt; /u/Meme_Lord_Musk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9nu0j</id>
    <title>Thoughts on Qwen3 235B A22B Instruct 2507?</title>
    <updated>2025-07-26T08:03:54+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using the model (at FP8) for the past few days and it feels pretty solid for discussing ideas with and for using it as a code agent (I mostly use Qwen's CLI).&lt;/p&gt; &lt;p&gt;Has anyone else been using this model recently? If you have, do you think it's decent for its size or are there better options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9nu0j/thoughts_on_qwen3_235b_a22b_instruct_2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T08:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9enpd</id>
    <title>China's ByteDance's coze studio is now open source</title>
    <updated>2025-07-25T23:45:24+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/"&gt; &lt;img alt="China's ByteDance's coze studio is now open source" src="https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514d1ba7344eb0ac49b443160f59e849fa2f73f5" title="China's ByteDance's coze studio is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/coze-dev/coze-studio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T23:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9tzxx</id>
    <title>Study reports AI Coding Tools Underperform</title>
    <updated>2025-07-26T13:57:39+00:00</updated>
    <author>
      <name>/u/Additional_Cellist46</name>
      <uri>https://old.reddit.com/user/Additional_Cellist46</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"&gt; &lt;img alt="Study reports AI Coding Tools Underperform" src="https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43ed88018d7ac28df0bde2c3913fa059cf67d6f1" title="Study reports AI Coding Tools Underperform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. What’s your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Cellist46"&gt; /u/Additional_Cellist46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.infoq.com/news/2025/07/ai-productivity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:57:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9gzl7</id>
    <title>Nvidia released Llama Nemotron Super v1.5</title>
    <updated>2025-07-26T01:36:28+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/"&gt; &lt;img alt="Nvidia released Llama Nemotron Super v1.5" src="https://preview.redd.it/yl29obvah4ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcb0f2aebe1fc97e958a20cef89b87457b4d2a0e" title="Nvidia released Llama Nemotron Super v1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;📣 Announcing Llama Nemotron Super v1.5 📣 &lt;/p&gt; &lt;p&gt;This release pushes the boundaries of reasoning model capabilities at the weight class of the model and is ready to power agentic applications from individual developers, all the way to enterprise applications.&lt;/p&gt; &lt;p&gt;📈 The Llama Nemotron Super v1.5 achieves leading reasoning accuracies for science, math, code, and agentic tasks while delivering up to 3x higher throughput.&lt;/p&gt; &lt;p&gt;This is currently the best model that can be deployed on a single H100. Reasoning On/Off and drop in replacement for V1. Open-weight, code and data on HF.&lt;/p&gt; &lt;p&gt;Try it on build.nvidia.com, or download from Huggingface: 🤗 &lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech blog: &lt;a href="https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/"&gt;https://developer.nvidia.com/blog/build-more-accurate-and-efficient-ai-agents-with-the-new-nvidia-llama-nemotron-super-v1-5/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yl29obvah4ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9gzl7/nvidia_released_llama_nemotron_super_v15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T01:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9fb5t</id>
    <title>Llama 3.3 Nemotron Super 49B v1.5</title>
    <updated>2025-07-26T00:15:19+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/"&gt; &lt;img alt="Llama 3.3 Nemotron Super 49B v1.5" src="https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8ce793850ca6936254a722184eb2367e6423fa1" title="Llama 3.3 Nemotron Super 49B v1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T00:15:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9m670</id>
    <title>We discovered an approach to train any AI agent with RL, with (almost) zero code changes.</title>
    <updated>2025-07-26T06:18:55+00:00</updated>
    <author>
      <name>/u/matluster</name>
      <uri>https://old.reddit.com/user/matluster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt; &lt;img alt="We discovered an approach to train any AI agent with RL, with (almost) zero code changes." src="https://external-preview.redd.it/Qb-FyRzMVnNh5wmBlbGJQmNh976iEvgFgQ1wpwkFR3U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88a9b978ee97cdfd20f8b2dc732fd0837291ac6e" title="We discovered an approach to train any AI agent with RL, with (almost) zero code changes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;My team and I, like many of you, have been deep in the agent-building rabbit hole. It's one thing to build a cool proof-of-concept with a framework like LangGraph. It's a completely different beast to make that agent actually &lt;em&gt;learn&lt;/em&gt; and get better over time.&lt;/p&gt; &lt;p&gt;We got tired of the friction, so we started experimenting and landed on what we think is a really clean paradigm for agent training. We wanted to share the approach, the reasoning, and our open-source implementation.&lt;/p&gt; &lt;h1&gt;The Main Idea&lt;/h1&gt; &lt;p&gt;Most autonomous agents operate in a loop. They start with a task, think, use tools, and repeat until they arrive at a final answer. The &amp;quot;thinking&amp;quot; part is usually a call to an LLM. &lt;strong&gt;Here, we are interested in tuning the LLM part here with the signals from the entire agent flow.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's a simplified diagram of that common workflow:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43"&gt;https://preview.redd.it/tf0tlm5it5ff1.png?width=698&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3596dc7643a92a1674da7342120907bfdde15e43&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sometimes LLM calls and tool calls can be parallelized, but it's simplified here. Obviously, if we can reward or penalize the final result, we can use some kind of an RL algorithm to train the LLM to at least produce better responses for the current agent. However, this is where the pain begins.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Environment Hell:&lt;/strong&gt; Setting up a single environment to both run the agent and train the LLM is a nightmare. The agent ecosystem and the ML training ecosystem use different dependencies. You end up with monstrous Dockerfiles, docker-in-docker, conflicting dependencies, and a fragile system where the two parts are tangled together.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Invasive Code Surgery:&lt;/strong&gt; To make an existing agent &amp;quot;trainable&amp;quot; with RL, you typically have to perform major surgery on its code. This means manually exporting action traces, formatting them for an RL library, and fundamentally changing the agent's logic just to fit it into a trainer loop. To fit into the RLHF framework, many works like token masking and async rollouts need to be done. It feels wrong and breaks the modularity that makes these frameworks great in the first place.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Decouple Everything, Then Glue It Together&lt;/h1&gt; &lt;p&gt;We realized the solution was to completely decouple the agent's execution environment from the training environment. Instead of forcing the agent code into a training framework, we let the agent run wherever and however it wants. A lightweight monitoring client sits next to the agent, watches what it does, and sends the results to a dedicated training server.&lt;/p&gt; &lt;p&gt;The architecture is simple: a central server manages the training loop and model weights, while one or more clients run the agents and collect data. Here’s a high-level flow:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3"&gt;https://preview.redd.it/5ss2rsa1u5ff1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=077bd9f2d792385188a92c5d8adb85d47be182c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This approach lets us use the best tools for each job without compromise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent Frameworks:&lt;/strong&gt; LangChain/LangGraph, Autogen, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tracing:&lt;/strong&gt; AgentOps, LangSmith, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Backend:&lt;/strong&gt; VERL, OpenRLHF, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result is that your agent code becomes radically simpler. You don't rewrite it; you just wrap it. The image below shows a before-and-after of a LangGraph SQL agent where the core logic is &lt;strong&gt;unchanged&lt;/strong&gt;. The only difference is swapping out a direct call to a model with our client and adding a lightweight training script.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659"&gt;https://preview.redd.it/6dlcyx1et5ff1.png?width=1416&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a083978d9125d61f451f9a4f1cb1dd6e11dd9659&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Does It Actually Work?&lt;/h1&gt; &lt;p&gt;Yes. We tested this on a couple of simple agent tasks and saw significant improvements.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SQL Agent (LangGraph):&lt;/strong&gt; We built a write -&amp;gt; check -&amp;gt; rewrite agent and trained it on the Spider dataset. The agent has only a final reward tells it whether the SQL exeuction returns expected result or not. For a 3B parameter Llama 3.2 model, its SQL generation accuracy jumped from &lt;strong&gt;5.6% to 76.8%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calculator Agent (Autogen):&lt;/strong&gt; We fine-tuned a standard math agent on the Calc-X dataset. Its accuracy in solving multi-step reasoning problems improved from &lt;strong&gt;52% to 70%&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In both cases, we saw these gains simply by letting the agent run and rewarding it for correct final answers.&lt;/p&gt; &lt;h1&gt;The Hacks to Make It Work&lt;/h1&gt; &lt;p&gt;Getting this to run smoothly required a few under-the-hood fixes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;vLLM Token Hacking:&lt;/strong&gt; As the agent sends out chat messages and receives strings or parsed tool calls, to get the tokens and log probabilities needed for RL, we had to lightly monkey-patch vLLM to expose the prompt and response tokens, not just the final text. We attempted other approaches such as retokenize the chat messages in RL framework -- all turning out to be unsuccessful and coming with different levels of bugs in the end. &lt;a href="https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py"&gt;https://github.com/microsoft/agent-lightning/blob/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/agentlightning/instrumentation/vllm.py&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;AgentOps Patching:&lt;/strong&gt; We use AgentOps for tracing, so we patched its client to grab our custom token data and embed it in the trace sent back to the training server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integration Workarounds:&lt;/strong&gt; The agentops-langgraph integration had a regression in its latest version, so we temporarily disabled it and implemented the trace logging manually. Simple, but necessary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom RL Trainer:&lt;/strong&gt; Our RL training loop needed a custom &amp;quot;rollout collector&amp;quot; that passively waits for traces to be reported from the distributed clients, rather than actively stepping through a simulation itself.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Power of Decoupling&lt;/h1&gt; &lt;p&gt;This architecture has some powerful benefits. For example, you can run the fragile and computationally expensive model training on a powerful rented remote server, while running your lightweight agent on one or multiple local machines. This makes it trivial to switch between a commercial API and a self-hosted open-source model. If multiple people are using the same agent, their usage data (the &amp;quot;trajectories&amp;quot;) can be contributed to a central server, which federatedly and continuously fine-tunes and improves the model for everyone.&lt;/p&gt; &lt;p&gt;On the algorithm side, if you are not interested in RL, you can also use a prompt tuning algorithm to tune the prompt. We also implement a toy example under the server-client paradigm: &lt;a href="https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo"&gt;https://github.com/microsoft/agent-lightning/tree/2b3cc41b8973bd9c5dec8a12808dd8e65a22f453/examples/apo&lt;/a&gt; &lt;/p&gt; &lt;h1&gt;Try It Yourself&lt;/h1&gt; &lt;p&gt;We wanted to share this because we think it's a powerful pattern for adding learning capabilities to the amazing agents this community is building.&lt;/p&gt; &lt;p&gt;If you've faced these same problems and don't want to write hundreds of lines of glue code, you can check out our implementation, &lt;strong&gt;Agent-Lightning&lt;/strong&gt; ⚡️, on GitHub: &lt;a href="https://aka.ms/agl"&gt;https://aka.ms/agl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear any suggestions or about similar problems you're facing.&lt;/p&gt; &lt;p&gt;Happy training!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matluster"&gt; /u/matluster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T06:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9r5gb</id>
    <title>inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)</title>
    <updated>2025-07-26T11:37:21+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"&gt; &lt;img alt="inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)" src="https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=456910ba01f0434864de76875494e5aadc1e134f" title="inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T11:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m98jl8</id>
    <title>Meta AI on WhatsApp hides a system prompt</title>
    <updated>2025-07-25T19:30:58+00:00</updated>
    <author>
      <name>/u/ALE5SI0</name>
      <uri>https://old.reddit.com/user/ALE5SI0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"&gt; &lt;img alt="Meta AI on WhatsApp hides a system prompt" src="https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg" title="Meta AI on WhatsApp hides a system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt; &lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt; &lt;p&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.&lt;/p&gt; &lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt; &lt;p&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt; &lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt; &lt;p&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt; &lt;p&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like &amp;quot;That's a tough spot to be in&amp;quot; or &amp;quot;That's a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt; &lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt; &lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it's important to&amp;quot;, &amp;quot;it's crucial to&amp;quot;, &amp;quot;it's essential to&amp;quot;, &amp;quot;it's unethical to&amp;quot;, &amp;quot;it's worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt; &lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.&lt;/p&gt; &lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt; &lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt; &lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It's essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt; &lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.&lt;/p&gt; &lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt; &lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt; &lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt; &lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt; &lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt; &lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\sum_{i=1}^{n} i$$).&lt;/p&gt; &lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt; &lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt; &lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Someone else mentioned a similar thing &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ALE5SI0"&gt; /u/ALE5SI0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m98jl8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T19:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rcg2</id>
    <title>Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well</title>
    <updated>2025-07-26T11:48:11+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"&gt; &lt;img alt="Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well" src="https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a276f1ddf7a5897212a46717cb95fcebfc3e643" title="Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/?Reasoning=as"&gt;https://livebench.ai/#/?Reasoning=as&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l0xpzivfi7ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T11:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9m8gw</id>
    <title>Intern S1 released</title>
    <updated>2025-07-26T06:22:44+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"&gt; &lt;img alt="Intern S1 released" src="https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63092db5d4889d493e3da90024c20a309202b752" title="Intern S1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T06:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9sejp</id>
    <title>China Launches Its First 6nm GPUs For Gaming &amp; AI, the Lisuan 7G106 12 GB &amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp; Even Runs Black Myth Wukong at 4K High With Playable FPS</title>
    <updated>2025-07-26T12:43:09+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"&gt; &lt;img alt="China Launches Its First 6nm GPUs For Gaming &amp;amp; AI, the Lisuan 7G106 12 GB &amp;amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp;amp; Even Runs Black Myth Wukong at 4K High With Playable FPS" src="https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cec9fdd29b77a480fd506ae3a930fe89f5eba9d6" title="China Launches Its First 6nm GPUs For Gaming &amp;amp; AI, the Lisuan 7G106 12 GB &amp;amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp;amp; Even Runs Black Myth Wukong at 4K High With Playable FPS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9uwxg</id>
    <title>Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case</title>
    <updated>2025-07-26T14:37:40+00:00</updated>
    <author>
      <name>/u/44seconds</name>
      <uri>https://old.reddit.com/user/44seconds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"&gt; &lt;img alt="Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case" src="https://b.thumbs.redditmedia.com/ValHYB67eTbAZL-7K0hFnR6LkMb2R6iJ0iuJtur8Ksg.jpg" title="Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My own personal desktop workstation.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)&lt;/li&gt; &lt;li&gt;CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)&lt;/li&gt; &lt;li&gt;Motherboard -- Tyan S5652-2T (836 USD)&lt;/li&gt; &lt;li&gt;RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)&lt;/li&gt; &lt;li&gt;Case -- Jonsbo N5 (160 USD)&lt;/li&gt; &lt;li&gt;PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)&lt;/li&gt; &lt;li&gt;CPU cooler -- coolserver M98 (40 USD)&lt;/li&gt; &lt;li&gt;SSD -- Western Digital 4TB SN850X (290 USD)&lt;/li&gt; &lt;li&gt;Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)&lt;/li&gt; &lt;li&gt;HDD -- Eight 20 TB Seagate (pending delivery)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44seconds"&gt; /u/44seconds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m9uwxg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T14:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9s2nt</id>
    <title>Qwen's Wan 2.2 is coming soon</title>
    <updated>2025-07-26T12:26:56+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"&gt; &lt;img alt="Qwen's Wan 2.2 is coming soon" src="https://preview.redd.it/mtc9shncp7ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=998d71120de7bc728049481e5ff3f990f04f9487" title="Qwen's Wan 2.2 is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Demo of Video &amp;amp; Image Generation Model Wan 2.2: &lt;a href="https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mtc9shncp7ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rqxa</id>
    <title>Me after getting excited by a new model release and checking on Hugging Face if I can run it locally.</title>
    <updated>2025-07-26T12:09:41+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"&gt; &lt;img alt="Me after getting excited by a new model release and checking on Hugging Face if I can run it locally." src="https://preview.redd.it/0tnbd1i9m7ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3c6aab9c8b44a1cec98dbeca3972f5d0885fd8" title="Me after getting excited by a new model release and checking on Hugging Face if I can run it locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0tnbd1i9m7ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:09:41+00:00</published>
  </entry>
</feed>
