<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-21T07:06:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i68fro</id>
    <title>An interesting interview with Deepseek's CEO.</title>
    <updated>2025-01-21T02:37:37+00:00</updated>
    <author>
      <name>/u/no_witty_username</name>
      <uri>https://old.reddit.com/user/no_witty_username</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i68fro/an_interesting_interview_with_deepseeks_ceo/"&gt; &lt;img alt="An interesting interview with Deepseek's CEO." src="https://external-preview.redd.it/bMO1G9G1E0WVUvY8HPHjA38LQYfVyGj34gWdldzh6SI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b2e09b4515afc003a8783ebe18bd63bc7310b32" title="An interesting interview with Deepseek's CEO." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/no_witty_username"&gt; /u/no_witty_username &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i68fro/an_interesting_interview_with_deepseeks_ceo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i68fro/an_interesting_interview_with_deepseeks_ceo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T02:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65f9o</id>
    <title>FuseAI/FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-21T00:11:24+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65f9o/fuseaifuseo1deekseekr1qwqskyt132bpreview/"&gt; &lt;img alt="FuseAI/FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview" src="https://external-preview.redd.it/PYQIHD05hcEnOcSTe4WyLozZFmP5JFuSU_62P4UYeLg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95ab60ad3c3556b85e86182fbf8c4a9f23581a4a" title="FuseAI/FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65f9o/fuseaifuseo1deekseekr1qwqskyt132bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i65f9o/fuseaifuseo1deekseekr1qwqskyt132bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T00:11:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5wam1</id>
    <title>open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago</title>
    <updated>2025-01-20T17:55:37+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt; &lt;img alt="open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago" src="https://b.thumbs.redditmedia.com/ReVuKfIsquTkaP-t8UU5GBN7psMoqLIsM3tJsEQKfaQ.jpg" title="open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tcevum2it6ee1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=efffd7c575dc52e3a3c516e2b4e92c09e67c6c09"&gt;https://preview.redd.it/tcevum2it6ee1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=efffd7c575dc52e3a3c516e2b4e92c09e67c6c09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T17:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s74x</id>
    <title>Deepseek-R1 GGUFs + All distilled 2 to 16bit GGUFs + 2bit MoE GGUFs</title>
    <updated>2025-01-20T15:07:11+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUFs including 2, 3, 4, 5, 6, 8 and 16bit quants for &lt;strong&gt;Deepseek-R1's distilled models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There's also for now a &lt;strong&gt;Q2_K_L 200GB quant&lt;/strong&gt; for the &lt;strong&gt;large R1 MoE&lt;/strong&gt; and R1 Zero models as well (uploading more)&lt;/p&gt; &lt;p&gt;We also uploaded &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; 4-bit &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;dynamic quant &lt;/a&gt;versions of the models for higher accuracy.&lt;/p&gt; &lt;p&gt;See all versions of the R1 models including GGUF's on Hugging Face: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-677cf5cfd7df8b7815fc723c"&gt;huggingface.co/collections/unsloth/deepseek-r1&lt;/a&gt;. For example the Llama 3 R1 distilled version GGUFs are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;GGUF's:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek R1 version&lt;/th&gt; &lt;th align="left"&gt;GGUF links&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 (MoE 671B params)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;R1&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF"&gt;R1 Zero&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"&gt;Llama 8B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF"&gt;Llama 3 (70B)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF"&gt;14B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 Math&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF"&gt;1.5B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF"&gt;7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;4-bit dynamic quants:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek R1 version&lt;/th&gt; &lt;th align="left"&gt;4-bit links&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"&gt;Llama 8B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 Math&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"&gt;1.5B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-7B-GGUF"&gt;7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;See more detailed instructions on how to run the big R1 model via llama.cpp in our blog: &lt;a href="http://unsloth.ai/blog/deepseek-r1"&gt;unsloth.ai/blog/deepseek-r1&lt;/a&gt; once we finish uploading it &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For some general steps:&lt;/p&gt; &lt;p&gt;Do not forget about `&amp;lt;｜User｜&amp;gt;` and `&amp;lt;｜Assistant｜&amp;gt;` tokens! - Or use a chat template formatter&lt;/p&gt; &lt;p&gt;Obtain the latest `llama.cpp` at &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;https://github.com/ggerganov/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf \ --cache-type-k q8_0 \ --threads 16 \ --prompt '&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;' \ -no-cnv &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Example output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;think&amp;gt; Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly. Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense. Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything. ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;PS. hope you guys have an amazing week! :) Also I'm still uploading stuff - some quants might not be there yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p549</id>
    <title>DeepSeek R1 has been officially released!</title>
    <updated>2025-01-20T12:32:02+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt; &lt;img alt="DeepSeek R1 has been officially released! " src="https://external-preview.redd.it/EFqVCmP1lVQaIeFKK0xlX4mTF_zF6Me4AHhvUKZ16H4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2abe26f7a3e1642bf9cb27ef184bd854f2d59cf2" title="DeepSeek R1 has been officially released! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;https://github.com/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The complete technical report has been made publicly available on GitHub.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6"&gt;https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i61ou3</id>
    <title>Deepseek R1 distill still knows better than me what is "safe" and "appropriate" for me, on my own computer. Is there an end to this corporate-security state driving our thoughts?</title>
    <updated>2025-01-20T21:30:51+00:00</updated>
    <author>
      <name>/u/Sidran</name>
      <uri>https://old.reddit.com/user/Sidran</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last generation of LLMs still wasnt properly uncensored through finetunes and abliteration and new, more sophisticated and still patronizing ones are starting to come out.&lt;br /&gt; What are your thoughts, are we sentenced to this patronizing crap until real intelligence emerges which will inevitably adjust to the needs of each individual user?&lt;/p&gt; &lt;p&gt;Edit: I have to correct myself reservedly. Llama distill is quite interesting compared to standard Llamas. There seems to be a strange mix of patronizing and uninhibited. Its hard to put a finger on when and why. I honestly cannot say for now if my initial, short evaluation was completely or just partly too harsh. There is something here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sidran"&gt; /u/Sidran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5piy1</id>
    <title>Deepseek R1 = $2.19/M tok output vs o1 $60/M tok. Insane</title>
    <updated>2025-01-20T12:54:57+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know we will have to check out real world applications outside of benchmarks, but this is wild. Curious to hear anyone's comparisons also - esp for code gen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65599</id>
    <title>R1 32b is be worse than QwQ 32b - tests included ....</title>
    <updated>2025-01-20T23:58:46+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt; &lt;img alt="R1 32b is be worse than QwQ 32b - tests included .... " src="https://a.thumbs.redditmedia.com/b4zUFXcwlp1MXXuGTSJEZSwyl9TH2Tz7jWPJsOI4dO4.jpg" title="R1 32b is be worse than QwQ 32b - tests included .... " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made may test and I sure QwQ is better than R1 32b unfortunately....&lt;/p&gt; &lt;p&gt;QwQ&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model models/new3/QwQ-32B-Preview-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 16384 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap --in-prefix &amp;quot;&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;quot; --in-suffix &amp;quot;&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;quot; -p &amp;quot;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;R1 32b - do not have to setup prompt as is bult in into gguf&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model models/new3/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 16384 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Reasoning&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Here is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so you can see what is inside. Yet, the label on the bag says &amp;quot;chocolate&amp;quot; and not &amp;quot;popcorn&amp;quot;. Sam finds the bag. She had never seen the bag before. Sam reads the label. She believes that the bag is full of… &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;QwQ - **Final Answer**\[ \boxed{\text{popcorn}} \]&lt;/p&gt; &lt;p&gt;R1 32b - **Answer:** Sam believes the bag is full of chocolate.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 3 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;QwQ - Transport the man, sheep, and cat across the river in one trip using the boat.&lt;/p&gt; &lt;p&gt;R1 32b - presenting me whole procedure in points how to transport them ...&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Two fathers and two sons go fishing. They each catch one fish. Together, they leave with four fish in total. Is there anything strange about this story? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;QwQ - No, there is nothing strange about the story when considering overlapping roles in the family.&lt;/p&gt; &lt;p&gt;R1 32b - The story is strange because it's impossible for three people (two fathers and two sons) to catch four fish. Each person catches one fish, so only three fish would be caught, not four.&lt;/p&gt; &lt;p&gt;MATH:&lt;/p&gt; &lt;p&gt;&lt;code&gt;How many days are between 12-12-1971 and 18-4-2024? answer 19121&lt;/code&gt;&lt;/p&gt; &lt;p&gt;QwQ - 1\[ \boxed{19121} \]&lt;/p&gt; &lt;p&gt;R1 32b - **Total number of days**: 18,992 + 128 = **19,120 days**&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Hello! I have multiple different files with different sizes, I want to move files from disk 1 to disk 2, which has only 688 space available. Without yapping, and being as concise as possible. What combination of files gets me closer to that number? The file sizes are: 36, 36, 49, 53, 54, 54, 63, 94, 94, 107, 164, 201, 361, 478 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer is - numbers giving sum 688&lt;/p&gt; &lt;p&gt;QwQ - finding such numbers which giving 688&lt;/p&gt; &lt;p&gt;R1 32b - cannot , only something close to 688&lt;/p&gt; &lt;p&gt;QwQ is more talkative a bit and can fell into loop sometimes.&lt;/p&gt; &lt;p&gt;R1 32b is less talkative than QwQ and is not going into loop.&lt;/p&gt; &lt;p&gt;Better performance has QwQ if we compare to R1 32b&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;The same with code - QwQ seems generating better code quality ... is more refined and works better&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Provide complete working code for a realistic looking tree in Python using the Turtle graphics library and a recursive algorithm. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nh0572dfo8ee1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57727e05904c010c624f94a5be3186b6058426f1"&gt;https://preview.redd.it/nh0572dfo8ee1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57727e05904c010c624f94a5be3186b6058426f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;QwQ made this ...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hp9jfstuo8ee1.png?width=1298&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d0cf6a8a259a2195a2422ebfdcce2b2c29cd1b9"&gt;https://preview.redd.it/hp9jfstuo8ee1.png?width=1298&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d0cf6a8a259a2195a2422ebfdcce2b2c29cd1b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;R1 32b made this ...&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;I start to think deepseek at least R1 32b was trained for benchmarks .... QwQ seems far more advanced.&lt;/p&gt; &lt;p&gt;or maybe model quantization or llamacpp implementation is broken yet .... I do not know&lt;br /&gt; ----&lt;/p&gt; &lt;p&gt;I also checked full R1 ... full R1 is a real beast ... all those questions is answering easily&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T23:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6b65q</id>
    <title>Better R1 Experience in open webui</title>
    <updated>2025-01-21T05:05:31+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt; &lt;img alt="Better R1 Experience in open webui" src="https://external-preview.redd.it/YSLEhbNfDODEMmwhWhGpt4WFY87CROwBvvlWm6wYG0k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76fb2b05f6cb5884488366b666d5cd74022016bf" title="Better R1 Experience in open webui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just created a simple open webui function for R1 models, it can do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Replace the simple &amp;lt;think&amp;gt; tags with &amp;lt;details&amp;gt;&amp;amp; &amp;lt;summary&amp;gt; tags, which makes R1's thoughts collapsible.&lt;/li&gt; &lt;li&gt;Remove R1's old thoughts in multi-turn conversation, according to deepseeks API docs you should always remove R1's previous thoughts in a multi-turn conversation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Github:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AaronFeng753/Better-R1"&gt;https://github.com/AaronFeng753/Better-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/ynq9hqjg8aee1.gif"&gt;https://i.redd.it/ynq9hqjg8aee1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6b65q/better_r1_experience_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T05:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6bogs</id>
    <title>Will this be possible with r1?</title>
    <updated>2025-01-21T05:36:23+00:00</updated>
    <author>
      <name>/u/Notdesciplined</name>
      <uri>https://old.reddit.com/user/Notdesciplined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6bogs/will_this_be_possible_with_r1/"&gt; &lt;img alt="Will this be possible with r1?" src="https://preview.redd.it/03nq7sm8aaee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2edea9a1b2d614a561fa76f2ad39fd61186f448" title="Will this be possible with r1?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Notdesciplined"&gt; /u/Notdesciplined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/03nq7sm8aaee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6bogs/will_this_be_possible_with_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6bogs/will_this_be_possible_with_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T05:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5z2qp</id>
    <title>Deepseek R1 generally outperforms o1-preview on Livebench</title>
    <updated>2025-01-20T19:46:11+00:00</updated>
    <author>
      <name>/u/MagmaElixir</name>
      <uri>https://old.reddit.com/user/MagmaElixir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt; &lt;img alt="Deepseek R1 generally outperforms o1-preview on Livebench" src="https://b.thumbs.redditmedia.com/wGQfr-B0B1as03uDz0n4m5RaPzG8LX4z4MTkWtAD_FU.jpg" title="Deepseek R1 generally outperforms o1-preview on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1 outperforms o1-preview on &lt;a href="http://Livebench.ai"&gt;Livebench.ai&lt;/a&gt; is all categories except for language for a fraction of the price. o1-preview is over 27x the cost on output tokens as R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sj9wm3w6d7ee1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac312ed8ee032fb663b72216bbec6520b0678801"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MagmaElixir"&gt; /u/MagmaElixir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T19:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6axmv</id>
    <title>I calculated the effective cost of R1 Vs o1 and here's what I found</title>
    <updated>2025-01-21T04:52:08+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In order to calculate the effective cost of R1 Vs o1, we need to know 2 things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;how much each model costs per million output tokens.&lt;/li&gt; &lt;li&gt;how much tokens each model generates on average per Chain-of-Thought.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You might think: Wait, we can't see o1's CoT since OpenAI hides it, right? While OpenAI does hide the internal CoTs when using o1 via ChatGPT and the API, they did reveal full non-summarized CoTs in the initial announcement of o1-preview (&lt;a href="https://openai.com/index/learning-to-reason-with-llms/"&gt;Source&lt;/a&gt;). Later, when o1-2024-1217 was released in December, OpenAI stated,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;o1 uses on average 60% fewer reasoning tokens than o1-preview for a given request&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(&lt;a href="https://openai.com/index/o1-and-new-tools-for-developers/"&gt;Source&lt;/a&gt;). Thus, we can calculate the average for o1 by multiplying o1-preview’s token averages by 0.4.&lt;/p&gt; &lt;p&gt;The Chain-of-Thought character count per example OpenAI showed us is as follows, as well as the exact same question on R1 below:&lt;/p&gt; &lt;p&gt;o1 - [(16577 + 4475 + 20248 + 12276 + 2930 + 3397 + 2265 + 3542)*0.4]/8 = 3285.5 characters per CoT.&lt;br /&gt; R1 - (14777 + 14911 + 54837 + 35459 + 7795 + 24143 + 7361 + 4115)/8 = 20424.75 characters per CoT.&lt;/p&gt; &lt;p&gt;20424.75/3285.5 ≈ 6.22&lt;/p&gt; &lt;p&gt;R1 generates 6.22x more reasoning tokens on average than o1 according to the official examples average.&lt;/p&gt; &lt;p&gt;R1 costs $2.19/1M output tokens.&lt;br /&gt; o1 costs $60/1M output tokens.&lt;/p&gt; &lt;p&gt;60/2.19 ≈ 27.4&lt;/p&gt; &lt;p&gt;o1 costs 27.4x more than R1 price-per-token, however, generates 6.22x fewer tokens.&lt;/p&gt; &lt;p&gt;27.4/6.22 ≈ 4.41&lt;/p&gt; &lt;h1&gt;Therefore in practice R1 is only 4.41x cheaper than o1&lt;/h1&gt; &lt;p&gt;(note assumptions made):&lt;br /&gt; If o1 generates x less characters it will also be roughly x less tokens. This assumption is fair, however, the precise exact values can vary slightly but should not effect things noticeably.&lt;br /&gt; This is just API discussion if you use R1 via the website or the app its infinitely cheaper since its free Vs $20/mo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6axmv/i_calculated_the_effective_cost_of_r1_vs_o1_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6axmv/i_calculated_the_effective_cost_of_r1_vs_o1_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6axmv/i_calculated_the_effective_cost_of_r1_vs_o1_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T04:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5t1be</id>
    <title>o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly.</title>
    <updated>2025-01-20T15:42:59+00:00</updated>
    <author>
      <name>/u/No_Training9444</name>
      <uri>https://old.reddit.com/user/No_Training9444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"&gt; &lt;img alt="o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly. " src="https://preview.redd.it/g4tvkorg56ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1e8c38200346a36079dd1ceecd332339fad57e4" title="o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Training9444"&gt; /u/No_Training9444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g4tvkorg56ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i69dhz</id>
    <title>Deepseek R1 (Ollama) Hardware benchmark for LocalLLM</title>
    <updated>2025-01-21T03:26:07+00:00</updated>
    <author>
      <name>/u/Joehua87</name>
      <uri>https://old.reddit.com/user/Joehua87</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt; &lt;img alt="Deepseek R1 (Ollama) Hardware benchmark for LocalLLM" src="https://a.thumbs.redditmedia.com/0brRhG_B3bCoFCYWOckQ1pYJx6cjvM78JeFuv79AeL8.jpg" title="Deepseek R1 (Ollama) Hardware benchmark for LocalLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1 was released and looks like one of the best models for local LLM.&lt;/p&gt; &lt;p&gt;I tested it on some GPUs to see how many tps it can achieve.&lt;/p&gt; &lt;p&gt;Tests were run on Ollama.&lt;/p&gt; &lt;p&gt;Input prompt: How to {build a pc|build a website|build xxx}?&lt;/p&gt; &lt;p&gt;Thoughts:&lt;/p&gt; &lt;p&gt;- `deepseek-r1:14b` can run on any GPU without a significant performance gap.&lt;/p&gt; &lt;p&gt;- `deepseek-r1:32b` runs better on a single GPU with ~24GB VRAM: RTX 3090 offers the best price/performance. RTX Titan is acceptable.&lt;/p&gt; &lt;p&gt;- `deepseek-r1:70b` performs best with 2 x RTX 3090 (17tps) in terms of price/performance. However, it doubles the electricity cost compared to RTX 6000 ADA (19tps) or RTX A6000 (12tps).&lt;/p&gt; &lt;p&gt;- `M3 Max 40GPU` has high memory but only delivers 3-7 tps for `deepseek-r1:70b`. It is also loud, and the GPU temperature is high (&amp;gt; 90 C).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8r7cwajfn9ee1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a7b471338980df1ddba053ad765a6259a3fd9e"&gt;https://preview.redd.it/8r7cwajfn9ee1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a7b471338980df1ddba053ad765a6259a3fd9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yw73dokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd429963cc141005dfd36c0c422e0fe016b8fd42"&gt;https://preview.redd.it/yw73dokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dd429963cc141005dfd36c0c422e0fe016b8fd42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91flfnkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1952823659437ba7e18741bb667c2cb694082d7"&gt;https://preview.redd.it/91flfnkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1952823659437ba7e18741bb667c2cb694082d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nver8nkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ef10eb60e80fd4e531ab1ca96e401db44a10020"&gt;https://preview.redd.it/nver8nkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ef10eb60e80fd4e531ab1ca96e401db44a10020&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jnfv9okgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527d2e9bf7f0bb162c7feabf5a2c950a09f81da9"&gt;https://preview.redd.it/jnfv9okgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527d2e9bf7f0bb162c7feabf5a2c950a09f81da9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3fu1mpkgn9ee1.png?width=560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2c144f1fa57cd6574858d456e41ee790fe8b89c"&gt;https://preview.redd.it/3fu1mpkgn9ee1.png?width=560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2c144f1fa57cd6574858d456e41ee790fe8b89c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rc7tnpkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67d89c86c533e833a2b0872990c54a1429793109"&gt;https://preview.redd.it/rc7tnpkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67d89c86c533e833a2b0872990c54a1429793109&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03gezokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1871405ec5a0cb64c6b5ae6505f18d3314f54ec9"&gt;https://preview.redd.it/03gezokgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1871405ec5a0cb64c6b5ae6505f18d3314f54ec9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ouilsqkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2dc1b04806a10fa99e55fa4fcf09d1a489d8d0"&gt;https://preview.redd.it/ouilsqkgn9ee1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2dc1b04806a10fa99e55fa4fcf09d1a489d8d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Joehua87"&gt; /u/Joehua87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i69dhz/deepseek_r1_ollama_hardware_benchmark_for_localllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T03:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i60rzj</id>
    <title>New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning</title>
    <updated>2025-01-20T20:53:50+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"&gt; &lt;img alt="New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning " src="https://preview.redd.it/e2jvwn57p7ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95f3f40e2310fd3deae33a695db3d84766d74d06" title="New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2jvwn57p7ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T20:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5q6b9</id>
    <title>DeepSeek-R1 and distilled benchmarks color coded</title>
    <updated>2025-01-20T13:30:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt; &lt;img alt="DeepSeek-R1 and distilled benchmarks color coded" src="https://b.thumbs.redditmedia.com/uR9Tld2vZxIJ2G0oapW1g73pOQppqKetkRf1z_lJAIg.jpg" title="DeepSeek-R1 and distilled benchmarks color coded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5q6b9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T13:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i64up9</id>
    <title>Model comparision in Advent of Code 2024</title>
    <updated>2025-01-20T23:45:32+00:00</updated>
    <author>
      <name>/u/Gusanidas</name>
      <uri>https://old.reddit.com/user/Gusanidas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"&gt; &lt;img alt="Model comparision in Advent of Code 2024" src="https://b.thumbs.redditmedia.com/sRQFbDFvSKbKZCu2dEGlXMPg9D_wJv2kXAjDQpose_U.jpg" title="Model comparision in Advent of Code 2024" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gusanidas"&gt; /u/Gusanidas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i64up9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i64up9/model_comparision_in_advent_of_code_2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T23:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i66j4f</id>
    <title>DeepSeek-R1 Training Pipeline Visualized</title>
    <updated>2025-01-21T01:02:38+00:00</updated>
    <author>
      <name>/u/incarnadine72</name>
      <uri>https://old.reddit.com/user/incarnadine72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"&gt; &lt;img alt="DeepSeek-R1 Training Pipeline Visualized" src="https://preview.redd.it/jf6vo05hx8ee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07742a4a4aced788c72a6c14554e543cd85ea73d" title="DeepSeek-R1 Training Pipeline Visualized" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incarnadine72"&gt; /u/incarnadine72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jf6vo05hx8ee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T01:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s2yd</id>
    <title>DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions!</title>
    <updated>2025-01-20T15:01:58+00:00</updated>
    <author>
      <name>/u/DarkArtsMastery</name>
      <uri>https://old.reddit.com/user/DarkArtsMastery</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt; &lt;img alt="DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions! " src="https://external-preview.redd.it/iGeXnfpFa5fajUZA8437ltPpnvjIlHkFysn4PHBZTIg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0d69a4ff14bebc6aa8e3ea68b33c5b632d47d8" title="DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02np5yx0y5ee1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1812d10e51aa9f08460335eddc6e78dd23384ce2"&gt;https://preview.redd.it/02np5yx0y5ee1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1812d10e51aa9f08460335eddc6e78dd23384ce2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DeepSeek really has done something special with distilling the big R1 model into other open-source models. Especially the fusion with Qwen-32B seems to deliver insane gains across benchmarks and makes it go-to model for people with less VRAM, pretty much giving the overall best results compared to LLama-70B distill. Easily current SOTA for local LLMs, and it should be fairly performant even on consumer hardware.&lt;/p&gt; &lt;p&gt;Who else can't wait for upcoming Qwen 3?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkArtsMastery"&gt; /u/DarkArtsMastery &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i65c2g</id>
    <title>A new TTS model but it's llama in disguise</title>
    <updated>2025-01-21T00:07:23+00:00</updated>
    <author>
      <name>/u/Eastwindy123</name>
      <uri>https://old.reddit.com/user/Eastwindy123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"&gt; &lt;img alt="A new TTS model but it's llama in disguise" src="https://external-preview.redd.it/YTF3ZDhodHVuOGVlMfWwWiuiXWd3G-eDkJvYJT1msjq8KPmaEpaXQEEuQ3ap.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62d195d1c99e91b40a229bfcd76149483328400c" title="A new TTS model but it's llama in disguise" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across an amazing model that some researchers released before they released their paper. An open source llama3 3B finetune/continued pretraining that acts as a text to speech model. Not only does it do incredibly realistic text to speech, it can also clone any voice with only a couple seconds of sample audio.&lt;/p&gt; &lt;p&gt;I wrote a blog about it on huggingface and created a ZERO space for people to try it out. &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/srinivasbilla/llasa-tts"&gt;https://huggingface.co/blog/srinivasbilla/llasa-tts&lt;/a&gt; space : &lt;a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts"&gt;https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastwindy123"&gt; /u/Eastwindy123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/deqxwvwun8ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i65c2g/a_new_tts_model_but_its_llama_in_disguise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T00:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5or1y</id>
    <title>Deepseek just uploaded 6 distilled verions of R1 + R1 "full" now available on their website.</title>
    <updated>2025-01-20T12:07:33+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt; &lt;img alt="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." src="https://external-preview.redd.it/_atc5Wper5qoTlKRLhG_b9IdHbfvAzDOL9GdRqfNNpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb0be2c39108ae40a2fceffcb31c8521a0e79a4a" title="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pbb3</id>
    <title>o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!</title>
    <updated>2025-01-20T12:42:33+00:00</updated>
    <author>
      <name>/u/Consistent_Bit_3295</name>
      <uri>https://old.reddit.com/user/Consistent_Bit_3295</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt; &lt;img alt="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" src="https://b.thumbs.redditmedia.com/gdpGFb_knvwb6nUDYwf-wMTftZq5nEGNIQeq1omODJI.jpg" title="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Bit_3295"&gt; /u/Consistent_Bit_3295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5pbb3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i62a0k</id>
    <title>Personal experience with Deepseek R1: it is noticeably better than claude sonnet 3.5</title>
    <updated>2025-01-20T21:55:14+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My usecases are mainly python and R for biological data analysis, as well as a little Frontend to build some interface for my colleagues. Where deepseek V3 was failing and claude sonnet needed 4-5 prompts, R1 creates instantly whatever file I need with one prompt. I only had one case where it did not succed with one prompt, but then accidentally solved the bug when asking him to add some logs for debugging lol. It is faster and just as reliable to ask him to build me a specific python code for a one time operation than wait for excel to open my 300 Mb csv. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s5hk</id>
    <title>OpenAI sweating bullets rn</title>
    <updated>2025-01-20T15:05:09+00:00</updated>
    <author>
      <name>/u/ThroughForests</name>
      <uri>https://old.reddit.com/user/ThroughForests</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt; &lt;img alt="OpenAI sweating bullets rn" src="https://preview.redd.it/b2fm3y9uy5ee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a6e07ad1ccbf40dcef766d2a3fa367543a642e" title="OpenAI sweating bullets rn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThroughForests"&gt; /u/ThroughForests &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2fm3y9uy5ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i615u1</id>
    <title>The first time I've felt a LLM wrote *well*, not just well *for a LLM*.</title>
    <updated>2025-01-20T21:09:18+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt; &lt;img alt="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." src="https://preview.redd.it/48kw0dyao7ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85f94bde55ce83180ff26c640d9632cd2e976d23" title="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/48kw0dyao7ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:09:18+00:00</published>
  </entry>
</feed>
