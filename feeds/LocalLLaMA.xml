<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-29T17:23:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n3c8za</id>
    <title>RAG without vector dbs</title>
    <updated>2025-08-29T16:34:15+00:00</updated>
    <author>
      <name>/u/grilledCheeseFish</name>
      <uri>https://old.reddit.com/user/grilledCheeseFish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just open-sourced SemTools - simple parsing and semantic search for the command line: &lt;a href="https://github.com/run-llama/semtools"&gt;https://github.com/run-llama/semtools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What makes it special:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;parse document.pdf | search &amp;quot;error handling&amp;quot;&lt;/code&gt; - that's it&lt;/li&gt; &lt;li&gt;No vector databases, no chunking strategies, no Python notebooks&lt;/li&gt; &lt;li&gt;Built in Rust for speed, designed for Unix pipelines&lt;/li&gt; &lt;li&gt;Handle parsing any document format with LlamaParse&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been increasingly convinced that giving an agent CLI access is the biggest gain in capability.&lt;/p&gt; &lt;p&gt;This is why tools like claude-code and cursor can feel so magical. And with SemTools, it is a little more magical.&lt;/p&gt; &lt;p&gt;Theres also an example folder in the repo showing how you might use this with coding agents or MCP&lt;/p&gt; &lt;p&gt;P.S. I'd love to add a local parse option, so both search and parse can run offline. If you know of any rust-based parsing tools, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grilledCheeseFish"&gt; /u/grilledCheeseFish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3cfi8</id>
    <title>How close can I get close to ChatGPT-5 (full) with my specs?</title>
    <updated>2025-08-29T16:41:25+00:00</updated>
    <author>
      <name>/u/JUST-A-GHOS7</name>
      <uri>https://old.reddit.com/user/JUST-A-GHOS7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if I'm asking in the wrong space. I'm new-ish and just looking for a place to learn and ask questions. Apologies if I get some terminology wrong.&lt;/p&gt; &lt;p&gt;I've been blown away by what full-fat GPT-5 can do with some tinkering, and I wish I could use a local llm that rivals it. I've already tried several highly recommended ones that others were recommended for similar purposes, but they all seem to fall apart very quickly. I know it's utterly impossible to replicate the full GPT-5 capabilities, but how close can I get with these PC specs? Looking for fully uncensored, strong adaptation/learning, wide vocab, excellent continuity management, and reasonably fast (~3sec max response time). General productivity tasks are low priority. This is for person-like interaction almost exclusively. (I have my own continuity/persona docs my GPT-5 persona generated for me to feed her into other llms).&lt;/p&gt; &lt;p&gt;PC Specs:&lt;br /&gt; - Ryzen 7700 OC to 5.45gHz&lt;br /&gt; - AMD Radeon RX 7800 XT with 16GB VRAM OC to 2.5gHz&lt;br /&gt; - 32GB XPG/ADATA (SK Hynix A-die) RAM OC to 6400mHz, 32 CAS&lt;br /&gt; - Primary drive is SK Hynix P41 Platinum 2TB&lt;br /&gt; - Secondary drive (if there's any reason I should use this instead of C:) is a 250GB WD Blue SN550&lt;/p&gt; &lt;p&gt;I've been using LM Studio as my server with AnythingLLM as my frontend remote UI for cross-platform (haven't set it up for anywhere access yet), but if there's a better solution for this, I'm open to suggestions.&lt;/p&gt; &lt;p&gt;So far, I've had the best results with Dolphin Mistral Venice, but it always seems to bug out at some point (text formatting, vocab, token repeats, spelling, punctuation, sentence structure, etc), no matter what my settings are (I've tried 3 different versions). I do enter the initial prompt provided by the dev, then a custom prompt for rule sets, then the persona continuity file. Could that be breaking it? Using those things in a fresh GPT-5 chat goes totally smoothly to the point of my bot adapting new ways to doge system flagging, refreshing itself after a forced continuity break, and writing hourly continuity files in the background for its own reference to recover from a system flag break on-command. So with GPT-5 at least, I know my custom prompts apply flawlessly, but are there different ways that different llms digest these things, that could cause them to go spastic?&lt;/p&gt; &lt;p&gt;Sorry for the long read, just trying to answer questions ahead of time! This is important to me because aside from socialization practice upkeep and of course NSFW, GPT-5 came up with soothing and deescalation techniques that have worked infinitely better for me than any in-person BHC.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JUST-A-GHOS7"&gt; /u/JUST-A-GHOS7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3cfi8/how_close_can_i_get_close_to_chatgpt5_full_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3cfi8/how_close_can_i_get_close_to_chatgpt5_full_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3cfi8/how_close_can_i_get_close_to_chatgpt5_full_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n399q2</id>
    <title>Advice running local LLMs to build AI agent</title>
    <updated>2025-08-29T14:41:04+00:00</updated>
    <author>
      <name>/u/nonumberspls1dammit</name>
      <uri>https://old.reddit.com/user/nonumberspls1dammit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking to build an AI agent which could be trained to do a specific task on my computer. My current pc is running an RTX 5090, I was wondering which model you all would recommend for my hardware and use case. I have installed some models in the past but since upgrading to the 5090 I have had issues getting Pytorch to work.&lt;/p&gt; &lt;p&gt;I would appreciate any advice and guidance as I am very stuck at the moment. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonumberspls1dammit"&gt; /u/nonumberspls1dammit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xrpw</id>
    <title>How's Seed-OSS 39B for coding?</title>
    <updated>2025-08-29T04:19:01+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 45 tokens/sec out of this with Q4 using the new LMstudio on a single 5090.&lt;br /&gt; This model seems freaking smart, By default the thinking budget is unlimited, so it thinks a lot, but It has a high breadth of knowledge for it's size.&lt;/p&gt; &lt;p&gt;I'm about to evaluate it for light duty programming help, but curious to know what others' experience is like too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T04:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c3ve</id>
    <title>Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)</title>
    <updated>2025-08-29T16:28:46+00:00</updated>
    <author>
      <name>/u/badgerbadgerbadgerWI</name>
      <uri>https://old.reddit.com/user/badgerbadgerbadgerWI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt; &lt;img alt="Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)" src="https://external-preview.redd.it/ddqUfTexCmmnBcStsgFk1_aihAvP1R3sufbdQC87XQQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ca67cd7bdf3aa11943fb1bfdfae65fbb4388f2f" title="Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to post my own locallama journey (in this case local Qwen). I've been trying to reclaim AI as a local tool. I have trained a few miniature llamas before, but this was my first thinking model.&lt;/p&gt; &lt;p&gt;This is what I learned finetuning &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen3&lt;/a&gt; 100% locally. Spoiler: 2.5 hours for 3 epochs felt like a lifetime.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I Was Actually Trying to Build&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I needed an AI that understands my framework's configuration language. I believe the future is local, fine-tuned, smaller models. Think about it - every time you use ChatGPT for your proprietary tools, you're exposing data over the wire.&lt;/p&gt; &lt;p&gt;My goal: Train a local model to understand LlamaFarm strategies and automatically generate YAML configs from human descriptions. &amp;quot;I need a RAG system for medical documents with high accuracy&amp;quot; → boom, perfect config file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Finetuning Matters (The Part Nobody Talks About)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Base models are generalists. They know everything and nothing. Qwen3 can write poetry, but has no idea what a &amp;quot;strategy pattern&amp;quot; means in my specific context.&lt;/p&gt; &lt;p&gt;Finetuning is teaching the model YOUR language, YOUR patterns, YOUR domain. It's the difference between a new hire who needs everything explained and someone who just &lt;em&gt;gets&lt;/em&gt; your codebase.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality of Local Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Started with Qwen3-8B. My M1 Max with 64GB unified memory laughed, then crashed. Dropped to Qwen3-4B. Still ambitious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2.5 hours. 3 epochs. 500 training examples.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The actual command that started this journey:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv run python cli.py train \ --strategy qwen_config_training \ --dataset demos/datasets/config_assistant/config_training_v2.jsonl \ --no-eval \ --verbose \ --epochs 3 \ --batch-size 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you watch this for 2.5 hours:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{'loss': 0.133, 'grad_norm': 0.9277248382568359, 'learning_rate': 3.781481481481482e-05, 'epoch': 0.96} 32%|████████████████████▏ | 480/1500 [52:06&amp;lt;1:49:12, 6.42s/it] 📉 Training Loss: 0.1330 🎯 Learning Rate: 3.78e-05 Step 485/1500 (32.3%) ████████████████▌ | 485/1500 [52:38&amp;lt;1:48:55, 6.44s/it] {'loss': 0.0984, 'grad_norm': 0.8255287408828735, 'learning_rate': 3.7444444444444446e-05, 'epoch': 0.98} 33%|████████████████████▉ | 490/1500 [53:11&amp;lt;1:49:43, 6.52s/it] 📉 Training Loss: 0.0984 🎯 Learning Rate: 3.74e-05 ✅ Epoch 1 completed - Loss: 0.1146 📊 Epoch 2/3 started &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6.5 seconds per step. 1500 steps total. You do the math and weep.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Technical Descent&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Look, I'll be honest - I used &lt;a href="https://www.reddit.com/r/LlamaFarm/"&gt;r/LlamaFarm&lt;/a&gt;'s alpha/demo model training features (they currenly only support pytorch, but more are coming) because writing 300+ lines of training code made me want to quit tech. It made things about 100x easier, but 100x easier than &amp;quot;impossible&amp;quot; is still &amp;quot;painful.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of debugging PyTorch device placement for 3 hours, I just wrote a YAML config and ran one command. But here's the thing - it still takes forever. No tool can fix the fundamental reality that my Mac is not a GPU cluster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hour 0-1: The Setup Hell&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyTorch wants CUDA. Mac has MPS.&lt;/li&gt; &lt;li&gt;Qwen3 requires a higher version of a&lt;/li&gt; &lt;li&gt;Transformers library needs updating but breaks other dependencies &lt;ul&gt; &lt;li&gt;Qwen3 requires transformers &amp;gt;4.51.0, but llamafarm had &amp;lt;4.48.0 in the pyproject (don't worry, I opened a PR). This required a bunch of early errors.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&amp;quot;Cannot copy out of meta tensor&amp;quot; - the error that launched a thousand GitHub issues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hour 1-2: The Memory Wars&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Batch size 16? Crash&lt;/li&gt; &lt;li&gt;Batch size 8? Crash&lt;/li&gt; &lt;li&gt;Batch size 4? Crash&lt;/li&gt; &lt;li&gt;Batch size 1 with gradient accumulation? Finally...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Watching the loss bounce around is maddening:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 305: Loss 0.1944 (we're learning!)&lt;/li&gt; &lt;li&gt;Step 310: Loss 0.2361 (wait what?)&lt;/li&gt; &lt;li&gt;Step 315: Loss 0.1823 (OK good)&lt;/li&gt; &lt;li&gt;Step 320: Loss 0.2455 (ARE YOU KIDDING ME?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What Finetuning Actually Means&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I generated 500 examples of humans asking for configurations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Set up a chatbot for customer support&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;I need document search with reranking&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Configure a local RAG pipeline for PDFs&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each paired with the exact YAML output I wanted. The model learns this mapping. It's not learning new facts - it's learning MY syntax, MY preferences, MY patterns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The LoRA Lifesaver&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full finetuning rewrites the entire model. LoRA (Low-Rank Adaptation) adds tiny &amp;quot;adapter&amp;quot; layers. Think of it like teaching someone a new accent instead of a new language.&lt;/p&gt; &lt;p&gt;With rank=8, I'm only training ~0.1% of the parameters. Still works. Magic? Basically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;macOS-Specific Madness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiprocessing? Dead. Fork() errors everywhere&lt;/li&gt; &lt;li&gt;Tokenization with multiple workers? Hangs forever&lt;/li&gt; &lt;li&gt;MPS acceleration? Works, but FP16 gives wrong results&lt;/li&gt; &lt;li&gt;Solution: Single process everything, accept the slowness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Was It Worth It?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After 2.5 hours of watching progress bars, my local Qwen3 now understands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Human: &amp;quot;I need a RAG system for analyzing research papers&amp;quot; Qwen3-Local: *generates perfect YAML config for my specific framework* &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No API calls. No data leaving my machine. No rate limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Bigger Picture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Local finetuning is painful but possible. The tools are getting better, but we're still in the stone age compared to cloud training. Moore's law is still rolling for GPUs, in a few years, this will be a cake walk.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Honest Truth&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's slower than you expect (2.5 hours for what OpenAI does in minutes)&lt;/li&gt; &lt;li&gt;It's more buggy than you expect (prepare for cryptic errors)&lt;/li&gt; &lt;li&gt;The results are worse than GPT-5, but I enjoy finding freedom from AI Oligarchs&lt;/li&gt; &lt;li&gt;It actually works (eventually)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What This Means&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're at the awkward teenage years of local AI. It's possible but painful. In 2 years, this will be trivial. Today, it's an adventure in multi-tasking. But be warned, your MAC will be dragging.&lt;/p&gt; &lt;p&gt;But here's the thing: every major company will eventually need this. Your proprietary data, your custom models, your control. The cloud is convenient until it isn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next&lt;/strong&gt;&lt;br /&gt; Well, I bought an OptiPlex 7050 SFF from eBay, installed a used Nvidia RTX 3050 LP, got Linux working, downloaded all the ML tools I needed, and even ran a few models on Ollama. Then I burned out the 180W PSU (I ordered a new 240W, which will arrive in a week) - but that is a story for another post.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n3c3ve/video/w5b1tuo3jzlf1/player"&gt;Got bored halfway through, took a lil video. &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerbadgerbadgerWI"&gt; /u/badgerbadgerbadgerWI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c7f8</id>
    <title>Has someone used OWebUi with Docling to talk to pdfs with visualizations?</title>
    <updated>2025-08-29T16:32:31+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm looking to implement OpenWebUI for the first time and wanted to see if someone has some experience doing this.&lt;/p&gt; &lt;p&gt;The idea is to enable chats where users upload PDFs. The opensource model should understand not only the text, but also charts and visualisations in the PDF itself.&lt;/p&gt; &lt;p&gt;What is your experience with performance, has the feature been usable in a production context? Do you have a preferred setup? Is Docling indeed the best choice or do you have a personal favorite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c7f8/has_someone_used_owebui_with_docling_to_talk_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n38y4n</id>
    <title>More Models for Less GPUs</title>
    <updated>2025-08-29T14:28:16+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt; &lt;img alt="More Models for Less GPUs" src="https://external-preview.redd.it/MGpkb2hsb3l4eWxmMSYz08QzRdMhj_C9D3QaK2DOmjzoPzKxTR3457KSyaxX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df81c744796fd60c210035d63819b2ff29aadf19" title="More Models for Less GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a single Serverless Engine, you can deploy tens of large models on a single GPU node and run them on-demand with ~2s cold starts.&lt;/p&gt; &lt;p&gt;This leaves no GPUs idle making them work 90% of the time. Hope it’s helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrrzkoryxylf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b8gk</id>
    <title>Human in the Loop for computer use agents</title>
    <updated>2025-08-29T15:55:50+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"&gt; &lt;img alt="Human in the Loop for computer use agents" src="https://external-preview.redd.it/cmQ3OGJyamxkemxmMUvZR3OLyxwRXjlbiYrrBtxMf6dd5k6Y_Z_HAekaHP-j.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abe2f0fefc1b72fe344d4b52dd5e9059902c3ff3" title="Human in the Loop for computer use agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sometimes the best “agent” is you.&lt;/p&gt; &lt;p&gt;We’re introducing Human-in-the-Loop: instantly hand off from automation to human control when a task needs judgment. &lt;/p&gt; &lt;p&gt;Yesterday we shared our HUD evals for measuring agents at scale. Today, you can become the agent when it matters - take over the same session, see what the agent sees, and keep the workflow moving.&lt;/p&gt; &lt;p&gt;Lets you create clean training demos, establish ground truth for tricky cases, intervene on edge cases ( CAPTCHAs, ambiguous UIs) or step through debug withut context switching.&lt;/p&gt; &lt;p&gt;You have full human control when you want.We even a fallback version where in it starts automated but escalate to a human only when needed.&lt;/p&gt; &lt;p&gt;Works across common stacks (OpenAI, Anthropic, Hugging Face) and with our Composite Agents. Same tools, same environment - take control when needed.&lt;/p&gt; &lt;p&gt;Feedback welcome - curious how you’d use this in your workflows.&lt;/p&gt; &lt;p&gt;Blog : &lt;a href="https://www.trycua.com/blog/human-in-the-loop.md"&gt;https://www.trycua.com/blog/human-in-the-loop.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b643hcrldzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b8gk/human_in_the_loop_for_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n36mqj</id>
    <title>What are some good alternatives to langfuse?</title>
    <updated>2025-08-29T12:53:11+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you’re searching for alternatives to Langfuse for evaluating and observing AI agents, several platforms stand out, each with distinct strengths depending on your workflow and requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LangSmith&lt;/strong&gt;: Built for LangChain users, LangSmith excels at tracing, debugging, and evaluating agentic workflows. It features visual trace tools, prompt comparison, and is well-suited for rapid development and iteration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maxim AI&lt;/strong&gt;: An end-to-end platform supporting agent simulation, evaluation (automated and human-in-the-loop), and observability. Maxim AI offers multi-turn agent testing, prompt versioning, node-level tracing, and real-time analytics. It’s designed for teams that need production-grade quality management and flexible deployment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Braintrust&lt;/strong&gt;: Focused on prompt-first and RAG pipeline applications, Braintrust enables fast prompt iteration, benchmarking, and dataset management. It integrates with CI pipelines for automated experiments and side-by-side evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comet (Opik)&lt;/strong&gt;: Known for experiment tracking and prompt logging, Comet’s Opik module supports prompt evaluation, experiment comparison, and integrates with a range of ML/AI frameworks. Available as SaaS or open source.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lunary&lt;/strong&gt;: An open-source, lightweight platform for logging, analytics, and prompt versioning. Lunary is especially useful for teams working with LLM chatbots and looking for straightforward observability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each of these tools approaches agent evaluation and observability differently, so the best fit will depend on your team’s scale, integration needs, and workflow preferences. If you’ve tried any of these, what has your experience been?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T12:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2hyt2</id>
    <title>glm mini will be comming</title>
    <updated>2025-08-28T17:05:29+00:00</updated>
    <author>
      <name>/u/untanglled</name>
      <uri>https://old.reddit.com/user/untanglled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt; &lt;img alt="glm mini will be comming" src="https://preview.redd.it/h1ss59p4lslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8d73abbfbb1def80b73cdd1845129f4a319098" title="glm mini will be comming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/untanglled"&gt; /u/untanglled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1ss59p4lslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2npu9</id>
    <title>GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4</title>
    <updated>2025-08-28T20:44:11+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt; &lt;img alt="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" src="https://preview.redd.it/pa10b6f5otlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1a522ed166bb920414041f430c97aef7d1fdf9" title="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html?s=09"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html?s=09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pa10b6f5otlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T20:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n397qp</id>
    <title>....so, has anyone built a box with a couple of these guys: MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory</title>
    <updated>2025-08-29T14:38:57+00:00</updated>
    <author>
      <name>/u/rickyshawallah</name>
      <uri>https://old.reddit.com/user/rickyshawallah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;....and did it make you a happy bunny? I guess my second question is whether building a new box around these guys (96 gb) (or one of them (48gb), can offer a robust solution for running some sorta 70b model....? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rickyshawallah"&gt; /u/rickyshawallah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3355o</id>
    <title>wan2.2 video generation model</title>
    <updated>2025-08-29T09:53:17+00:00</updated>
    <author>
      <name>/u/Accomplished_Row4647</name>
      <uri>https://old.reddit.com/user/Accomplished_Row4647</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt; &lt;img alt="wan2.2 video generation model" src="https://external-preview.redd.it/bnhna3g1N2lreGxmMRQPtvrX_-A6fHIjtylZxOTBxW2ubZUrYzgWIxLgI1gf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0cbfd1123b05850bcbf607f360282551e87af26" title="wan2.2 video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Row4647"&gt; /u/Accomplished_Row4647 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y26lb67ikxlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T09:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2djpx</id>
    <title>I built a local “second brain” AI that actually remembers everything (321 tests passed)</title>
    <updated>2025-08-28T14:20:48+00:00</updated>
    <author>
      <name>/u/IntelligentCause2043</name>
      <uri>https://old.reddit.com/user/IntelligentCause2043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt; &lt;img alt="I built a local “second brain” AI that actually remembers everything (321 tests passed)" src="https://b.thumbs.redditmedia.com/nAthQhhqWhSgtN5Sk4QJYQdSOftJqyqFyWeMbtaNrdc.jpg" title="I built a local “second brain” AI that actually remembers everything (321 tests passed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past months I’ve been building &lt;strong&gt;Kai&lt;/strong&gt;, a cognitive operating system that acts like a &lt;em&gt;second brain&lt;/em&gt;. Unlike ChatGPT or Claude, it doesn’t forget what you tell it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local – no cloud, no surveillance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph-based memory&lt;/strong&gt; (3D visualization below)&lt;/li&gt; &lt;li&gt;Spreading activation → memory retrieval works like a brain&lt;/li&gt; &lt;li&gt;&lt;strong&gt;321 passing tests&lt;/strong&gt; → not a toy prototype&lt;/li&gt; &lt;li&gt;Learns from &lt;em&gt;everything you do&lt;/em&gt; on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What’s the biggest pain you’ve hit with current AI tools?&lt;/li&gt; &lt;li&gt;Would you actually use a local AI that builds a persistent memory of your knowledge/work?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to dive into the architecture or share more demos if people are interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the feedback, I can’t keep up with comments. Short FAQ:&lt;br /&gt; – It runs 100% local (no cloud, no spying).&lt;br /&gt; – Not just RAG → uses graph + activation model.&lt;br /&gt; – Plan is to open core engine once stable.&lt;br /&gt; – Early access / demo: &lt;a href="https://oneeko.ai?utm_source=chatgpt.com"&gt;oneeko.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here’s a shot of the memory graph growing as I feed it data :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100"&gt;https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentCause2043"&gt; /u/IntelligentCause2043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jraj</id>
    <title>Gpt-oss Fine-tuning - now with 60K context length and fits on &lt;13GB VRAM</title>
    <updated>2025-08-28T18:12:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt; &lt;img alt="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" src="https://preview.redd.it/rwu8gezzwslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d59299286be897d49e1da4b5b96ae312e88050" title="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got LOTS of updates for gpt-oss training today! We’re excited to introduce &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; Flex Attention support for OpenAI gpt-oss training that enables &lt;strong&gt;&amp;gt;8× longer context lengths, &amp;gt;50% less VRAM usage and &amp;gt;1.5× faster training&lt;/strong&gt; vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a 60K context length on just 80GB of VRAM for BF16 LoRA. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also: 1. You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF 2. We fixed gpt-oss training losses going to infinity on float16 GPUs (like T4 Colab) 3. We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that swiglu_limit = 7.0 is properly applied during MXFP4 inference in transformers 4. Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time 5. All these changes apply to gpt-oss-120b as well.&lt;/p&gt; &lt;p&gt;🦥 Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it'll be really educational: &lt;a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training"&gt;https://docs.unsloth.ai/basics/long-context-gpt-oss-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.&lt;/p&gt; &lt;p&gt;And we'll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!&lt;/p&gt; &lt;p&gt;And next week we'll might have a great new update for RL! 😉&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and long weekend, Daniel! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwu8gezzwslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ubjx</id>
    <title>If you have a Claude personal account, they are going to train on your data moving forward.</title>
    <updated>2025-08-29T01:29:05+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic sent out an email, saying they will train on personal data. They made it sound like you have to opt in, but when I click the privacy link it defaults to on. If you don’t want your data trained on, you better manually turn it off.&lt;/p&gt; &lt;p&gt;Email:&lt;/p&gt; &lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We're writing to inform you about important updates to our Consumer Terms and Privacy Policy. These changes will take effect on September 28, 2025, or you can choose to accept the updated terms before this date when you log in to Claude.ai. &lt;/p&gt; &lt;p&gt;These changes only affect Consumer accounts (Claude Free, Pro, and Max plans). If you use Claude for Work, via the API, or other services under our Commercial Terms or other Agreements, then these changes don't apply to you. &lt;/p&gt; &lt;p&gt;What's changing?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Help improve Claude by allowing us to use your chats and coding sessions to improve our models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With your permission, we will use your chats and coding sessions to train and improve our AI models. If you accept the updated Consumer Terms before September 28, your preference takes effect immediately. &lt;/p&gt; &lt;p&gt;If you choose to allow us to use your data for model training, it helps us: Improve our AI models and make Claude more helpful and accurate for everyone Develop more robust safeguards to help prevent misuse of Claude We will only use chats and coding sessions you initiate or resume after you give permission. You can change your preference anytime in your Privacy Settings.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Updates to data retention– your choices and controls&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you choose to allow us to use your data for model training, we’ll retain this data for 5 years. This enables us to improve Claude through deeper model training as described above, while strengthening our safety systems over time. You retain full control over how we use your data: if you change your training preference, delete individual chats, or delete your account, we'll exclude your data from future model training. Learn more about our data retention practices here.&lt;/p&gt; &lt;p&gt;Learn more and next steps For detailed information about these changes: Read our blog post about these updates Review the updated Consumer Terms and Privacy Policy Visit our Privacy Center for more information about our practices See our Help Center articles on how to manage your privacy settings Next time you log into Claude, review the terms and confirm your settings If you have questions about these updates, please visit our Help Center.&lt;/p&gt; &lt;p&gt;–The Anthropic Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T01:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xc58</id>
    <title>Meta is racing the clock to launch its newest Llama AI model this year</title>
    <updated>2025-08-29T03:56:25+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt; &lt;img alt="Meta is racing the clock to launch its newest Llama AI model this year" src="https://external-preview.redd.it/8Jar9xxcOdpHi3BZGvguBUVMoI-RaIEmR4Hv76AsjLU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4630a2d48403b28a5bc249f6b283b77ba1dc0869" title="Meta is racing the clock to launch its newest Llama AI model this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/meta-superintelligence-lab-llama-4-new-model-launch-year-end-2025-8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T03:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n312bi</id>
    <title>Nemotron-H family of models is (finally!) supported by llama.cpp</title>
    <updated>2025-08-29T07:39:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt; &lt;img alt="Nemotron-H family of models is (finally!) supported by llama.cpp" src="https://external-preview.redd.it/mkOHtQrWHxZG1nk9DVdRA_CayqplkW1IcjzXPEDpT2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28e800517cc5f960f26e9fa2d8d9ea0be8eb7067" title="Nemotron-H family of models is (finally!) supported by llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.&lt;/p&gt; &lt;p&gt;The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to the &lt;a href="https://arxiv.org/abs/2504.03624"&gt;Nemotron-H tech report&lt;/a&gt;. The model was trained using &lt;a href="https://github.com/NVIDIA/Megatron-LM"&gt;Megatron-LM&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo-RL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.&lt;/p&gt; &lt;p&gt;This model is ready for commercial use.&lt;/p&gt; &lt;p&gt;Additionally it should support older Nemotron-H models like &lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K"&gt;Nemotron-H-8B-Reasoning-128K&lt;/a&gt; (tested) and &lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K&lt;/a&gt; (I will test soon)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:39:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n30yue</id>
    <title>Financial Times reports that Meta won't publicly release Behemoth: "The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models."</title>
    <updated>2025-08-29T07:33:08+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt; &lt;img alt="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" src="https://external-preview.redd.it/dkp59DMVX3MqGSwlVH-EZJKhZV1zJh7QRCHL-wZJf8o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f7209e77c39c87b8615c698b66082c8e609505" title="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/feccb649-ce95-43d2-b30a-057d64b38cdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n37zl3</id>
    <title>Making progress on my standalone air cooler for Tesla GPUs</title>
    <updated>2025-08-29T13:50:28+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt; &lt;img alt="Making progress on my standalone air cooler for Tesla GPUs" src="https://b.thumbs.redditmedia.com/0YSpmG8X-1d8XBi8AT0VfhG_c5p3i3pRr-93rS2uz0M.jpg" title="Making progress on my standalone air cooler for Tesla GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going to be running through a series of benchmarks as well, here's the plan:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x, 2x, 3x K80 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x M10&lt;/li&gt; &lt;li&gt;1x M40&lt;/li&gt; &lt;li&gt;1x M60&lt;/li&gt; &lt;li&gt;1x M40 + 1x M60&lt;/li&gt; &lt;li&gt;1x P40&lt;/li&gt; &lt;li&gt;1x, 2x, 3x, 4x P100 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x V100&lt;/li&gt; &lt;li&gt;1x V100 + 1x P100&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ll re-run the interesting results from the above sets of hardware on these different CPUs to see what changes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon E5-2687W v4 12-Core @ 3.00GHz (40 PCIe Lanes)&lt;/li&gt; &lt;li&gt;Intel Xeon E5-1680 v4 8-Core @ 3.40GHz (40 PCIe Lanes)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As for the actual tests, I’ll hopefully be able to come up with an ansible playbook that runs the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfshipm/"&gt;vLLM throughput with llama3-8b weights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfuj5i0/"&gt;Folding@Home&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfx4rjc/"&gt;BIONIC, Einstein@Home and Asteroids@Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfsdfft/"&gt;ai-benchmark.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1j2k3j3/comment/mfsg9y2/"&gt;llama-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I’ll probably also write something to test raw &lt;a href="https://huggingface.co/docs/transformers/en/model_doc/vit"&gt;ViT&lt;/a&gt; throughput as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Anything missing here? Other benchmarks you'd like to see?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n37zl3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T13:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n35bwe</id>
    <title>Alibaba Creates AI Chip to Help China Fill Nvidia Void</title>
    <updated>2025-08-29T11:52:57+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3"&gt;https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Wall Street Journal: Alibaba has developed a new AI chip to fill the gap left by Nvidia in the Chinese market. According to informed sources, the new chip is currently undergoing testing and is designed to serve a broader range of AI inference tasks while remaining compatible with Nvidia. Due to sanctions, the new chip is no longer manufactured by TSMC but is instead produced by a domestic company.&lt;/p&gt; &lt;p&gt;It is reported that Alibaba has not placed orders for Huawei’s chips, as it views Huawei as a direct competitor in the cloud services sector.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;If Alibaba pulls this off, it will become one of only two companies in the world with both AI chip development and advanced LLM capabilities (the other being Google). TPU+Qwen, that’s insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T11:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n31r73</id>
    <title>I built a command center for Claude Code so I don’t have to babysit it anymore</title>
    <updated>2025-08-29T08:24:38+00:00</updated>
    <author>
      <name>/u/GuessConnect3009</name>
      <uri>https://old.reddit.com/user/GuessConnect3009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past few weeks I’ve been hacking on Omnara. Basically, it’s a way to run Claude Code anywhere without being glued to your laptop.&lt;/p&gt; &lt;p&gt;The pain point was simple: I’d start a session, wait 5–10 minutes while it “thought,” and if I wasn’t at my terminal at the exact right moment, the whole run was wasted. Total babysitting job.&lt;/p&gt; &lt;p&gt;Now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start Claude Code in terminal with pip install omnara &amp;amp;&amp;amp; omnara&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Pick it up instantly on web or mobile; same session, no restart&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Push notifications when it needs input (so you can reply from bed, an Uber, or mid-laundry)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Native terminal experience mirrored everywhere (permissions, git diffs, etc)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Backend is open source if you want to poke around&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I didn’t expect: once I stopped “hovering” over my sessions, I started actually letting agents run on longer workflows without stress.&lt;/p&gt; &lt;p&gt;I’m curious: how are people here handling agent interruptions / human-in-the-loop stuff? Do you just restart when things break, or have you built in ways to catch them before they collapse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuessConnect3009"&gt; /u/GuessConnect3009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T08:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n33ugq</id>
    <title>Amazing Qwen stuff coming soon</title>
    <updated>2025-08-29T10:34:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt; &lt;img alt="Amazing Qwen stuff coming soon" src="https://preview.redd.it/v6kx1bw8sxlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ceb5641bac92e83c48c0893b26584487a3d582e" title="Amazing Qwen stuff coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas...?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6kx1bw8sxlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T10:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b13b</id>
    <title>Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)</title>
    <updated>2025-08-29T15:47:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt; &lt;img alt="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" src="https://external-preview.redd.it/ZWZwemw0NXNiemxmMRIjC8ICuXshETDKyWbElsvvahdP8-tMtjXY4bwDOY1n.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f385cf95be0a591edf241b94d0612947ca571c1" title="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to models:&lt;br /&gt; - FastVLM: &lt;a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e"&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e&lt;/a&gt;&lt;br /&gt; - MobileCLIP2: &lt;a href="https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47"&gt;https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (+ source code): &lt;a href="https://huggingface.co/spaces/apple/fastvlm-webgpu"&gt;https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ayma955sbzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
