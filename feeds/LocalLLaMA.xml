<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-01T00:28:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j0cmfn</id>
    <title>Contemplating the Radeon 9070 32GB</title>
    <updated>2025-02-28T17:05:03+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, AMD today released the 9070 and 9070 XT with &lt;strong&gt;624 GB/s&lt;/strong&gt; memory bandwidth and 16GB DDR6 VRAM (256 bit). There are &lt;em&gt;still&lt;/em&gt; rumors about upcoming cards with 32GB of DDR6 memory. These would cost an extra 250-300€ or USD, so the cards would be slightly less than 1000€.&lt;/p&gt; &lt;p&gt;Let's assume that these cards indeed make it to the market and they're based on the 9070 which draws 220W. What does this offer us?&lt;/p&gt; &lt;p&gt;We could add 32GB of VRAM per 2-slot GPU. The VRAM would be 2.43x faster than the new AMD Ryzen AI Max+ 395 PCs like the Framework Desktop which manages &lt;strong&gt;256GB/s&lt;/strong&gt; with its Quad channel LPDDR5X-8000. The RAM would be slower than the &lt;strong&gt;936 GB/s&lt;/strong&gt; of a RTX 3090 24GB with 384bit DDR6X. The price per GB of VRAM would be similar to that of a used RTX 3090 24GB (assuming a price of 720€).&lt;/p&gt; &lt;p&gt;The cost of a system with 128GB VRAM would be around 4000€ for the 4 GPUs plus around 3000€ for the EPYC system that provides enough PCIe 5.0 lanes. (for example, EPYC 9115 16 core cpu for around 940€ and a ASRock Rack GENOAD8X-2T/BCM mainboard with 7 PCIe slots for around 1260€).&lt;/p&gt; &lt;p&gt;We end up with a system that is likely to be around &lt;strong&gt;2.4x faster&lt;/strong&gt; during inference, but also &lt;strong&gt;3x more expensive&lt;/strong&gt; than a Framework Desktop system, with a significantly higher power draw (probably around 1100 Watts). Given some extra budget we could plug in more than 4 GPUs into the mainboard (using PCIe extenders) to add even more VRAM, that's something you can't do with the current generation of AMD AI systems. With 6 GPUs we have 192GB of VRAM. Pretty enticing. Until now, getting more than 24GB of VRAM on a card has meant spending thousands of dollars per card or getting something rather obsolete.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cmfn/contemplating_the_radeon_9070_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cmfn/contemplating_the_radeon_9070_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cmfn/contemplating_the_radeon_9070_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0jwtn</id>
    <title>Jetson Nano + 2 ESP32s = X-Ray vision</title>
    <updated>2025-02-28T22:16:31+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So they use wifi signals to bounce off walls and such to a receiver and detect people/etc through walls, lol. Not my repo just saw and guess I have to go get a Nano now?? ha&lt;/p&gt; &lt;p&gt;Code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Timothy2105/theia"&gt;https://github.com/Timothy2105/theia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Article:&lt;/p&gt; &lt;p&gt;&lt;a href="https://0xredj.medium.com/how-we-built-a-vr-headset-that-sees-people-through-walls-treehacks-25-e517cb805b9c"&gt;https://0xredj.medium.com/how-we-built-a-vr-headset-that-sees-people-through-walls-treehacks-25-e517cb805b9c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0jwtn/jetson_nano_2_esp32s_xray_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0jwtn/jetson_nano_2_esp32s_xray_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0jwtn/jetson_nano_2_esp32s_xray_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T22:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00pjp</id>
    <title>Is it not possible for NVIDIA to make VRAM extensions for other PCIE slots? Or other dedicated AI hardware?</title>
    <updated>2025-02-28T05:42:32+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it not possible for NVIDIA to make a new (or old idk) kind of hardware to just expand your vram?&lt;/p&gt; &lt;p&gt;I'm assuming the PCIE slots carry the same data speeds but if this is not possible at all, i will ask could NVIDIA then make a dedicated AI module rather than a graphics card?&lt;/p&gt; &lt;p&gt;Seems like the market for such a thing might not be huge but couldn't they do a decent markup and make them in smaller batches?&lt;/p&gt; &lt;p&gt;Just seems like 32gb vram is pretty small for the storage options we have today? But idk maybe the speeds they operate at are much more expensive to make?&lt;/p&gt; &lt;p&gt;Very curious to see in the future if we get actual AI hardware or we just keep working off what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00pjp/is_it_not_possible_for_nvidia_to_make_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1izf4zf</id>
    <title>Pythagoras : i should've guessed first hand 😩 !</title>
    <updated>2025-02-27T12:59:00+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt; &lt;img alt="Pythagoras : i should've guessed first hand 😩 !" src="https://preview.redd.it/m3vrfaz8jole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e5f34f13dda8627c1b31cbceebdf6cfb503c19e" title="Pythagoras : i should've guessed first hand 😩 !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3vrfaz8jole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00wiz</id>
    <title>LongRoPE2: Near-Lossless LLM Context Window Scaling</title>
    <updated>2025-02-28T05:55:04+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.20082"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00wiz/longrope2_nearlossless_llm_context_window_scaling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00wiz/longrope2_nearlossless_llm_context_window_scaling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0a2g0</id>
    <title>Open source knowledge base llm chat application?</title>
    <updated>2025-02-28T15:19:52+00:00</updated>
    <author>
      <name>/u/kohlerm</name>
      <uri>https://old.reddit.com/user/kohlerm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for an open source application with the following features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Be able to define several knowledge bases, each of them defined by a set of documents&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Be able to ask questions/ chat about the knowledge base &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The answer needs to contain references to the knowledge base &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Use configurable LLMs,including local ones (preferably on Macs ATM) &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Basically it should be quite similar to notebooklm by Google, i just do not need the audio/podcast features. &lt;/p&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kohlerm"&gt; /u/kohlerm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0a2g0/open_source_knowledge_base_llm_chat_application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0a2g0/open_source_knowledge_base_llm_chat_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0a2g0/open_source_knowledge_base_llm_chat_application/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T15:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1izt03h</id>
    <title>I have to share this with you - Free-Form Chat for writing, 100% local</title>
    <updated>2025-02-27T22:55:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt; &lt;img alt="I have to share this with you - Free-Form Chat for writing, 100% local" src="https://preview.redd.it/7781ripihrle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=503141823b634142caf9d0103c96974965baa8ef" title="I have to share this with you - Free-Form Chat for writing, 100% local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7781ripihrle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T22:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1izwh49</id>
    <title>DeepSeek OpenSourceWeek Day 5</title>
    <updated>2025-02-28T01:45:14+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fire-Flyer File System (3FS)&lt;/p&gt; &lt;p&gt;Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.&lt;/p&gt; &lt;p&gt;⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster.&lt;/p&gt; &lt;p&gt;⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster.&lt;/p&gt; &lt;p&gt;⚡ 40+ GiB/s peak throughput per client node for KVCache lookup.&lt;/p&gt; &lt;p&gt;🧬 Disaggregated architecture with strong consistency semantics.&lt;/p&gt; &lt;p&gt;✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp;amp; KVCache lookups for inference in V3/R1.&lt;/p&gt; &lt;p&gt;🔗 3FS → &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Smallpond - data processing framework on 3FS → &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j08glt</id>
    <title>Is LLM based Learning Really Usefull?</title>
    <updated>2025-02-28T14:07:28+00:00</updated>
    <author>
      <name>/u/Remarkable-Ad723</name>
      <uri>https://old.reddit.com/user/Remarkable-Ad723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow Redditors,&lt;/p&gt; &lt;p&gt;I’m a Software Engineer looking to upskill, and I’ve been exploring different ways to learn effectively. With LLM-powered tools like ChatGPT, Claude, Gemini, and various AI-driven learning platforms, it feels like we’re entering a new era of AI based learning. These tools look promising when it comes to breaking down complex topics in simple terms, generating some exercises, and even providing feedback on our understanding.&lt;/p&gt; &lt;p&gt;But I’m wondering—how effective are these tools really? Have any of you successfully used AI tools to learn new skills, prepare for exams, or level up in your careers? Or do you think traditional methods (books, courses, hands-on practice) are still the best way to go?&lt;/p&gt; &lt;p&gt;Would love to hear your experiences—what worked, what didn’t, and whether AI can be trusted as a learning tool.&lt;/p&gt; &lt;p&gt;Looking forward to your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Ad723"&gt; /u/Remarkable-Ad723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j08glt/is_llm_based_learning_really_usefull/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j08glt/is_llm_based_learning_really_usefull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j08glt/is_llm_based_learning_really_usefull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T14:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1izoyxk</id>
    <title>A diffusion based 'small' coding LLM that is 10x faster in token generation than transformer based LLMs (apparently 1000 tok/s on H100)</title>
    <updated>2025-02-27T20:01:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Karpathy post: &lt;a href="https://xcancel.com/karpathy/status/1894923254864978091"&gt;https://xcancel.com/karpathy/status/1894923254864978091&lt;/a&gt; (covers some interesting nuance about transformer vs diffusion for image/video vs text)&lt;/p&gt; &lt;p&gt;Artificial analysis comparison: &lt;a href="https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig"&gt;https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo video: &lt;a href="https://xcancel.com/InceptionAILabs/status/1894847919624462794"&gt;https://xcancel.com/InceptionAILabs/status/1894847919624462794&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The chat link (down rn, probably over capacity) &lt;a href="https://chat.inceptionlabs.ai/"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's interesting here is that this thing generates all tokens at once and then goes through refinements as opposed to transformer based one token at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0b6xn</id>
    <title>CohereForAI/c4ai-command-r7b-arabic-02-2025 · Hugging Face</title>
    <updated>2025-02-28T16:07:08+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"&gt; &lt;img alt="CohereForAI/c4ai-command-r7b-arabic-02-2025 · Hugging Face" src="https://external-preview.redd.it/73KAWnSegMjFP_fsyR6QSQ02fnqaTKaAAHSwlrGelw8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57eecf29ad1f482ee744a918c100afc4a28bea9f" title="CohereForAI/c4ai-command-r7b-arabic-02-2025 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1izy4by</id>
    <title>2 diffusion LLMs in one day -&gt; don't undermine the underdog</title>
    <updated>2025-02-28T03:12:18+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, its awesome that we're getting frequent and amazing model releases - seemingly by the day right now. &lt;/p&gt; &lt;p&gt;Inception labs released&lt;a href="https://www.inceptionlabs.ai/news"&gt; mercury coder&lt;/a&gt;, a (by my testing) somewhat competent model which can code on a 1 to 2 year old SOTA (as good as the best models 1-2 years ago), having the benefit of being really cool to see the diffusion process. Really scratches an itch (perhaps one of some interpretability?). &lt;strong&gt;Promises 700-1000 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reason why I say time period instead of model - it suffers from many of the same issues I remember GPT 4 (and turbo) suffering from. You should check it out anyways.&lt;/p&gt; &lt;p&gt;And, for some reason on the same day (at least model weights uploaded, preprint earlier), we get &lt;a href="https://arxiv.org/pdf/2502.09992"&gt;LLaDA&lt;/a&gt;, an open-source diffusion model which seems to be somewhat of a contender for llama 3 8b with benchmarks, and gives some degree of freedom in terms of guiding (not forcing, sometimes doesn't work) the nth word to be a specified one. I found the quality in the &lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;demo &lt;/a&gt;to be much worse than any recent models, but I also noticed it improved a TON as I played around and adjusted certain prompting (and word targets, really cool). Check this out too - its different from mercury. &lt;/p&gt; &lt;p&gt;TLDR; 2 cool new diffusion-based LLMs, a closed-source one comparable to GPT-4 (based on my vibe checking) promising 700-1000 t/s (technically 2 different models by size), and an open-source one reported to be LLaMa3.1-8b-like, but testing (again, mine only) shows more testing is needed lol.&lt;/p&gt; &lt;p&gt;Don't let the open source model be overshadowed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T03:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0l2kp</id>
    <title>OpenArc upcoming olmoOCR, Qwen2-VL support</title>
    <updated>2025-02-28T23:08:30+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I have been talking about &lt;a href="https://github.com/SearchSavior"&gt;OpenArc&lt;/a&gt; a lot this week. Even have a release planned this weekend.&lt;/p&gt; &lt;p&gt;Right now I am excited because I was just able to convert &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview"&gt;olmOCR-7B-0225-preview&lt;/a&gt; to OpenVINO format!!! Weights will be posted this weekend and OpenArc will support serving eventually with the full gamut of Qwen2-VL input/output. For those who don't know, this is a pretty big deal; olmoOCR seems very powerful for document analysis and Qwen2-VL can be tooled to be a very effect edge compute vision agent on top of it's own image vision capabilities. Pair that with omniparser or paddle... we'll have capable local intel vsion agents very soon. Not sooner than text only however.&lt;/p&gt; &lt;p&gt;Holy crap this is very exciting do a lot of work with CPU only OCR and am very familair with the literature in this area- Qwen2-VL was my first major llm project. For reference, on my 2x xeon 6242 server I was getting ~7t/s for 100 dpi images with Qwen2-VL-7B which was 7x faster than full precision; I replicated their accuracy int4 results from the qwen 2 vl paper with openvino int4 in my work on dense table analysis.&lt;/p&gt; &lt;p&gt;Anyway, stay tuned for text only, a gradio dashboard with baked in documentation and openai endpoints this weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T23:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d54k</id>
    <title>I a designing intent_blocks to solve the dreaded problem of how much context should I send to an LLM, especially in a multi-turn conversations. Your feedback would help</title>
    <updated>2025-02-28T17:26:14+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One dreaded and underrated aspect about building RAG apps is to figure out how and when to rephrase the last user query so that you can improve retrieval. For example:&lt;/p&gt; &lt;p&gt;User: Tell me about all the great accomplishments of George Washington&lt;br /&gt; Assistant: &amp;lt;some response about George Washington&amp;gt;&lt;br /&gt; User: what about his siblings?&lt;/p&gt; &lt;p&gt;Now if you only look at the last user query your retrieval system will return junk because it doesn’t under stand “this”. You could pass the full history then your response would at best include both the accomplishments of GW and his siblings or worse be inaccurate. The other approach is send the full context to an LLM and ask it to rephrase or re-write the last user prompt so that the intent is fully represented in it. This is generally slow, excessive in token costs, and hard to debug if things go wrong - but does have higher chances of success.&lt;/p&gt; &lt;p&gt;A couple of &lt;a href="https://github.com/katanemo/archgw"&gt;releases ago&lt;/a&gt; I added support for multi-turn detection in archgw, where I would extract critical information (relation=siblings, person=George Washington) in a multi-turn scenario and route to an endpoint that was expecting these parameters to improve retrieval. This works fine but requires developers to define usage patterns more precisely. It’s not abstract enough to handle more nuanced retrieval scenarios.&lt;/p&gt; &lt;p&gt;So now I am designing &amp;lt;intent-blocks&amp;gt;: essentially markers applied to messages history that would indicate on what messages and assistant responses are related as metadata of the conversational history, which can then be used to rephrase the last query to improve retrieval. This means you can ignore certain blocks because they are not related and improve speed, cost and retrieval accuracy&lt;/p&gt; &lt;p&gt;Would this be useful to you? How do you go about solving this problem today? How else would you like for me to improve the designs to accommodate your needs? 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d2xo</id>
    <title>MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!</title>
    <updated>2025-02-28T17:23:41+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"&gt; &lt;img alt="MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!" src="https://external-preview.redd.it/nDrA4WA4Kllguv9k8ov6r62GHYnppkRZHGSgvQCCQm4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ba35156b1076a74a3437bd13d8bec19e91a359e" title="MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/PmJGlafRqpI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1izvwck</id>
    <title>DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp; smallpond (A lightweight data processing framework)</title>
    <updated>2025-02-28T01:15:12+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt; &lt;img alt="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" src="https://external-preview.redd.it/HvC95tBfvHDGJxAbUH6W9PmwC54Tm2U3z7QQDPE9EaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f064ebf3053086f57b27efe553869e937081d60d" title="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't believe DeepSeek has even revolutionized storage architecture... The last time I was amazed by a network file system was with HDFS and CEPH. But those are disk-oriented distributed file systems. Now, a truly modern SSD and RDMA network-oriented file system has been born!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3FS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;smallpond&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lightweight data processing framework built on &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; and &lt;a href="https://github.com/deepseek-ai/3FS"&gt;3FS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82"&gt;https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0kgyn</id>
    <title>99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900</title>
    <updated>2025-02-28T22:41:24+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt; &lt;img alt="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" src="https://external-preview.redd.it/HFws3DDkcEP1xVBovP3WDu-ptb9oIschwqz-M_5LaEc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1b52e8ca906335c7d16f3c20d0abb029388892" title="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ZWBQPKc.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T22:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00v4y</id>
    <title>"Crossing the uncanny valley of conversational voice" post by Sesame - realtime conversation audio model rivalling OpenAI</title>
    <updated>2025-02-28T05:52:31+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So this is one of the craziest voice demos I've heard so far, and they apparently want to release their models under an Apache-2.0 license in the future: I've never heard of Sesame, they seem to be very new.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Our models will be available under an Apache 2.0 license&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Your thoughts? Check the demo first: &lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No public weights yet, we can only dream and hope, but this easily matches or beats OpenAI's Advanced Voice Mode. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0gs1g</id>
    <title>AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play</title>
    <updated>2025-02-28T20:00:12+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt; &lt;img alt="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" src="https://external-preview.redd.it/GK3DdqrCYGvJGL-l88rd-LZtTxw8isk9OD7o602ntRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20e1333c4dbdf7f381d26b7f87d3f296cf306559" title="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-Vulkan-SPIR-V-Wide-AI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T20:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j088yg</id>
    <title>RX 9070 XT Potential performance discussion</title>
    <updated>2025-02-28T13:57:36+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As some of you might have seen, AMD just revealed the new RDNA 4 GPUS. RX 9070 XT for $599 and RX 9070 for $549&lt;/p&gt; &lt;p&gt;Looking at the numbers, 9070 XT offers &amp;quot;2x&amp;quot; in FP16 per compute unit compared to 7900 XTX [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;], so at 64U vs 96U that means RX 9070 XT would have 33% compute uplift.&lt;/p&gt; &lt;p&gt;The issue is the bandwitdh - at 256bit GDDR6 we get ~630GB/s compared to 960GB/s on a 7900 XTX.&lt;/p&gt; &lt;p&gt;BUT! According to the same presentation [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;] they mention they've added INT8 and INT8 &lt;em&gt;with sparsity&lt;/em&gt; computations to RDNA 4, which make it 4x and 8x faster than RDNA 3 &lt;em&gt;per unit&lt;/em&gt;, which would make it 2.67x and 5.33x times faster than RX 7900 XTX.&lt;/p&gt; &lt;p&gt;I wonder if newer model architectures that are less limited by memory bandwidth could use these computations and make new AMD GPUs great inference cards. What are your thoughts?&lt;/p&gt; &lt;p&gt;EDIT: Updated links after they cut the video. Both are now the same, originallly I quoted two different parts of the video.&lt;/p&gt; &lt;p&gt;EDIT2: I missed it, but hey also mention 4-bit tensor types!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j09mfx</id>
    <title>is 9070xt any good for localAI on windows ?</title>
    <updated>2025-02-28T15:00:48+00:00</updated>
    <author>
      <name>/u/gfy_expert</name>
      <uri>https://old.reddit.com/user/gfy_expert</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt; &lt;img alt="is 9070xt any good for localAI on windows ?" src="https://a.thumbs.redditmedia.com/IHRW4iuZw3RS3QXGe1CwjVtw2Kt1kBPYjq-WbucEht0.jpg" title="is 9070xt any good for localAI on windows ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfy_expert"&gt; /u/gfy_expert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j09mfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T15:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0c53c</id>
    <title>Inference speed comparisons between M1 Pro and maxed-out M4 Max</title>
    <updated>2025-02-28T16:45:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Won’t Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I don't think these models are actually utilizing the CPU. But I'm not definitive on this.&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I added a github repo in case anyone wants to contribute their own speed tests. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0cbvs</id>
    <title>🗣️ Free &amp; Open-Source AI TTS: Kokoro Web v0.1.0</title>
    <updated>2025-02-28T16:53:25+00:00</updated>
    <author>
      <name>/u/EduardoDevop</name>
      <uri>https://old.reddit.com/user/EduardoDevop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Excited to share &lt;strong&gt;Kokoro Web&lt;/strong&gt;, a fully open-source AI text-to-speech tool that you can use for free. No paywalls, no restrictions—just high-quality, local-friendly TTS. &lt;/p&gt; &lt;h2&gt;🔥 Why It Matters:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Open-Source&lt;/strong&gt;: No locked features, no subscriptions.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hostable&lt;/strong&gt;: Run it locally or on your own server.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI API Compatible&lt;/strong&gt;: Drop-in replacement for AI projects.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Generate speech in different accents.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built on Kokoro v1.0&lt;/strong&gt;: One of the top-ranked models in &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;TTS Arena&lt;/a&gt;, just behind ElevenLabs.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;🚀 Try It Out:&lt;/h2&gt; &lt;p&gt;Live demo: &lt;a href="https://voice-generator.pages.dev"&gt;https://voice-generator.pages.dev&lt;/a&gt; &lt;/p&gt; &lt;h2&gt;🔧 Self-Hosting:&lt;/h2&gt; &lt;p&gt;Spin it up with Docker in minutes: &lt;a href="https://github.com/eduardolat/kokoro-web"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts—feedback, contributions, and ideas are always welcome! 🖤 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EduardoDevop"&gt; /u/EduardoDevop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j045xn</id>
    <title>I trained a reasoning model that speaks French—for just $20! 🤯🇫🇷</title>
    <updated>2025-02-28T09:51:24+00:00</updated>
    <author>
      <name>/u/TheREXincoming</name>
      <uri>https://old.reddit.com/user/TheREXincoming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j045xn/video/mvudzukrpule1/player"&gt;https://reddit.com/link/1j045xn/video/mvudzukrpule1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheREXincoming"&gt; /u/TheREXincoming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T09:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d30g</id>
    <title>There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day</title>
    <updated>2025-02-28T17:23:46+00:00</updated>
    <author>
      <name>/u/unixmachine</name>
      <uri>https://old.reddit.com/user/unixmachine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt; &lt;img alt="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" src="https://external-preview.redd.it/VGXH8e8_7pJ5Oyuiy8alPcJzC5slCQHySpimzMU8-QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7786514b391f52128b399c1e789d18e7f6f10a" title="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixmachine"&gt; /u/unixmachine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-ROCm-RX-9070-Launch-Day"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:46+00:00</published>
  </entry>
</feed>
