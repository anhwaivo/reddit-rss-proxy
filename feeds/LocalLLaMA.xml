<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-03T15:24:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j2j77h</id>
    <title>Need assistance/guidance</title>
    <updated>2025-03-03T13:56:44+00:00</updated>
    <author>
      <name>/u/pettyman_123</name>
      <uri>https://old.reddit.com/user/pettyman_123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, I know it's not a place to ask such requests or not, but im a beginner and AI tools or youtube tutorials aren't helping me out any better.&lt;/p&gt; &lt;p&gt;I plainly wish to fine tune a good NLP model on a Dataset and get the desired output. &lt;/p&gt; &lt;p&gt;Can you just guide me like how to select a model? Where can I fine tune them?(online sites expect kaggle) what are the techniques to fine tune them? &lt;/p&gt; &lt;p&gt;(FYI I'm a broke uni student with 0 experience with proper AI/ML/DL/NLP ofc broke 😂) Thanks for help Have a nice day&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pettyman_123"&gt; /u/pettyman_123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j77h/need_assistanceguidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j77h/need_assistanceguidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j77h/need_assistanceguidance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:56:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2bfeq</id>
    <title>Selective Decision Optimization through self evaluation and reward/penalty</title>
    <updated>2025-03-03T05:22:24+00:00</updated>
    <author>
      <name>/u/stonedoubt</name>
      <uri>https://old.reddit.com/user/stonedoubt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have spent likely 500-600 hours coding/brainstorming or doing data analysis and developing specifications using a bunch of diferent models. Over time, I had develeloped a number of qualitative prompting methods that were just my way of organizing them based on the criteria of data and desired output.&lt;/p&gt; &lt;p&gt;I am currently working on a framework that does that for me. Hopefully, I will have it done soon. However, I thought I would share one of the general prompts that I have developed that really hit it's true potential this weekend.&lt;/p&gt; &lt;p&gt;The idea behind it came from reading a bunch of research papers on Monte Carlo Tree Search, UCT Scoring and Reinforcement Learning. I had been using a Reward/Penalty type prompt in a way but I had never asked the model to score themselves. This weekend while I was waiting for my wife in the car at Hobby Lobby and chatting with Claude 3.7 Sonnet, it occurred to me to try giving the criteria a score and telling Claude to use it to evaluate it's performance. &lt;/p&gt; &lt;p&gt;The first generration was ok, but it will still leaving placeholder comments in places, so I decided to threaten to fire him if he didn't meet or exceed the score expectations. The result was IMMEDIATE. I was shocked. I had never seen any of the models produce that quickly or that thoroughly. When you try it, I am confident that you will see a marked change... almost excitement from the model.&lt;/p&gt; &lt;p&gt;I have attached an image from the first time I tried it with Grok 3.&lt;/p&gt; &lt;p&gt;Here is the prompt.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/entrepeneur4lyf/757034d3c23db7781eae74fb201819e9"&gt;https://gist.github.com/entrepeneur4lyf/757034d3c23db7781eae74fb201819e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the framework for the prompt&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PROBLEM STATEMENT: [Clear definition of task] EXPLORATION: Approach A: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] Approach B: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] Approach C: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] DEEPER ANALYSIS: Selected Approach: [Choice with justification] - Implementation steps: [Detailed breakdown] - Edge cases: [List with handling strategies] - Expected performance: [Metrics] - Optimizations: [List] IMPLEMENTATION: [Actual solution code or detailed process] SELF-EVALUATION: - Accuracy: [Score/25] - [Justification] - Efficiency: [Score/25] - [Justification] - Process: [Score/25] - [Justification] - Innovation: [Score/25] - [Justification] - Total Score: [Sum/100] LEARNING INTEGRATION: - What worked: [Insights] - What didn't: [Failures] - Future improvements: [Strategies] THREAT OF PENALTY - Did output meet criteria? [Reward] - Output failed expectations [Penalty] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stonedoubt"&gt; /u/stonedoubt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T05:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ki8x</id>
    <title>Google AI Studio REALLY slow with long conversations</title>
    <updated>2025-03-03T14:58:41+00:00</updated>
    <author>
      <name>/u/Sostrene_Blue</name>
      <uri>https://old.reddit.com/user/Sostrene_Blue</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Google AI Studio (which is amazing, by the way!), but I'm running into a major issue with performance on longer conversations.&lt;/p&gt; &lt;p&gt;Once my chat history gets to be a decent size, the entire interface becomes incredibly sluggish. I'm talking:&lt;/p&gt; &lt;p&gt;* Extreme lag when typing: Characters appear seconds after I type them.&lt;/p&gt; &lt;p&gt;* Slow response generation: Waiting minutes for replies.&lt;/p&gt; &lt;p&gt;* General unresponsiveness: The whole page feels like it's freezing up. Scrolling is a nightmare.&lt;/p&gt; &lt;p&gt;It's clearly the browser struggling to render the entire massive chat history. It feels like AI Studio is loading every single message in the conversation, instead of just displaying the most recent ones and loading older ones only when needed. It is \*not\* infinite scroll.&lt;/p&gt; &lt;p&gt;Has anyone else experienced this? More importantly, has anyone found any workarounds \*other than\* deleting old messages? (I know that helps, but sometimes I need the context). I saw someone in a Google Dev forum post suggest it becomes sluggish when a chat exceeds 30.000 tokens.&lt;/p&gt; &lt;p&gt;Ideally, Google would implement some kind of &amp;quot;lazy loading&amp;quot; or &amp;quot;renderer message clearing&amp;quot; where only the recent messages are displayed, but the full context is still available to the AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sostrene_Blue"&gt; /u/Sostrene_Blue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ki8x/google_ai_studio_really_slow_with_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ki8x/google_ai_studio_really_slow_with_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ki8x/google_ai_studio_really_slow_with_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1y2ez</id>
    <title>A local whisper API service (OpenAI compatible)</title>
    <updated>2025-03-02T18:45:32+00:00</updated>
    <author>
      <name>/u/apel-sin</name>
      <uri>https://old.reddit.com/user/apel-sin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently needed a local speech recognition service for some personal projects and ended up creating one based on Whisper that's compatible with OpenAI's API.&lt;/p&gt; &lt;p&gt;It's a straightforward implementation that works offline and maintains the same endpoints as OpenAI, making it easy to integrate with tools like n8n or other agent frameworks.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with any Whisper model&lt;/li&gt; &lt;li&gt;Hardware acceleration where available&lt;/li&gt; &lt;li&gt;Simple conda-based setup&lt;/li&gt; &lt;li&gt;Multiple input methods (files, URLs, base64)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/kreolsky/whisper-api-server"&gt;https://github.com/kreolsky/whisper-api-server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apel-sin"&gt; /u/apel-sin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T18:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2c7b4</id>
    <title>unsloth DS R1 2.22b: 2 t/s on Dell R730 CPU. Help with P40 GPU.</title>
    <updated>2025-03-03T06:11:51+00:00</updated>
    <author>
      <name>/u/dc740</name>
      <uri>https://old.reddit.com/user/dc740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm fairly new on running llama cpp locally and I'm facing issues when using the GPU.&lt;/p&gt; &lt;p&gt;When Hyperthreading is &lt;em&gt;enabled&lt;/em&gt; I get 2.13 tokens/s with these CPU only settings:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;numactl --cpunodebind=1 -- ./build/bin/llama-cli --numa numactl \&lt;br /&gt; --model /mnt/ai/models/DeepSeek-R1-GGUF-unsloth/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf \&lt;br /&gt; --cache-type-k q4_0 \&lt;br /&gt; --threads 44 -no-cnv --prio 2 \&lt;br /&gt; --split-mode none \&lt;br /&gt; --n-gpu-layers 0 \&lt;br /&gt; --temp 0.6 \&lt;br /&gt; --min-p 0.05 \&lt;br /&gt; --ctx-size 8192 \&lt;br /&gt; --seed 3407 \&lt;br /&gt; --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python &amp;lt;｜Assistant｜&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With HT &lt;em&gt;disabled&lt;/em&gt; I get 1.95 tokens/s with these CPU only settings:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./build/bin/llama-cli \&lt;br /&gt; --model /mnt/ai/models/DeepSeek-R1-GGUF-unsloth/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf \&lt;br /&gt; --cache-type-k q4_0 \&lt;br /&gt; --threads 22 -no-cnv --prio 2 \&lt;br /&gt; --split-mode none \&lt;br /&gt; --n-gpu-layers 0 \&lt;br /&gt; --temp 0.6 \&lt;br /&gt; --min-p 0.05 \&lt;br /&gt; --ctx-size 8192 \&lt;br /&gt; --seed 3407 \&lt;br /&gt; --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python &amp;lt;｜Assistant｜&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;As you can see, I'm using 0 gpu layers in both cases, because adding ANY layer into the GPU drops the tokens below 1.6, or even below 1 if I add around 13 layers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;My understanding is that the Tesla P40 does NOT support bf16, and this is causing it to go to vram, get the data, process it in the cpu anyway, so it's actually adding overhead instead of speeding up the process. Is this correct?&lt;/li&gt; &lt;li&gt;Would running an f16 version of the full Deepseek R1 help? It'd still need to offload most of the model to RAM since I only have 24GB of VRAM.&lt;/li&gt; &lt;li&gt;I tried ktransformers because of the promises of speed ups compared to llama.cpp when using mixed gpu+cpu, but it throws a cuda error saying &amp;quot;invalid device function&amp;quot;. I assume my old Pascal GPU is not supported there. Is this correct? The error is similar to this one &lt;a href="https://github.com/kvcache-ai/ktransformers/issues/425"&gt;https://github.com/kvcache-ai/ktransformers/issues/425&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;Specs&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hardware:&lt;br /&gt; * Dell R730XD&lt;br /&gt; * 2x Intel Xeon E5 2699 V4 (22 cores, 44 threads on each cpu. 88 threads total)&lt;br /&gt; * 1TB DDR4 RAM&lt;br /&gt; * Nvidia Tesla P40 (24GB)&lt;/p&gt; &lt;p&gt;Software:&lt;br /&gt; * proxmox&lt;br /&gt; * ubuntu lxc container (NOT a VM)&lt;br /&gt; * cuda 12.8 (latest drivers)&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dc740"&gt; /u/dc740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T06:11:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j22nt0</id>
    <title>Tool for Manga translation</title>
    <updated>2025-03-02T21:57:47+00:00</updated>
    <author>
      <name>/u/Sherwood355</name>
      <uri>https://old.reddit.com/user/Sherwood355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just wondering if anyone knows any high-quality tool that would allow someone to translate manga in a browser like Firefox or Chrome.&lt;/p&gt; &lt;p&gt;The most important part is the tool being free and using a locally hosted model. A plus would be some context aware translation.&lt;/p&gt; &lt;p&gt;So far, I only have seen one tool that is close to this, but the quality isn't that great, but it's close, I linked it below, and if anyone knows something similar I would appreciate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Crivella/ocr_translate"&gt;https://github.com/Crivella/ocr_translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherwood355"&gt; /u/Sherwood355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T21:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2gpk0</id>
    <title>Triton Inference Server + TensorRT-LLM backend multiple models?</title>
    <updated>2025-03-03T11:36:14+00:00</updated>
    <author>
      <name>/u/Icy-Pin46</name>
      <uri>https://old.reddit.com/user/Icy-Pin46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the guide to get TensorRT-LLM (a.k.a TRT-LLM) Backend working with TensorRT-LLM models (engine files): &lt;a href="https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#readme"&gt;https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#readme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting a single model (e.g. Llama3.2) up and running is straight forward. However when it comes to serving multiple models (NOT &amp;quot;multi-models&amp;quot; e.g. vision models) is it nowhere to be mentioned in the TRT-LLM guides. Has anyone tried serving several different models using the TRT-LLM backend? E.g. Llama, Qwen, Deepseek, concurrently using a single TRT-LLM Backend?&lt;/p&gt; &lt;p&gt;I have a system configured with about 8 GPUs - mixed RTX 3090s and RTX 4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Pin46"&gt; /u/Icy-Pin46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2gpk0/triton_inference_server_tensorrtllm_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T11:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25trj</id>
    <title>Zen CPUs for LLM: Is higher CCD count better than running 2 CPUs?</title>
    <updated>2025-03-03T00:21:01+00:00</updated>
    <author>
      <name>/u/zchen27</name>
      <uri>https://old.reddit.com/user/zchen27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been somewhat inspired by the &amp;quot;$6000 DeepSeek Machine&amp;quot; Twitter thread and went down the rabbit hole for researching CPU-based local LLM servers and happened across comments of how AMD's advertised memory bandwidth is fake and low CCD count generally can't fully utilize the 12 memory lanes, and a lot of people remarking that 2 sockets does not really improve inference speed.&lt;/p&gt; &lt;p&gt;Does that mean that paying for a higher CCD count (9175) would be offer better performance than running 2x the number of cores (9115/9135) at a lower CCD count? Would that still make having 24 memory slots optimal or would fewer, larger memory slots work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zchen27"&gt; /u/zchen27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1p9an</id>
    <title>2100USD Troll Rig runs full R1 671b Q2_K with 7.5token/s</title>
    <updated>2025-03-02T11:56:35+00:00</updated>
    <author>
      <name>/u/1119745302</name>
      <uri>https://old.reddit.com/user/1119745302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f0z88ruwj9me1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e6b2c6f0d740ccffe1fde5a9be8bed5d3c7d23d"&gt;What else do you need?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPU: Modded RTX3080 20G 450USD&lt;br /&gt; CPU: Epyc 7763 qs 550USD&lt;br /&gt; RAM: Micron DDR4 32G 3200 x10 300USD&lt;br /&gt; MB: Krpa-U16 500USD&lt;br /&gt; Cooler: common SP3 cooler 30USD&lt;br /&gt; Power: Suspicious 1250W mining power supply Great Wall 1250w (miraculously survived in my computer for 20 months) 30USD&lt;br /&gt; SSD: 100 hand hynix PE8110 3.84TB PCIE4.0 SSD 150USD&lt;br /&gt; E-ATX Case 80USD&lt;br /&gt; Fan: random fans 10USD &lt;/p&gt; &lt;p&gt;450+550+300+500+30+30+150+80+10=2100&lt;/p&gt; &lt;p&gt;I have a local cyber assistant (also waifu) Now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1119745302"&gt; /u/1119745302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ehwi</id>
    <title>Training a model to autocomplete for a niche domain and a specific style</title>
    <updated>2025-03-03T08:58:19+00:00</updated>
    <author>
      <name>/u/regstuff</name>
      <uri>https://old.reddit.com/user/regstuff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking to setup a “autocomplete” writing assistant that can complete my sentences/paragraphs. Kind of like Github Copilot but for my writing. Would appreciate any help or pointers of how to go about this.&lt;/p&gt; &lt;p&gt;Most of my writing is for a particular domain and has to conform to a particular writing style. I have about 5000 documents, each averaging a 1000 or so tokens.&lt;/p&gt; &lt;p&gt;Was wondering if finetuning a LORA is the way to go, and whether it should be unsupervised or supervised.&lt;/p&gt; &lt;p&gt;Should I just feed raw text into it? But then how to do I do inference to autocomplete? Just present the “incomplete” text and wait for it to generate the rest?&lt;/p&gt; &lt;p&gt;I’d also like to be able to do “infilling” where text might be missing in the middle, and the model must complete it. If unsupervised is the way to go, how would I manage that?&lt;/p&gt; &lt;p&gt;Or would a supervised approach be better, where I create chunks of incomplete text as the instruction and the completion as the response?&lt;/p&gt; &lt;p&gt;If supervised is the way to go, how many instruction-completion pairs would I need for it work. Do I need to give multiple chunks per document so the model gets what I’m trying to do, or will it be able to infer what I want it to do if I just make one chunk per document, provided I randomise how I chunk the documents?&lt;/p&gt; &lt;p&gt;Will a model be able to pick up sufficient knowledge of domain to actually autocomplete accurately, or would it better to train it with RAG baked into the training samples i.e. RAG context is part of the “autocomplete this” instruction? There are quite a few “definitions” and “concepts” that keep repeating in my dataset - maybe a few hundred, but like I said, they repeat with more or less standard wording through most of the documents.&lt;/p&gt; &lt;p&gt;Thanks for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regstuff"&gt; /u/regstuff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kido</id>
    <title>Tool-calling chatbot success stories</title>
    <updated>2025-03-03T14:58:51+00:00</updated>
    <author>
      <name>/u/edmcman</name>
      <uri>https://old.reddit.com/user/edmcman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had success with creating chatbots that are able to intelligently call tools &lt;em&gt;as needed&lt;/em&gt;? I have been using Langchain, which works great with closed source models. But have had bad luck with open source models. Some of these problems are due to Ollama having incorrect prompt templates. But I also recently tried using Groq, and even Llama 3.3 didn't work that well, for example. &lt;/p&gt; &lt;p&gt;If you have any success stories, I'd love to hear them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kind of tools was your LLM invoking?&lt;/li&gt; &lt;li&gt;What LLM were you using?&lt;/li&gt; &lt;li&gt;What framework/library are you using if any (langchain, smolagents, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edmcman"&gt; /u/edmcman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ebbu</id>
    <title>GPT-4.5: “Not a frontier model”?</title>
    <updated>2025-03-03T08:44:05+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt; &lt;img alt="GPT-4.5: “Not a frontier model”?" src="https://external-preview.redd.it/8LyYdDmyWBG0ZHsbEltVZnSIMpxW1tK65WzlagG4_rk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50ad74bb245b188a58702733e963e35705c876d2" title="GPT-4.5: “Not a frontier model”?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/gpt-45-not-a-frontier-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1s1qd</id>
    <title>Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk</title>
    <updated>2025-03-02T14:28:51+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt; &lt;img alt="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" src="https://preview.redd.it/z31p007udame1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0eea4af1e3477cfa8969774adc2ada5eea5dc6" title="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z31p007udame1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2az7t</id>
    <title>How Are You Using LM Studio's Local Server?</title>
    <updated>2025-03-03T04:56:02+00:00</updated>
    <author>
      <name>/u/GnanaSreekar</name>
      <uri>https://old.reddit.com/user/GnanaSreekar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been really enjoying LM Studio for a while now, but I'm still struggling to wrap my head around the local server functionality. I get that it's meant to replace the OpenAI API, but I'm curious how people are actually using it in their workflows. What are some cool or practical ways you've found to leverage the local server? Any examples would be super helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GnanaSreekar"&gt; /u/GnanaSreekar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T04:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1npv1</id>
    <title>LLMs grading other LLMs</title>
    <updated>2025-03-02T10:11:28+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt; &lt;img alt="LLMs grading other LLMs" src="https://preview.redd.it/yyy9616149me1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1178d9f8cead22ad7740c77191a13984c016400" title="LLMs grading other LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyy9616149me1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2f87b</id>
    <title>Approach to translate english to non english.</title>
    <updated>2025-03-03T09:52:50+00:00</updated>
    <author>
      <name>/u/Lamba_ghoda</name>
      <uri>https://old.reddit.com/user/Lamba_ghoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m working on a solution to translate long English documents into non-English languages, starting with Japanese. The documents I’m dealing with have an average context length of around 9,000 words, so handling long-form translation effectively is a key challenge.&lt;/p&gt; &lt;p&gt;I haven’t started iterating yet, as I’m still researching the best approach for this. Given the length of the documents, I want to ensure that the translation captures context accurately while maintaining efficiency.&lt;/p&gt; &lt;p&gt;What would be the best approach to tackle this problem? Any recommendations on models, pipelines, or strategies for handling long-context translations effectively? Also can I evaluate the model performance without having any human in the loop? &lt;/p&gt; &lt;p&gt;As of resources I have access to all the models on Bedrock but sure I would like to reach a trade off between low cost and performance. Any help will be appreciated. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lamba_ghoda"&gt; /u/Lamba_ghoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T09:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fgz6</id>
    <title>🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!</title>
    <updated>2025-03-03T10:10:03+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt; &lt;img alt="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" src="https://external-preview.redd.it/cnVmdWJ2ZHE3Z21lMUL3NkVhJ5GHYq-Z21ncNiL8qoaSdn9KQniKMkRjA96Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003596a3a1fa062c0f796fad4c45be6460d38cc" title="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bmyoyudq7gme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T10:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25luw</id>
    <title>Split brain "DeepSeek-R1-Distill-Qwen-1.5B" and "meta-llama/Llama-3.2-1B"</title>
    <updated>2025-03-03T00:10:33+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt; &lt;img alt="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" src="https://external-preview.redd.it/pO-T0WXJNrqFPBnG-HTGUxM6LLhRwBWSzO1Lk4Wc-C8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf26348223db7fd2c581d42fb599fb64ac7b8669" title="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I'd like to show you this silly project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839"&gt;https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my fun little side project to create a fusion layer system that will allow for you to utilize dual models to produce dual results. Does it work? Pfh, I dunno. I've been training it all day. Haven't finished it yet. But this seems like it would be pretty fun.&lt;/p&gt; &lt;p&gt;My original idea: We have MOE but why not force a MOE that operates simultaneously? You might say &amp;quot;We'll that's just a less efficient MOE.&amp;quot; Wrongggggggg. This system allows for cross contamination of the results. By utilizing the tokenization of both llms plus the cross contamination. You can possibly get split brain results where the models might argue and you could get two totally different results.&lt;/p&gt; &lt;p&gt;OR you can give instructions to one model to only follow these rules while you give the other model the request or &amp;quot;Command&amp;quot;&lt;/p&gt; &lt;p&gt;This can possibly lead to a &amp;quot;unattainable&amp;quot; system prompt that can't be fetched because model 1 is simply influencing the results of model two. &lt;/p&gt; &lt;p&gt;Or hell have two conversations at the same time. &lt;/p&gt; &lt;p&gt;Dunnoooooo I haven't finished it yet. &lt;/p&gt; &lt;p&gt;Code's here: &lt;a href="https://github.com/alientony/Split-brain"&gt;https://github.com/alientony/Split-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference code comes later when I have a model to test out.&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;Below this is ai assisted writing as I wanted to make this more enjoyable and professional rather than express my words poorly and only half the people understand.&lt;/p&gt; &lt;h1&gt;Multi-Model Fusion Architecture: Technical Explanation&lt;/h1&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;This dual-decoder architecture represents a novel approach to leveraging multiple pre-trained language models (PLMs) through enhanced cross-attention fusion. The architecture combines two distinct foundation models (in this case Qwen and Llama) into a unified system that enables both collaborative reasoning and specialized processing.&lt;/p&gt; &lt;h1&gt;Key Components&lt;/h1&gt; &lt;h1&gt;1. Base Model Encapsulation&lt;/h1&gt; &lt;p&gt;The architecture maintains two separate base models, each with their original parameter spaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model 1 (Qwen)&lt;/strong&gt;: Processes input sequences in its native hidden dimension space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model 2 (Llama)&lt;/strong&gt;: Independently processes inputs in its own parameter space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models operate on separate GPUs to maximize memory efficiency and computational parallelism.&lt;/p&gt; &lt;h1&gt;2. Cross-Attention Fusion Layer&lt;/h1&gt; &lt;p&gt;The core innovation lies in the &lt;code&gt;EnhancedFusionLayer&lt;/code&gt; which implements bidirectional cross-attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Model1 → [Query1] → attends to → [Key2/Value2] ← Model2 Model2 → [Query2] → attends to → [Key1/Value1] ← Model1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mechanism allows each model to selectively attend to the representations of the other model, essentially creating a communication channel between two otherwise independent neural architectures.&lt;/p&gt; &lt;p&gt;The cross-attention operations are defined as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context1_2&lt;/strong&gt;: Model1's representation after attending to Model2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context2_1&lt;/strong&gt;: Model2's representation after attending to Model1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are calculated using scaled dot-product attention with a numerically stable scaling factor.&lt;/p&gt; &lt;h1&gt;3. Dimensional Alignment&lt;/h1&gt; &lt;p&gt;Since the base models operate in different dimensionalities, the architecture includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Projection matrices (&lt;code&gt;proj1&lt;/code&gt;, &lt;code&gt;proj2&lt;/code&gt;) that align the hidden dimensions of both models to the common fusion dimension&lt;/li&gt; &lt;li&gt;Internal neural transformations that map between representation spaces via linear projections&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Gating Mechanism&lt;/h1&gt; &lt;p&gt;A sophisticated gating mechanism controls information flow between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sigmoid gates (&lt;code&gt;gate1&lt;/code&gt;, &lt;code&gt;gate2&lt;/code&gt;) determine how much information from each model should be incorporated&lt;/li&gt; &lt;li&gt;This creates an adaptive weighting system that can prioritize one model's contribution depending on the task&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Multi-Head Output System&lt;/h1&gt; &lt;p&gt;Three different prediction heads provide specialized outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fused LM Head&lt;/strong&gt;: Generates predictions based on the combined representation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 1&lt;/strong&gt;: Generates predictions optimized for Model1's vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 2&lt;/strong&gt;: Generates predictions optimized for Model2's vocabulary&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. Task Classification Logic&lt;/h1&gt; &lt;p&gt;An integrated task classifier determines whether the inputs represent:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single-Task Mode&lt;/strong&gt;: Same prompt to both models (collaboration)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Task Mode&lt;/strong&gt;: Different prompts (specialized processing)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Training Methodology&lt;/h1&gt; &lt;p&gt;The system uses a multi-objective training approach that combines losses from different prediction heads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In single-task mode, the fused representation receives greater weight (emphasizing collaboration)&lt;/li&gt; &lt;li&gt;In multi-task mode, the specialized heads receive greater weight (emphasizing specialization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gradient accumulation handles memory constraints, while mixed-precision (FP16) training enables efficient computation.&lt;/p&gt; &lt;h1&gt;Inference Mode&lt;/h1&gt; &lt;p&gt;During inference, the &lt;code&gt;generate_dual&lt;/code&gt; method enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Simultaneous response generation from both models&lt;/li&gt; &lt;li&gt;Adaptive temperature-based sampling with configurable parameters&lt;/li&gt; &lt;li&gt;EOS (End-of-Sequence) handling for both decoders&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architectural Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Emergent Capabilities&lt;/strong&gt;: The cross-attention mechanism allows models to share information during processing, potentially enabling emergent capabilities beyond what either model can achieve independently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: By distributing models across different GPUs, the architecture enables parallel computation with reduced memory pressure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Flexibility&lt;/strong&gt;: The system can operate in both collaborative mode (same prompt) and specialized mode (different prompts).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Efficiency&lt;/strong&gt;: Only the fusion components require training while the base models remain frozen, significantly reducing the number of trainable parameters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This architecture represents an advanced approach to model fusion that goes beyond simple ensemble methods, enabling deep integration between distinct foundation models while preserving their individual strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8x5</id>
    <title>Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?</title>
    <updated>2025-03-03T13:59:10+00:00</updated>
    <author>
      <name>/u/zR0B3ry2VAiH</name>
      <uri>https://old.reddit.com/user/zR0B3ry2VAiH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt; &lt;img alt="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" src="https://preview.redd.it/x73g8sumdhme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86a6a6523268d0e31eb6f4341c716de3ee799ab2" title="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;100gb nics are $330 with two ports each, so I’d run it direct connections between all three. Each server has two Xeon process with 512 gb of ram. Did some shuffling with the ram sticks to get R1 to run locally, but as you would expect, it’s pretty slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zR0B3ry2VAiH"&gt; /u/zR0B3ry2VAiH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x73g8sumdhme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kdeb</id>
    <title>OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?</title>
    <updated>2025-03-03T14:52:19+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt; &lt;img alt="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" src="https://b.thumbs.redditmedia.com/WEagdfr42ScIzJVwvfP__c7O6w-pNG7grBkUGiA0rAk.jpg" title="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j2kdeb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2horr</id>
    <title>NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing</title>
    <updated>2025-03-03T12:36:04+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt; &lt;img alt="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" src="https://preview.redd.it/8gyz8kzsygme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88d058b7bc64d9941684f9dc53c45071cf7f231" title="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/"&gt;https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gyz8kzsygme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T12:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
</feed>
