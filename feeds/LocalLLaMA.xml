<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-12T18:08:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kkqgjy</id>
    <title>Best app to write novels?</title>
    <updated>2025-05-12T11:46:58+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, &lt;/p&gt; &lt;p&gt;Absolutely just plain idea, I know that in vscode I can use cline to automate writing code, wondering if there is that conbo specialised for writing stories?&lt;/p&gt; &lt;p&gt;Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqgjy/best_app_to_write_novels/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqgjy/best_app_to_write_novels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqgjy/best_app_to_write_novels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T11:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkyzaz</id>
    <title>Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn</title>
    <updated>2025-05-12T17:48:33+00:00</updated>
    <author>
      <name>/u/Heavy-Charity-3509</name>
      <uri>https://old.reddit.com/user/Heavy-Charity-3509</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt; &lt;img alt="Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn" src="https://external-preview.redd.it/qq3lKSb4oQnaoe9n5uHCfWaVVo7d03JOuY5gJaSBhFM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75d019b2a5a3ea4ffc46975b3d1157ef1b6e3ce2" title="Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Manus is impressive. I'm trying to build a local Manus alternative AI agent desktop app, that can easily install in MacOS and windows. The goal is to build a general purpose agent with expertise in product marketing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/55e3jptr1e0f1.png?width=3034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bdd1c225c0ef95b497ab08bd740382062d6711"&gt;https://preview.redd.it/55e3jptr1e0f1.png?width=3034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34bdd1c225c0ef95b497ab08bd740382062d6711&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is available in &lt;a href="https://github.com/11cafe/local-manus/"&gt;https://github.com/11cafe/local-manus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use Ollama to run the &lt;strong&gt;Qwen3 30B&lt;/strong&gt; model locally, and connect it with modular toolchains (MCPs) like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;playwright-mcp&lt;/strong&gt; for browser automation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;filesystem-mcp&lt;/strong&gt; for file read/write&lt;/li&gt; &lt;li&gt;custom MCPs for &lt;strong&gt;code execution, image &amp;amp; video editing&lt;/strong&gt;, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Why a local AI agent?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;One major advantage is &lt;strong&gt;persistent login&lt;/strong&gt; across websites. Many real-world tasks (e.g. searching or interacting on LinkedIn, Twitter, or TikTok) require an authenticated session. Unlike cloud agents, a local agent can &lt;strong&gt;reuse your logged-in browser session&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This unlocks use cases like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;automatic &lt;strong&gt;job searching&lt;/strong&gt; and application in Linkedin,&lt;/li&gt; &lt;li&gt;finding/&lt;strong&gt;reaching potential customers&lt;/strong&gt; in Twitter/Instagram,&lt;/li&gt; &lt;li&gt;write once and &lt;strong&gt;cross-posting&lt;/strong&gt; to multiple sites&lt;/li&gt; &lt;li&gt;automating social media promotions, and finding potential customers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;1. ü§ñ Qwen3/Claude/GPT agent ability comparison&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the LLM model, I tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;qwen3:30b-a3b using ollama,&lt;/li&gt; &lt;li&gt;Chatgpt-4o,&lt;/li&gt; &lt;li&gt;Claude 3.7 sonnet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I found that &lt;strong&gt;claude 3.7 &amp;gt; gpt 4o &amp;gt; qwen3:30b&lt;/strong&gt; in terms of their abilities to call tools like browser. A simple create and submit post task, Claude 3.7 can reliably finish while gpt and qwen sometimes stuck. I think maybe claude 3.7 has some post training for tool call abilities?&lt;/p&gt; &lt;p&gt;To make LLM execute in agent mode, I made it run in a ‚Äúchat loop‚Äù once received a prompt, and added a ‚Äúfinish_task‚Äù function tool to it and enforce that it must call it to finish the chat.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SYSTEM_TOOLS = [ { &amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;finish&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;You MUST call this tool when you think the task is finished or you think you can't do anything more. Otherwise, you will be continuously asked to do more about this task indefinitely. Calling this tool will end your turn on this task and hand it over to the user for further instructions.&amp;quot;, &amp;quot;parameters&amp;quot;: None, } } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. ü¶ô Qwen3 + Ollama local deploy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I deployed qwen3:30b-a3b using Mac M1 64GB computer, the speed is great and smooth. But Ollama has a bug that it cannot stream chat if function call tools enabled for the LLM. They have many issues complaining about this bug and it seems they are &lt;a href="https://github.com/ollama/ollama/pull/10415"&gt;baking a fix&lt;/a&gt; currently....&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. üåê Playwright MCP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I used this mcp for browser automation, it's great. The only problem is that file uploading related functions are not working well, and the website snapshot string returned are not paginated, sometimes it can exhaust 10k+ tokens just for the snapshot itself. So I plan to fork it to add pagination and fix uploading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. üîî Human-in-loop actions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sometimes, agent can be blocked by captcha, login page, etc. In this scenerio, it needs to notify human to help unblock them. Like shown in screenshots, my agent will send a dialog notification through function call to ask the user to open browser and login, or to confirm if the draft content is good to post. Human just needs to click buttons in presented UI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eqe4d0722e0f1.png?width=2382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a5b5c9851744d0d03577d54404e5851a4fa7566"&gt;AI prompt user to open browser to login to website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0d4ukfl52e0f1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62f6a6102a7ebe4d44f3f7d50171e037d5cc7907"&gt;https://preview.redd.it/0d4ukfl52e0f1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62f6a6102a7ebe4d44f3f7d50171e037d5cc7907&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also looking for collaborators in this project with me, if you are interested, please do not hesitant to DM me! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heavy-Charity-3509"&gt; /u/Heavy-Charity-3509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkz1z7</id>
    <title>Local fine tuning - CPU for 5090</title>
    <updated>2025-05-12T17:51:21+00:00</updated>
    <author>
      <name>/u/Legitimate-Week3916</name>
      <uri>https://old.reddit.com/user/Legitimate-Week3916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to hear your recomendations for CPU for local fine-tune of LLM models for RTX 5090 based setup. I dont think I plan to add any other GPU soon.&lt;/p&gt; &lt;p&gt;I am tergeting models max to 15B params (mostly smaller ones 7-11B) and with datasets &amp;lt; 10GB. &lt;/p&gt; &lt;p&gt;I am not constrained too much by budget, the goal is to avoid bottlenecking of GPU and dont hugely overpay it.&lt;br /&gt; Any recommendations, tips etc welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Legitimate-Week3916"&gt; /u/Legitimate-Week3916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkz1z7/local_fine_tuning_cpu_for_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkz1z7/local_fine_tuning_cpu_for_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkz1z7/local_fine_tuning_cpu_for_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:51:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkkhkf</id>
    <title>"How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)". Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there.</title>
    <updated>2025-05-12T05:07:01+00:00</updated>
    <author>
      <name>/u/Ein-neiveh-blaw-bair</name>
      <uri>https://old.reddit.com/user/Ein-neiveh-blaw-bair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"&gt; &lt;img alt="&amp;quot;How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)&amp;quot;. Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there." src="https://preview.redd.it/dg50eahh8a0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca25176cc0131c24ad4b3dda3c0fc89336805285" title="&amp;quot;How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)&amp;quot;. Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ein-neiveh-blaw-bair"&gt; /u/Ein-neiveh-blaw-bair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg50eahh8a0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T05:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkvrjh</id>
    <title>What is the best way to return code snippets in a structured output?</title>
    <updated>2025-05-12T15:43:20+00:00</updated>
    <author>
      <name>/u/Infrared12</name>
      <uri>https://old.reddit.com/user/Infrared12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;pretty much the title, afaik, returning code in JSON (e.g {&amp;quot;thought&amp;quot;:..., &amp;quot;code&amp;quot;: ...}), degrades performance a bit, what do you guys usually do if you want to output code snippets reliably along side other &amp;quot;keys&amp;quot; (like &amp;quot;thought&amp;quot;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infrared12"&gt; /u/Infrared12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvrjh/what_is_the_best_way_to_return_code_snippets_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvrjh/what_is_the_best_way_to_return_code_snippets_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvrjh/what_is_the_best_way_to_return_code_snippets_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkiif9</id>
    <title>Ktransformer VS Llama CPP</title>
    <updated>2025-05-12T03:10:02+00:00</updated>
    <author>
      <name>/u/Bluesnow8888</name>
      <uri>https://old.reddit.com/user/Bluesnow8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking into Ktransformer lately (&lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt;), but I have not tried it myself yet.&lt;/p&gt; &lt;p&gt;Based on its readme, it can handle very large model , such as the Deepseek 671B or Qwen3 235B with only 1 or 2 GPUs.&lt;/p&gt; &lt;p&gt;However, I don't see it gets discussed a lot here. I wonder why everyone still uses Llama CPP? Will I gain more performance by switching to Ktransformer? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bluesnow8888"&gt; /u/Bluesnow8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T03:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkxiio</id>
    <title>what's the best way to choose and fine-tune llms on hugging face?</title>
    <updated>2025-05-12T16:51:59+00:00</updated>
    <author>
      <name>/u/jamesftf</name>
      <uri>https://old.reddit.com/user/jamesftf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I'm new to Hugging Face and fine-tuning.&lt;/p&gt; &lt;p&gt;I've used OpenAI's playground for fine-tuning, which seems good, but I'm exploring other LLMs and feeling a bit lost.&lt;/p&gt; &lt;p&gt;I have a few newbie questions (I've searched online and used AI for answers), but I value personal experience.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What's the best way to choose from all available LLMs? Should I rely on leaderboards? They don't specify which models excel at content creation.&lt;/li&gt; &lt;li&gt;I can't fine-tune locally, so I must use cloud services. I've found paid and free options. Is the free option sufficient, or are there downsides?&lt;/li&gt; &lt;li&gt;Once I find the best LLM, where should I host it? The same place where I fine-tuned it?&lt;/li&gt; &lt;li&gt;Why use Hugging Face LLMs when Gemini, Claude, and OpenAI offer fine-tunable models?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamesftf"&gt; /u/jamesftf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxiio/whats_the_best_way_to_choose_and_finetune_llms_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxiio/whats_the_best_way_to_choose_and_finetune_llms_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxiio/whats_the_best_way_to_choose_and_finetune_llms_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkx4ev</id>
    <title>Best local inference provider?</title>
    <updated>2025-05-12T16:36:11+00:00</updated>
    <author>
      <name>/u/TechnicalGeologist99</name>
      <uri>https://old.reddit.com/user/TechnicalGeologist99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried ollama and vllm. &lt;/p&gt; &lt;p&gt;I liked the ability to swap models in ollama. But I found vllm is faster. Though if I'm not mistaken, vllm doesn't support model swapping.&lt;/p&gt; &lt;p&gt;What I need: - ability to swap models - run as a server via docker/compose - run multiple models at the same time - able to use finetuned checkpoints - server handles it's own queue of requests - openai like API&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnicalGeologist99"&gt; /u/TechnicalGeologist99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkocfx</id>
    <title>llama.cpp not using kv cache effectively?</title>
    <updated>2025-05-12T09:36:43+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp not using kv cache effectively?&lt;/p&gt; &lt;p&gt;I'm running the unsloth UD q4 quanto of qwen3 30ba3b and noticed that when adding new responses in a chat, it seemed to re-process the whole conversation instead of using the kv cache.&lt;/p&gt; &lt;p&gt;any ideas?&lt;/p&gt; &lt;p&gt;``` May 12 09:33:13 llm llm[948025]: srv params&lt;em&gt;from&lt;/em&gt;: Chat format: Content-only May 12 09:33:13 llm llm[948025]: slot launch&lt;em&gt;slot&lt;/em&gt;: id 0 | task 105562 | processing task May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | new prompt, n_ctx_slot = 40960, n_keep = 0, n_prompt_tokens = 15411 May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [3, end) May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 2051, n_tokens = 2048, progress = &amp;gt; May 12 09:33:16 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [2051, end) May 12 09:33:16 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 4099, n_tokens = 2048, progress = &amp;gt; May 12 09:33:18 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [4099, end) May 12 09:33:18 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 6147, n_tokens = 2048, progress = &amp;gt; May 12 09:33:21 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [6147, end) May 12 09:33:21 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 8195, n_tokens = 2048, progress = &amp;gt; May 12 09:33:25 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [8195, end)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;EDIT: I suspect Open WebUI client. The KV cache works fine with the CLI 'llm' tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T09:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkjyvk</id>
    <title>A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B.</title>
    <updated>2025-05-12T04:34:06+00:00</updated>
    <author>
      <name>/u/sqli</name>
      <uri>https://old.reddit.com/user/sqli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"&gt; &lt;img alt="A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B." src="https://preview.redd.it/u28jb5s74a0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05bcf2f1fef0da32184133fff187d14aa925a27d" title="A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm Thomas, I created &lt;a href="https://awfulsec.com/introducing_awful_security_news.html"&gt;Awful Security News&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I found that prompt engineering is quite difficult for those who don't like Python and prefer to use command line tools over comprehensive suites like Silly Tavern.&lt;/p&gt; &lt;p&gt;I also prefer being able to run inference without access to the internet, on my local machine. I saw that LM Studio now supports Open-AI tool calling and Response Formats and long wanted to learn how this works without wasting hundreds of dollars and hours using Open-AI's products.&lt;/p&gt; &lt;p&gt;I was pretty impressed with the capabilities of Qwen's models and needed a distraction free way to read the news of the day. Also, the speed of the news cycles and the firehouse of important details, say &lt;em&gt;Named Entities&lt;/em&gt; and &lt;em&gt;Dates&lt;/em&gt; makes recalling these facts when necessary for the conversation more of a workout than necessary.&lt;/p&gt; &lt;p&gt;I was interested in the fact that Qwen is a multilingual model made by the long renown Chinese company Alibaba. I know that when I'm reading foreign languages, written by native speakers in their country of origin, things like Named Entities might not always translate over in my brain. It's easy to confuse a title or name for an action or an event. For instance, the Securities Exchange Commission could mean that Investments are trading each other bonuses they made on sales or &amp;quot;Securities are exchanging commission.&amp;quot; Things like this can be easily disregarded as &amp;quot;bad translation.&amp;quot;&lt;/p&gt; &lt;p&gt;I thought it may be easier to parse news as a brief summary (crucially one that links to the original source), followed by a list and description of each named Entity, why they are important to the story and the broader context. Then a list of important dates and timeframes mentioned in the article.&lt;/p&gt; &lt;p&gt;mdBook provides a great, distraction-free reading experience in the style of a book. I hate databases and extra layers of complexity so this provides the basis for the web based version of the final product. The code also builds a JSON API that allows you to plumb the data for interesting trends or find a needle in a haystack.&lt;/p&gt; &lt;p&gt;For example we can collate all of the Named Entites listed, alongside a given Named Entity, for all of the articles in a publication.&lt;/p&gt; &lt;p&gt;&lt;code&gt;mdBook&lt;/code&gt; also provides for us a fantastic search feature that requires no external database as a dependency. The entire project website is made of static, flat-files.&lt;/p&gt; &lt;p&gt;The Rust library that calls Open-AI compatible API's for model inference, &lt;code&gt;aj&lt;/code&gt; is available on my Github: &lt;a href="https://github.com/graves/awful%5C_aj"&gt;https://github.com/graves/awful\_aj&lt;/a&gt;. The blog post linked to at the top of this post contains details on how the prompt engineering works. It uses &lt;code&gt;yaml&lt;/code&gt; files to specify everything necessary. Personally, I find it much easier to work with, when actually typing, than &lt;code&gt;json&lt;/code&gt; or in-line code. This library can also be used as a command line client to call Open-AI compatible APIs AND has a home-rolled custom Vector Database implementation that allows your conversation to recall memories that fall outside of the conversation context. There is an &lt;code&gt;interactive&lt;/code&gt; mode and an &lt;code&gt;ask&lt;/code&gt; mode that will just print the LLM inference response content to stdout.&lt;/p&gt; &lt;p&gt;The Rust command line client that uses &lt;code&gt;aj&lt;/code&gt; as dependency and actually organizes Qwen's responses into a daily news publication fit for &lt;code&gt;mdBook&lt;/code&gt; is also available on my Github: &lt;a href="https://github.com/graves/awful%5C_text%5C_news"&gt;https://github.com/graves/awful\_text\_news&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;mdBook&lt;/code&gt; project I used as a starting point for the first few runs is also available on my Github: &lt;a href="https://github.com/graves/awful_security_news"&gt;https://github.com/graves/awful_security_news&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are some interesting things I'd like to do like add the astrological moon phase to each edition (without using an external service). I'd also like to build parody site to act as a mirror to the world's events, and use the &lt;a href="https://huggingface.co/teknium/Mistral-Trismegistus-7B"&gt;Mistral Trismegistus model&lt;/a&gt; to rewrite the world's events from the &lt;strong&gt;perspective of angelic intervention being the initiating factor of each key event.&lt;/strong&gt; üòáüåôüòá&lt;/p&gt; &lt;p&gt;Contributions to the code are welcome and both the site and API are free to use and will remain free to use as long as I am physically capable of keeping them running.&lt;/p&gt; &lt;p&gt;I would love any feedback, tips, or discussion on how to make the site or tools that build it more useful. ‚ô•Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sqli"&gt; /u/sqli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u28jb5s74a0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T04:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkshqc</id>
    <title>Qwen3 repeats itself forever at the end of its output</title>
    <updated>2025-05-12T13:27:07+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using ollama 0.6.8 with ollama run &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;There are 30 people in a class and each person chooses 5 other people in the class that they want to be in a new class with. The new classes will each be of size 10 (the classes don't overlap and cover all 30 people). Is it ever impossible for everyone to be with at least one of their chosen five?&lt;/p&gt; &lt;p&gt;Or alternatively, show that it is always possible.&lt;/p&gt; &lt;p&gt;I know that there is a set of choices that makes the classes impossible to arrange if each person chooses 4 others. He also shows that there is no such bad set of choices if each person chooses 8 others.&lt;/p&gt; &lt;p&gt;For 5, 6, 7 choices the question is currently open.&lt;/p&gt; &lt;p&gt;The question is asking about a set of preferences that is impossible to satisfy for any partition.&lt;/p&gt; &lt;p&gt;-----&lt;/p&gt; &lt;p&gt;After thinking for a while it outputs this on a loop which never seems to terminate:&lt;/p&gt; &lt;p&gt;**Final Answer:** \boxed{Yes}&lt;/p&gt; &lt;p&gt;**Reasoning:** The high connectivity of the graph (with 5 choices per person) ensures that a suitable partition&lt;/p&gt; &lt;p&gt;into 3 groups is always possible, as supported by probabilistic arguments. Hence, the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;**Final Answer**&lt;/p&gt; &lt;p&gt;\boxed{Yes}&lt;/p&gt; &lt;p&gt;**Reasoning:** The high connectivity of the graph (with 5 choices per person) ensures that a suitable partition&lt;/p&gt; &lt;p&gt;into 3 groups is always possible, as supported by probabilistic arguments. Hence, the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;I think the answer is \boxed{Yes}.&lt;/p&gt; &lt;p&gt;**Step-by-Step Explanation:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Understanding the Problem:**- There are 30 people, each with 5 choices (presumably of other people).- We are to divide them into 3 groups of 10.- In each group, for every person, at least one of their 5 choices must be in the same group.&lt;/li&gt; &lt;li&gt;**Key Observations:**- Each person has 5 &amp;quot;preferences&amp;quot; (choices).- The graph formed by these choices is a directed graph with 30 nodes and 5 outgoing edges per node.- The goal is to partition this graph into 3 subgraphs (groups), each of size 10, such that for every node, at&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;least one of its 5 outgoing edges leads to a node in the same subgraph.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Random Assignment and Probabilities:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- If we randomly assign each person to one of the 3 groups, the probability that a specific person's group&lt;/p&gt; &lt;p&gt;contains none of their 5 choices is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;\left(\frac{2}{3}\right)^5 \approx 0.1316&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;- Therefore, the probability that a person is &amp;quot;satisfied&amp;quot; (i.e., at least one of their choices is in the same&lt;/p&gt; &lt;p&gt;group) is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;1 - \left(\frac{2}{3}\right)^5 \approx 0.8684&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;- The expected number of &amp;quot;unsatisfied&amp;quot; people across all 30 is:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;30 \times 0.1316 \approx 3.95&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Existence Argument (Probabilistic Method):**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- While the expected number of unsatisfied people is about 4, this does not guarantee that there exists a partition with zero unsatisfied people.&lt;/p&gt; &lt;p&gt;- However, the graph is highly connected (each node has 5 outgoing edges), and such a partition is likely to exist due to the high density of connections.&lt;/p&gt; &lt;p&gt;- The probabilistic method suggests that, given the high connectivity, it is highly probable that such a partition exists.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Conclusion:**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Based on the high connectivity of the graph and the probabilistic argument, it is always possible to divide the 30 people into 3 groups of 10 such that each person has at&lt;/p&gt; &lt;p&gt;least one of their 5 choices in the same group.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkshqc/qwen3_repeats_itself_forever_at_the_end_of_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T13:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkbhxr</id>
    <title>Wow! DeerFlow is OSS now: LLM + Langchain + tools (web search, crawler, code exec)</title>
    <updated>2025-05-11T21:11:32+00:00</updated>
    <author>
      <name>/u/behradkhodayar</name>
      <uri>https://old.reddit.com/user/behradkhodayar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance (the company behind TikTok), opensourced DeerFlow (&lt;strong&gt;D&lt;/strong&gt;eep &lt;strong&gt;E&lt;/strong&gt;xploration and &lt;strong&gt;E&lt;/strong&gt;fficient &lt;strong&gt;R&lt;/strong&gt;esearch &lt;strong&gt;Flow&lt;/strong&gt;), such a great give-back.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bytedance/deer-flow"&gt;https://github.com/bytedance/deer-flow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/behradkhodayar"&gt; /u/behradkhodayar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T21:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkea2w</id>
    <title>LPT: Got an old low VRAM GPU you're not using? Use it to increase your VRAM pool.</title>
    <updated>2025-05-11T23:23:26+00:00</updated>
    <author>
      <name>/u/pneuny</name>
      <uri>https://old.reddit.com/user/pneuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got an RTX 5060 Ti 16GB, but 16GB is still not enough to fit something like Qwen 3 30b-a3b. That's where the old GTX 1060 I got in return for handing down a 3060 Ti comes in handy. In LMStudio, using the Vulkan backend, with full GPU offloading to both the RTX and GTX cards, I managed to get 43 t/s, which is way better than the ~13 t/s with partial CPU offloading when using CUDA 12.&lt;/p&gt; &lt;p&gt;So yeah, if you have a 16GB card, break out that old card and add it to your system if your motherboard has the PCIE slot to spare.&lt;/p&gt; &lt;p&gt;PS: This also gives you 32 bit physx support on your RTX 50 series if the old card is Nvidia.&lt;/p&gt; &lt;p&gt;TL;DR: RTX 5060 Ti 16GB + GTX 1060 6GB = 43t/s on Qwen3 30b-a3b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pneuny"&gt; /u/pneuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T23:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk97m7</id>
    <title>We made an open source agent builder and framework designed to work with local llms!</title>
    <updated>2025-05-11T19:31:37+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt; &lt;img alt="We made an open source agent builder and framework designed to work with local llms!" src="https://preview.redd.it/ha9ptoygf70f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89d2d79a3d2e7586b294f58dfb84c68117b05a1a" title="We made an open source agent builder and framework designed to work with local llms!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ha9ptoygf70f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kko4xu</id>
    <title>Support for InternVL has been merged into llama.cpp</title>
    <updated>2025-05-12T09:21:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13422"&gt;https://github.com/ggml-org/llama.cpp/pull/13422&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13443"&gt;https://github.com/ggml-org/llama.cpp/pull/13443&lt;/a&gt;&lt;/p&gt; &lt;p&gt;when GGUF? ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T09:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkl39r</id>
    <title>Findings from LoRA Finetuning for Qwen3</title>
    <updated>2025-05-12T05:46:44+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Fine-tuned Qwen3-8B with a small LoRA setup to preserve its ability to switch behaviors using &lt;code&gt;/think&lt;/code&gt; (reasoning) and &lt;code&gt;/no_think&lt;/code&gt; (casual) prompts. Rank 8 gave the best results. Training took ~30 minutes for 8B using 4,000 examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;LoRA Rank Testing Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Rank 8&lt;/strong&gt;: Best outcome‚Äîpreserved both &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; behavior.&lt;/li&gt; &lt;li&gt;‚ùå &lt;strong&gt;Rank 32&lt;/strong&gt;: Model started ignoring the &lt;code&gt;/think&lt;/code&gt; prompt.&lt;/li&gt; &lt;li&gt;üíÄ &lt;strong&gt;Rank 64&lt;/strong&gt;: Completely broke‚Äîoutput became nonsensical.&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Rank 128&lt;/strong&gt;: Overfit hard‚Äîmodel became overly STUPID&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Configuration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Applied LoRA to: &lt;code&gt;q_proj&lt;/code&gt;, &lt;code&gt;k_proj&lt;/code&gt;, &lt;code&gt;v_proj&lt;/code&gt;, &lt;code&gt;o_proj&lt;/code&gt;, &lt;code&gt;gate_proj&lt;/code&gt;, &lt;code&gt;up_proj&lt;/code&gt;, &lt;code&gt;down_proj&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Rank: 8&lt;/li&gt; &lt;li&gt;Alpha: 16&lt;/li&gt; &lt;li&gt;Dropout: 0.05&lt;/li&gt; &lt;li&gt;Bias: Disabled&lt;/li&gt; &lt;li&gt;Gradient Checkpointing: Enabled to reduce memory usage&lt;/li&gt; &lt;li&gt;Batch Size: 2&lt;/li&gt; &lt;li&gt;Gradient Accumulation: 4 steps&lt;/li&gt; &lt;li&gt;Learning Rate: 2e-4&lt;/li&gt; &lt;li&gt;Epochs: 1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tested whether full finetuning or using the model without 4-bit quantization would help. Neither approach gave better results. In fact, the model sometimes performed worse or became inconsistent in responding to &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt;. This confirmed that lightweight LoRA with rank 8 was the ideal trade-off between performance and resource use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Collection:&lt;/strong&gt; üëâ &lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;GrayLine-Qwen3 Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Plans:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;Try fine-tuning Qwen3-30B-A3B (MoE version) to see if it handles behavior switching better at scale.&lt;/li&gt; &lt;li&gt;Run full benchmark evaluations using LM-Eval to better understand model performance across reasoning, safety, and general capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you want me to try any other configs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T05:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkxguj</id>
    <title>Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!</title>
    <updated>2025-05-12T16:50:07+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt; &lt;img alt="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" src="https://external-preview.redd.it/dK0eCIEzcM5j6_jBCsj8F3QxdQmtEzB-3y5sTAPZ79w.png?width=140&amp;amp;height=78&amp;amp;crop=140:78,smart&amp;amp;auto=webp&amp;amp;s=3aad984995ecdfbd3d86d0e8b6cdf2b4633f38be" title="Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on everything around vision LMs ü§ó&lt;/p&gt; &lt;p&gt;We just shipped a compilation blog post on everything new about vision language models, of course focusing on open models:&lt;/p&gt; &lt;p&gt;- multimodal agents&lt;/p&gt; &lt;p&gt;- multimodal RAG&lt;/p&gt; &lt;p&gt;- video language models&lt;/p&gt; &lt;p&gt;- Omni/any-to-any models, and more! &lt;/p&gt; &lt;p&gt;Looking forward to discuss with you all under the blog ü§†&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3"&gt;https://preview.redd.it/ohcrk58krd0f1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4230152ccb900753ca9479d16b39be6191ab61c3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T16:50:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkox2l</id>
    <title>alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b</title>
    <updated>2025-05-12T10:14:51+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt; &lt;img alt="alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b" src="https://external-preview.redd.it/brLGj73HpwGowL4BCPItOOK8Jxb9y38Hisluq85GQSc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fb2f98106483af9f91d56a5a868a3f07d4b752b" title="alibaba's MNN Chat App now supports qwen 2.5 omni 3b and 7b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md"&gt;Github Page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fh8ydmulsb0f1.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4868777573338de97f98442b4ac0f90bf28a3bd0"&gt;https://preview.redd.it/fh8ydmulsb0f1.png?width=1776&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4868777573338de97f98442b4ac0f90bf28a3bd0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the pull request has just been merged, If you have any problem, please report an issue in github, or comment below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkox2l/alibabas_mnn_chat_app_now_supports_qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T10:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkvqti</id>
    <title>Qwen3 throughput benchmarks on 2x 3090, almost 1000 tok/s using 4B model and vLLM as the inference engine</title>
    <updated>2025-05-12T15:42:32+00:00</updated>
    <author>
      <name>/u/kms_dev</name>
      <uri>https://old.reddit.com/user/kms_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;System:&lt;/p&gt; &lt;p&gt;CPU: Ryzen 5900x RAM: 32GB GPUs: 2x 3090 (pcie 4.0 x16 + pcie 4.0 x4) allowing full 350W on each card&lt;/p&gt; &lt;p&gt;Input tokens per request: 4096&lt;/p&gt; &lt;p&gt;Generated tokens per request: 1024&lt;/p&gt; &lt;p&gt;Inference engine: vLLM&lt;/p&gt; &lt;h3&gt;Benchmark results&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model name&lt;/th&gt; &lt;th&gt;Quantization&lt;/th&gt; &lt;th&gt;Parallel Structure&lt;/th&gt; &lt;th&gt;Output token throughput (TG)&lt;/th&gt; &lt;th&gt;Total token throughput (TG+PP)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;749&lt;/td&gt; &lt;td&gt;3811&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;790&lt;/td&gt; &lt;td&gt;4050&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;833&lt;/td&gt; &lt;td&gt;4249&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-4b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;981&lt;/td&gt; &lt;td&gt;4995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;387&lt;/td&gt; &lt;td&gt;1993&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-8b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;581&lt;/td&gt; &lt;td&gt;3000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;214&lt;/td&gt; &lt;td&gt;1105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;267&lt;/td&gt; &lt;td&gt;1376&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-14b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;382&lt;/td&gt; &lt;td&gt;1947&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;95&lt;/td&gt; &lt;td&gt;514&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;dp2&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;td&gt;431&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W4A16&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;125&lt;/td&gt; &lt;td&gt;674&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;AWQ&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;124&lt;/td&gt; &lt;td&gt;670&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3-32b&lt;/td&gt; &lt;td&gt;W8A8&lt;/td&gt; &lt;td&gt;tp2&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;393&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;dp: Data parallel, tp: Tensor parallel&lt;/p&gt; &lt;h3&gt;Conclusions&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;When running smaller models (model + context fit within one card), using data parallel gives higher throughput&lt;/li&gt; &lt;li&gt;INT8 quants run faster on Ampere cards compared to FP8 (as FP8 is not supported at hardware level, this is expected)&lt;/li&gt; &lt;li&gt;For models in 32b range, use AWQ quant to optimize throughput and FP8 to optimize quality&lt;/li&gt; &lt;li&gt;When the model almost fills up one card with less vram for context, better to do tensor parallel compared to data parallel. qwen3-32b using W4A16 dp gave 77 tok/s whereas tp yielded 125 tok/s.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;How to run the benchmark&lt;/h3&gt; &lt;p&gt;start the vLLM server by&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;specify --max-model-len xxx if you get CUDA out of memory when running higher quants&lt;/h1&gt; &lt;p&gt;vllm serve Qwen/Qwen3-32B-AWQ --enable-reasoning --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.85 --disable-log-requests -tp 2 ```&lt;/p&gt; &lt;p&gt;and in a separate terminal run the benchmark&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash vllm bench serve --model Qwen/Qwen3-32B-AWQ --random_input_len 4096 --random_output_len 1024 --num_prompts 100 &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kms_dev"&gt; /u/kms_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkqud8</id>
    <title>Continuous Thought Machines - Sakana AI</title>
    <updated>2025-05-12T12:07:03+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt; &lt;img alt="Continuous Thought Machines - Sakana AI" src="https://external-preview.redd.it/301MLdXBGS0U_36M44Bby0bKZg0NibAojUn2aDi7Aao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dedee1849a1301ba66f6e5516f26d39f420baa8" title="Continuous Thought Machines - Sakana AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/ctm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkqud8/continuous_thought_machines_sakana_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkgzip</id>
    <title>INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning</title>
    <updated>2025-05-12T01:46:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt; &lt;img alt="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" src="https://external-preview.redd.it/C1X5HGKGzXyAtD9lvvvB3VxlaW_Pl5NuFtz4_fp414w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8977006bf732e56b214f916d46801909a0bb97fa" title="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:46:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkuq7m</id>
    <title>Qwen suggests adding presence penalty when using Quants</title>
    <updated>2025-05-12T15:01:27+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt; &lt;img alt="Qwen suggests adding presence penalty when using Quants" src="https://external-preview.redd.it/A0CJkaVhWSJlS1H3jMo88QQ29sV2UK4TZDFuCwfIrfE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e16adf99126adf2234ecfd290e3742cbf83a7a0" title="Qwen suggests adding presence penalty when using Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Image 1: Qwen 32B&lt;/li&gt; &lt;li&gt;Image 2: Qwen 32B GGUF Interesting to spot this,i have always used recomended parameters while using quants, is there any other model that suggests this?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kkuq7m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T15:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kky1sg</id>
    <title>Meta has released an 8B BLT model</title>
    <updated>2025-05-12T17:12:33+00:00</updated>
    <author>
      <name>/u/ThiccStorms</name>
      <uri>https://old.reddit.com/user/ThiccStorms</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThiccStorms"&gt; /u/ThiccStorms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&amp;amp;utm_medium=organic%20social&amp;amp;utm_content=video&amp;amp;utm_campaign=fair"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kky1sg/meta_has_released_an_8b_blt_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T17:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkq8q8</id>
    <title>Microsoft Researchers Introduce ARTIST</title>
    <updated>2025-05-12T11:34:55+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt; &lt;img alt="Microsoft Researchers Introduce ARTIST" src="https://preview.redd.it/90acs85p7c0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c24f942d10fedd4f933d6f856346cbfea33433" title="Microsoft Researchers Introduce ARTIST" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft Research introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a framework that combines agentic reasoning, reinforcement learning, and dynamic tool use to enhance LLMs. ARTIST enables models to autonomously decide when, how, and which tools to use during multi-step reasoning, learning robust strategies without step-level supervision. The model improves reasoning and interaction with external environments through integrated tool queries and outputs. Evaluated on challenging math and function-calling benchmarks, ARTIST outperforms top models like GPT-4o, achieving up to 22% gains. It demonstrates emergent agentic behaviors, setting a new standard in generalizable and interpretable problem-solving. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/"&gt;https://www.marktechpost.com/2025/05/10/microsoft-researchers-introduce-artist-a-reinforcement-learning-framework-that-equips-llms-with-agentic-reasoning-and-dynamic-tool-use/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: &lt;a href="https://arxiv.org/abs/2505.01441"&gt;https://arxiv.org/abs/2505.01441&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90acs85p7c0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkq8q8/microsoft_researchers_introduce_artist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkrgyl</id>
    <title>Qwen releases official quantized models of Qwen3</title>
    <updated>2025-05-12T12:39:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt; &lt;img alt="Qwen releases official quantized models of Qwen3" src="https://preview.redd.it/ok2e3kp5jc0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32d02567371fef442da1e95968e95dba1cbebc18" title="Qwen releases official quantized models of Qwen3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre officially releasing the quantized models of Qwen3 today!&lt;/p&gt; &lt;p&gt;Now you can deploy Qwen3 via Ollama, LM Studio, SGLang, and vLLM ‚Äî choose from multiple formats including GGUF, AWQ, and GPTQ for easy local deployment.&lt;/p&gt; &lt;p&gt;Find all models in the Qwen3 collection on Hugging Face.&lt;/p&gt; &lt;p&gt;Hugging FaceÔºö&lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ok2e3kp5jc0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkrgyl/qwen_releases_official_quantized_models_of_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T12:39:07+00:00</published>
  </entry>
</feed>
