<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-18T03:52:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k10yak</id>
    <title>Honest thoughts on the OpenAI release</title>
    <updated>2025-04-17T01:22:49+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"&gt; &lt;img alt="Honest thoughts on the OpenAI release" src="https://preview.redd.it/inywb6g3rave1.gif?width=216&amp;amp;crop=smart&amp;amp;s=01f6b7c9517ca3a073290dfd9cde5f09acbb00d9" title="Honest thoughts on the OpenAI release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay bring it on&lt;/p&gt; &lt;p&gt;o3 and o4-mini:&lt;br /&gt; - We all know full well from many open source research (like DeepseekMath and Deepseek-R1) that if you keep scaling up the RL, it will be better -&amp;gt; OpenAI just scale it up and sell an APIs, there are a few different but so how much better can it get?&lt;br /&gt; - More compute, more performance, well, well, &lt;strong&gt;more tokens?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;codex?&lt;br /&gt; - Github copilot used to be codex&lt;br /&gt; - Acting like there are not like a &lt;strong&gt;tons of things out there: Cline, RooCode, Cursor, Windsurf,...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Worst of all they &lt;strong&gt;are hyping up the community, the open source, local, community, for their commercial interest,&lt;/strong&gt; throwing out vague information about Open and Mug of OpenAI on ollama account etc...&lt;/p&gt; &lt;p&gt;Talking about 4.1 ? coding halulu, delulu yes benchmark is good.&lt;/p&gt; &lt;p&gt;Yeah that's my rant, downvote me if you want. I have been in this thing since 2023, and I find it more and more annoying following these news. It's misleading, it's boring, it has nothing for us to learn about, it has nothing for us to do except for paying for their APIs and maybe contributing to their open source client, which they are doing because they know there is no point just close source software. &lt;/p&gt; &lt;p&gt;This is pointless and sad development of the AI community and AI companies in general, we could be so much better and so much more, accelerating so quickly, yes we are here, paying for one more token and learn nothing &lt;strong&gt;(if you can call scaling RL which we all know is a LEARNING AT ALL).&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/inywb6g3rave1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k10yak/honest_thoughts_on_the_openai_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T01:22:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k14k6a</id>
    <title>JetBrains AI now has local llms integration and is free with unlimited code completions</title>
    <updated>2025-04-17T04:40:53+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"&gt; &lt;img alt="JetBrains AI now has local llms integration and is free with unlimited code completions" src="https://b.thumbs.redditmedia.com/2k-Ribx9nGO9Xazuaac7EOKvTqA2Do41B3wRLWXenuQ.jpg" title="JetBrains AI now has local llms integration and is free with unlimited code completions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.jetbrains.com/rider/whatsnew/"&gt;What's New in Rider&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Rider goes AI&lt;/p&gt; &lt;p&gt;JetBrains AI Assistant has received a major upgrade, making AI-powered development more accessible and efficient. With this release, &lt;strong&gt;AI features are now free in JetBrains IDEs&lt;/strong&gt;, including unlimited code completion, support for local models, and credit-based access to cloud-based features. &lt;a href="https://www.jetbrains.com/ai-ides/buy/"&gt;A new subscription system&lt;/a&gt; makes it easy to scale up with AI Pro and AI Ultimate tiers.&lt;/p&gt; &lt;p&gt;This release introduces major enhancements to boost productivity and reduce repetitive work, including smarter code completion, support for new cloud models like GPT-4.1 (—Åoming soon), Claude 3.7, and Gemini 2.0, advanced RAG-based context awareness, and a new Edit mode for multi-file edits directly from chat&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k14k6a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k14k6a/jetbrains_ai_now_has_local_llms_integration_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T04:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1v9rq</id>
    <title>CSM 1B is real-time now and has fine-tuning</title>
    <updated>2025-04-18T03:21:15+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidbrowne17/csm-streaming"&gt;https://github.com/davidbrowne17/csm-streaming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not sure if many of you have been following this model, but the open-source community has managed to reach real-time with streaming and figured out fine-tuning. This is my repo with fine-tuning and a real-time local chat demo, my version of fine-tuning is lora but there is also full fine tuning out there as well. Give it a try and let me know how it compares to other TTS models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T03:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k12i6l</id>
    <title>Trump administration reportedly considers a US DeepSeek ban</title>
    <updated>2025-04-17T02:44:14+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"&gt; &lt;img alt="Trump administration reportedly considers a US DeepSeek ban" src="https://preview.redd.it/80uc8c906bve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=027d4187d5af43a99f8442134a91d40393c2dc07" title="Trump administration reportedly considers a US DeepSeek ban" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/04/16/trump-administration-reportedly-considers-a-us-deepseek-ban/"&gt;https://techcrunch.com/2025/04/16/trump-administration-reportedly-considers-a-us-deepseek-ban/&lt;/a&gt;&lt;br /&gt; Washington Takes Aim at DeepSeek and Its American Chip Supplier, Nvidia: &lt;a href="https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html"&gt;https://www.nytimes.com/2025/04/16/technology/nvidia-deepseek-china-ai-trump.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80uc8c906bve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k12i6l/trump_administration_reportedly_considers_a_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T02:44:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1drtz</id>
    <title>I really didn't expect this.</title>
    <updated>2025-04-17T14:11:03+00:00</updated>
    <author>
      <name>/u/Educational_Grab_473</name>
      <uri>https://old.reddit.com/user/Educational_Grab_473</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1drtz/i_really_didnt_expect_this/"&gt; &lt;img alt="I really didn't expect this." src="https://preview.redd.it/jmyzbmtrkeve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3621e8ccd66f0f6b56d231728f888ec361fb1bc" title="I really didn't expect this." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Grab_473"&gt; /u/Educational_Grab_473 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmyzbmtrkeve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1drtz/i_really_didnt_expect_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1drtz/i_really_didnt_expect_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T14:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1h5eh</id>
    <title>DreamGen Lucid Nemo 12B: Story-Writing &amp; Role-Play Model</title>
    <updated>2025-04-17T16:31:53+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I am happy to share my latest model &lt;strong&gt;focused on story-writing and role-play&lt;/strong&gt;: &lt;a href="https://huggingface.co/dreamgen/lucid-v1-nemo"&gt;dreamgen/lucid-v1-nemo&lt;/a&gt; (GGUF and EXL2 available - thanks to bartowski, mradermacher and lucyknada).&lt;/p&gt; &lt;p&gt;Is Lucid worth your precious bandwidth, disk space and time? I don't know, but here's a bit of info about Lucid to help you decide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Focused on role-play &amp;amp; story-writing. &lt;ul&gt; &lt;li&gt;Suitable for all kinds of writers and role-play enjoyers:&lt;/li&gt; &lt;li&gt;For world-builders who want to specify every detail in advance: plot, setting, writing style, characters, locations, items, lore, etc.&lt;/li&gt; &lt;li&gt;For intuitive writers who start with a loose prompt and shape the narrative through instructions (OCC) as the story / role-play unfolds.&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;multi-character role-plays&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;Model can automatically pick between characters.&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;inline writing instructions (OOC)&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;Controlling plot development (say what should happen, what the characters should do, etc.)&lt;/li&gt; &lt;li&gt;Controlling pacing.&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;inline writing assistance&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;Planning the next scene / the next chapter / story.&lt;/li&gt; &lt;li&gt;Suggesting new characters.&lt;/li&gt; &lt;li&gt;etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Support for &lt;strong&gt;reasoning (opt-in)&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If that sounds interesting, I would love it if you check it out and let me know how it goes!&lt;/p&gt; &lt;p&gt;The README has &lt;strong&gt;extensive documentation, examples and SillyTavern presets!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1h5eh/dreamgen_lucid_nemo_12b_storywriting_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1h5eh/dreamgen_lucid_nemo_12b_storywriting_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1h5eh/dreamgen_lucid_nemo_12b_storywriting_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T16:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1rfe7</id>
    <title>I made this extension that applies the AI's changes semi-automatically without using an API.</title>
    <updated>2025-04-17T23:57:18+00:00</updated>
    <author>
      <name>/u/Delicious-Trash6988</name>
      <uri>https://old.reddit.com/user/Delicious-Trash6988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rfe7/i_made_this_extension_that_applies_the_ais/"&gt; &lt;img alt="I made this extension that applies the AI's changes semi-automatically without using an API." src="https://external-preview.redd.it/bnM5aHUyNGFmaHZlMfXWszA2aBwWyFSKhA5ZAJVyEflwSFKnBokwQQKV24Gs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77abeaf5703afbf932017ff900fc3d226488600f" title="I made this extension that applies the AI's changes semi-automatically without using an API." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, the AI responds in a certain format, and when you paste it into the extension, it automatically executes the commands ‚Äî creates files, etc. I made it in a short amount of time and wanted to know what you think. The idea was to have something that doesn't rely on APIs, which usually have a lot of limitations. It can be used with any AI ‚Äî you just need to set the system instructions.&lt;/p&gt; &lt;p&gt;If I were to continue developing it, I'd add more efficient editing (without needing to show the entire code), using search and replace, and so on.&lt;/p&gt; &lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items/?itemName=FelpolinColorado.buildy"&gt;https://marketplace.visualstudio.com/items/?itemName=FelpolinColorado.buildy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LIMITATIONS AND WARNING:&lt;/strong&gt; this extension is not secure at all. Even though it has a checkpoint system, it doesn‚Äôt ask for any permissions, so be very careful if you choose to use it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Trash6988"&gt; /u/Delicious-Trash6988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jxyur24afhve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rfe7/i_made_this_extension_that_applies_the_ais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rfe7/i_made_this_extension_that_applies_the_ais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T23:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k183aa</id>
    <title>Where is Qwen 3?</title>
    <updated>2025-04-17T08:48:36+00:00</updated>
    <author>
      <name>/u/Special_System_6627</name>
      <uri>https://old.reddit.com/user/Special_System_6627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was a lot of hype around the launch of Qwen 3 ( GitHub PRs, tweets and all) Where did the hype go all of a sudden?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special_System_6627"&gt; /u/Special_System_6627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k183aa/where_is_qwen_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k183aa/where_is_qwen_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k183aa/where_is_qwen_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T08:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1tdpa</id>
    <title>Instantly allocate more graphics memory on your Mac VRAM Pro</title>
    <updated>2025-04-18T01:38:15+00:00</updated>
    <author>
      <name>/u/DazzlingHedgehog6650</name>
      <uri>https://old.reddit.com/user/DazzlingHedgehog6650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/"&gt; &lt;img alt="Instantly allocate more graphics memory on your Mac VRAM Pro" src="https://b.thumbs.redditmedia.com/uSwR7bMLHrm2g93GDEfykSelIThQL7debX7qx7INGsU.jpg" title="Instantly allocate more graphics memory on your Mac VRAM Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a tiny macOS utility that does one very specific thing:&lt;br /&gt; &lt;strong&gt;It unlocks additional GPU memory on Apple Silicon Macs.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why? Because macOS doesn‚Äôt give you any control over VRAM ‚Äî and hard caps it, leading to swap issues in certain use cases.&lt;/p&gt; &lt;p&gt;I needed it for performance in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running large LLMs&lt;/li&gt; &lt;li&gt;Blender and After Effects&lt;/li&gt; &lt;li&gt;Unity and Unreal previews&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So‚Ä¶ I made &lt;strong&gt;VRAM Pro&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß† Simple: Just sits in your menubar&lt;/li&gt; &lt;li&gt;üîì Lets you allocate more VRAM &lt;/li&gt; &lt;li&gt;üîê Notarized, signed, autoupdates&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì¶ Download:&lt;/h1&gt; &lt;p&gt;&lt;a href="https://vrampro.com/"&gt;https://VRAMPro.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you need this app? No! You can do this with various commands in terminal. But wanted a nice and easy GUI way to do this.&lt;/p&gt; &lt;p&gt;Would love feedback, and happy to tweak it based on use cases!&lt;br /&gt; Also ‚Äî if you‚Äôve got other obscure GPU tricks on macOS, I‚Äôd &lt;em&gt;love&lt;/em&gt; to hear them.&lt;/p&gt; &lt;p&gt;Thanks Reddit üôè&lt;/p&gt; &lt;p&gt;PS: after I made this app someone created am open source copy: &lt;a href="https://github.com/PaulShiLi/Siliv"&gt;https://github.com/PaulShiLi/Siliv&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DazzlingHedgehog6650"&gt; /u/DazzlingHedgehog6650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k1tdpa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T01:38:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1o71i</id>
    <title>SecondMe/Mindverse - stay away</title>
    <updated>2025-04-17T21:26:07+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1o71i/secondmemindverse_stay_away/"&gt; &lt;img alt="SecondMe/Mindverse - stay away" src="https://preview.redd.it/0fu8ie5eqgve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a7d0d6e3cf5517e18cc0d17a997e82ee9ab5ee0" title="SecondMe/Mindverse - stay away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a heads up - Mindverse/SecondMe are lowkey scamming to funnel people to their product.&lt;/p&gt; &lt;p&gt;How do I know? I received an email above, seemingly an invitation to proceed with my application to their AI startup. But here's the thing: - I only use this email address on GitHub - so I know it was sourced from there - I never applied to any jobs from Mindverse, I'm happily employed&lt;/p&gt; &lt;p&gt;This is the same entity that was promoting SecondMe here and on other LLM subs a week or so ago - their posts were questionable but nothing out of ordinary for LLM/AI projects. However email above is at least misleading and at most just a scam - so be aware and stay away.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0fu8ie5eqgve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1o71i/secondmemindverse_stay_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1o71i/secondmemindverse_stay_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T21:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1myni</id>
    <title>LMArena public beta officially releases with a new UI. (No more gradio) | https://beta.lmarena.ai</title>
    <updated>2025-04-17T20:33:00+00:00</updated>
    <author>
      <name>/u/HostFit8686</name>
      <uri>https://old.reddit.com/user/HostFit8686</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1myni/lmarena_public_beta_officially_releases_with_a/"&gt; &lt;img alt="LMArena public beta officially releases with a new UI. (No more gradio) | https://beta.lmarena.ai" src="https://b.thumbs.redditmedia.com/5-5toRbipdayS6ZmMRoDrI4l0QrDd2eTZnBkQ_mJICU.jpg" title="LMArena public beta officially releases with a new UI. (No more gradio) | https://beta.lmarena.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HostFit8686"&gt; /u/HostFit8686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k1myni"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1myni/lmarena_public_beta_officially_releases_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1myni/lmarena_public_beta_officially_releases_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T20:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1fk88</id>
    <title>FULL LEAKED Devin AI System Prompts and Tools</title>
    <updated>2025-04-17T15:26:48+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 17/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get full official Devin AI system prompts, including its tools. Over 400 lines.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fk88/full_leaked_devin_ai_system_prompts_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fk88/full_leaked_devin_ai_system_prompts_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fk88/full_leaked_devin_ai_system_prompts_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T15:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k18pb4</id>
    <title>Gemma's license has a provision saying "you must make "reasonable efforts to use the latest version of Gemma"</title>
    <updated>2025-04-17T09:34:02+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k18pb4/gemmas_license_has_a_provision_saying_you_must/"&gt; &lt;img alt="Gemma's license has a provision saying &amp;quot;you must make &amp;quot;reasonable efforts to use the latest version of Gemma&amp;quot;" src="https://preview.redd.it/pn9z3hg67dve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86c7084ad25d72a8afaffab2fd42032b322889a8" title="Gemma's license has a provision saying &amp;quot;you must make &amp;quot;reasonable efforts to use the latest version of Gemma&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pn9z3hg67dve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k18pb4/gemmas_license_has_a_provision_saying_you_must/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k18pb4/gemmas_license_has_a_provision_saying_you_must/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T09:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1mnqk</id>
    <title>Every time I see an open source alternative to a trending proprietary agent</title>
    <updated>2025-04-17T20:20:11+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1mnqk/every_time_i_see_an_open_source_alternative_to_a/"&gt; &lt;img alt="Every time I see an open source alternative to a trending proprietary agent" src="https://preview.redd.it/ueghfakmegve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726cf7ec57e3a0ca01f38535f67b5b186eeb9f0b" title="Every time I see an open source alternative to a trending proprietary agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueghfakmegve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1mnqk/every_time_i_see_an_open_source_alternative_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1mnqk/every_time_i_see_an_open_source_alternative_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T20:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1fe1d</id>
    <title>Scrappy underdog GLM-4-9b still holding onto the top spot (for local models) for lowest hallucination rate</title>
    <updated>2025-04-17T15:19:43+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fe1d/scrappy_underdog_glm49b_still_holding_onto_the/"&gt; &lt;img alt="Scrappy underdog GLM-4-9b still holding onto the top spot (for local models) for lowest hallucination rate" src="https://preview.redd.it/63jtqmp0xeve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6ad8da98a91331c0ffb08d26fe5050f440773c8" title="Scrappy underdog GLM-4-9b still holding onto the top spot (for local models) for lowest hallucination rate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4-9b appreciation post here (the older version, not the new one). This little model has been a production RAG workhorse for me for like the last 4 months or so. I‚Äôve tried it against so many other models and it just crushes at fast RAG. To be fair, QwQ-32b blows it out of the water for RAG when you have time to spare, but if you need a fast answer or are resource limited, GLM-4-9b is still the GOAT in my opinion.&lt;/p&gt; &lt;p&gt;The fp16 is only like 19 GB which fits well on a 3090 with room to spare for context window and a small embedding model like Nomic.&lt;/p&gt; &lt;p&gt;Here‚Äôs the specific version I found seems to work best for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/glm4:9b-chat-fp16"&gt;https://ollama.com/library/glm4:9b-chat-fp16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs consistently held the top spot for local models on Vectara‚Äôs Hallucinations Leaderboard for quite a while now despite new ones being added to the leaderboard fairly frequently. Last update was April 10th. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vectara/hallucination-leaderboard?tab=readme-ov-file"&gt;https://github.com/vectara/hallucination-leaderboard?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm very eager to try all the new GLM models that were released earlier this week. Hopefully Ollama will add support for them soon, if they don‚Äôt, then I guess I‚Äôll look into LM Studio. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/63jtqmp0xeve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fe1d/scrappy_underdog_glm49b_still_holding_onto_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fe1d/scrappy_underdog_glm49b_still_holding_onto_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T15:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1rjm1</id>
    <title>How to run Llama 4 fast, even though it's too big to fit in RAM</title>
    <updated>2025-04-18T00:03:01+00:00</updated>
    <author>
      <name>/u/Klutzy-Snow8016</name>
      <uri>https://old.reddit.com/user/Klutzy-Snow8016</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: in your llama.cpp command, add:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 49 --override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Explanation:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 49&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;offload all 49 layers to GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;--override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;...except for the MOE weights&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;--ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;process the prompt in batches of 1 at a time (instead of the default 512 - otherwise your SSD will be the bottleneck and prompt processing will be slower)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This radically speeds up inference by taking advantage of LLama 4's MOE architecture. LLama 4 Maverick has 400 billion total parameters, but only 17 billion active parameters. Some are needed on every token generation, while others are only occasionally used. So if we put the parameters that are always needed onto GPU, those will be processed quickly, and there will just be a small number that need to be handled by the CPU. This works so well that the weights don't even need to all fit in your CPU's RAM - many of them can memory mapped from NVMe.&lt;/p&gt; &lt;p&gt;My results with Llama 4 Maverick:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unsloth's UD-Q4_K_XL quant is 227GB&lt;/li&gt; &lt;li&gt;Unsloth's Q8_0 quant is 397GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both of those are much bigger than my RAM + VRAM (128GB + 3x24GB). But with these tricks, I get 15 tokens per second with the UD-Q4_K_M and 6 tokens per second with the Q8_0.&lt;/p&gt; &lt;p&gt;Full llama.cpp server commands:&lt;/p&gt; &lt;p&gt;Note: the &lt;code&gt;--override-tensor&lt;/code&gt; command is tweaked because I had some extra VRAM available, so I offloaded most of the MOE layers to CPU, but loaded a few onto each GPU.&lt;/p&gt; &lt;p&gt;UD-Q4_K_XL:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00001-of-00005.gguf -ngl 49 -fa -c 16384 --override-tensor &amp;quot;([1][1-9]|[2-9][0-9]).ffn_.*_exps.=CPU,([0-2]).ffn_.*_exps.=CUDA0,([3-6]).ffn_.*_exps.=CUDA1,([7-9]|[1][0]).ffn_.*_exps.=CUDA2&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Q8_0:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m Llama-4-Maverick-17B-128E-Instruct-Q8_0-00001-of-00009.gguf -ngl 49 -fa -c 16384 --override-tensor &amp;quot;([6-9]|[1-9][0-9]).ffn_.*_exps.=CPU,([0-1]).ffn_.*_exps.=CUDA0,([2-3]).ffn_.*_exps.=CUDA1,([4-5]).ffn_.*_exps.=CUDA2&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Credit goes to the people behind Unsloth for this knowledge. I hadn't seen people talking about this here, so I thought I'd make a post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klutzy-Snow8016"&gt; /u/Klutzy-Snow8016 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T00:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1io81</id>
    <title>Geobench - A benchmark to measure how well llms can pinpoint the location based on a Google Streetview image.</title>
    <updated>2025-04-17T17:34:19+00:00</updated>
    <author>
      <name>/u/Jupaoqqq</name>
      <uri>https://old.reddit.com/user/Jupaoqqq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1io81/geobench_a_benchmark_to_measure_how_well_llms_can/"&gt; &lt;img alt="Geobench - A benchmark to measure how well llms can pinpoint the location based on a Google Streetview image." src="https://b.thumbs.redditmedia.com/MabugYj2dwcrZmMJIFEFGGjxMBEfPs-rLfLDdfOS5BM.jpg" title="Geobench - A benchmark to measure how well llms can pinpoint the location based on a Google Streetview image." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://geobench.org/"&gt;https://geobench.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically it makes llms play the game GeoGuessr, and find out how well each model performs on common metrics in the GeoGuessr community - if it guess the correct country, the distance between its guess and the actual location (measured by average and median score)&lt;/p&gt; &lt;p&gt;Credit to the original site creator Illusion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jupaoqqq"&gt; /u/Jupaoqqq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k1io81"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1io81/geobench_a_benchmark_to_measure_how_well_llms_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1io81/geobench_a_benchmark_to_measure_how_well_llms_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T17:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1ifw5</id>
    <title>What are the people dropping &gt;10k on a setup using it for?</title>
    <updated>2025-04-17T17:24:48+00:00</updated>
    <author>
      <name>/u/Ashefromapex</name>
      <uri>https://old.reddit.com/user/Ashefromapex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Surprisingly often I see people on here asking for advice on what to buy for local llm inference/training with a budget of &amp;gt;10k $. As someone who uses local llms as a hobby, I myself have bought a nice macbook and a rtx3090 (making it a pretty expensive hobby). But i guess when spending this kind of money, it serves a deeper purpose than just for a hobby right? So what are yall spending this kind of money using it for? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashefromapex"&gt; /u/Ashefromapex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ifw5/what_are_the_people_dropping_10k_on_a_setup_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ifw5/what_are_the_people_dropping_10k_on_a_setup_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ifw5/what_are_the_people_dropping_10k_on_a_setup_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T17:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1av1x</id>
    <title>Medium sized local models already beating vanilla ChatGPT - Mind blown</title>
    <updated>2025-04-17T11:52:34+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was used to stupid &amp;quot;Chatbots&amp;quot; by companies, who just look for some key words in your question to reference some websites.&lt;/p&gt; &lt;p&gt;When ChatGPT came out, there was nothing comparable and for me it was mind blowing how a chatbot is able to really talk like a human about everything, come up with good advice, was able to summarize etc.&lt;/p&gt; &lt;p&gt;Since ChatGPT (GPT-3.5 Turbo) is a huge model, I thought that todays small and medium sized models (8-30B) would still be waaay behind ChatGPT (and this was the case, when I remember the good old llama 1 days).&lt;br /&gt; Like:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Tier 1: The big boys (GPT-3.5/4, Deepseek V3, Llama Maverick, etc.)&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Tier 2: Medium sized (100B), pretty good, not perfect, but good enough when privacy is a must&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Tier 3: The children area (all 8B-32B models)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Since the progress in AI performance is gradually, I asked myself &amp;quot;How much better now are we from vanilla ChatGPT?&amp;quot;. So I tested it against Gemma3 27B with IQ3_XS which fits into 16GB VRAM with some prompts about daily advice, summarizing text or creative writing.&lt;/p&gt; &lt;p&gt;And hoooly, &lt;strong&gt;we have reached and even surpassed vanilla ChatGPT (GPT-3.5) and it runs on consumer hardware&lt;/strong&gt;!!!&lt;/p&gt; &lt;p&gt;I thought I mention this so we realize how far we are now with local open source models, because we are always comparing the newest local LLMs with the newest closed source top-tier models, which are being improved, too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1av1x/medium_sized_local_models_already_beating_vanilla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1av1x/medium_sized_local_models_already_beating_vanilla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1av1x/medium_sized_local_models_already_beating_vanilla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T11:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1hm53</id>
    <title>BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!</title>
    <updated>2025-04-17T16:50:53+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"&gt; &lt;img alt="BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!" src="https://a.thumbs.redditmedia.com/W3FZqegPamqJWv9vRdKzCg1qWqTclI5Hf1QcM-4Jjj8.jpg" title="BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/gargighosh/status/1912908118939541884"&gt;https://x.com/gargighosh/status/1912908118939541884&lt;/a&gt; &lt;a href="https://github.com/facebookresearch/blt/pull/97"&gt;https://github.com/facebookresearch/blt/pull/97&lt;/a&gt; &lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/"&gt;https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2412.09871"&gt;https://arxiv.org/abs/2412.09871&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k1hm53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T16:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1nle9</id>
    <title>Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)</title>
    <updated>2025-04-17T21:00:19+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"&gt; &lt;img alt="Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)" src="https://external-preview.redd.it/Z2JsMWRwbGRrZ3ZlMYv6snaPu9pbXPiAyfaNKBLULaPCIm0pnygnovycxPje.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eab23a4ac22667d41e99959dfc570a8f5b1ece0" title="Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eni76fldkgve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T21:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1ahr4</id>
    <title>Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that‚Äôs specifically optimized for machine learning applications</title>
    <updated>2025-04-17T11:31:44+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"&gt; &lt;img alt="Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that‚Äôs specifically optimized for machine learning applications" src="https://preview.redd.it/d044iigqrdve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45e7348a9b7348bb6993397c3dbf74ba198c4943" title="Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that‚Äôs specifically optimized for machine learning applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Verge: &lt;a href="https://www.theverge.com/news/650467/wikipedia-kaggle-partnership-ai-dataset-machine-learning"&gt;https://www.theverge.com/news/650467/wikipedia-kaggle-partnership-ai-dataset-machine-learning&lt;/a&gt;&lt;br /&gt; Wikipedia Kaggle Dataset using Structured Contents Snapshot: &lt;a href="https://enterprise.wikimedia.com/blog/kaggle-dataset/"&gt;https://enterprise.wikimedia.com/blog/kaggle-dataset/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d044iigqrdve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T11:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1ujk4</id>
    <title>It's been hours already without a new open source SOTA being released. Is open source LLMs dead? Is this it???</title>
    <updated>2025-04-18T02:40:50+00:00</updated>
    <author>
      <name>/u/DamiaHeavyIndustries</name>
      <uri>https://old.reddit.com/user/DamiaHeavyIndustries</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RIP LLMS!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DamiaHeavyIndustries"&gt; /u/DamiaHeavyIndustries &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ujk4/its_been_hours_already_without_a_new_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ujk4/its_been_hours_already_without_a_new_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ujk4/its_been_hours_already_without_a_new_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T02:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1qpr6</id>
    <title>microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft</title>
    <updated>2025-04-17T23:22:11+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt; &lt;img alt="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" src="https://external-preview.redd.it/oacNTVfe15Ozahhiv8YMZ-Teu__pPBVygtAgzE9FP3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=140a805e8a294974bbef97d4e1035ef969130c5a" title="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MAI-DS-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T23:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1fi5w</id>
    <title>New society is taking shape</title>
    <updated>2025-04-17T15:24:23+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt; &lt;img alt="New society is taking shape" src="https://preview.redd.it/05n7cxquxeve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3503ecb35d404084ae9522bb771aecd69177ee0c" title="New society is taking shape" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/05n7cxquxeve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T15:24:23+00:00</published>
  </entry>
</feed>
