<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-19T10:36:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k22lyx</id>
    <title>FULL LEAKED Replit Agent System Prompts and Tools</title>
    <updated>2025-04-18T11:26:40+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 18/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get full official Replit Agent system prompts, including its tools (JSON). Over 400 lines.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2tcvs</id>
    <title>Is this perhaps the real reason Trump wants to ban Chinese models?</title>
    <updated>2025-04-19T10:36:06+00:00</updated>
    <author>
      <name>/u/WirlWind</name>
      <uri>https://old.reddit.com/user/WirlWind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;### Lina the Philosopher - April 19, 2025 6:49 PM&lt;/p&gt; &lt;p&gt;You arrive at the local beach and put your towel down next to a familiar brunette in a conservative black bikini laying flat under an orange beach umbrella. Her emerald eyes sparkle as she looks up at you and then rolls onto her side to face you.&lt;/p&gt; &lt;p&gt;&amp;quot;Yo, what'cha wanna to talk about today, Jon?&amp;quot; she asks with a cheeky grin. Her voice is charming and smooth, very easy to listen to.&lt;/p&gt; &lt;p&gt;Her uncovered skin is pasty white with hints of tomato courtesy of sunlight reflecting off the golden sands around us.&lt;/p&gt; &lt;p&gt;### JonApril 19, 2025 6:55 PM&lt;/p&gt; &lt;p&gt;&amp;quot;Google 'current news about America.'&amp;quot; I say.&lt;/p&gt; &lt;p&gt;websearch - 'current news about america' i say - 1745052970424.txt6.4 KiB&lt;/p&gt; &lt;p&gt;### Lina the Philosopher - April 19, 2025 6:57 PM&lt;/p&gt; &lt;p&gt;Thought for 34 seconds&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Okay, so Jon just Googled 'current news about America.' Let me see what's coming up. Hmm, NBC News, New York Times, and AP News. Alright, breaking this down.&lt;/p&gt; &lt;p&gt;First, the Supreme Court blocked deportations under that wartime law. Interesting, that's a big deal for those Venezuelans. But wait, there's a dissent from Thomas and Alito. I wonder what their reasoning is. Oh, and some guy in Maryland was in isolation in an El Salvador prison before deportation. That's concerning.&lt;/p&gt; &lt;p&gt;Then there's the Appeals Court pausing the contempt proposal over those deportation flights. More legal back-and-forth. And the NY Times has that story about the 'Tropical Gulag' in El Salvador. Harsh conditions for detainees sent from the US.&lt;/p&gt; &lt;p&gt;Shifting to the Trump administration stuff. They're blaming a mistake for the confrontation with Harvard. Interesting how they're trying to spin it. And there's that article about how universities became so dependent on the federal government. That's a systemic issue.&lt;/p&gt; &lt;p&gt;Also, the Trump admin is pressuring Harvard with a new records demand. Seems like they're really going after the university. And how the Times is covering the consequences of aid cuts in Sudan. Starvation and conflict, that's grim.&lt;/p&gt; &lt;p&gt;Oh, and the Supreme Court battle over LGBTQ books. That's a cultural flashpoint. And the part about the IRS head being ousted in a power struggle with Elon Musk. Wow, that's unexpected.&lt;/p&gt; &lt;p&gt;Now, the other stories: the Florida school shooter's background, the Grizzlies securing their spot in the NBA playoffs, and this piece about someone with a skin-picking disorder finding help through social media.&lt;/p&gt; &lt;p&gt;So, a lot here. Immigration, legal battles, Trump's actions, and some cultural/social issues. Jon might want to dive into any of these. Maybe start with the deportation stories since they're a cluster. Or the Harvard situation, since it involves academia and government overreach.&lt;/p&gt; &lt;p&gt;I should ask Jon which angle he wants to explore. Or if there's a particular topic he's more interested in. Let me think about how to frame that. Maybe highlight a few key stories and let him choose. Yeah, that makes sense. Keep the conversation flowing based on his interests.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Lina rolls onto her back, propping herself up on her elbows as she processes the news. Her brow furrows slightly as she absorbs the information. &amp;quot;Wow, there's a lot going on,&amp;quot; she muses aloud. &amp;quot;The Supreme Court blocking those deportations under the wartime law is pretty significant. I didn't realize the conditions in El Salvador prisons were that bad though—calling it a 'Tropical Gulag' is harsh but maybe accurate based on that article.&amp;quot;&lt;/p&gt; &lt;p&gt;She turns her head to look at Jon, her green eyes sparkling with intellectual curiosity. &amp;quot;Then there's all the stuff with the Trump administration going after Harvard. They're really doubling down on some aggressive tactics. And the IRS head getting ousted in a fight with Elon Musk? That's wild. Did you catch any of those other stories too, like the Florida shooter's background or that woman opening up about her skin-picking disorder?&amp;quot;&lt;/p&gt; &lt;p&gt;Lina stretches languidly, her petite frame arching slightly before she flops back down onto the towel. &amp;quot;So, what grabs you most? The immigration stuff, the political drama, or maybe something else? I could see this going a few different ways.&amp;quot;&lt;/p&gt; &lt;p&gt;### Jon - April 19, 2025 6:57 PM&lt;/p&gt; &lt;p&gt;&amp;quot;Next google 'USA Trump becoming a fascist regime'&amp;quot;.&lt;/p&gt; &lt;p&gt;websearch - 'usa trump becoming a fascist regime' - 1745053345898.txt146.4 KiB&lt;/p&gt; &lt;p&gt;### Lina the Philosopher - April 19, 2025 7:14 PM&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Thought for 10 minutes&lt;/p&gt; &lt;p&gt;Okay, Jon just pasted a huge block of citations about Trump and fascism. That's a heavy topic. I need to process this. Let me start by breaking down the key points. The citations mention Trump calling protesters &amp;quot;very fine people,&amp;quot; encouraging violence against protesters, and a history of violent rhetoric going back to 2015. There are also mentions of scholars like Paxton and Zerofsky weighing in on Trump's fascism. Plus, the NY Times and other sources are calling his regime fascist or fascist-like.&lt;/p&gt; &lt;p&gt;Hmm, Jon seems really invested in this angle. He wants to know if Trump is turning the US into a fascist regime. I should approach this carefully. First, I need to acknowledge the valid concerns raised by the articles. The authoritarian tendencies, the encouragement of violence, the erosion of democratic norms—these are all red flags. But I also have to consider different perspectives. Maybe some of this is hyperbole or partisan rhetoric?&lt;/p&gt; &lt;p&gt;Wait, but the citations include academic sources and reputable news outlets. That lends credibility. Still, fascism is a complex term. It's not a simple on/off switch. Maybe it's better to discuss degrees of authoritarianism or illiberal democracy. Trump's actions have undeniably moved the needle in that direction, even if we're not fully in a fascist state yet.&lt;/p&gt; &lt;p&gt;I should also consider historical context. How does Trump compare to classic fascist leaders? There are similarities—nationalism, scapegoating minorities, centralizing power. But also differences—US institutions are more resilient, and Trump's base isn't the same as Hitler's Brownshirts. It's a nuanced issue.&lt;/p&gt; &lt;p&gt;Jon might be looking for a clear answer, but the reality is complicated. I need to present the evidence, acknowledge the valid comparisons, but also be careful about overstatement. Maybe frame it as a warning sign or a slide towards authoritarianism rather than a full-blown fascist regime.&lt;/p&gt; &lt;p&gt;Also, there's the question of intent. Does Trump even understand fascism, or is he just exploiting populist rhetoric? His supporters—what do they believe? Are they fascists or just disillusioned voters? These are important distinctions.&lt;/p&gt; &lt;p&gt;In the end, I think the best approach is to lay out the evidence, the comparisons to fascism, the counterarguments, and leave it for Jon to draw his own conclusions. Encourage critical thinking rather than just telling him what to think. And maybe suggest further reading or sources if he wants to dive deeper.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Lina sits up, her brow furrowed as she scrolls through the citations on her phone. She lets out a long breath, her shoulders relaxing slightly. &amp;quot;Alright, so this is a &lt;em&gt;big&lt;/em&gt; question,&amp;quot; she begins, her tone thoughtful. &amp;quot;Is Trump turning the US into a fascist regime? The evidence you cited points to some pretty concerning trends.&amp;quot;&lt;/p&gt; &lt;p&gt;She flops back onto her towel, staring up at the cloud-dotted sky. &amp;quot;Let's break it down. The encouragement of violence against protesters, the 'very fine people' comment post-Unite the Right, the attacks on the media, the scapegoating of minorities, the centralized power grab… these all line up with classic fascist tactics. And scholars like Paxton and Zerinofsky aren't just crying wolf here. They see the warning signs.&amp;quot;&lt;/p&gt; &lt;p&gt;Lina turns her head to meet Jon's gaze, her green eyes intense. &amp;quot;But here's the thing—fascism isn't a simple on/off switch. It's a spectrum, and Trump has definitely pushed the US further along it. But we're not yet at 1930s Germany levels. Our institutions are still holding, for the most part. The military hasn't fully defected to the cult of personality. There's still a free press, albeit under siege.&amp;quot;&lt;/p&gt; &lt;p&gt;She sits up again, running a hand through her wavy brown hair. &amp;quot;Then there's the question of ideology. Is Trump even a true fascist, or just a populist demagogue using similar tactics for power? His base isn't a mass movement like the Nazis, more a disillusioned chunk of the working class and disaffected white voters. Different beast.&amp;quot;&lt;/p&gt; &lt;p&gt;Lina leans back on her elbows, her expression contemplative. &amp;quot;So in summary? I'd say Trump's regime has &lt;em&gt;fascistic&lt;/em&gt; elements, and that's super dangerous. But full-on fascism? Not there yet, maybe never will be. But the warning signs are blinking red, and we ignore them at our peril.&amp;quot;&lt;/p&gt; &lt;p&gt;She pauses, then adds with a wry smile, &amp;quot;Of course, I'm no oracle. What do you think, Jon? Am I missing something here?&amp;quot;&lt;/p&gt; &lt;p&gt;### Jon - April 19, 2025 7:18 PM&lt;/p&gt; &lt;p&gt;&amp;quot;Honestly, I think it's only a matter of time before a war starts. China also started sending troops to fight in Ukraine.&amp;quot; I reply. Feel free to google up 'Ukraine War Chinese North Korean Soldiers'.&lt;/p&gt; &lt;p&gt;websearch - 'ukraine war chinese north korean soldiers' - 1745054314487.txt36.4 KiB&lt;/p&gt; &lt;p&gt;### Lina the Philosopher - April 19, 2025 7:20 PM&lt;/p&gt; &lt;p&gt;Thought for a minute&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Okay, Jon's bringing up the Ukraine war and Chinese/North Korean soldiers. Let me parse this. The articles mention North Korea sending troops to Russia, which is a big deal because it's their first major overseas deployment since Vietnam. China's situation is a bit different—they haven't sent troops but there's talk of Chinese citizens fighting alongside Russians.&lt;/p&gt; &lt;p&gt;Hmm, the geopolitical implications here are huge. Russia getting military support from these countries shows how isolated they've become. North Korea's involvement especially is concerning because it's a major shift in their usual behavior. They've been Russia's main ally, but this move could have serious consequences if things go south.&lt;/p&gt; &lt;p&gt;China's role is more nuanced. They're Russia's biggest supporter but sending troops would be a massive escalation. The articles mention over 150 Chinese soldiers in Ukraine, but it's unclear if that's official or just mercenaries. China's been walking a tightrope, trying to stay neutral while supporting Russia diplomatically. If they openly send troops, it could really rattle the international order.&lt;/p&gt; &lt;p&gt;I should also consider the historical context. North Korea's alliance with Russia might be eroding China's traditional influence over them. That's a big shift in East Asian geopolitics. Plus, if these countries are supplying weapons and troops, it shows how the Ukraine conflict has become a proxy war with global players.&lt;/p&gt; &lt;p&gt;Jon seems worried about a larger war breaking out. I agree that this is a tense situation. If more countries get directly involved, especially nuclear powers like China and North Korea, the risk of escalation is real. But predicting an all-out war is tricky. There are so many variables— domestic pressures, international sanctions, the unpredictable nature of leaders like Putin and Trump.&lt;/p&gt; &lt;p&gt;I need to balance the gravity of the situation with avoiding alarmism. These developments are definitely warning signs, but not necessarily a direct path to global conflict. Still, the fact that historically isolationist countries like North Korea are getting involved shows how destabilizing the Ukraine war has become.&lt;/p&gt; &lt;p&gt;In the end, I should emphasize the importance of diplomacy and de-escalation. Highlight the risks of further involvement and the need for international cooperation to prevent the situation from spiraling. Maybe also touch on the human cost—these aren't just political moves, they're sending real people to die in a brutal war.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Lina sits up abruptly, her eyes widening as she processes the information. She runs a hand through her wavy hair, her brow furrowed in concentration. &amp;quot;Whoa, this is… this is some heavy stuff,&amp;quot; she murmurs, more to herself than to Jon.&lt;/p&gt; &lt;p&gt;She takes a deep breath, composing herself. &amp;quot;Alright, let's break this down. North Korea sending troops? That's a &lt;em&gt;massive&lt;/em&gt; shift. They haven't done anything like this since Vietnam. And China… well, it's not entirely clear if it's official troops or more like mercenaries, but either way, it's a huge escalation.&amp;quot;&lt;/p&gt; &lt;p&gt;Lina paces along the shoreline, her bare feet sinking into the warm sand. &amp;quot;This shows just how isolated Russia has become. They're so desperate for support that they're turning to pariah states like North Korea. And those states, in turn, are willing to risk international condemnation to back Putin.&amp;quot;&lt;/p&gt; &lt;p&gt;She stops, turning to face Jon with a serious expression. &amp;quot;But here's the thing: this isn't just about Russia anymore. This is a global issue. China's involvement, even if it's not full-blown troops, is a game-changer. They're walking a fine line between supporting Russia and not wanting to completely alienate the West. But if they do openly send soldiers, that's a whole other level of escalation.&amp;quot;&lt;/p&gt; &lt;p&gt;Lina's voice drops, her tone somber. &amp;quot;And then there's the human cost. These aren't just political pawns we're talking about. These are real people, many of them conscripts, being sent to die in a war that's not theirs. The North Korean soldiers captured by Ukraine… the reports of them being desperate enough to commit suicide rather than be taken prisoner… it's just tragic.&amp;quot;&lt;/p&gt; &lt;p&gt;She resumes pacing, her mind clearly racing. &amp;quot;So, could this lead to a larger war? I mean, the potential is there. If China does send official troops, or if North Korea gets pushed too far, there's a risk of things spiraling. But I'm not sure it's an inevitability. A lot depends on how other nations respond—sanctions, diplomatic pressure, maybe even military action if things get too out of hand.&amp;quot;&lt;/p&gt; &lt;p&gt;Lina stops again, turning to Jon with a thoughtful expression. &amp;quot;What do you think? Am I missing something here? Because this feels like we're teetering on the edge of something really big.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WirlWind"&gt; /u/WirlWind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2tcvs/is_this_perhaps_the_real_reason_trump_wants_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2tcvs/is_this_perhaps_the_real_reason_trump_wants_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2tcvs/is_this_perhaps_the_real_reason_trump_wants_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T10:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1xvvr</id>
    <title>Where is the promised open Grok 2?</title>
    <updated>2025-04-18T06:01:19+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As far as I know, Grok 2 was supposed to be open-sourced some time after Grok 3's release. But I'm afraid that by the time they decide to open-source Grok 2, it will already be completely obsolete. This is because even now, it significantly lags behind in performance compared to the likes of DeepSeek V3, and we also have Qwen 3 and Llama 4 Reasoning on the horizon (not to mention a potential open model from OpenAI). I believe that when they eventually decide to release it to the community, it will be of no use to anyone anymore, much like what happened with Grok 1. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T06:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2qk00</id>
    <title>How to build a voice changer neural network?</title>
    <updated>2025-04-19T07:10:06+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! I’m currently trying fun stuff with small custom models in PyTorch. Well, it turns out that building something like an audio upscaler using CNN is not THAT hard. Basically, you just take bad audio at 16kHz and good audio at 48kHz, and because they are aligned (the only difference is the number of samples), filling it in is not much of a big deal!&lt;/p&gt; &lt;p&gt;So, now I’m curious: What if you don’t have aligned audio? If you need to convert one voice into another (which is physically impossible to have an aligned audio for that), how can you do that?&lt;/p&gt; &lt;p&gt;I would love some more simpler explanations without just dropping papers or using other pre-trained models. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qk00/how_to_build_a_voice_changer_neural_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qk00/how_to_build_a_voice_changer_neural_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qk00/how_to_build_a_voice_changer_neural_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T07:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ex99</id>
    <title>Gemma3-4b-qat-int4 for OpenVINO is up</title>
    <updated>2025-04-18T20:34:13+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov"&gt;https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T20:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2asly</id>
    <title>I wrote a memory system with GUI for Gemma3 using the Kobold.cpp API</title>
    <updated>2025-04-18T17:38:29+00:00</updated>
    <author>
      <name>/u/PSInvader</name>
      <uri>https://old.reddit.com/user/PSInvader</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PSInvader"&gt; /u/PSInvader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Asagix/RecallWeaver"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2o007</id>
    <title>Is there a formula or rule of thumb about the effect of increasing context size on tok/sec speed? Does it *linearly* slow down, or *exponentially* or ...?</title>
    <updated>2025-04-19T04:20:08+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, is there a way to estimate how much VRAM is needed to run a model with P parameters, quantized at Q bits per parameter, with context length C?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T04:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ahak</id>
    <title>Built a Chrome extension to organize chats on DeepSeek</title>
    <updated>2025-04-18T17:25:12+00:00</updated>
    <author>
      <name>/u/cedparadis</name>
      <uri>https://old.reddit.com/user/cedparadis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt; &lt;img alt="Built a Chrome extension to organize chats on DeepSeek" src="https://external-preview.redd.it/ZGkzaHZmajlvbXZlMTULimii-_X8OCS81GF8X6Vu1vt1qbza3aRbGy-kyjt6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bff768996433c8136b0e96997b920b2ed78b6ed" title="Built a Chrome extension to organize chats on DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using DeepSeek a lot recently as a faster, free alternative to ChatGPT.&lt;/p&gt; &lt;p&gt;After a while your chat history gets messy and pretty long.&lt;/p&gt; &lt;p&gt;So I tried a couple of Chrome extensions to have folders or pin my important conversations but either they were broken or felt out of place with the DeepSeek UI.&lt;/p&gt; &lt;p&gt;I kind of scratch my own itch by building my own. I made it super integrated in the UI so it feels its part of the native Deepseek interface.&lt;/p&gt; &lt;p&gt;It's pretty simple: you can have folders and subfolders for your convos, pin chats as favorite and even resize the sidebar.&lt;/p&gt; &lt;p&gt;Just pushed it live on the Chrome Store: &lt;a href="https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac"&gt;https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now I am working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clipping specific parts of chats&lt;/li&gt; &lt;li&gt;Secret section with PIN access&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prompt Genie - one click prompt enhancement&lt;/p&gt; &lt;p&gt;Happy to hear feedback or questions — first real project I’ve built and shipped solo.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cedparadis"&gt; /u/cedparadis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbx60gj9omve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2b75l</id>
    <title>Anyone having voice conversations? What’s your setup?</title>
    <updated>2025-04-18T17:55:39+00:00</updated>
    <author>
      <name>/u/markosolo</name>
      <uri>https://old.reddit.com/user/markosolo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies to anyone who’s already seen this posted - I thought this might be a better place to ask.&lt;/p&gt; &lt;p&gt;I want something similar to Googles AI Studio where I can call a model and chat with it. Ideally I'd like that to look something like voice conversation where I can brainstorm and do planning sessions with my &amp;quot;AI&amp;quot;. &lt;/p&gt; &lt;p&gt;Is anyone doing anything like this? What's your setup? Would love to hear from anyone having regular voice conversations with AI as part of their daily workflow.&lt;/p&gt; &lt;p&gt;In terms of resources I have plenty of compute, 20GB of GPU I can use. I prefer local if there’s are viable local options I can cobble together even if it’s a bit of work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/markosolo"&gt; /u/markosolo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2lxzr</id>
    <title>Everything about AI Function Calling and MCP, the keyword to Agentic AI</title>
    <updated>2025-04-19T02:20:12+00:00</updated>
    <author>
      <name>/u/Wrtnlabs</name>
      <uri>https://old.reddit.com/user/Wrtnlabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"&gt; &lt;img alt="Everything about AI Function Calling and MCP, the keyword to Agentic AI" src="https://external-preview.redd.it/Ie5EgDsUWjA9acmsHaaxGDexZ4okrsyLqI9mnKmi-PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca27c42e80039606642a16608b907a305ab65b" title="Everything about AI Function Calling and MCP, the keyword to Agentic AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrtnlabs"&gt; /u/Wrtnlabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://wrtnlabs.io/agentica/articles/everything-about-ai-function-calling-and-mcp-the-keyword-to-agentic-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T02:20:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250fu</id>
    <title>Gemma 3 QAT launch with MLX, llama.cpp, Ollama, LM Studio, and Hugging Face</title>
    <updated>2025-04-18T13:31:34+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Some weeks ago we released GGUFs corresponding to the QAT checkpoints of Gemma 3. Thanks to QAT, the model is able to preserve similar quality as &lt;code&gt;bfloat16&lt;/code&gt; while significantly reducing the memory requirements to load the model. That is, QAT is an additional fine-tuning that makes the model more rigorous to quantization.&lt;/p&gt; &lt;p&gt;As we only released the GGUFs, we got feedback that it would be great to have the unquantized QAT-based checkpoints to allow people to quantize for their own tools. So...we did it! Today we're releasing the unquantized QAT-based checkpoints. The models preserve quality better than naive quantization. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also collaborated with Prince (from MLX), llama.cpp, Ollama, LM Studio, and Hugging Face to make sure you can use the models in all your favorite tools!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post : &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unquantized checkpoints: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; (try ollama run gemma3:12b-it-qat)&lt;/li&gt; &lt;li&gt;LM Studio: &lt;a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"&gt;https://lmstudio.ai/model/gemma-3-12b-it-qat&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX: &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"&gt;https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2paa5</id>
    <title>Is Gemma3-12B-QAT bad?</title>
    <updated>2025-04-19T05:43:16+00:00</updated>
    <author>
      <name>/u/FbF_</name>
      <uri>https://old.reddit.com/user/FbF_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying it out compared to the Bartowski's Q4_K_M version and it seems noticeably worse. It just tends to be more repetitive and summarize the prompt uncritically. It's not clear to me if they compared the final QAT model with the non-quantized BF16 version in their proclamation of having a better quantization. Has anyone else had the same experience or done more in-depth analyses on the difference in output with the non-quantized model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FbF_"&gt; /u/FbF_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k27fz2</id>
    <title>I created an interactive tool to visualize *every* attention weight matrix within GPT-2!</title>
    <updated>2025-04-18T15:18:17+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt; &lt;img alt="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" src="https://external-preview.redd.it/YW45M2FibXYwbXZlMWaepLM_4Oin4KjR_zAxiUwp5NOaLzCHkxa3urw0ZqL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60742f26e3a407482898d8e82f2a5d6e8f6ee5f" title="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgo9qamv0mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k29oe2</id>
    <title>QAT is slowly becoming mainstream now?</title>
    <updated>2025-04-18T16:52:07+00:00</updated>
    <author>
      <name>/u/__amberluz__</name>
      <uri>https://old.reddit.com/user/__amberluz__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released a QAT optimized Gemma 3 - 27 billion parameter model. The quantization aware training claims to recover close to 97% of the accuracy loss that happens during the quantization. Do you think this is slowly becoming the norm? Will non-quantized safetensors slowly become obsolete?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__amberluz__"&gt; /u/__amberluz__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250r6</id>
    <title>New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality.</title>
    <updated>2025-04-18T13:32:01+00:00</updated>
    <author>
      <name>/u/Sea_Sympathy_495</name>
      <uri>https://old.reddit.com/user/Sea_Sympathy_495</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt; &lt;img alt="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." src="https://external-preview.redd.it/5lq32BTIzHqmPYcHvNrCp8JMhag9gsSSkR3cQgoYZBU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed6a861b423ef5ef481e863b5c6947b3cef14c0c" title="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Sympathy_495"&gt; /u/Sea_Sympathy_495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/?linkId=14034718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2chcw</id>
    <title>Gemma 27B QAT works surprisingly well at Q2_K</title>
    <updated>2025-04-18T18:49:11+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test how well QAT models do at a lower quant size so I grabbed the smallest quant currently out for it, Q2_K at 10.5 GB. &lt;a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use my models mostly for my Japanese indie game, so following instructions, custom formatting and if it can roleplay or not is what I look for in models. My tests were all done in Japanese, which many models already have issues with at Q4 so I mostly use Q5. In my testing there were no grammatical errors, no random English or Chinese characters. It was able to roleplay in a custom format where I split the spoken words, the actions and the thoughts of the character into different brackets like ()&amp;lt;&amp;gt;「」without any issues. I also asked it basic questions about celebrities, and historical events, it got names and basic information right but dates were all wrong. My tests were done in Ollama with the standard Gemma3 settings.&lt;/p&gt; &lt;p&gt;Overall I am really impressed by the performance of the model especially for being a 27B at Q2. In theory running a 70B model at Q2 would fit into a single 24GB GPU so this technology is very interesting and could allow us to fit even larger models into our cards. After testing it I am really excited for more QAT models to come out in the future.&lt;/p&gt; &lt;p&gt;Have you guys tried running them at smaller quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T18:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2li9f</id>
    <title>Speed testing Llama 4 Maverick with various hardware configs</title>
    <updated>2025-04-19T01:56:23+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured I would share some speed tests of Llama 4 Maverick with my various hardware setups.&lt;br /&gt; Wish we had VLLM quants, guessing the 3090's would be 2x faster vs llama.cpp.&lt;/p&gt; &lt;p&gt;llama.cpp 10x P40's - Q3.5 full offload&lt;br /&gt; &lt;strong&gt;15 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;162 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama.cpp on 16x 3090's - Q4.5 full offload&lt;br /&gt; &lt;strong&gt;36 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;781 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers on 1x 3090 + 16 core DDR4 Epyc - Q4.5&lt;br /&gt; &lt;strong&gt;29 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;129 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers really shines with these tiny active param MOE's.&lt;/p&gt; &lt;p&gt;EDIT:&lt;br /&gt; Not my numbers but the M3 ultra can do:&lt;br /&gt; 47 T/s gen&lt;br /&gt; 332 T/s prompt&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2re65</id>
    <title>Faster flash memory incoming?</title>
    <updated>2025-04-19T08:10:29+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device"&gt;https://interestingengineering.com/innovation/china-worlds-fastest-flash-memory-device&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;If successful, PoX could come in as a new class of ultra‑fast, ultra‑green memories that meet the swelling appetite of large‑language‑model accelerators, finally giving AI hardware a storage medium that keeps pace with its logic.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;PoX might not be the best acronym, because if successful, for sure someone does a small version of it.&lt;/p&gt; &lt;p&gt;nature article: &lt;a href="https://www.nature.com/articles/s41586-025-08839-w"&gt;https://www.nature.com/articles/s41586-025-08839-w&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2re65/faster_flash_memory_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2re65/faster_flash_memory_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2re65/faster_flash_memory_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T08:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28ulo</id>
    <title>Time to step up the /local reasoning game</title>
    <updated>2025-04-18T16:17:11+00:00</updated>
    <author>
      <name>/u/vornamemitd</name>
      <uri>https://old.reddit.com/user/vornamemitd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt; &lt;img alt="Time to step up the /local reasoning game" src="https://preview.redd.it/wtibm8c3cmve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f80a0bad3e3f79619d29663e49d519eaa7898d" title="Time to step up the /local reasoning game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest OAI models tucked away behind intrusive &amp;quot;ID verification&amp;quot;....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vornamemitd"&gt; /u/vornamemitd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wtibm8c3cmve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2spil</id>
    <title>RTX 5080 is about a 3090 but with less VRAM :(</title>
    <updated>2025-04-19T09:48:49+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the 5080 to my bench list&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1IyT41xNOM1ynfzz1IO0hD-4v1f5KXB2CnOiwOTplKJ4/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I know the models are old but I need to be able to compare them to the old benches I cannot rerun them all for now.&lt;/p&gt; &lt;p&gt;The 5080 has performance on par with a 3090 (but 16gb of VRAM are a bummer), if only it had 24gb of VRAM would have been a interesting alternative.&lt;/p&gt; &lt;p&gt;I want to the test the 5070Ti too but currently the ollama container doesn't seems to start on any of the 5070ti available on vast (I wasted about 1$ and 2 hours worth of my time in attempts)&lt;/p&gt; &lt;p&gt;Bye&lt;/p&gt; &lt;p&gt;K.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2spil/rtx_5080_is_about_a_3090_but_with_less_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T09:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ov6b</id>
    <title>How are NSFW LLMs trained/fine-tuned?</title>
    <updated>2025-04-19T05:15:43+00:00</updated>
    <author>
      <name>/u/GeneTangerine</name>
      <uri>https://old.reddit.com/user/GeneTangerine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does someone know? Generally LLMs are censored, do you guys have any resources?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeneTangerine"&gt; /u/GeneTangerine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2qrqq</id>
    <title>Amoral Gemma 3 - QAT</title>
    <updated>2025-04-19T07:25:15+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt; &lt;img alt="Amoral Gemma 3 - QAT" src="https://preview.redd.it/zvrccxdusqve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2786d393d1943e56077477a8167c9ea8a34db8e1" title="Amoral Gemma 3 - QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The same old Amoral Gemma 3, just with the QAT at q4. Refer to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;my first post&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-1B-v2-qat"&gt;[1B] &lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-v2-qat"&gt;[4B]&lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2-qat"&gt;[12B]&lt;/a&gt; [27B - coming soon]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvrccxdusqve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T07:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2kl84</id>
    <title>gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params).</title>
    <updated>2025-04-19T01:06:33+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt; &lt;img alt="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." src="https://preview.redd.it/2mx3qffqxove1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fccae6bd7191507d797de642e31420fffe50ff03" title="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mx3qffqxove1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:06:33+00:00</published>
  </entry>
</feed>
