<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-24T13:24:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i867k8</id>
    <title>First 5090 LLM results, compared to 4090 and 6000 ada</title>
    <updated>2025-01-23T15:42:34+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt; &lt;img alt="First 5090 LLM results, compared to 4090 and 6000 ada" src="https://external-preview.redd.it/PnzUTHeUQDah3Madq3JF5tCDBdEWLySpwcBRwh4t1-o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5868e7a86ac26107edab60570277f4095ed053" title="First 5090 LLM results, compared to 4090 and 6000 ada" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source:&lt;br /&gt; &lt;a href="https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration"&gt;https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4"&gt;https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073"&gt;https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update:&lt;br /&gt; Also form Level 1 Tech:&lt;br /&gt; &lt;a href="https://forum.level1techs.com/t/nvidia-rtx-5090-has-launched/2245"&gt;https://forum.level1techs.com/t/nvidia-rtx-5090-has-launched/2245&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First glance it appears that for small models it is compute limited for small models and you get a 30% gain.&lt;br /&gt; For bigger models the memory bandwidth might come into play (up to 80% faster in theory)&lt;/p&gt; &lt;p&gt;5090 specific quantisations might helpt a lot as well but not many good benchmarks yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9q1qzv9dkwee1.png?width=1504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693c193d478785150be0f7321798ecffd4fa6f7d"&gt;https://preview.redd.it/9q1qzv9dkwee1.png?width=1504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=693c193d478785150be0f7321798ecffd4fa6f7d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1mi7bw9dkwee1.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90cb507de66436c569548fb66122e14d578d02b1"&gt;https://preview.redd.it/1mi7bw9dkwee1.png?width=1496&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90cb507de66436c569548fb66122e14d578d02b1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4h0oquh8kwee1.png?width=3839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19930d9ba22695e3b028e083abee261390f65ef4"&gt;https://preview.redd.it/4h0oquh8kwee1.png?width=3839&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19930d9ba22695e3b028e083abee261390f65ef4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8k3i3</id>
    <title>deepseek-r1-distill-qwen-32b benchmark results on LiveBench</title>
    <updated>2025-01-24T01:39:57+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/"&gt; &lt;img alt="deepseek-r1-distill-qwen-32b benchmark results on LiveBench" src="https://b.thumbs.redditmedia.com/5Vr5jIqo1NpOLHRc4jS1-09Zk2u_NtzxJLbO0J7nnsY.jpg" title="deepseek-r1-distill-qwen-32b benchmark results on LiveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zipfmjlpiuee1.png?width=1461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dba6d02d3273cc0b0eb35355a5fc267237b2a7e"&gt;https://preview.redd.it/zipfmjlpiuee1.png?width=1461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dba6d02d3273cc0b0eb35355a5fc267237b2a7e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T01:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8rzts</id>
    <title>NVIDIA 50 series bottlenecks</title>
    <updated>2025-01-24T09:55:35+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't know how it translates to workloads regarding AI, but there was some questions about why we don't see better performance when the memory bandwidth is substantially higher. And this review mentions that there could potentially be a CPU or PCIe bottleneck. There also seems to be problems with older risers, for anyone that tries to cram a bunch of cards in the same case...&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/5TJk_P2A0Iw"&gt;https://youtu.be/5TJk_P2A0Iw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rzts/nvidia_50_series_bottlenecks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rzts/nvidia_50_series_bottlenecks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rzts/nvidia_50_series_bottlenecks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T09:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81iim</id>
    <title>Been ages since google released an open model</title>
    <updated>2025-01-23T11:43:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt; &lt;img alt="Been ages since google released an open model" src="https://preview.redd.it/fa91scqqdqee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f78c10935c8984f8f9d17834c7720f182fed482" title="Been ages since google released an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa91scqqdqee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8ptsj</id>
    <title>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</title>
    <updated>2025-01-24T07:08:28+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.10799"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ptsj/stepkto_optimizing_mathematical_reasoning_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ptsj/stepkto_optimizing_mathematical_reasoning_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T07:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8srsv</id>
    <title>Simple Open source tool like AI (Apple Intelligence) but completely private / local using Ollama and Kokoro</title>
    <updated>2025-01-24T10:51:58+00:00</updated>
    <author>
      <name>/u/namuan</name>
      <uri>https://old.reddit.com/user/namuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8srsv/simple_open_source_tool_like_ai_apple/"&gt; &lt;img alt="Simple Open source tool like AI (Apple Intelligence) but completely private / local using Ollama and Kokoro" src="https://external-preview.redd.it/ZGpreW02NWU5eGVlMaNbrH4VawvqtBNIlI_TN6ZdlIQIRM_6iX5iv_gMzF0s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6705fa9a9d3760b8b461e07042fb465480bf217a" title="Simple Open source tool like AI (Apple Intelligence) but completely private / local using Ollama and Kokoro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/namuan"&gt; /u/namuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3lmdx75e9xee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8srsv/simple_open_source_tool_like_ai_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8srsv/simple_open_source_tool_like_ai_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T10:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8svno</id>
    <title>"R1-Pro" - a 'Deep Think' Mode for DeepSeek-R1-Distilled Models – Boost Reasoning Effort for Local LLMs</title>
    <updated>2025-01-24T10:59:52+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8svno/r1pro_a_deep_think_mode_for_deepseekr1distilled/"&gt; &lt;img alt="&amp;quot;R1-Pro&amp;quot; - a 'Deep Think' Mode for DeepSeek-R1-Distilled Models – Boost Reasoning Effort for Local LLMs" src="https://external-preview.redd.it/eGF3YmJreDBheGVlMS3Azf7ct8pfQm1z9XYMY4QGWDLBtvNoRs9oapizu3FU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dac3f476e3c4ee2348f56005b736e1b0adac0d81" title="&amp;quot;R1-Pro&amp;quot; - a 'Deep Think' Mode for DeepSeek-R1-Distilled Models – Boost Reasoning Effort for Local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qdylplx0axee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8svno/r1pro_a_deep_think_mode_for_deepseekr1distilled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8svno/r1pro_a_deep_think_mode_for_deepseekr1distilled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T10:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8r0on</id>
    <title>I actually really like the idea of this. It won’t be long before they can look at your PC on call as well.</title>
    <updated>2025-01-24T08:39:17+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8r0on/i_actually_really_like_the_idea_of_this_it_wont/"&gt; &lt;img alt="I actually really like the idea of this. It won’t be long before they can look at your PC on call as well." src="https://preview.redd.it/3vxre6rklwee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ddbef961cfc76e4dbc466509afe79469c4b921c" title="I actually really like the idea of this. It won’t be long before they can look at your PC on call as well." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3vxre6rklwee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8r0on/i_actually_really_like_the_idea_of_this_it_wont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8r0on/i_actually_really_like_the_idea_of_this_it_wont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T08:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i856wr</id>
    <title>Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !</title>
    <updated>2025-01-23T14:57:59+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt; &lt;img alt="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" src="https://preview.redd.it/lxwhx4eicree1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6a7b23129dc6a37671b0f77472359990567d0e4" title="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lxwhx4eicree1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T14:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8fpza</id>
    <title>SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU.</title>
    <updated>2025-01-23T22:17:19+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"&gt; &lt;img alt="SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU." src="https://external-preview.redd.it/NTYzZXAwOXdpdGVlMeNP1riRHGftFiyraDTq8M0dXNR_Xk41nSkLrV2F0EOo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aefeccdf988fce29d96f197804b176f76684cb54" title="SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qikrzy8witee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T22:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8i41v</id>
    <title>Openai is ahead only till china reverse engineers...</title>
    <updated>2025-01-24T00:04:50+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"&gt; &lt;img alt="Openai is ahead only till china reverse engineers..." src="https://preview.redd.it/zy8ljay42uee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e8626d46d75e083e4343a6f229defc70f8055d8" title="Openai is ahead only till china reverse engineers..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zy8ljay42uee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T00:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i86e4y</id>
    <title>Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model</title>
    <updated>2025-01-23T15:50:30+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt; &lt;img alt="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" src="https://external-preview.redd.it/QaGEWAoaN73yKpJcRFLASUVmy5TY0ehTzhGZuFAVhPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05b9252fa9a2aa815d8f1c4c41bc8b680d1e4628" title="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/01/23/scale-ai-ceo-says-china-has-quickly-caught-the-us-with-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8hqp0</id>
    <title>DeepSeek R1 (reasoner) can use internet there o1 still can't</title>
    <updated>2025-01-23T23:47:38+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"&gt; &lt;img alt="DeepSeek R1 (reasoner) can use internet there o1 still can't" src="https://b.thumbs.redditmedia.com/NWZ7upEkaFaUDhq-5d7CT7VuzH9PhcF4ym1RrfuBo1M.jpg" title="DeepSeek R1 (reasoner) can use internet there o1 still can't" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Funny ... DeepSeek doing more for free than paid o1...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i8hqp0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T23:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87fkl</id>
    <title>Deepseek R1 is the only one that nails this new viral benchmark</title>
    <updated>2025-01-23T16:34:21+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"&gt; &lt;img alt="Deepseek R1 is the only one that nails this new viral benchmark" src="https://external-preview.redd.it/dTNsOXYwcnJ0cmVlMQcGL6cDuoI_ROA8VT0SlOGuG2iHRRkQmxqkRS_k8D6O.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f916bf51cb83dcc443906e66c7cbbd04f4cf9cc" title="Deepseek R1 is the only one that nails this new viral benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4skrezsntree1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8chpr</id>
    <title>Deepseek-r1-Qwen 1.5B's overthinking is adorable</title>
    <updated>2025-01-23T20:01:44+00:00</updated>
    <author>
      <name>/u/Ill-Still-6859</name>
      <uri>https://old.reddit.com/user/Ill-Still-6859</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"&gt; &lt;img alt="Deepseek-r1-Qwen 1.5B's overthinking is adorable" src="https://external-preview.redd.it/azZ1d2EzZ2x1c2VlMWwcsRUdCKlecN3EYDmX-jmw1aKFL7Ec90KkMpgcpWxW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ffd8792fef56427e2ddfd654527bbf426c6fbf8" title="Deepseek-r1-Qwen 1.5B's overthinking is adorable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Still-6859"&gt; /u/Ill-Still-6859 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b5coo5glusee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T20:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8a9qb</id>
    <title>Deepmind learning from Deepseek. Power of open source!</title>
    <updated>2025-01-23T18:30:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"&gt; &lt;img alt="Deepmind learning from Deepseek. Power of open source!" src="https://preview.redd.it/xouhskggesee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2d75d3fe9869f9aa59bf1661a57a8050b9bde4" title="Deepmind learning from Deepseek. Power of open source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xouhskggesee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T18:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8u9jk</id>
    <title>DeepSeek-R1 appears on LMSYS Arena Leaderboard</title>
    <updated>2025-01-24T12:29:12+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8u9jk/deepseekr1_appears_on_lmsys_arena_leaderboard/"&gt; &lt;img alt="DeepSeek-R1 appears on LMSYS Arena Leaderboard" src="https://b.thumbs.redditmedia.com/Cbe6Zl-znSiMPkTrr0J7qqua6y3OL0gAQYUhGg_4B2M.jpg" title="DeepSeek-R1 appears on LMSYS Arena Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i8u9jk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8u9jk/deepseekr1_appears_on_lmsys_arena_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8u9jk/deepseekr1_appears_on_lmsys_arena_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T12:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80cwf</id>
    <title>deepseek is a side project</title>
    <updated>2025-01-23T10:22:48+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt; &lt;img alt="deepseek is a side project" src="https://preview.redd.it/zdvrlxahzpee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df808bd8bc2e4ba90db2fdb005eaae092d5d8206" title="deepseek is a side project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zdvrlxahzpee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8qmwv</id>
    <title>Economist: "China’s AI industry has almost caught up with America’s"</title>
    <updated>2025-01-24T08:09:10+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8qmwv/economist_chinas_ai_industry_has_almost_caught_up/"&gt; &lt;img alt="Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" src="https://a.thumbs.redditmedia.com/ZQ8Jd3yNsMvXFQX-vWNUOP50oj7_BpX4CfG1prbLj84.jpg" title="Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a recent article, The Economist claims that Chinese AI models are &amp;quot;more open and more effective&amp;quot; and &amp;quot;DeepSeek’s llm is not only bigger than many of its Western counterparts—it is also better, matched only by the proprietary models at Google and Openai.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ucks1vgggwee1.png?width=360&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57e7a71f13589a314f53cda9a39bd3ba318ec59b"&gt;https://preview.redd.it/ucks1vgggwee1.png?width=360&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57e7a71f13589a314f53cda9a39bd3ba318ec59b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The article goes on to explain how DeepSeek is more effective thanks to a series of improvements, and more open, not only in terms of availability but also of research transparency: &amp;quot;This permissiveness is matched by a remarkable openness: the two companies publish papers whenever they release new models that provide a wealth of detail on the techniques used to improve their performance.&amp;quot;&lt;/p&gt; &lt;p&gt;Worth a read: &lt;a href="https://archive.is/vAop1#selection-1373.91-1373.298"&gt;https://archive.is/vAop1#selection-1373.91-1373.298&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8qmwv/economist_chinas_ai_industry_has_almost_caught_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8qmwv/economist_chinas_ai_industry_has_almost_caught_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8qmwv/economist_chinas_ai_industry_has_almost_caught_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T08:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8mwpc</id>
    <title>Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)</title>
    <updated>2025-01-24T04:06:24+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8mwpc/coming_soon_100_local_video_understanding_engine/"&gt; &lt;img alt="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" src="https://external-preview.redd.it/a3BrbTdjZjVzdWVlMe21Biif0sGFU8GTsH3N7D_CJugYvIxsEVZ-nvrUed0U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e198b50c956615f546356a1784f29f65d0e8c5ea" title="Coming soon: 100% Local Video Understanding Engine (an open-source project that can classify, caption, transcribe, and understand any video on your local device)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v8xdjbf5suee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8mwpc/coming_soon_100_local_video_understanding_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8mwpc/coming_soon_100_local_video_understanding_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T04:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8996r</id>
    <title>I think it's forced. DeepSeek did its best...</title>
    <updated>2025-01-23T17:49:34+00:00</updated>
    <author>
      <name>/u/Alexs1200AD</name>
      <uri>https://old.reddit.com/user/Alexs1200AD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"&gt; &lt;img alt="I think it's forced. DeepSeek did its best..." src="https://preview.redd.it/b3n1jpj17see1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55a5ea362bf2cb802996106f2fc698c1f579cfff" title="I think it's forced. DeepSeek did its best..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alexs1200AD"&gt; /u/Alexs1200AD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b3n1jpj17see1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T17:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8ifxd</id>
    <title>Ollama is confusing people by pretending that the little distillation models are "R1"</title>
    <updated>2025-01-24T00:20:07+00:00</updated>
    <author>
      <name>/u/blahblahsnahdah</name>
      <uri>https://old.reddit.com/user/blahblahsnahdah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was baffled at the number of people who seem to think they're using &amp;quot;R1&amp;quot; when they're actually running a Qwen or Llama finetune, until I saw a screenshot of the Ollama interface earlier. Ollama is misleadingly pretending in their UI and command line that &amp;quot;R1&amp;quot; is a series of differently-sized models and that distillations are just smaller sizes of &amp;quot;R1&amp;quot;. Rather than what they actually are which is some quasi-related experimental finetunes of other models that Deepseek happened to release at the same time.&lt;/p&gt; &lt;p&gt;It's not just annoying, it seems to be doing reputational damage to Deepseek as well, because a lot of low information Ollama users are using a shitty 1.5B model, noticing that it sucks (because it's 1.5B), and saying &amp;quot;wow I don't see why people are saying R1 is so good, this is terrible&amp;quot;. Plus there's misleading social media influencer content like &amp;quot;I got R1 running on my phone!&amp;quot; (no, you got a Qwen-1.5B finetune running on your phone).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blahblahsnahdah"&gt; /u/blahblahsnahdah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T00:20:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i88g4y</id>
    <title>Meta panicked by Deepseek</title>
    <updated>2025-01-23T17:15:55+00:00</updated>
    <author>
      <name>/u/Optimal_Hamster5789</name>
      <uri>https://old.reddit.com/user/Optimal_Hamster5789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt; &lt;img alt="Meta panicked by Deepseek" src="https://preview.redd.it/ek65oz361see1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd236f1570226e841c54a41cd8f2a2e7c6328a8c" title="Meta panicked by Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimal_Hamster5789"&gt; /u/Optimal_Hamster5789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek65oz361see1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T17:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8tx5z</id>
    <title>I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)</title>
    <updated>2025-01-24T12:08:50+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt; &lt;img alt="I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)" src="https://preview.redd.it/es9l38ezmxee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a66f2c3fda0b03915eea1c0a72185b32e17e660" title="I benchmarked (almost) every model that can fit in 24GB VRAM (Qwens, R1 distils, Mistrals, even Llama 70b gguf)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/es9l38ezmxee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T12:08:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8rujw</id>
    <title>Notes on Deepseek r1: Just how good it is compared to OpenAI o1</title>
    <updated>2025-01-24T09:44:13+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally, there is a model worthy of the hype it has been getting since Claude 3.6 Sonnet. Deepseek has released something anyone hardly expected: a reasoning model on par with OpenAI’s o1 within a month of the v3 release, with an MIT license and 1/20th of o1’s cost.&lt;/p&gt; &lt;p&gt;This is easily the best release since GPT-4. It's wild; the general public seems excited about this, while the big AI labs are probably scrambling. It feels like things are about to speed up in the AI world. And it's all thanks to this new DeepSeek-R1 model and how they trained it. &lt;/p&gt; &lt;p&gt;Some key details from the paper&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pure RL (GRPO) on v3-base to get r1-zero. (No Monte-Carlo Tree Search or Process Reward Modelling)&lt;/li&gt; &lt;li&gt;The model uses “Aha moments” as pivot tokens to reflect and reevaluate answers during CoT.&lt;/li&gt; &lt;li&gt;To overcome r1-zero’s readability issues, v3 was SFTd on cold start data.&lt;/li&gt; &lt;li&gt;Distillation works, small models like Qwen and Llama trained over r1 generated data show significant improvements.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here’s an overall r0 pipeline&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;v3 base + RL (GRPO) → r1-zero &lt;/p&gt; &lt;p&gt;r1 training pipeline.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-V3 Base&lt;/strong&gt; + SFT (Cold Start Data) → &lt;strong&gt;Checkpoint 1&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint 1&lt;/strong&gt; + RL (GRPO + Language Consistency) → &lt;strong&gt;Checkpoint 2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint 2&lt;/strong&gt; used to Generate Data (Rejection Sampling)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-V3 Base&lt;/strong&gt; + SFT (Generated Data + Other Data) → &lt;strong&gt;Checkpoint 3&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint 3&lt;/strong&gt; + RL (Reasoning + Preference Rewards) → &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We know the benchmarks, but just how good is it?&lt;/p&gt; &lt;h1&gt;Deepseek r1 vs OpenAI o1.&lt;/h1&gt; &lt;p&gt;So, for this, I tested r1 and o1 side by side on complex reasoning, math, coding, and creative writing problems. These are the questions that o1 solved only or by none before.&lt;/p&gt; &lt;p&gt;Here’s what I found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For &lt;strong&gt;reasoning&lt;/strong&gt;, it is much better than any previous SOTA model until o1. It is better than o1-preview but a notch below o1. This is also shown in the ARC AGI bench.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;: It's also the same for mathematics; r1 is a killer, but o1 is better.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt;: I didn’t get to play much, but on first look, it’s up there with o1, and the fact that it costs 20x less makes it the practical winner.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writing&lt;/strong&gt;: This is where R1 takes the lead. It gives the same vibes as early Opus. It’s free, less censored, has much more personality, is easy to steer, and is very creative compared to the rest, even o1-pro.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What interested me was how free the model sounded and thought traces were, akin to human internal monologue. Perhaps this is because of the less stringent RLHF, unlike US models.&lt;/p&gt; &lt;p&gt;The fact that you can get r1 from v3 via pure RL was the most surprising.&lt;/p&gt; &lt;p&gt;For in-depth analysis, commentary, and remarks on the Deepseek r1, check out this blog post: &lt;a href="https://composio.dev/blog/notes-on-the-new-deepseek-r1/"&gt;Notes on Deepseek r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What are your experiences with the new Deepseek r1? Did you find the model useful for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes_on_deepseek_r1_just_how_good_it_is_compared/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes_on_deepseek_r1_just_how_good_it_is_compared/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes_on_deepseek_r1_just_how_good_it_is_compared/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T09:44:13+00:00</published>
  </entry>
</feed>
