<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-27T19:05:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iaqajh</id>
    <title>deepseek is a side project pt. 2</title>
    <updated>2025-01-26T21:02:46+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt; &lt;img alt="deepseek is a side project pt. 2" src="https://preview.redd.it/bawhrb3ekefe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e236c7b8478b2b0b98ff8cb74b60fac011ead97e" title="deepseek is a side project pt. 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bawhrb3ekefe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T21:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib9hvd</id>
    <title>DeepSeek Chat Started to Slow Down after all the News and Hype</title>
    <updated>2025-01-27T13:49:32+00:00</updated>
    <author>
      <name>/u/lake_trade</name>
      <uri>https://old.reddit.com/user/lake_trade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Issues logging in, it takes forever, or it says login failed.&lt;br /&gt; If logged in successfully, chat history is not loading, opening the chat takes a longer time and sometimes nothing comes up in the chat.&lt;br /&gt; It is giving clear message in the chat that heavy traffic is there and retry later.&lt;br /&gt; So it seems the infra of the DeepSeek chat has hit the limit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lake_trade"&gt; /u/lake_trade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib9hvd/deepseek_chat_started_to_slow_down_after_all_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib9hvd/deepseek_chat_started_to_slow_down_after_all_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib9hvd/deepseek_chat_started_to_slow_down_after_all_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T13:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibedgi</id>
    <title>Jailbreaking DeepSeek: Sweary haiku about [redacted]</title>
    <updated>2025-01-27T17:16:41+00:00</updated>
    <author>
      <name>/u/Time-Winter-4319</name>
      <uri>https://old.reddit.com/user/Time-Winter-4319</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibedgi/jailbreaking_deepseek_sweary_haiku_about_redacted/"&gt; &lt;img alt="Jailbreaking DeepSeek: Sweary haiku about [redacted]" src="https://external-preview.redd.it/OXRiYXdwdHFra2ZlMRTVpkWRwftFqNp2hlTpeOI6hMMMLBV-dAtEKMuzejtZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ff7094b967b76a98a32643a7178cb58ede56321" title="Jailbreaking DeepSeek: Sweary haiku about [redacted]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time-Winter-4319"&gt; /u/Time-Winter-4319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o86owjtqkkfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibedgi/jailbreaking_deepseek_sweary_haiku_about_redacted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibedgi/jailbreaking_deepseek_sweary_haiku_about_redacted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ffq</id>
    <title>From this week's The Economist: "China‚Äôs AI industry has almost caught up with America‚Äôs"</title>
    <updated>2025-01-27T05:04:42+00:00</updated>
    <author>
      <name>/u/comfyui_user_999</name>
      <uri>https://old.reddit.com/user/comfyui_user_999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt; &lt;img alt="From this week's The Economist: &amp;quot;China‚Äôs AI industry has almost caught up with America‚Äôs&amp;quot;" src="https://external-preview.redd.it/C2yiTG1ri5iDyKLc-Vn_3V_ISkOymhbpYsu1RacQ-tE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1461e932e29591186828398f4de9362c5ad8106" title="From this week's The Economist: &amp;quot;China‚Äôs AI industry has almost caught up with America‚Äôs&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/comfyui_user_999"&gt; /u/comfyui_user_999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.economist.com/briefing/2025/01/23/chinas-ai-industry-has-almost-caught-up-with-americas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibai2q</id>
    <title>DeepSeek API has been down most of the morning</title>
    <updated>2025-01-27T14:36:47+00:00</updated>
    <author>
      <name>/u/RazzmatazzReal4129</name>
      <uri>https://old.reddit.com/user/RazzmatazzReal4129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://status.deepseek.com/"&gt;https://status.deepseek.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Haven't been able to get a response the last couple hours. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RazzmatazzReal4129"&gt; /u/RazzmatazzReal4129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibai2q/deepseek_api_has_been_down_most_of_the_morning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibai2q/deepseek_api_has_been_down_most_of_the_morning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibai2q/deepseek_api_has_been_down_most_of_the_morning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T14:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd5t8</id>
    <title>Janus-Pro - improving both multimodal understanding and visual generation of Deepseek Janus</title>
    <updated>2025-01-27T16:28:13+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Deepseek hits a homerun&lt;/strong&gt; with a new release just &lt;strong&gt;7 days after R1&lt;/strong&gt;: &lt;strong&gt;Janus-Pro&lt;/strong&gt;, an MLLM (Text-Image to Text-Image) model! üéâ&lt;/p&gt; &lt;p&gt;Imagine applying the &lt;strong&gt;R1 GRPO algorithm&lt;/strong&gt; alongside the new paper: &lt;em&gt;‚Äú&lt;/em&gt;&lt;a href="https://huggingface.co/papers/2501.13926"&gt;Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step&lt;/a&gt;&lt;em&gt;‚Äù&lt;/em&gt; and its &lt;strong&gt;Potential Assessment Reward Model (PARM)&lt;/strong&gt;‚Äîthis could lead to a groundbreaking &lt;strong&gt;Deepseek-VR1 model&lt;/strong&gt;! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5t8/januspro_improving_both_multimodal_understanding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5t8/januspro_improving_both_multimodal_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5t8/januspro_improving_both_multimodal_understanding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T16:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib7y8e</id>
    <title>Qwen2.5 14B-1M Impressions</title>
    <updated>2025-01-27T12:41:09+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running 14B-1M-Q8 gguf + 64K Q8 KV Cache, still have 2gb vram left on my 24gb card &lt;/p&gt; &lt;p&gt;I tested it using long podcast scripts from WAN Show and Waveform, it can capture most of the details, definitely not all of them, but it's pretty good for a 14B-Q8 model, plus I'm using Q8 quantnized context &lt;/p&gt; &lt;p&gt;I also compared it with the original 14b-32k model, using Waveform scripts which can fit in 32k context, I found 1M version always generate longer summarisation then 32k, and for most times it's better structured and more detailed than 32k &lt;/p&gt; &lt;p&gt;In conclusion, this model definitely worth downloading, and it replaced 32B-IQ4-XS as my main model for text summarisation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7y8e/qwen25_14b1m_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7y8e/qwen25_14b1m_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7y8e/qwen25_14b1m_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T12:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibelba</id>
    <title>Guess who's jealous</title>
    <updated>2025-01-27T17:25:33+00:00</updated>
    <author>
      <name>/u/No-Point-6492</name>
      <uri>https://old.reddit.com/user/No-Point-6492</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"&gt; &lt;img alt="Guess who's jealous" src="https://preview.redd.it/pfhlaz5jmkfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ca8fb4c59047cde6f8c3383a1d8424c6f63e76" title="Guess who's jealous" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Point-6492"&gt; /u/No-Point-6492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfhlaz5jmkfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibffe6</id>
    <title>DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B</title>
    <updated>2025-01-27T17:59:06+00:00</updated>
    <author>
      <name>/u/aichiusagi</name>
      <uri>https://old.reddit.com/user/aichiusagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"&gt; &lt;img alt="DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B" src="https://external-preview.redd.it/A2EEjszaCCukFYACldeB-sC0pfmrpnJQLkGODk-spiI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55b85aef606aeca155936b8d61992b31b38c1d84" title="DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aichiusagi"&gt; /u/aichiusagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/Janus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib7mg4</id>
    <title>I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446</title>
    <updated>2025-01-27T12:25:45+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"&gt; &lt;img alt="I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446" src="https://b.thumbs.redditmedia.com/alSIx8B8I53oq9CVNU61AjABH8VeEa2cyNOSjfEiIDc.jpg" title="I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/11446"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T12:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4qrg</id>
    <title>It was fun while it lasted.</title>
    <updated>2025-01-27T09:50:11+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt; &lt;img alt="It was fun while it lasted." src="https://preview.redd.it/f4z3rtg5dife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2e9e389c105a5198f86f71e16e2dc7186ad1daa" title="It was fun while it lasted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f4z3rtg5dife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe7dn</id>
    <title>Nvidia faces $465 billion loss as DeepSeek disrupts AI market, largest in US market history</title>
    <updated>2025-01-27T17:09:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.financialexpress.com/business/investing-abroad-nvidia-faces-465-billion-loss-as-deepseek-disrupts-ai-market-3728093/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib2uuz</id>
    <title>I created a "Can you run it" tool for open source LLMs</title>
    <updated>2025-01-27T07:46:52+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Raskoll2/LLMcalc"&gt;https://github.com/Raskoll2/LLMcalc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's extremly simple but tells you a tk/s estimate of all the quants, and how to run them e.g. 80% layer offload, KV offload, all on GPU. &lt;/p&gt; &lt;p&gt;I have no clue if it'll run on anyone else's systems. I've tried with with linux + 1x Nvidia GPU, if anyone on other systems or multi GPU systems could relay some error messages that would be great&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T07:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib8wfw</id>
    <title>Nvidia pre-market down 12% due Deepseek</title>
    <updated>2025-01-27T13:23:17+00:00</updated>
    <author>
      <name>/u/puffyarizona</name>
      <uri>https://old.reddit.com/user/puffyarizona</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html"&gt;NVidia Prƒô-market down 12% due Deepseek&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puffyarizona"&gt; /u/puffyarizona &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T13:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iasyc3</id>
    <title>Deepseek is #1 on the U.S. App Store</title>
    <updated>2025-01-26T22:52:07+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt; &lt;img alt="Deepseek is #1 on the U.S. App Store" src="https://preview.redd.it/sr4kvvnv3ffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a82ab88b43a6f7f3f1aa6d284ecb8edff2e4630" title="Deepseek is #1 on the U.S. App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr4kvvnv3ffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T22:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibc2vx</id>
    <title>Deepseek currently restricts new registrations to Chinese phone numbers only</title>
    <updated>2025-01-27T15:43:22+00:00</updated>
    <author>
      <name>/u/SysPsych</name>
      <uri>https://old.reddit.com/user/SysPsych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See: &lt;a href="https://i.imgur.com/9WLAnko.png"&gt;https://i.imgur.com/9WLAnko.png&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SysPsych"&gt; /u/SysPsych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib99ei</id>
    <title>Same size as the old gpt2 model. Insane.</title>
    <updated>2025-01-27T13:38:32+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"&gt; &lt;img alt="Same size as the old gpt2 model. Insane." src="https://preview.redd.it/cotf3wm1ijfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba2f6e3e7a8a1708887fa7db558a92d1d0c83b4e" title="Same size as the old gpt2 model. Insane." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cotf3wm1ijfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T13:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibeub5</id>
    <title>llama.cpp PR with 99% of code written by Deepseek-R1</title>
    <updated>2025-01-27T17:35:22+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt; &lt;img alt="llama.cpp PR with 99% of code written by Deepseek-R1" src="https://preview.redd.it/pfm0xpbaokfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3147a5b22a1d22f5ba48a7737734a1af6de28d53" title="llama.cpp PR with 99% of code written by Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfm0xpbaokfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib5yuk</id>
    <title>deepseek r1 tops the creative writing rankings</title>
    <updated>2025-01-27T11:00:18+00:00</updated>
    <author>
      <name>/u/Still_Potato_415</name>
      <uri>https://old.reddit.com/user/Still_Potato_415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"&gt; &lt;img alt="deepseek r1 tops the creative writing rankings" src="https://preview.redd.it/yslsnd6fpife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3efe198ef9c53da2bdad4be41459ab90d46ec7" title="deepseek r1 tops the creative writing rankings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Still_Potato_415"&gt; /u/Still_Potato_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yslsnd6fpife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T11:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4ksj</id>
    <title>How *exactly* is Deepseek so cheap?</title>
    <updated>2025-01-27T09:40:04+00:00</updated>
    <author>
      <name>/u/micamecava</name>
      <uri>https://old.reddit.com/user/micamecava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek's all the rage. I get it, 95-97% reduction in costs. &lt;/p&gt; &lt;p&gt;How *exactly*? &lt;/p&gt; &lt;p&gt;Aside from cheaper training (not doing RLHF), quantization, and caching (semantic input HTTP caching I guess?), where's the reduction coming from? &lt;/p&gt; &lt;p&gt;This can't be all, because supposedly R1 isn't quantized. Right?&lt;/p&gt; &lt;p&gt;Is it subsidized? Is OpenAI/Anthropic just...charging too much? What's the deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/micamecava"&gt; /u/micamecava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd5x0</id>
    <title>DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model).</title>
    <updated>2025-01-27T16:28:21+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt; &lt;img alt="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." src="https://external-preview.redd.it/n5r1wVoNriwXCNjXkrw2Ab2zRN5UbL6aXFXA0wRQWRU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef80d96659edc7101cd569ddae687c24437596ba" title="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibb8rr</id>
    <title>Qwen3.0 MOE? New Reasoning Model?</title>
    <updated>2025-01-27T15:09:24+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt; &lt;img alt="Qwen3.0 MOE? New Reasoning Model?" src="https://preview.redd.it/0vnua5vqxjfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12861d7e6664e9cd7e45dd0710b87280d3a92aff" title="Qwen3.0 MOE? New Reasoning Model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0vnua5vqxjfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe1ro</id>
    <title>Thoughts? I kinda feel happy about this...</title>
    <updated>2025-01-27T17:03:47+00:00</updated>
    <author>
      <name>/u/Butefluko</name>
      <uri>https://old.reddit.com/user/Butefluko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt; &lt;img alt="Thoughts? I kinda feel happy about this..." src="https://preview.redd.it/6b78kpulikfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f14a6edf107ecbf10ce8c437a2e0826bef5af67" title="Thoughts? I kinda feel happy about this..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Butefluko"&gt; /u/Butefluko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6b78kpulikfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbloy</id>
    <title>1.58bit DeepSeek R1 - 131GB Dynamic GGUF</title>
    <updated>2025-01-27T15:24:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt; &lt;img alt="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" src="https://external-preview.redd.it/uHOmNdCTHW-Q1CBdw01aifeSpeyvgfhjJI_lcC-SH5c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc7cd6ab7b35a273b107dce1a4113ba2c9dcca51" title="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I managed to &lt;strong&gt;dynamically quantize&lt;/strong&gt; the full DeepSeek R1 671B MoE to 1.58bits in GGUF format. The trick is &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but quantize only the MoE layers to 1.5bit, and leave attention and other layers in 4 or 6bit.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;131GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;158GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;183GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;212GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can get &lt;strong&gt;140 tokens / s&lt;/strong&gt; on 2x H100 80GB GPUs with all layers offloaded. A 24GB GPU like RTX 4090 should be able to get at least 1 to 3 tokens / s.&lt;/p&gt; &lt;p&gt;If we naively quantize all layers to 1.5bit (-1, 0, 1), the model will fail dramatically, since it'll produce &lt;strong&gt;gibberish&lt;/strong&gt; and &lt;strong&gt;infinite repetitions&lt;/strong&gt;. I selectively leave all attention layers in 4/6bit, and leave the first 3 transformer dense layers in 4/6bit. The MoE layers take up 88% of all space, so we can leave them in 1.5bit. We get in total a weighted sum of 1.58bits!&lt;/p&gt; &lt;p&gt;I asked it the 1.58bit model to create Flappy Bird with 10 conditions (like random colors, a best score etc), and it did pretty well! Using a generic non dynamically quantized model will fail miserably - there will be no output at all!&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k8nfun2ezjfe1.gif"&gt;Flappy Bird game made by 1.58bit R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's more details in the blog here: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt; The link to the 1.58bit GGUF is here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S&lt;/a&gt; You should be able to run it in your favorite inference tool if it supports i matrix quants. No need to re-update llama.cpp.&lt;/p&gt; &lt;p&gt;A reminder on DeepSeek's chat template (for distilled versions as well) - it auto adds a BOS - do not add it manually!&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;What is 1+1?&amp;lt;ÔΩúAssistantÔΩú&amp;gt;It's 2.&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&amp;lt;ÔΩúUserÔΩú&amp;gt;Explain more!&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To know how many layers to offload to the GPU, I approximately calculated it as below:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;File Size&lt;/th&gt; &lt;th align="left"&gt;24GB GPU&lt;/th&gt; &lt;th align="left"&gt;80GB GPU&lt;/th&gt; &lt;th align="left"&gt;2x80GB GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;131GB&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;All layers 61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;158GB&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;183GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;212GB&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All other GGUFs for R1 are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF&lt;/a&gt; There's also GGUFs and dynamic 4bit bitsandbytes quants and others for all other distilled versions (Qwen, Llama etc) at &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employee‚Äôs reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employee‚Äôs reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employee‚Äôs reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
</feed>
