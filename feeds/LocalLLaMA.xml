<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-23T03:35:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m641zg</id>
    <title>MegaTTS 3 Voice Cloning is Here</title>
    <updated>2025-07-22T03:53:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt; &lt;img alt="MegaTTS 3 Voice Cloning is Here" src="https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13bd3c86a79666218395f17439b714df6a5fc52c" title="MegaTTS 3 Voice Cloning is Here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt; &lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt; &lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href="https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt; &lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href="https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt; &lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T03:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ufm4</id>
    <title>Has anyone tried Hierarchical Reasoning Models yet?</title>
    <updated>2025-07-23T00:04:00+00:00</updated>
    <author>
      <name>/u/jackboulder33</name>
      <uri>https://old.reddit.com/user/jackboulder33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackboulder33"&gt; /u/jackboulder33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T00:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6gq8e</id>
    <title>I wrote 2000 LLM test cases so you don't have to</title>
    <updated>2025-07-22T15:12:44+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt; &lt;h1&gt;The problem: too many options&lt;/h1&gt; &lt;p&gt;I've been building &lt;a href="https://github.com/kiln-ai/kiln"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt; &lt;p&gt;Here's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt; &lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt; &lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt; &lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt; &lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt; &lt;h1&gt;Wait, doesn't that cost a lot of money and take forever?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.&lt;/p&gt; &lt;p&gt;Our blog has some details on the &lt;a href="https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;The Result&lt;/h1&gt; &lt;p&gt;The end result is that it's much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt; &lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, you're in control. You can always override any suggestion.&lt;/p&gt; &lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt; &lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt; &lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt; &lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt; &lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.&lt;/p&gt; &lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.getkiln.ai/docs/quickstart"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T15:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6bddm</id>
    <title>AMD's Strix Halo "Ryzen AI MAX" APUs Come To DIY PC Builders With New MoDT "Mini-ITX" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory</title>
    <updated>2025-07-22T11:18:22+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt; &lt;img alt="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" src="https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01" title="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6vbds</id>
    <title>Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed</title>
    <updated>2025-07-23T00:45:03+00:00</updated>
    <author>
      <name>/u/Sudden-Tap3484</name>
      <uri>https://old.reddit.com/user/Sudden-Tap3484</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt; &lt;img alt="Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed" src="https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg" title="Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href="https://www.boson.ai/technologies/voice"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt; &lt;p&gt;Listen to this &lt;a href="https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt; &lt;p&gt;I tried a few examples in their HF &lt;a href="https://huggingface.co/spaces/smola/higgs_audio_v2"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden-Tap3484"&gt; /u/Sudden-Tap3484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T00:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6u0gt</id>
    <title>Unsloth quants already starting to roll out for Qwen3-Coder</title>
    <updated>2025-07-22T23:45:30+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"&gt; &lt;img alt="Unsloth quants already starting to roll out for Qwen3-Coder" src="https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae" title="Unsloth quants already starting to roll out for Qwen3-Coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T23:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6u3kd</id>
    <title>Qwen3-Coder is available on OpenRouter</title>
    <updated>2025-07-22T23:49:17+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"&gt; &lt;img alt="Qwen3-Coder is available on OpenRouter" src="https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7" title="Qwen3-Coder is available on OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/qwen/qwen3-coder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T23:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ct7u</id>
    <title>Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000</title>
    <updated>2025-07-22T12:31:02+00:00</updated>
    <author>
      <name>/u/aidanjustsayin</name>
      <uri>https://old.reddit.com/user/aidanjustsayin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt; &lt;img alt="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" src="https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47d0469ef510365b725dc72ec2ab1d98d266e09a" title="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt; &lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context: 4096 (default)&lt;/li&gt; &lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt; &lt;li&gt;CPU Thread Pool: 16&lt;/li&gt; &lt;li&gt;... all else default besides ...&lt;/li&gt; &lt;li&gt;Flash Attention: On&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt; &lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aidanjustsayin"&gt; /u/aidanjustsayin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1x5u9hrp5fef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6cfzi</id>
    <title>The ik_llama.cpp repository is back! \o/</title>
    <updated>2025-07-22T12:13:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6orbr</id>
    <title>Anyone here who has been able to reproduce their results yet?</title>
    <updated>2025-07-22T20:11:38+00:00</updated>
    <author>
      <name>/u/Original_Log_9899</name>
      <uri>https://old.reddit.com/user/Original_Log_9899</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt; &lt;img alt="Anyone here who has been able to reproduce their results yet?" src="https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5" title="Anyone here who has been able to reproduce their results yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://x.com/makingAGI/status/1947286324735856747"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original_Log_9899"&gt; /u/Original_Log_9899 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cfffg12fghef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T20:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6rsym</id>
    <title>Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models</title>
    <updated>2025-07-22T22:11:21+00:00</updated>
    <author>
      <name>/u/arcanemachined</name>
      <uri>https://old.reddit.com/user/arcanemachined</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"&gt; &lt;img alt="Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models" src="https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd" title="Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcanemachined"&gt; /u/arcanemachined &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T22:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qkse</id>
    <title>It's here guys and qwen nailed it !!</title>
    <updated>2025-07-22T21:22:09+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"&gt; &lt;img alt="It's here guys and qwen nailed it !!" src="https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg" title="It's here guys and qwen nailed it !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m6qkse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mfic</id>
    <title>Qwen3-Coder Available on chat.qwen.ai</title>
    <updated>2025-07-22T18:44:49+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt; &lt;img alt="Qwen3-Coder Available on chat.qwen.ai" src="https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83" title="Qwen3-Coder Available on chat.qwen.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1M token context length&lt;/p&gt; &lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href="https://chat.qwen.ai"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8xj4raow0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6medy</id>
    <title>Qwen3-Coder is imminent</title>
    <updated>2025-07-22T18:43:38+00:00</updated>
    <author>
      <name>/u/Dudensen</name>
      <uri>https://old.reddit.com/user/Dudensen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt; &lt;img alt="Qwen3-Coder is imminent" src="https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809" title="Qwen3-Coder is imminent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dudensen"&gt; /u/Dudensen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mruaiodv0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qc8c</id>
    <title>Qwen/Qwen3-Coder-480B-A35B-Instruct</title>
    <updated>2025-07-22T21:12:52+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"&gt; &lt;img alt="Qwen/Qwen3-Coder-480B-A35B-Instruct" src="https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1547f625cbccf70a7763a9c35af1919246072a2e" title="Qwen/Qwen3-Coder-480B-A35B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6wgs7</id>
    <title>Qwen3-Coder Unsloth dynamic GGUFs</title>
    <updated>2025-07-23T01:38:45+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Qwen3-Coder Unsloth dynamic GGUFs" src="https://preview.redd.it/s9cwrvwg1jef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75c9ba63f5cc1768819789d0934d7d2a1e5a5926" title="Qwen3-Coder Unsloth dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We made dynamic 2bit to 8bit dynamic Unsloth quants for the 480B model! Dynamic 2bit needs 182GB of space (down from 512GB). Also, we're making &lt;strong&gt;1M context length variants&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;You can achieve &amp;gt;6 tokens/s on &lt;strong&gt;182GB unified memory or 158GB RAM + 24GB VRAM&lt;/strong&gt; via MoE offloading. You do not need 182GB of VRAM, since llama.cpp can offload MoE layers to RAM via &lt;/p&gt; &lt;pre&gt;&lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately 1bit models cannot be made since there are some quantization issues (similar to Qwen 235B) - we're investigating why this happens.&lt;/p&gt; &lt;p&gt;You can also run the &lt;strong&gt;un-quantized 8bit / 16bit&lt;/strong&gt; versions also using llama,cpp offloading! Use Q8_K_XL which will be completed in an hour or so.&lt;/p&gt; &lt;p&gt;To increase performance and context length, use KV cache quantization, especially the _1 variants (higher accuracy than _0 variants). More details &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder#how-to-fit-long-context-256k-to-1m"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--cache-type-k q4_1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Enable flash attention as well and also try llama.cpp's NEW high throughput mode for multi user inference (similar to vLLM). Details on how to are &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder#improving-generation-speed"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Qwen3-Coder-480B-A35B GGUFs (still ongoing) are at &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1 million context length variants will be up at &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs on how to run it are here: &lt;a href="https://docs.unsloth.ai/basics/qwen3-coder"&gt;https://docs.unsloth.ai/basics/qwen3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s9cwrvwg1jef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wgs7/qwen3coder_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T01:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6wb5o</id>
    <title>Recent Qwen Benchmark Scores are Questionable</title>
    <updated>2025-07-23T01:31:34+00:00</updated>
    <author>
      <name>/u/Electronic_Ad8889</name>
      <uri>https://old.reddit.com/user/Electronic_Ad8889</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"&gt; &lt;img alt="Recent Qwen Benchmark Scores are Questionable" src="https://preview.redd.it/8gjn0yhf1jef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5550a5410e5e1c751c0140c16c192e6bd86fddd" title="Recent Qwen Benchmark Scores are Questionable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Ad8889"&gt; /u/Electronic_Ad8889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gjn0yhf1jef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6wb5o/recent_qwen_benchmark_scores_are_questionable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T01:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mlbk</id>
    <title>Qwen3-Coder-480B-A35B-Instruct</title>
    <updated>2025-07-22T18:50:48+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hyperolic already has it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ny2q</id>
    <title>Qwen3-Coder Web Development</title>
    <updated>2025-07-22T19:41:12+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt; &lt;img alt="Qwen3-Coder Web Development" src="https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=113bee11066829bd35da182aa0ce00847ecb4ea0" title="Qwen3-Coder Web Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt; &lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt; &lt;p&gt;Creds &lt;a href="https://www.youtube.com/@TheFeatureCrew"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ob9yhvcjahef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:41:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qixu</id>
    <title>Qwen out here releasing models like it’s a Costco sample table</title>
    <updated>2025-07-22T21:20:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"&gt; &lt;img alt="Qwen out here releasing models like it’s a Costco sample table" src="https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f24e0235850da677693988507655dde73bf8e60" title="Qwen out here releasing models like it’s a Costco sample table" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5eb8n31sshef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6nxh2</id>
    <title>Everyone brace up for qwen !!</title>
    <updated>2025-07-22T19:40:36+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt; &lt;img alt="Everyone brace up for qwen !!" src="https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=855c907a55cf3f70afe582932d52350878ef5e68" title="Everyone brace up for qwen !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mn8auem2bhef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6lf9s</id>
    <title>Could this be Deepseek?</title>
    <updated>2025-07-22T18:07:46+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt; &lt;img alt="Could this be Deepseek?" src="https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e224ff9a214f929b3917304102fe92d67371e639" title="Could this be Deepseek?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qzkjkgegugef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qnpq</id>
    <title>Qwen3 coder will be in multiple sizes</title>
    <updated>2025-07-22T21:25:25+00:00</updated>
    <author>
      <name>/u/dinesh2609</name>
      <uri>https://old.reddit.com/user/dinesh2609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"&gt; &lt;img alt="Qwen3 coder will be in multiple sizes" src="https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1547f625cbccf70a7763a9c35af1919246072a2e" title="Qwen3 coder will be in multiple sizes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinesh2609"&gt; /u/dinesh2609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mew9</id>
    <title>Qwen3- Coder 👀</title>
    <updated>2025-07-22T18:44:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt; &lt;img alt="Qwen3- Coder 👀" src="https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1" title="Qwen3- Coder 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Available in &lt;a href="https://chat.qwen.ai"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnhuwe801hef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qdet</id>
    <title>Qwen3-Coder is here!</title>
    <updated>2025-07-22T21:14:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt; &lt;img alt="Qwen3-Coder is here!" src="https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b" title="Qwen3-Coder is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Qwen3-Coder is here! ✅&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;We’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀&lt;/p&gt; &lt;p&gt;Alongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0cowg3grrhef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:14:07+00:00</published>
  </entry>
</feed>
