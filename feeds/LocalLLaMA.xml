<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-19T04:49:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jegjjx</id>
    <title>I wrote a small piece: the rise of intelligent infrastructure for AI-native apps</title>
    <updated>2025-03-18T21:31:20+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jegjjx/i_wrote_a_small_piece_the_rise_of_intelligent/"&gt; &lt;img alt="I wrote a small piece: the rise of intelligent infrastructure for AI-native apps" src="https://preview.redd.it/eaqaz10znipe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=107550d36ff0cfdcf5166f103e576126ed92ed54" title="I wrote a small piece: the rise of intelligent infrastructure for AI-native apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an infrastructure and could services builder- who built services at AWS. I joined the company in 2012 just when cloud computing was reinventing the building blocks needed for web and mobile apps&lt;/p&gt; &lt;p&gt;With the rise of AI apps I feel a new reinvention of the building blocks (aka infrastructure primitives) is underway to help developers build high-quality, reliable and production-ready LLM apps. While the shape of infrastructure building blocks will look the same, it will have very different properties and attributes.&lt;/p&gt; &lt;p&gt;Hope you enjoy the read üôè - &lt;a href="https://www.archgw.com/blogs/the-rise-of-intelligent-infrastructure-for-llm-applications"&gt;https://www.archgw.com/blogs/the-rise-of-intelligent-infrastructure-for-llm-applications&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eaqaz10znipe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jegjjx/i_wrote_a_small_piece_the_rise_of_intelligent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jegjjx/i_wrote_a_small_piece_the_rise_of_intelligent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jempy6</id>
    <title>Cohere Command A Reviews?</title>
    <updated>2025-03-19T02:19:35+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a few days since Cohere's released their new 111B &amp;quot;Command A&amp;quot;.&lt;/p&gt; &lt;p&gt;Has anyone tried this model? Is it actually good in a specific area (coding, general knowledge, RAG, writing, etc.) or just benchmaxxing?&lt;/p&gt; &lt;p&gt;Honestly I can't really justify downloading a huge model when I could be using Gemma 3 27B or the new Mistral 3.1 24B...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jempy6/cohere_command_a_reviews/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jempy6/cohere_command_a_reviews/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jempy6/cohere_command_a_reviews/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T02:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1je7hs0</id>
    <title>ollama 0.6.2 pre-release makes Gemma 3 actually work and not suck</title>
    <updated>2025-03-18T15:22:19+00:00</updated>
    <author>
      <name>/u/vertigo235</name>
      <uri>https://old.reddit.com/user/vertigo235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally can use Gemma 3 without memory errors when increasing context size with this new pre-release. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.2"&gt;https://github.com/ollama/ollama/releases/tag/v0.6.2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vertigo235"&gt; /u/vertigo235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je7hs0/ollama_062_prerelease_makes_gemma_3_actually_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:22:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jefu92</id>
    <title>NVIDIA Enters The AI PC Realm With DGX Spark &amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory</title>
    <updated>2025-03-18T21:01:48+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"&gt; &lt;img alt="NVIDIA Enters The AI PC Realm With DGX Spark &amp;amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory" src="https://external-preview.redd.it/5AEU0ER6vvoF_Vwmm5Rb5FZlbnmQmmesAV1c3JbTGmg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74d56d71efdf31cc89b6b1f1afb29f86801be909" title="NVIDIA Enters The AI PC Realm With DGX Spark &amp;amp; DGX Station Desktops: 72 Core Grace CPU, Blackwell GPUs, Up To 784 GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-enters-ai-pc-realm-dgx-spark-dgx-station-desktops-72-core-grace-cpu-blackwell-gpus-up-to-784-gb-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jefu92/nvidia_enters_the_ai_pc_realm_with_dgx_spark_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:01:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeepw8</id>
    <title>EXAONE-Deep-7.8B might be the worst reasoning model I've tried.</title>
    <updated>2025-03-18T20:16:11+00:00</updated>
    <author>
      <name>/u/LSXPRIME</name>
      <uri>https://old.reddit.com/user/LSXPRIME</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeepw8/exaonedeep78b_might_be_the_worst_reasoning_model/"&gt; &lt;img alt="EXAONE-Deep-7.8B might be the worst reasoning model I've tried." src="https://external-preview.redd.it/3YF4nrsMnN9z6X1T24Gum7_PWW0AKnzm1HYDM1LJrSo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b4a8dd18197b7c65843b6c58a95e672dcc37943" title="EXAONE-Deep-7.8B might be the worst reasoning model I've tried." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jvufke0w9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0601e794d69e361d0b791fa4faf137825a7b6f2a"&gt;https://preview.redd.it/jvufke0w9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0601e794d69e361d0b791fa4faf137825a7b6f2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j6j9cibw9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23357dc6f56aac238d6d5963368fbf97abcb74b1"&gt;https://preview.redd.it/j6j9cibw9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23357dc6f56aac238d6d5963368fbf97abcb74b1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/db7datlw9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c9eedef3a64625d991524b6b3039304301cde3"&gt;https://preview.redd.it/db7datlw9ipe1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47c9eedef3a64625d991524b6b3039304301cde3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With an average of 12K tokens of unrelated thoughts, I am a bit disappointed as it's the first EXAONE model I try. On the other hand, other reasoning models of similar size often produce results with less than 1K tokens, even if they can be hit-or-miss. However, this model consistently fails to hit the mark or follow the questions. I followed the template and settings provided in their GitHub repository.&lt;/p&gt; &lt;p&gt;I see a praise posts around for its smaller sibling (2.4B). Have I missed something?&lt;/p&gt; &lt;p&gt;I used the Q4_K_M quant from &lt;a href="https://huggingface.co/mradermacher/EXAONE-Deep-7.8B-i1-GGUF"&gt;https://huggingface.co/mradermacher/EXAONE-Deep-7.8B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LM Studio Instructions from EXAONE repo &lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep#lm-studio"&gt;https://github.com/LG-AI-EXAONE/EXAONE-Deep#lm-studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LSXPRIME"&gt; /u/LSXPRIME &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeepw8/exaonedeep78b_might_be_the_worst_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeepw8/exaonedeep78b_might_be_the_worst_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeepw8/exaonedeep78b_might_be_the_worst_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T20:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdw7bg</id>
    <title>After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS</title>
    <updated>2025-03-18T03:41:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt; &lt;img alt="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" src="https://preview.redd.it/3lujka2ucdpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a9e581fb20610cdc9e8940f25cf7e4e9277ff42" title="After these last 2 weeks of exciting releases, the only thing I know for certain is that benchmarks are largely BS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3lujka2ucdpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdw7bg/after_these_last_2_weeks_of_exciting_releases_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T03:41:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1je17el</id>
    <title>Open source 7.8B model beats o1 mini now on many benchmarks</title>
    <updated>2025-03-18T09:45:26+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt; &lt;img alt="Open source 7.8B model beats o1 mini now on many benchmarks" src="https://preview.redd.it/211jtna16fpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e1d30576311226c74c3be9ef4b897ba63782ca9" title="Open source 7.8B model beats o1 mini now on many benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/211jtna16fpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je17el/open_source_78b_model_beats_o1_mini_now_on_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T09:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeg10y</id>
    <title>Mistral Small 3.1 performance on benchmarks not included in their announcement</title>
    <updated>2025-03-18T21:09:42+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeg10y/mistral_small_31_performance_on_benchmarks_not/"&gt; &lt;img alt="Mistral Small 3.1 performance on benchmarks not included in their announcement" src="https://preview.redd.it/egmm33owjipe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e4892d34337abb591cf89a45908889eda93ea40" title="Mistral Small 3.1 performance on benchmarks not included in their announcement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/egmm33owjipe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeg10y/mistral_small_31_performance_on_benchmarks_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeg10y/mistral_small_31_performance_on_benchmarks_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedum8</id>
    <title>NVIDIA‚Äôs Llama-nemotron models</title>
    <updated>2025-03-18T19:40:57+00:00</updated>
    <author>
      <name>/u/gizcard</name>
      <uri>https://old.reddit.com/user/gizcard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning ON/OFF. Currently on HF with entire post training data under CC-BY-4. &lt;a href="https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b"&gt;https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gizcard"&gt; /u/gizcard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedum8/nvidias_llamanemotron_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jehxiw</id>
    <title>LLAMA 4 in April?!?!?!?</title>
    <updated>2025-03-18T22:31:24+00:00</updated>
    <author>
      <name>/u/Sea_Anywhere896</name>
      <uri>https://old.reddit.com/user/Sea_Anywhere896</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt; &lt;img alt="LLAMA 4 in April?!?!?!?" src="https://external-preview.redd.it/uRHwEOU98rM_55MIJyA8g2IjSi4Ibl9Ab1kLsdGuLI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44af8b7574c0a4b26360d529db34c1b06ffcafcc" title="LLAMA 4 in April?!?!?!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zalw2xetbipe1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=530f772a86a3ec93733a34053e7d66f91df342a0"&gt;https://preview.redd.it/zalw2xetbipe1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=530f772a86a3ec93733a34053e7d66f91df342a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google did similar thing with Gemma 3, so... llama 4 soon?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.llama.com/"&gt;https://www.llama.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Anywhere896"&gt; /u/Sea_Anywhere896 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jehxiw/llama_4_in_april/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T22:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1je7u2e</id>
    <title>ASUS DIGITS</title>
    <updated>2025-03-18T15:37:13+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"&gt; &lt;img alt="ASUS DIGITS" src="https://preview.redd.it/oidvhqtswgpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a48278147831b957d3817b71e1537dbde46ab70" title="ASUS DIGITS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When we got the online presentation, a while back, and it was in collaboration with PNY, it seemed like they would manufacture them. Now it seems like there will be more, like I guessed when I saw it.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.techpowerup.com/334249/asus-unveils-new-ascent-gx10-mini-pc-powered-nvidia-gb10-grace-blackwell-superchip?amp"&gt;https://www.techpowerup.com/334249/asus-unveils-new-ascent-gx10-mini-pc-powered-nvidia-gb10-grace-blackwell-superchip?amp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Archive: &lt;a href="https://web.archive.org/web/20250318102801/https://press.asus.com/news/press-releases/asus-ascent-gx10-ai-supercomputer-nvidia-gb10/"&gt;https://web.archive.org/web/20250318102801/https://press.asus.com/news/press-releases/asus-ascent-gx10-ai-supercomputer-nvidia-gb10/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oidvhqtswgpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je7u2e/asus_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1je4eka</id>
    <title>SmolDocling - 256M VLM for document understanding</title>
    <updated>2025-03-18T13:01:55+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! I'm andi and I work at HF for everything multimodal and vision ü§ù Yesterday with IBM we released SmolDocling, a new smol model (256M parameters ü§èüèªü§èüèª) to transcribe PDFs into markdown, it's state-of-the-art and outperforms much larger models Here's some TLDR if you're interested:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The text is rendered into markdown and has a new format called DocTags, which contains location info of objects in a PDF (images, charts), it can caption images inside PDFs Inference takes 0.35s on single A100 This model is supported by transformers and friends, and is loadable to MLX and you can serve it in vLLM Apache 2.0 licensed Very curious about your opinions ü•π&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je4eka/smoldocling_256m_vlm_for_document_understanding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T13:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedlum</id>
    <title>DGX Sparks / Nvidia Digits</title>
    <updated>2025-03-18T19:31:07+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt; &lt;img alt="DGX Sparks / Nvidia Digits" src="https://preview.redd.it/4ydasblh2ipe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22bbb5dc941a9764664463ef883da62ce8b80a06" title="DGX Sparks / Nvidia Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have now official Digits/DGX Sparks specs&lt;/p&gt; &lt;p&gt;|| || |Architecture|NVIDIA Grace Blackwell| |GPU|Blackwell Architecture| |CPU|20 core Arm, 10 Cortex-X925 + 10 Cortex-A725 Arm| |CUDA Cores|Blackwell Generation| |Tensor Cores|5th Generation| |RT Cores|4th Generation| |&lt;sup&gt;1&lt;/sup&gt;Tensor Performance |1000 AI TOPS| |System Memory|128 GB LPDDR5x, unified system memory| |Memory Interface|256-bit| |Memory Bandwidth|273 GB/s| |Storage|1 or 4 TB NVME.M2 with self-encryption| |USB|4x USB 4 TypeC (up to 40Gb/s)| |Ethernet|1x RJ-45 connector 10 GbE| |NIC|ConnectX-7 Smart NIC| |Wi-Fi|WiFi 7| |Bluetooth|BT 5.3 w/LE| |Audio-output|HDMI multichannel audio output| |Power Consumption|170W| |Display Connectors|1x HDMI 2.1a| |NVENC | NVDEC|1x | 1x| |OS|&lt;sup&gt;‚Ñ¢&lt;/sup&gt; NVIDIA DGX OS| |System Dimensions|150 mm L x 150 mm W x 50.5 mm H| |System Weight|1.2 kg|&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4ydasblh2ipe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedlum/dgx_sparks_nvidia_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jee2b2</id>
    <title>NVIDIA DGX Spark (Project DIGITS) Specs Are Out</title>
    <updated>2025-03-18T19:49:56+00:00</updated>
    <author>
      <name>/u/spectrography</name>
      <uri>https://old.reddit.com/user/spectrography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Memory bandwidth: 273 GB/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spectrography"&gt; /u/spectrography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jee2b2/nvidia_dgx_spark_project_digits_specs_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1je58r5</id>
    <title>Wen GGUFs?</title>
    <updated>2025-03-18T13:42:29+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt; &lt;img alt="Wen GGUFs?" src="https://preview.redd.it/vv2vg9xbcgpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fce563f7a834755ce5916e4567c3e30f30949f6" title="Wen GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vv2vg9xbcgpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je58r5/wen_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T13:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeh199</id>
    <title>Gemma 3 27B and Mistral Small 3.1 LiveBench results</title>
    <updated>2025-03-18T21:52:48+00:00</updated>
    <author>
      <name>/u/Vivid_Dot_6405</name>
      <uri>https://old.reddit.com/user/Vivid_Dot_6405</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"&gt; &lt;img alt="Gemma 3 27B and Mistral Small 3.1 LiveBench results" src="https://preview.redd.it/ss640j7rripe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5889de41f1225137aeb588b78ba1420f488711e8" title="Gemma 3 27B and Mistral Small 3.1 LiveBench results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Dot_6405"&gt; /u/Vivid_Dot_6405 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ss640j7rripe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeh199/gemma_3_27b_and_mistral_small_31_livebench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T21:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeddoy</id>
    <title>bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF</title>
    <updated>2025-03-18T19:22:10+00:00</updated>
    <author>
      <name>/u/nicklauzon</name>
      <uri>https://old.reddit.com/user/nicklauzon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The man, the myth, the legend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicklauzon"&gt; /u/nicklauzon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeddoy/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jej4s5</id>
    <title>Uncensored Gemma 3</title>
    <updated>2025-03-18T23:24:33+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B"&gt;https://huggingface.co/soob3123/amoral-gemma3-12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just finetuned this gemma 3 a day ago. Havent gotten it to refuse to anything yet. &lt;/p&gt; &lt;p&gt;Please feel free to give me feedback! This is my first finetuned model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jej4s5/uncensored_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T23:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeod23</id>
    <title>Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!</title>
    <updated>2025-03-19T03:49:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt; &lt;img alt="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" src="https://b.thumbs.redditmedia.com/XFSJpkliPR8uY1jFq2k76gGRqvjuRsWCKVnvxNMCY1M.jpg" title="Still can't believe it. Got this A6000 (Ampere) beauty, working perfectly for 1300USD on Chile!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jeod23"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeod23/still_cant_believe_it_got_this_a6000_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T03:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jef8pr</id>
    <title>Llama-3.3-Nemotron-Super-49B-v1 benchmarks</title>
    <updated>2025-03-18T20:37:26+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"&gt; &lt;img alt="Llama-3.3-Nemotron-Super-49B-v1 benchmarks" src="https://preview.redd.it/9mswvzt3eipe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24e550f4997f92ee036e88cce665df61a6cfcb83" title="Llama-3.3-Nemotron-Super-49B-v1 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mswvzt3eipe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jef8pr/llama33nemotronsuper49bv1_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T20:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedtdx</id>
    <title>NVIDIA RTX PRO 6000 "Blackwell" Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM</title>
    <updated>2025-03-18T19:39:33+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" src="https://external-preview.redd.it/5BAv36T_l_eU_QSz_Xpf7tieC4_Jhv4Dc1Rz9SQmcqY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=164c15c74e9b11e253100c5a08166759fbfab7a1" title="NVIDIA RTX PRO 6000 &amp;quot;Blackwell&amp;quot; Series Launched: Flagship GB202 GPU With 24K Cores, 96 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-launch-flagship-gb202-gpu-24k-cores-96-gb-600w-tdp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedtdx/nvidia_rtx_pro_6000_blackwell_series_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jedy17</id>
    <title>Nvidia digits specs released and renamed to DGX Spark</title>
    <updated>2025-03-18T19:44:57+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/"&gt;https://www.nvidia.com/en-us/products/workstations/dgx-spark/&lt;/a&gt; Memory Bandwidth 273 GB/s&lt;/p&gt; &lt;p&gt;Much cheaper for running 70gb - 200 gb models than a 5090. Cost $3K according to nVidia. Previously nVidia claimed availability in May 2025. Will be interesting tps versus &lt;a href="https://frame.work/desktop"&gt;https://frame.work/desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1je8axe</id>
    <title>I'm not one for dumb tests but this is a funny first impression</title>
    <updated>2025-03-18T15:56:57+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt; &lt;img alt="I'm not one for dumb tests but this is a funny first impression" src="https://preview.redd.it/s5k3j9z70hpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7562163bfe7eb6adc1234ea41f7c18eca73fb49c" title="I'm not one for dumb tests but this is a funny first impression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s5k3j9z70hpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je8axe/im_not_one_for_dumb_tests_but_this_is_a_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T15:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jeczzz</id>
    <title>New reasoning model from NVIDIA</title>
    <updated>2025-03-18T19:07:23+00:00</updated>
    <author>
      <name>/u/mapestree</name>
      <uri>https://old.reddit.com/user/mapestree</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt; &lt;img alt="New reasoning model from NVIDIA" src="https://external-preview.redd.it/S69zjX2lDQklr7v9YvMlzRoANZizsM0E74iOfCibG0E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2980af0c07cf4d3c4c52a935239567d59c9b3be3" title="New reasoning model from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapestree"&gt; /u/mapestree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/5kluqad.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jeczzz/new_reasoning_model_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T19:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1je6ns1</id>
    <title>Meta talks about us and open source source AI for over 1 Billion downloads</title>
    <updated>2025-03-18T14:46:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt; &lt;img alt="Meta talks about us and open source source AI for over 1 Billion downloads" src="https://preview.redd.it/gcql3piongpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58b8393e9781f3853aac114d10af307ef017ca59" title="Meta talks about us and open source source AI for over 1 Billion downloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gcql3piongpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1je6ns1/meta_talks_about_us_and_open_source_source_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-18T14:46:25+00:00</published>
  </entry>
</feed>
