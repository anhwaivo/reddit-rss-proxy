<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-04T11:23:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lrfo4i</id>
    <title>What kind of models can I run with my new hardware?</title>
    <updated>2025-07-04T10:43:26+00:00</updated>
    <author>
      <name>/u/GTurkistane</name>
      <uri>https://old.reddit.com/user/GTurkistane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;RTX 3090, 24GB VRAM&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;Ryzen 9 9950X3D, 32 threads, 192MB L3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;192GB DDR5 3600hz&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I am using webui as a back end, what type of GGUF (30b/70b models with 8/4 quantization...etc) models can I run? How much should I off load to GPU and how much to CPU with reasonable t/s?&lt;/p&gt; &lt;p&gt;Also, is there a way for me to utilize the 2g VRAM in the CPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTurkistane"&gt; /u/GTurkistane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrfo4i/what_kind_of_models_can_i_run_with_my_new_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrfo4i/what_kind_of_models_can_i_run_with_my_new_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrfo4i/what_kind_of_models_can_i_run_with_my_new_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T10:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr2z7q</id>
    <title>How do tools like ChatGPT, Gemini, and Grok derive context from a video?</title>
    <updated>2025-07-03T22:37:42+00:00</updated>
    <author>
      <name>/u/Familiar_Engine718</name>
      <uri>https://old.reddit.com/user/Familiar_Engine718</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I uploaded a 10 second clip of myself playing minigolf, and it could even tell that I hit a hole in one. It gave me an accurate timeline description of the clip. I know it has to do with multi-modal capabilities but I am still somewhat confused from a technical perspective?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Familiar_Engine718"&gt; /u/Familiar_Engine718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr2z7q/how_do_tools_like_chatgpt_gemini_and_grok_derive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr2z7q/how_do_tools_like_chatgpt_gemini_and_grok_derive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr2z7q/how_do_tools_like_chatgpt_gemini_and_grok_derive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T22:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lraotq</id>
    <title>How to set up MCP for fast code</title>
    <updated>2025-07-04T05:21:31+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to be able to ask my local LLM to give me fast code for a particular function. Ideally it would give the code, run it locally and time it, then change the code to try to speed it up and repeat.&lt;/p&gt; &lt;p&gt;I would probably run this in docker to stop it accidentally damaging my system. &lt;/p&gt; &lt;p&gt;I am new to MCP. Are there any guides on how to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lraotq/how_to_set_up_mcp_for_fast_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lraotq/how_to_set_up_mcp_for_fast_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lraotq/how_to_set_up_mcp_for_fast_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T05:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrgcd4</id>
    <title>Can I use ollama to run https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF/blob/main/DeepSeek-R1-0528-Qwen3-8B-UD-Q4_K_XL.gguf ?</title>
    <updated>2025-07-04T11:23:03+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand when you can use ollama to run huggingface models. Can that model be used with ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgcd4/can_i_use_ollama_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgcd4/can_i_use_ollama_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgcd4/can_i_use_ollama_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T11:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrdrzi</id>
    <title>Fine-tuning LLM PoC</title>
    <updated>2025-07-04T08:40:20+00:00</updated>
    <author>
      <name>/u/QueRoub</name>
      <uri>https://old.reddit.com/user/QueRoub</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I have only worked with big enterprise models so far.&lt;/p&gt; &lt;p&gt;I would like to run a fine-tuning PoC for a small pretrained model.&lt;/p&gt; &lt;p&gt;Please suggest up to 3 selections for the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Dataset selection (dataset for text classification or sentiment analysis)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Model selection (which are the best small models to fine-tune for this use case (like Gemma, Mistral Small etc))&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fine-tuning libraries (like LoRa, QLoRa)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Optimization techniques (to reduce model size or inference latency)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QueRoub"&gt; /u/QueRoub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrdrzi/finetuning_llm_poc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrdrzi/finetuning_llm_poc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrdrzi/finetuning_llm_poc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T08:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr3eh1</id>
    <title>Client-side STT version of Moonshine released</title>
    <updated>2025-07-03T22:57:15+00:00</updated>
    <author>
      <name>/u/petewarden</name>
      <uri>https://old.reddit.com/user/petewarden</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/"&gt; &lt;img alt="Client-side STT version of Moonshine released" src="https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82b86b66380c9a5e7514f8812884ee2c803923e1" title="Client-side STT version of Moonshine released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1lr3eh1/video/x813klchapaf1/player"&gt;https://reddit.com/link/1lr3eh1/video/x813klchapaf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm happy to say we have released our first version of &lt;a href="https://github.com/moonshine-ai/moonshine-js"&gt;MoonshineJS&lt;/a&gt;, an open source speech to text library based on the fast-but-accurate &lt;a href="https://github.com/moonshine-ai/moonshine"&gt;Moonshine models&lt;/a&gt;, including &lt;a href="https://huggingface.co/UsefulSensors/moonshine-es"&gt;new Spanish versions&lt;/a&gt; available under a non-commercial license (English and code are all MIT). The video above shows captions being generated in the browser, all running locally on the client, and &lt;a href="https://www.moonshine.ai/examples/video-captioner/index.html"&gt;here's a live demo&lt;/a&gt;. The code to do this is literally:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import * as Moonshine from &amp;quot;https://cdn.jsdelivr.net/npm/@moonshine-ai/moonshine-js@0.1.29/dist/moonshine.min.js&amp;quot; var video = document.getElementById(&amp;quot;video&amp;quot;); var videoCaptioner = new Moonshine.VideoCaptioner(video, &amp;quot;model/base&amp;quot;, false); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We also have &lt;a href="https://github.com/moonshine-ai/moonshine-js-webrtc/"&gt;a more extensive example that shows how to both transcribe and translate a WebRTC video call&lt;/a&gt; in real time, which you can &lt;a href="https://webrtc.moonshine.ai/"&gt;try live here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1lr3eh1/video/bkgvxedvjqaf1/player"&gt;https://reddit.com/link/1lr3eh1/video/bkgvxedvjqaf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are more examples and documentation at &lt;a href="http://dev.moonshine.ai"&gt;dev.moonshine.ai&lt;/a&gt;, along with our SDKs for other languages. The largest model (equivalent in accuracy to Whisper Base) is 60MB in size, so hopefully that won't bloat your pages too much.&lt;/p&gt; &lt;p&gt;I've been a long-time lurker here, it's great to see so many things happening in the world of local inference, and if you do build anything with these models I'd love to hear from you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petewarden"&gt; /u/petewarden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T22:57:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr9594</id>
    <title>DnD LLMs - Prompt to LoRA github</title>
    <updated>2025-07-04T03:53:03+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To the 2 dozen people that were waiting on this code and were disappointed when you checked the link after the !remindme today only to find nothing: &lt;a href="https://github.com/sanowl/Drag-and-Drop-LLMs-Zero-Shot-Prompt-to-Weights"&gt;https://github.com/sanowl/Drag-and-Drop-LLMs-Zero-Shot-Prompt-to-Weights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just stumbled upon it in my github activity&lt;/p&gt; &lt;p&gt;looks like they just didn't update the github.io page&lt;/p&gt; &lt;p&gt;original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/uyaWHReUW8"&gt;https://www.reddit.com/r/LocalLLaMA/s/uyaWHReUW8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr9594/dnd_llms_prompt_to_lora_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr9594/dnd_llms_prompt_to_lora_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr9594/dnd_llms_prompt_to_lora_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T03:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqx16</id>
    <title>Kyutai Unmute (incl. TTS) released</title>
    <updated>2025-07-03T14:25:11+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmute github: &lt;a href="https://github.com/kyutai-labs/unmute"&gt;https://github.com/kyutai-labs/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unmute blog: &lt;a href="https://kyutai.org/next/unmute"&gt;https://kyutai.org/next/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS blog with a demo: &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS weights: &lt;a href="https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29"&gt;https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;STT was released earlier so the whole component stack is now out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqxs6n</id>
    <title>Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen</title>
    <updated>2025-07-03T18:58:08+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt; &lt;img alt="Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen" src="https://b.thumbs.redditmedia.com/b-h03xyvoled26aXh7C0uCGNcWCl9E1UZV5LDMiLKeM.jpg" title="Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a"&gt;https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;9.8t/s on a 235b model with just a 16GB card?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Now 11.7 t/s with 16 threads. Even my 3060 can do 10.2 t/s it seems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-server.exe -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot exps=CPU -c 30000 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA0 -md Qwen3-0.6B-BF16.gguf -devd CUDA0 -ngld 99&lt;/p&gt; &lt;p&gt;prompt eval time = 10924.78 ms / 214 tokens ( 51.05 ms per token, 19.59 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 594651.64 ms / 5826 tokens ( 102.07 ms per token, 9.80 tokens per second)&lt;/p&gt; &lt;p&gt;total time = 605576.42 ms / 6040 tokens&lt;/p&gt; &lt;p&gt;slot print_timing: id 0 | task 0 |&lt;/p&gt; &lt;p&gt;draft acceptance rate = 0.86070 ( 4430 accepted / 5147 generated)&lt;/p&gt; &lt;p&gt;I've now tried quite a few Qwen 0.6b draft models. TLDR, Q80 is marginally faster BUT FOR SOME REASON the bf16 draft model produces better outputs than all the others. Also, look at that acceptance rate. 86%!&lt;/p&gt; &lt;p&gt;This was the classic flappy bird test and here's the code it produced:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import pygame import random import sys # Initialize pygame pygame.init() # Set up display width, height = 400, 600 screen = pygame.display.set_mode((width, height)) pygame.display.set_caption(&amp;quot;Flappy Bird&amp;quot;) # Set up game clock clock = pygame.time.Clock() # Bird parameters bird_x = width // 4 bird_y = height // 2 bird_velocity = 0 gravity = 0.5 acceleration = -8 bird_size = 30 bird_shape = random.choice(['square', 'circle', 'triangle']) bird_color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)) # Land parameters land_height = random.choice([50, 100]) land_color = random.choice([(139, 69, 19), (255, 255, 0)]) # Pipe parameters pipe_width = 60 pipe_gap = 150 pipe_velocity = 3 pipes = [] pipe_colors = [(0, 100, 0), (165, 105, 55), (60, 60, 60)] # Score score = 0 best_score = 0 font = pygame.font.Font(None, 36) # Background background_color = (173, 216, 230) # light blue # Game state game_active = True def create_pipe(): pipe_height = random.randint(100, height - pipe_gap - land_height - 50) top_pipe = pygame.Rect(width, 0, pipe_width, pipe_height) bottom_pipe = pygame.Rect(width, pipe_height + pipe_gap, pipe_width, height - pipe_height - pipe_gap) color = random.choice(pipe_colors) return [top_pipe, bottom_pipe, color, False] # False for scored status def draw_bird(): if bird_shape == 'square': pygame.draw.rect(screen, bird_color, (bird_x, bird_y, bird_size, bird_size)) elif bird_shape == 'circle': pygame.draw.circle(screen, bird_color, (bird_x + bird_size//2, bird_y + bird_size//2), bird_size//2) elif bird_shape == 'triangle': points = [(bird_x, bird_y + bird_size), (bird_x + bird_size//2, bird_y), (bird_x + bird_size, bird_y + bird_size)] pygame.draw.polygon(screen, bird_color, points) def check_collision(): # Create bird rect bird_rect = pygame.Rect(bird_x, bird_y, bird_size, bird_size) # Check collision with pipes for pipe in pipes: if pipe[0].colliderect(bird_rect) or pipe[1].colliderect(bird_rect): return True # Check collision with ground or ceiling if bird_y &amp;gt;= height - land_height or bird_y &amp;lt;= 0: return True return False # Initial pipe pipes.append(create_pipe()) # Main game loop while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() if event.type == pygame.KEYDOWN: if event.key == pygame.K_SPACE: if game_active: bird_velocity = acceleration else: # Restart game bird_y = height // 2 bird_velocity = 0 pipes = [create_pipe()] score = 0 game_active = True if event.key == pygame.K_q or event.key == pygame.K_ESCAPE: pygame.quit() sys.exit() if game_active: # Update bird position bird_velocity += gravity bird_y += bird_velocity # Update pipes if not pipes or pipes[-1][0].x &amp;lt; width - 200: pipes.append(create_pipe()) for pipe in pipes: pipe[0].x -= pipe_velocity pipe[1].x -= pipe_velocity # Remove off-screen pipes pipes = [pipe for pipe in pipes if pipe[0].x + pipe_width &amp;gt; 0] # Check for collision if check_collision(): game_active = False best_score = max(score, best_score) # Check for score update for pipe in pipes: if not pipe[3]: # If not scored yet if pipe[0].x + pipe_width &amp;lt; bird_x: score += 1 pipe[3] = True # Draw everything screen.fill(background_color) # Draw pipes for pipe in pipes: pygame.draw.rect(screen, pipe[2], pipe[0]) pygame.draw.rect(screen, pipe[2], pipe[1]) # Draw bird draw_bird() # Draw land pygame.draw.rect(screen, land_color, (0, height - land_height, width, land_height)) # Draw score score_text = font.render(f&amp;quot;Score: {score}&amp;quot;, True, (0, 0, 0)) best_score_text = font.render(f&amp;quot;Best: {best_score}&amp;quot;, True, (0, 0, 0)) screen.blit(score_text, (width - 150, 20)) screen.blit(best_score_text, (width - 150, 50)) if not game_active: game_over_text = font.render(&amp;quot;Game Over! Press SPACE to restart&amp;quot;, True, (0, 0, 0)) screen.blit(game_over_text, (width//2 - 150, height//2 - 50)) pygame.display.flip() clock.tick(60) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I had no intention of using this model, I was just trying to see how badly it would run however, I'm starting to think there may be some sort of synergy between Unsloth's Q2K 235b and their BF16 0.6b as a draft model.&lt;/p&gt; &lt;p&gt;The game seems to run and play fine, too:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e"&gt;https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T18:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqxm89</id>
    <title>We Built an Open Source Clone of Lovable</title>
    <updated>2025-07-03T18:51:34+00:00</updated>
    <author>
      <name>/u/velobro</name>
      <uri>https://old.reddit.com/user/velobro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI-coding agents like Lovable and Bolt are taking off, but it's still not widely known how they actually work.&lt;/p&gt; &lt;p&gt;We decided to build an open-source Lovable clone that includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Structured prompts using BAML (like RPCs for LLMs)&lt;/li&gt; &lt;li&gt;Secure sandboxing for generated code&lt;/li&gt; &lt;li&gt;Real-time previews with WebSockets and FastAPI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're curious about how agentic apps work under the hood or want to build your own, this might help. Everything we learned is in the blog post below, and you can see all the code on Github.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog Post&lt;/strong&gt;: &lt;a href="https://www.beam.cloud/blog/agentic-apps"&gt;https://www.beam.cloud/blog/agentic-apps&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/beam-cloud/lovable-clone"&gt;https://github.com/beam-cloud/lovable-clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know if you have feedback or if there's anything we missed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/velobro"&gt; /u/velobro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T18:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrc8pk</id>
    <title>Give me some ideas</title>
    <updated>2025-07-04T06:57:44+00:00</updated>
    <author>
      <name>/u/ajmusic15</name>
      <uri>https://old.reddit.com/user/ajmusic15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning, everyone.&lt;/p&gt; &lt;p&gt;I wanted to discuss with you some ideas for getting the most out of my 5080 (it has 16 GB). What AI applications could I use it for? Currently, I can run Flux Dev on FP8 smoothly, and I can also run models as large as Devstral 24B on IQ2_XXS or Qwen3-30B-A3B on IQ3_XXS (the first at 48-56 tk/s and the last at almost 130 tk/s).&lt;/p&gt; &lt;p&gt;What else can I do? I want to try out NVFP4, but I don't know if vLLM or SGLang support it right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajmusic15"&gt; /u/ajmusic15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrc8pk/give_me_some_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrc8pk/give_me_some_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrc8pk/give_me_some_ideas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T06:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr5g8x</id>
    <title>Productivity Tracker that uses Gemma3:4BB</title>
    <updated>2025-07-04T00:36:48+00:00</updated>
    <author>
      <name>/u/Far-Incident822</name>
      <uri>https://old.reddit.com/user/Far-Incident822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I built this two months ago over the course of a few days. It's very much alpha software. It's a productivity tracker that measures whether you're being productive, and tries to nudge you when you're being unproductive. Let me know what you think. Once again, super alpha codebase. You'll need to add your own model files to the models directory to get the app to run.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grunsab/Time-Tracker-Mac/"&gt;https://github.com/grunsab/Time-Tracker-Mac/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Incident822"&gt; /u/Far-Incident822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqw2yg</id>
    <title>Deep Dive into Deep Research with Qwen3-30b-a3b</title>
    <updated>2025-07-03T17:50:31+00:00</updated>
    <author>
      <name>/u/charlie-woodworking</name>
      <uri>https://old.reddit.com/user/charlie-woodworking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"&gt; &lt;img alt="Deep Dive into Deep Research with Qwen3-30b-a3b" src="https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893874a6c9a7feb3582a1c15d40e9a7e7c407abe" title="Deep Dive into Deep Research with Qwen3-30b-a3b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recorded an explanation of how I architected, experimented with, and iterated on a custom deep research application using Qwen3-30b-a3b as the base model for a multi-agent orchestrated flow. Sprinkled in there are a few lessons I learned along the way.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=PCuBNUyS8Bc"&gt;https://www.youtube.com/watch?v=PCuBNUyS8Bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to hit me up with questions or discussions. This is the primary demo I'm giving at a tech conference in a few weeks so definitely open to improving it based on what folks want to know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charlie-woodworking"&gt; /u/charlie-woodworking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=PCuBNUyS8Bc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr1ypr</id>
    <title>Best current models for 72GB VRAM</title>
    <updated>2025-07-03T21:52:04+00:00</updated>
    <author>
      <name>/u/GregoryfromtheHood</name>
      <uri>https://old.reddit.com/user/GregoryfromtheHood</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just managed to cobble together a machine with 3x24GB GPUs, looking to see of the models currently available, what are the best ones I should be looking at now.&lt;/p&gt; &lt;p&gt;I know &amp;quot;best model&amp;quot; isn't entirely a thing, some are better than others at certain things. Like so far of the 70b and 110b models I've tried on my previous 48gb of VRAM, none came even close to Gemma3 27b for creative writing and instruction following. But I'm wondering if there are some bigger ones that might beat it.&lt;/p&gt; &lt;p&gt;Also coding, would anything I can run now beat Qwen2.5-coder 32b?&lt;/p&gt; &lt;p&gt;So far I haven't yet found anything in the ~70b range that can beat these smaller models, but maybe something bigger can?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregoryfromtheHood"&gt; /u/GregoryfromtheHood &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T21:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrerwe</id>
    <title>pytorch 2.7.x no longer supports Pascal architecture?</title>
    <updated>2025-07-04T09:46:51+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got these warnings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:262: UserWarning: Found GPU0 NVIDIA GeForce GT 1030 which is of cuda capability 6.1. PyTorch no longer supports this GPU because it is too old. The minimum cuda capability supported by this library is 7.5. warnings.warn( /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning: NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120. If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And then crash with this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1 Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried the 2.7.0 with both cuda 12.6 and 12.8 and they both gave me this error. So I should stick with 2.6.0?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnwih</id>
    <title>I can't believe it actually runs - Qwen 235b @ 16GB VRAM</title>
    <updated>2025-07-03T12:07:58+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I decided to try my luck with Qwen 235b so downloaded Unsloth's Q2XL. I've got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).&lt;/p&gt; &lt;p&gt;My runtime args:&lt;/p&gt; &lt;p&gt;llama-cli -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;/p&gt; &lt;p&gt;Super simple user prompt because I wasn't expecting miracles:&lt;/p&gt; &lt;p&gt;tell me a joke&lt;/p&gt; &lt;p&gt;Result:&lt;br /&gt; 8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven't tried any actual work on it yet.&lt;/p&gt; &lt;p&gt;cli output blurb:&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 24.81 ms / 476 runs ( 0.05 ms per token, 19183.49 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 16979.96 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 1497.01 ms / 12 tokens ( 124.75 ms per token, 8.02 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 85040.21 ms / 463 runs ( 183.67 ms per token, 5.44 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 100251.11 ms / 475 tokens&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;p&gt;It looks like I'm only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Managed to fill out the rest of the VRAM with a draft model. &lt;/p&gt; &lt;p&gt;Generation went up to 9.8t/s:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T12:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqxhq</id>
    <title>I have made a True Reasoning LLM</title>
    <updated>2025-07-03T14:25:42+00:00</updated>
    <author>
      <name>/u/moilanopyzedev</name>
      <uri>https://old.reddit.com/user/moilanopyzedev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt; &lt;p&gt;You can get it here&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/moelanoby/phi-3-M3-coder"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moilanopyzedev"&gt; /u/moilanopyzedev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr0i8p</id>
    <title>Smartphone SoC inference performance by year and series</title>
    <updated>2025-07-03T20:49:52+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"&gt; &lt;img alt="Smartphone SoC inference performance by year and series" src="https://b.thumbs.redditmedia.com/zFmb1fKtOa4tHvXdPcDmgPMDWJtC_rwKAXVsetdAeCg.jpg" title="Smartphone SoC inference performance by year and series" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr0i8p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T20:49:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lre3x9</id>
    <title>Apple M4 Max or AMD Ryzen AI Max+ 395 (Framwork Desktop)</title>
    <updated>2025-07-04T09:02:33+00:00</updated>
    <author>
      <name>/u/zeltbrennt</name>
      <uri>https://old.reddit.com/user/zeltbrennt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a LLM-Project for my CS Degree where I need to run a models locally, because of sensitive data. My current Desktop PC is quite old now (Windows, i5-6600K, 16GB RAM, GTX 1060 6GB) and only capable of running small models, so I want to upgrade it anyway. I saw a few people reccomending Apples ARM for the job, but they are very expensive. I am looking at &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mac Studio M4 Max&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple M4 Max&lt;/li&gt; &lt;li&gt;16 Core CPU&lt;/li&gt; &lt;li&gt;40 Core GPU&lt;/li&gt; &lt;li&gt;16 Core NE&lt;/li&gt; &lt;li&gt;546 GB/s memory bandwidth &lt;/li&gt; &lt;li&gt;128 GB RAM&lt;/li&gt; &lt;li&gt;1TB SSD&lt;/li&gt; &lt;li&gt;MacOS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the Edu-Store they sell in my country it for &lt;strong&gt;4,160€&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I found another alternative: Framework. I knew they build nice Laptops, but one might also preorder their new Desktops (Charge 11 is estimated to ship in Q3). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Framework Desktop Max+ 395&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen AI Max+ 395&lt;/li&gt; &lt;li&gt;16 Core CPU&lt;/li&gt; &lt;li&gt;40 Core GPU&lt;/li&gt; &lt;li&gt;265 GB/s memory bandwidth &lt;/li&gt; &lt;li&gt;128 GB RAM&lt;/li&gt; &lt;li&gt;1TB SSD&lt;/li&gt; &lt;li&gt;Fedora&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So with the (on paper) equivalent configuration I arrive at &lt;strong&gt;2,570€&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That is a lot of money saved! Plus I would be running Linux instead of MacOS. I like not being boxed in an ecosystem. The replacement parts are much cheaper. The only downside would be a few programs like Lightroom are not availabe on Linux (I would cancel my subscription, wich also saves money). Gaming on this thing might also be better.&lt;/p&gt; &lt;p&gt;Has anybody expierence with this System for LLMs? Would this be a good alternative? What benefit am I getting in the Max version and is it worth the premium price?&lt;/p&gt; &lt;p&gt;Edit: fixed CPU core count, added memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeltbrennt"&gt; /u/zeltbrennt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr217c</id>
    <title>Cheaper Transcriptions, Pricier Errors!</title>
    <updated>2025-07-03T21:55:12+00:00</updated>
    <author>
      <name>/u/TelloLeEngineer</name>
      <uri>https://old.reddit.com/user/TelloLeEngineer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"&gt; &lt;img alt="Cheaper Transcriptions, Pricier Errors!" src="https://preview.redd.it/zznx9kqgdqaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b6c6aa04a999c4484b5ede2e12f9048adef610c" title="Cheaper Transcriptions, Pricier Errors!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was a post going around recently, &lt;a href="https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/"&gt;OpenAI Charges by the Minute, So Make the Minutes Shorter&lt;/a&gt;, proposing to speed up audio to lower inference / api costs for speech recognition / transcription / stt. I for one was intrigued by the results but given that they were based primarily on anecdotal evidence I felt compelled to perform a proper evaluation. &lt;a href="https://github.com/LeonEricsson/stt-speedup-bench"&gt;This repo&lt;/a&gt; contains the full experiments, and below is the TLDR, accompanying the figure.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Performance degradation is exponential, at 2× playback most models are already 3–5× worse; push to 2.5× and accuracy falls off a cliff, with 20× degradation not uncommon. There are still sweet spots, though: Whisper-large-turbo only drifts from 5.39 % to 6.92 % WER (≈ 28 % relative hit) at 1.5×, and GPT-4o tolerates 1.2 × with a trivial ~3 % penalty.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TelloLeEngineer"&gt; /u/TelloLeEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zznx9kqgdqaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T21:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr18jg</id>
    <title>Serene Pub v0.3.0 Alpha Released — Offline AI Roleplay Client w/ Lorebooks+</title>
    <updated>2025-07-03T21:20:25+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"&gt; &lt;img alt="Serene Pub v0.3.0 Alpha Released — Offline AI Roleplay Client w/ Lorebooks+" src="https://b.thumbs.redditmedia.com/dvo_5ERmmo1diIOh_NZezoiMzqhJNXXLibR4Rm1MveY.jpg" title="Serene Pub v0.3.0 Alpha Released — Offline AI Roleplay Client w/ Lorebooks+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;🌟 Serene Pub v0.3.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Serene Pub&lt;/strong&gt; is an open source, locally hosted AI client built specifically for immersive roleplay and storytelling. It focuses on presenting a clean interface and easy configuration for users who would rather not feel like they need a PHD in AI or software development. With built-in real-time sync and offline-first design, Serene Pub helps you stay in character, not in the configuration menu.&lt;/p&gt; &lt;p&gt;After weeks of refinement and feedback, I’m excited to announce the &lt;strong&gt;0.3.0 alpha release&lt;/strong&gt; of &lt;strong&gt;Serene Pub&lt;/strong&gt; — a modern, open source AI client focused on ease of use and role-playing.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;✨ What's New in 0.3.0 Alpha&lt;/h2&gt; &lt;h3&gt;📚 Lorebooks+&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Create and manage &lt;strong&gt;World Lore&lt;/strong&gt;, &lt;strong&gt;Character Lore&lt;/strong&gt;, and &lt;strong&gt;History&lt;/strong&gt; entries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Bindings:&lt;/strong&gt; Hot-swappable character and persona bindings to your lorebook. Bindings are used to dynamically insert names into your lore book entries, or link character lore.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;World Lore:&lt;/strong&gt; Traditional lorebook entries that you are already familiar with. Describe places, items, organizations—anything relevant to your world.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Lore:&lt;/strong&gt; Lore entries that are attached to character bindings. These lore entries extend your character profiles.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;History:&lt;/strong&gt; Chronological lore entries that can represent a year, month or day. Provide summaries of past events or discussions. The latest entry is considered the &amp;quot;current date,&amp;quot; which can be automatically referenced in your context configuration.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;🧰 Other Updates&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;In-app update notifications&lt;/strong&gt; – Serene Pub will now (politely) notify you when a new release is available on GitHub.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Preset connection configurations&lt;/strong&gt; – Built-in presets make it easy to connect to services like OpenRouter, Ollama, and other OpenAI-compatible APIs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;UI polish &amp;amp; bug fixes&lt;/strong&gt; – Ongoing improvements to mobile layout, theming, and token/prompt statistics.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;⚡ Features Recap&lt;/h2&gt; &lt;p&gt;Serene Pub already includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;WebSocket-based real-time sync&lt;/strong&gt; across windows/devices&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;Custom prompt instruction blocks&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;10+ themes&lt;/strong&gt; and dark mode&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;Offline/local-first&lt;/strong&gt; — no account or cloud required&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;🚀 Try It Now&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/doolijb/serene-pub/releases"&gt;Download the latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Extract the archive and execute &lt;code&gt;run.sh&lt;/code&gt; (Linux/MacOS) or &lt;code&gt;run.cmd&lt;/code&gt; (Windows)&lt;/li&gt; &lt;li&gt;Visit &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Add a model, create a character, and start chatting!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Reminder: This project is in Alpha. It is being actively developed, expect bugs and significant changes!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🆙 Upgrading from 0.2.2 to 0.3.x&lt;/h2&gt; &lt;p&gt;Serene Pub now uses a new database backend powered by &lt;strong&gt;PostgreSQL via pglite&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upgrading your data from &lt;strong&gt;0.2.2 to 0.3.x&lt;/strong&gt; is &lt;strong&gt;supported only during the 0.3.x release window&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Future releases (e.g. &lt;strong&gt;0.4.x and beyond&lt;/strong&gt;) &lt;strong&gt;will not support direct migration from 0.2.2&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;⚠️ To preserve your data, please upgrade to 0.3.x before jumping to future versions.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;📹 Video Guide Coming Soon&lt;/h2&gt; &lt;p&gt;I will try to record an in-depth walk-through in the next week!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🧪 Feedback Needed&lt;/h2&gt; &lt;p&gt;This release was only tested on &lt;strong&gt;Linux x64&lt;/strong&gt; and &lt;strong&gt;Windows x64&lt;/strong&gt;. Support for other platforms is experimental and feedback is urgently needed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you run into issues, please &lt;a href="https://github.com/doolijb/serene-pub/issues"&gt;open an issue&lt;/a&gt; or reach out.&lt;/li&gt; &lt;li&gt;Bug patches will be released in the coming days/weeks based on feedback and severity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Your testing and suggestions are extremely appreciated!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;🐞 Known Issues&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;LM Chat support is currently disabled&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;The native LM Chat API has been disabled due to bugs in their SDK.&lt;/li&gt; &lt;li&gt;Their OpenAI-compatible endpoint also has unresolved issues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Use &lt;strong&gt;Ollama&lt;/strong&gt; for the most stable and user-friendly local model experience.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;🔮 Coming Soon (0.4.0 – 0.6.0)&lt;/h2&gt; &lt;p&gt;These features are currently being planned and will hopefully make it into upcoming releases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Seamless chat and lorebook vectorization&lt;/strong&gt; – enable smarter memory and retrieval for characters and world info.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Management Console&lt;/strong&gt; – download, manage, and switch models directly within Serene Pub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Serene Pub Assistant Chat&lt;/strong&gt; – get help from a built-in assistant for documentation, feature walkthroughs, or character design.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tags&lt;/strong&gt; – organize personas, characters, chats, and lorebooks with flexible tagging.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;🗨️ Final Thoughts&lt;/h2&gt; &lt;p&gt;Thank you to everyone who has tested, contributed, or shared ideas! Your support continues to shape Serene Pub. Try it out, file an issue, and let me know what features you’d love to see next. Reach out on Github, Reddit or Discord.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr18jg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T21:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqycp0</id>
    <title>Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation</title>
    <updated>2025-07-03T19:20:57+00:00</updated>
    <author>
      <name>/u/pheonis2</name>
      <uri>https://old.reddit.com/user/pheonis2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt; &lt;img alt="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" src="https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb529167e09a3692724b342df0121216749b7bd" title="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd"&gt;https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.&lt;/p&gt; &lt;p&gt;It’s super fast, starting to generate audio in just ~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.&lt;/p&gt; &lt;p&gt;You can also clone voices with just 10 seconds of audio.&lt;/p&gt; &lt;p&gt;And yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/&lt;/a&gt;&lt;br /&gt; Huggingface: &lt;a href="https://huggingface.co/kyutai/tts-1.6b-en_fr"&gt;https://huggingface.co/kyutai/tts-1.6b-en_fr&lt;/a&gt;&lt;br /&gt; &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pheonis2"&gt; /u/pheonis2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T19:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lreu44</id>
    <title>30-60tok/s on 4bit local LLM, iPhone 16.</title>
    <updated>2025-07-04T09:50:53+00:00</updated>
    <author>
      <name>/u/Specific_Opinion_573</name>
      <uri>https://old.reddit.com/user/Specific_Opinion_573</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"&gt; &lt;img alt="30-60tok/s on 4bit local LLM, iPhone 16." src="https://preview.redd.it/1pi871kgxtaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b45a353c7618f09017647270bb57fb0e37b72933" title="30-60tok/s on 4bit local LLM, iPhone 16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I’m an AI/LLM enthusiast coming from a mobile dev background (iOS, Swift). I’ve been building a local inference engine, tailored for Metal-first, real-time inference on iOS (iPhone + iPad).&lt;/p&gt; &lt;p&gt;I’ve been benchmarking on iPhone 16 and hitting what seem to be high token/s rates for 4-bit quantized models.&lt;/p&gt; &lt;p&gt;Current Benchmarks (iPhone 16 Plus, all 4-bit):&lt;/p&gt; &lt;p&gt;Model Size - Token/s Range 0.5B–1.7B - 30–64 tok/s 2B - 20–48 tok/s 3B - 15–30 tok/s 4B - 7–16 tok/s 7B - often crashes due to RAM, 5–12 tok/s max&lt;/p&gt; &lt;p&gt;I haven’t seen any PrivateLLM, MLC-LLM, or llama.cpp shipping these numbers with live UI streaming, so I’d love validation: 1. iPhone 16 / 15 Pro users willing to test, can you reproduce these numbers on A17/A18? 2. If you’ve profiled PrivateLLM or MLC at 2-3 B, please drop raw tok/s + device specs.&lt;/p&gt; &lt;p&gt;Happy to share build structure and testing info if helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specific_Opinion_573"&gt; /u/Specific_Opinion_573 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pi871kgxtaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqvovt</id>
    <title>A project to bring CUDA to non-Nvidia GPUs is making major progress</title>
    <updated>2025-07-03T17:35:16+00:00</updated>
    <author>
      <name>/u/OwnWitness2836</name>
      <uri>https://old.reddit.com/user/OwnWitness2836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt; &lt;img alt="A project to bring CUDA to non-Nvidia GPUs is making major progress" src="https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f" title="A project to bring CUDA to non-Nvidia GPUs is making major progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnWitness2836"&gt; /u/OwnWitness2836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T17:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrbwmz</id>
    <title>Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search</title>
    <updated>2025-07-04T06:36:12+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt; &lt;img alt="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" src="https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a18a6b5aef4f1d48310d2918ee6ff6f6c5943c2" title="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/CAE"&gt;https://github.com/MVPandey/CAE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I'm a terrible frontend dev)&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/dqws3fzgysaf1.gif"&gt;https://i.redd.it/dqws3fzgysaf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates multiple response candidates at each conversation state&lt;/li&gt; &lt;li&gt;Simulates how conversations might unfold down each branch (using the LLM to predict user responses)&lt;/li&gt; &lt;li&gt;Scores each trajectory on metrics like empathy, goal achievement, coherence&lt;/li&gt; &lt;li&gt;Uses MCTS with UCB1 to efficiently explore the most promising paths&lt;/li&gt; &lt;li&gt;Selects the response that leads to the best expected outcome&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FastAPI backend with async SQLAlchemy (PostgreSQL)&lt;/li&gt; &lt;li&gt;Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible endpoint&lt;/li&gt; &lt;li&gt;Dual-purpose: works as both a standard chat API and on-demand analysis engine&lt;/li&gt; &lt;li&gt;No agentic framework dependencies &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)&lt;/li&gt; &lt;li&gt;Branch pruning is naive - just threshold-based instead of something smarter like progressive widening&lt;/li&gt; &lt;li&gt;Memory usage grows with tree size - haven't implemented node recycling yet&lt;/li&gt; &lt;li&gt;The pgvector embedding code is there but commented out (wanted semantic search over conversation history)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Originally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.&lt;/p&gt; &lt;p&gt;Example output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.&lt;/p&gt; &lt;p&gt;Curious if anyone's tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.&lt;/p&gt; &lt;p&gt;Anyway, please let me know any thoughts, criticisms, feedback, etc! :) &lt;/p&gt; &lt;p&gt;I also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T06:36:12+00:00</published>
  </entry>
</feed>
