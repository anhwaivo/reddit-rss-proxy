<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-11T07:05:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hy7m1y</id>
    <title>Which Local LLMs know best when to speak and when to STFU in group chat agent-to-agent conversations?</title>
    <updated>2025-01-10T15:58:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So we’re experimenting with Autogen AG2 group chat right now and it’s kind of going horribly with regards to agent-to-agent conversations, like really bad. So far Llams3.3 seems to be the absolute worst at group chats, sometimes it just flat out refuses to provide any response like it’s the shy kid in school and gets timed out. Nemotron seems better but still not great. &lt;/p&gt; &lt;p&gt;My question is, which models have you found are good at multi-LLM multi-turn conversations in group chat scenarios?&lt;/p&gt; &lt;p&gt;We have tried Llama3.3, Nemotron, and Qwen2.5-72b mainly so far and are about to try a range of small to midsize models (Phi-4, Dolphin3, Exaone, and Falcon). &lt;/p&gt; &lt;p&gt;Our roles are Project Manager, Engineer, Researcher, Writer, and Critic. Also, we are exploring both group chat and swarm methods of communication between agents. &lt;/p&gt; &lt;p&gt;I would love to hear from anyone who has already “plowed this ground” before. Any insights you have are appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy7m1y/which_local_llms_know_best_when_to_speak_and_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxwtpy</id>
    <title>Energy efficiency of 5090 is slightly worse than 4090</title>
    <updated>2025-01-10T04:56:48+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Despite the big jump in energy efficiency in the previous two generations. Nividia dropped the ball this time. It is only saved by the higher VRAM size and significantly higher memory bandwidth.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX TITAN&lt;/th&gt; &lt;th align="left"&gt;3090&lt;/th&gt; &lt;th align="left"&gt;4090&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;65.25&lt;/td&gt; &lt;td align="left"&gt;142.32&lt;/td&gt; &lt;td align="left"&gt;330.4&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;280W&lt;/td&gt; &lt;td align="left"&gt;350W&lt;/td&gt; &lt;td align="left"&gt;450W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;233.03&lt;/td&gt; &lt;td align="left"&gt;406.63&lt;/td&gt; &lt;td align="left"&gt;734.22&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some might attribute the energy efficiency gain can be constrained by smaller transistor size. But if you look at the 96W MacBook Pro 14in using the Max chips, their energy efficiency gain is steady. The only conclusion is that Nvidia did a poorer job at chip design going from 4090 to 5090.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Chip&lt;/th&gt; &lt;th align="left"&gt;M1 Max&lt;/th&gt; &lt;th align="left"&gt;M3 Max&lt;/th&gt; &lt;th align="left"&gt;M4 Max&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;21.2992&lt;/td&gt; &lt;td align="left"&gt;28.672&lt;/td&gt; &lt;td align="left"&gt;34.4064&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;221.87&lt;/td&gt; &lt;td align="left"&gt;298.67&lt;/td&gt; &lt;td align="left"&gt;358.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxwtpy/energy_efficiency_of_5090_is_slightly_worse_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T04:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyg0f5</id>
    <title>What is the best Text-to-speech model to run locally?</title>
    <updated>2025-01-10T21:51:20+00:00</updated>
    <author>
      <name>/u/fewsats</name>
      <uri>https://old.reddit.com/user/fewsats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think llama 4 will have multimodality (including audio input/output) but until then, what do people use for going from text to speech + how do they run it locally (Ollama does not support this kind of models does it?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fewsats"&gt; /u/fewsats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyg0f5/what_is_the_best_texttospeech_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyqcir</id>
    <title>stable-diffusion.cpp context size</title>
    <updated>2025-01-11T06:56:43+00:00</updated>
    <author>
      <name>/u/goingsplit</name>
      <uri>https://old.reddit.com/user/goingsplit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anybody using this tool? I noticed the context size being clipped to some given size when the inference starts. I wonder if anybody figured how to control that parameter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goingsplit"&gt; /u/goingsplit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqcir/stablediffusioncpp_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T06:56:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyqd53</id>
    <title>Any recommendations for prompt enhancement ggufs?</title>
    <updated>2025-01-11T06:58:01+00:00</updated>
    <author>
      <name>/u/YT_Brian</name>
      <uri>https://old.reddit.com/user/YT_Brian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Uncensored ones as huggingface only has censored ones. Kind of amazed there is not a single one uncensored. They keep coming back &amp;quot;I can't do that Dave, it is violence Dave. It is unsafe Dave.&amp;quot;&lt;/p&gt; &lt;p&gt;Honestly if they did that I wouldn't be mildly annoyed at having to find one specifically for this.&lt;/p&gt; &lt;p&gt;So, does anyone know one not on huggingface? Enjoy my privacy so would have to be able to run local.&lt;/p&gt; &lt;p&gt;Any help is appreciated. &lt;/p&gt; &lt;p&gt;I have just been using the LLMs themselves. &amp;quot;Enhance the following prompt to better work in this LLM.&amp;quot; But I haven't seen one that really does it right yet.&lt;/p&gt; &lt;p&gt;Would a better, well, prompting for the enhancement work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YT_Brian"&gt; /u/YT_Brian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqd53/any_recommendations_for_prompt_enhancement_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqd53/any_recommendations_for_prompt_enhancement_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqd53/any_recommendations_for_prompt_enhancement_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T06:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyqfyj</id>
    <title>inference help</title>
    <updated>2025-01-11T07:03:07+00:00</updated>
    <author>
      <name>/u/Alternative_Bee_2142</name>
      <uri>https://old.reddit.com/user/Alternative_Bee_2142</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a total beginner to ML, and am currently experimenting with a project where I try to see how prompt responses vary across LLaMA base, instruct and downstream finetuned models. The trouble I'm having is with the chat template to use for prompting. Currently, I've defined a custom function that uses the LLaMA 3 defined template since apply_chat_template() method is not defined for many of the finetuned model's tokenizers (from huggingface) but they've said they follow the LLaMA 3 prompting template. However, the inference responses often produce answers like &amp;quot;assistantassistant&amp;quot; or other problematic responses, which makes me question if I'm doing something wrong. in the same vein, I'm also unsure about how to prompt the base model. While I understand the model is not instruction tuned and actually just continues generating text based on the input, when I use OLLAMA and load the base model, it gives coherent responses. What template does it use under the hood? Any help would be super appreciated, I've been stuck on this for a while now. For reference, here's my function:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def apply_chat_template(messages, add_generation_prompt=True): formatted_message = &amp;quot;&amp;lt;|begin_of_text|&amp;gt;&amp;quot; for message in messages: role = message[&amp;quot;role&amp;quot;] content = message[&amp;quot;content&amp;quot;] if role == &amp;quot;system&amp;quot;: formatted_message += f&amp;quot;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;{content}&amp;lt;|eot_id|&amp;gt;&amp;quot; elif role == &amp;quot;user&amp;quot;: formatted_message += f&amp;quot;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;{content}&amp;lt;|eot_id|&amp;gt;&amp;quot; elif role == &amp;quot;assistant&amp;quot;: formatted_message += f&amp;quot;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;{content}&amp;lt;|eot_id|&amp;gt;&amp;quot; if add_generation_prompt: formatted_message += &amp;quot;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&amp;quot; return formatted_message &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alternative_Bee_2142"&gt; /u/Alternative_Bee_2142 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqfyj/inference_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqfyj/inference_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyqfyj/inference_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T07:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy60n8</id>
    <title>why is there no LMStudio/Msty/GPT4All type app that supports backends other than llama.cpp?</title>
    <updated>2025-01-10T14:48:48+00:00</updated>
    <author>
      <name>/u/gaspoweredcat</name>
      <uri>https://old.reddit.com/user/gaspoweredcat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im curious, ive heard that other backends, especially exllamav2 can be faster than llama.cpp in many cases especially when multiple cards or even multiple machines are running on it, model files are readily available so there is demand for it&lt;/p&gt; &lt;p&gt;yet any of the apps i find even ones that support a sort of pluggable backend generally offer llama.cpp cpu, llama.cpp metal, llama.cpp cuda, llama.cpp vulkan and thats it, exllama seems to only be supported by the often somewhat janky and not that great to use webUIs like oogabooga or LoLLMs &lt;/p&gt; &lt;p&gt;so my question is why not? are exllama and other backends really that difficult to implement that no one wants to even touch it, llama.cpp has LM studio, Msty, GPT4All, Jan, Jellybox, and several other options, some even support stablediffusion models but for text gen it seems no one wants to integrate it and i just wondered if theres a good reason most apps etc generally use llama.cpp over anything else &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaspoweredcat"&gt; /u/gaspoweredcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy60n8/why_is_there_no_lmstudiomstygpt4all_type_app_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyi24a</id>
    <title>Help With Chunking a PDF Textbook for GraphRAG Applications</title>
    <updated>2025-01-10T23:22:31+00:00</updated>
    <author>
      <name>/u/GapElectrical8507</name>
      <uri>https://old.reddit.com/user/GapElectrical8507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm building a project where we're trying to create a physics course based off a knowledge graph database we're building. I've found a couple open source physics textbooks which I'm trying to essentially extract concepts, definitions, and equations out of to put in a knowledge graph that maps relationships between concepts, equations, and definitions. &lt;/p&gt; &lt;p&gt;However, I'm not sure how to get started on chunking the PDF to extract relevant information because it's a massive PDF, and because there's images, tables, and examples that are used throughout the textbook content when I'm just trying to extract concepts, definitions, and derivations of equations out of the PDF. Any help would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GapElectrical8507"&gt; /u/GapElectrical8507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyi24a/help_with_chunking_a_pdf_textbook_for_graphrag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyhyoo</id>
    <title>Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!</title>
    <updated>2025-01-10T23:17:59+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"&gt; &lt;img alt="Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!" src="https://external-preview.redd.it/Ol0yzFMM8hLJHUMhOdEYg0wDLH6o_8qd6oG6sXR9vKE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daf80009e9ca850a0a5df31935aecd6e4c27a673" title="Notate 1.0.3 - Lots of platform fixes + I've included installers on Github. If you have requests or feedback or issues lemme know!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/CNTRLAI/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyhyoo/notate_103_lots_of_platform_fixes_ive_included/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ii4</id>
    <title>freeact: A Lightweight Library for Code-Action Based Agents</title>
    <updated>2025-01-10T16:37:39+00:00</updated>
    <author>
      <name>/u/krasserm</name>
      <uri>https://old.reddit.com/user/krasserm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt; &lt;img alt="freeact: A Lightweight Library for Code-Action Based Agents" src="https://external-preview.redd.it/LzADpwKw8mUc5MZG38ssaaBnDyw6elEdCSjne9inj0g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b672149c7219c55d19b0e99adc3b4bbb5b158c" title="freeact: A Lightweight Library for Code-Action Based Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! We just released &lt;a href="https://github.com/gradion-ai/freeact"&gt;freeact&lt;/a&gt;, a lightweight agent library that empowers language models to act as autonomous agents through executable &lt;strong&gt;code actions&lt;/strong&gt;. By enabling agents to express their actions directly in code rather than through constrained formats like JSON, freeact provides a flexible and powerful approach to solving complex, open-ended problems that require dynamic solution paths.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports dynamic installation and utilization of Python packages at runtime&lt;/li&gt; &lt;li&gt;Agents learn from feedback and store successful code actions as reusable skills in long-term memory&lt;/li&gt; &lt;li&gt;Skills can be interactively developed and refined in collaboration with freeact agents&lt;/li&gt; &lt;li&gt;Agents compose skills and any other Python modules to build increasingly sophisticated capabilities&lt;/li&gt; &lt;li&gt;Code actions are executed in &lt;a href="https://github.com/gradion-ai/ipybox"&gt;ipybox&lt;/a&gt;, a secure Docker + IPython sandbox that runs locally or remotely&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/gradion-ai/freeact"&gt;https://github.com/gradion-ai/freeact&lt;/a&gt;&lt;br /&gt; Evaluation: &lt;a href="https://gradion-ai.github.io/freeact/evaluation/"&gt;https://gradion-ai.github.io/freeact/evaluation/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback!&lt;/p&gt; &lt;p&gt;See it in action:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player"&gt;https://reddit.com/link/1hy8ii4/video/rs73092327ce1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krasserm"&gt; /u/krasserm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ii4/freeact_a_lightweight_library_for_codeaction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxm0ep</id>
    <title>Anyone want the script to run Moondream 2b's new gaze detection on any video?</title>
    <updated>2025-01-09T20:12:41+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt; &lt;img alt="Anyone want the script to run Moondream 2b's new gaze detection on any video?" src="https://external-preview.redd.it/cmk5cnZsYXZ6MGNlMeEGpTWo5MaI3KFBwDeey6o_wri3pXWzYnC4YTD3TTIr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0330607dc613a7fe62b098a05b7a0c3c3e3495dc" title="Anyone want the script to run Moondream 2b's new gaze detection on any video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n9beslavz0ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxm0ep/anyone_want_the_script_to_run_moondream_2bs_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T20:12:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8ehf</id>
    <title>Does anyone know how to replicate this setup for coding ?</title>
    <updated>2025-01-10T16:32:48+00:00</updated>
    <author>
      <name>/u/Alive-Tax3189</name>
      <uri>https://old.reddit.com/user/Alive-Tax3189</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt; &lt;img alt="Does anyone know how to replicate this setup for coding ?" src="https://external-preview.redd.it/enU0ZnVheGwxN2NlMZs7egYfaDsCtkR_AYCrnVuq-88BdYMxPb_V_Fpy742y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=311e897110b819f7974eee9811f460d255b45faf" title="Does anyone know how to replicate this setup for coding ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Tax3189"&gt; /u/Alive-Tax3189 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wz5qfaxl17ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8ehf/does_anyone_know_how_to_replicate_this_setup_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjhch</id>
    <title>Any good LLM benchmarks that rank ability to document code and explain code?</title>
    <updated>2025-01-11T00:29:02+00:00</updated>
    <author>
      <name>/u/palindsay</name>
      <uri>https://old.reddit.com/user/palindsay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems current coding benchmarks like Aider and bigcode, etc. focus on code refactoring and generation. What about strength in code documentation and explanation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palindsay"&gt; /u/palindsay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjhch/any_good_llm_benchmarks_that_rank_ability_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy4onq</id>
    <title>OCR tools for really very bad handwriting!</title>
    <updated>2025-01-10T13:44:05+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt; &lt;img alt="OCR tools for really very bad handwriting!" src="https://preview.redd.it/ww1i5y5h76ce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c64901dd13fc181007e945126d45f11e6e021c" title="OCR tools for really very bad handwriting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww1i5y5h76ce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy4onq/ocr_tools_for_really_very_bad_handwriting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T13:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyio5w</id>
    <title>Beginner Guide - Creating LLM Datasets with Python</title>
    <updated>2025-01-10T23:50:58+00:00</updated>
    <author>
      <name>/u/0xlisykes</name>
      <uri>https://old.reddit.com/user/0xlisykes</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xlisykes"&gt; /u/0xlisykes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://toolworks.dev/docs/Guides/creating-llm-datasets-python"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy5l18</id>
    <title>Local TTS models that can match ElevenLabs in terms of quality and consistency</title>
    <updated>2025-01-10T14:28:17+00:00</updated>
    <author>
      <name>/u/_megazz</name>
      <uri>https://old.reddit.com/user/_megazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should probably start by stating that I'm somewhat new to running AI models locally, but I've tinkered with Ollama + Open WebUI before and was able to get some models running through WSL2 on my RTX 4080 and was pretty impressed with the results.&lt;/p&gt; &lt;p&gt;With that said, I'm now looking for a good local TTS model and I was honestly disappointed with what I could find. Most projects seem to not be updated in months or are simply dead.&lt;/p&gt; &lt;p&gt;From what I've read, the general consensus seems to be that XTTS-v2 is still the best overall model to this day, which is from a startup that has &lt;a href="https://coqui.ai/"&gt;shut down&lt;/a&gt;. I figured I'd try it anyway and I was able to get it running through &lt;a href="https://github.com/daswer123/xtts-webui"&gt;this simple portable version&lt;/a&gt;, but I was honestly disappointed with the results I got, all very inconsistent and not natural sounding, even after tinkering a lot with its different parameters and voices. Not even close to what I can get from ElevenLabs, which could easily pass as real person speaking, but that service is very pricey for me, unfortunately.&lt;/p&gt; &lt;p&gt;There are other popular suggestions like Fish Speech or F5-TTS, but since I need the model to speak Portuguese, that limits my options a lot.&lt;/p&gt; &lt;p&gt;Right now I feel like I'm just wasting my time and that nothing that I can run locally can match EvenLabs currently, but as I said, I'm new to this and maybe I'm missing something obvious. In any case, I'd appreciate any input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_megazz"&gt; /u/_megazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyomxu</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push</title>
    <updated>2025-01-11T05:04:42+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push?leadSource=reddit_wall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyh9y3</id>
    <title>DeepSeek-V3 imatrix quants by team mradermacher</title>
    <updated>2025-01-10T22:47:05+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt; &lt;img alt="DeepSeek-V3 imatrix quants by team mradermacher" src="https://external-preview.redd.it/m-G04wn3IB1jswcKbDUS8jJlKetCzX6HK1WoeuTcULY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059108de9020787af36b4f6d446ccbfc92d4ba7e" title="DeepSeek-V3 imatrix quants by team mradermacher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/DeepSeek-V3-i1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T22:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy91m1</id>
    <title>0.5B Distilled QwQ, runnable on IPhone</title>
    <updated>2025-01-10T16:59:44+00:00</updated>
    <author>
      <name>/u/Lord_of_Many_Memes</name>
      <uri>https://old.reddit.com/user/Lord_of_Many_Memes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt; &lt;img alt="0.5B Distilled QwQ, runnable on IPhone" src="https://external-preview.redd.it/hOvT7Zh2EDTGcuqajUYbM7IboIMuAwdCFsY0UWAS0pU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85274e9584cd8dc27f3835483f32b47ea48f28f0" title="0.5B Distilled QwQ, runnable on IPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lord_of_Many_Memes"&gt; /u/Lord_of_Many_Memes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/kz919/Mini-QwQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyapzu</id>
    <title>Phi-4 Finetuning - now with &gt;128K context length + Bug Fix Details</title>
    <updated>2025-01-10T18:09:05+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt; &lt;img alt="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Phi-4 with &amp;gt;128K context lengths using &lt;a href="https://github.com/unslothai/unsloth/"&gt;Unsloth&lt;/a&gt;! That's 12x longer than Hugging Face + FA2’s 11K on a 48GB GPU.&lt;/p&gt; &lt;p&gt;Phi-4 Finetuning Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also previously announced bug fixes for Phi-4 and so we’ll reveal the details.&lt;/p&gt; &lt;p&gt;But, before we do, some of you were curious if our fixes actually worked? Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4"&gt;Open LLM Leaderboard&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e"&gt;https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you even tested it to show greatly improved results in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 1: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/"&gt;Multiple-choice tasks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316"&gt;https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/"&gt;ASCII art generation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada"&gt;https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Bug Fix Details&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Tokenizer Fix: Phi-4 incorrectly uses &amp;lt;|endoftext|&amp;gt; as EOS instead of &amp;lt;|im_end|&amp;gt;.&lt;/li&gt; &lt;li&gt;Finetuning Fix: Use a proper padding token (e.g., &amp;lt;|dummy_87|&amp;gt;).&lt;/li&gt; &lt;li&gt;Chat Template Fix: Avoid adding an assistant prompt unless specified to prevent serving issues.&lt;/li&gt; &lt;li&gt;More in-depth in our blog: &lt;a href="https://unsloth.ai/blog/phi4"&gt;https://unsloth.ai/blog/phi4&lt;/a&gt; or &lt;a href="https://twitter.com/danielhanchen/status/1877781452818968615"&gt;tweet&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For all other model uploads, see &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;our docs&lt;/a&gt;&lt;br /&gt; I know this post was a bit long, but I hope it was informative and please ask any questions!! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T18:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyf1pf</id>
    <title>Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro?</title>
    <updated>2025-01-10T21:09:12+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt; &lt;img alt="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hydavt</id>
    <title>New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b</title>
    <updated>2025-01-10T19:56:16+00:00</updated>
    <author>
      <name>/u/iamephemeral</name>
      <uri>https://old.reddit.com/user/iamephemeral</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt; &lt;img alt="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" src="https://external-preview.redd.it/r4CGqgcRPLr1eA9JfvNHSBaN_-4tgT5j575hGH0pgUU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239946d045e3a552b2d863b9157de34884befd7f" title="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamephemeral"&gt; /u/iamephemeral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Goodfire/Llama-3.3-70B-Instruct-SAE-l50"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T19:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy34ir</id>
    <title>WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js</title>
    <updated>2025-01-10T12:16:13+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt; &lt;img alt="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" src="https://external-preview.redd.it/a3B0bmYzbTJyNWNlMYVrWG7q5Ym6r9MYEdNpGfavLsbyjmwCsGU7oHTw1w8w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06dd6f09c82183918afdcca9863994fcffe8274f" title="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vmfpb2m2r5ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjoau</id>
    <title>This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)</title>
    <updated>2025-01-11T00:38:10+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt; &lt;img alt="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" src="https://b.thumbs.redditmedia.com/niNscGOj9hur8A-QVwFzrElx4sAsFt-GLXQ2A5RCLGw.jpg" title="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hyjoau"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
</feed>
