<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-27T16:39:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jkotcs</id>
    <title>Delving deep into Llama.cpp and exploiting Llama.cpp's Heap Maze, from Heap-Overflow to Remote-Code Execution.</title>
    <updated>2025-03-26T22:36:54+00:00</updated>
    <author>
      <name>/u/FitItem2633</name>
      <uri>https://old.reddit.com/user/FitItem2633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://retr0.blog/blog/llama-rpc-rce"&gt;https://retr0.blog/blog/llama-rpc-rce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitItem2633"&gt; /u/FitItem2633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T22:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkbh4f</id>
    <title>Google releases TxGemma, open models for therapeutic applications</title>
    <updated>2025-03-26T13:13:38+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt; &lt;img alt="Google releases TxGemma, open models for therapeutic applications" src="https://external-preview.redd.it/w4gdQx4Lq4f5EYpFmMZH_AWSFA12WrreFd38e_AppFM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4444c30b578119b10e027ae74a8e53550f6800b" title="Google releases TxGemma, open models for therapeutic applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We're excited to share TxGemma! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemma 2-based model for multiple therapeutic tasks &lt;ul&gt; &lt;li&gt;Classification (will molecule cross blood-brain barrier)&lt;/li&gt; &lt;li&gt;Regression (drug's binding affinity)&lt;/li&gt; &lt;li&gt;Generation (given product of some reaction, generate reactant set)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2B, 9B, and 27B, with 27B being SOTA for many tasks, including versus single-task models&lt;/li&gt; &lt;li&gt;Chat version for general reasoning, to answer questions and engage in discussions&lt;/li&gt; &lt;li&gt;Fine-tunable with transformers, with an example notebook&lt;/li&gt; &lt;li&gt;Agentic-Tx for agentic systems, powered with Gemini, and using TxGemma as a tool&lt;/li&gt; &lt;li&gt;Models on HF: &lt;a href="https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"&gt;https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/?linkId=13647386"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkxevv</id>
    <title>What's the background for the current image generating improvements?</title>
    <updated>2025-03-27T06:11:17+00:00</updated>
    <author>
      <name>/u/Jentano</name>
      <uri>https://old.reddit.com/user/Jentano</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI image generation seems to improve a lot across the board. &lt;/p&gt; &lt;p&gt;The new GPT4o image generation is very good, although it has a lot of blocking compliance rules like not wanting to modify real fotos.&lt;/p&gt; &lt;p&gt;But others also seem to be progressing a lot in image accuracy, image-text precision amd prompt following.&lt;/p&gt; &lt;p&gt;Were there any paper breakthroughs or is this mostly better training, perhaps text insertion and more correction loops?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jentano"&gt; /u/Jentano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T06:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkkqs3</id>
    <title>Free Search: Making Search Free 4 All</title>
    <updated>2025-03-26T19:45:09+00:00</updated>
    <author>
      <name>/u/Far-Celebration-470</name>
      <uri>https://old.reddit.com/user/Far-Celebration-470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hi all! &lt;/p&gt; &lt;p&gt;For any AI agent, internet search üîé is an important tool. However, with APIs like Tavily and Exa, it becomes really difficult to keep up with the cost. In some cases, these Internet APIs cost more than the LLM. &lt;/p&gt; &lt;p&gt;To solve, this, I am making a playwright wrapper API on top of publicly available searXNG instances. This will enable agent applications to fetch internet results for free. &lt;/p&gt; &lt;p&gt;Currently, I have set up a basic GitHub repo, and I will continue developing advanced search features, such as image search üñºÔ∏è &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/HanzlaJavaid/Free-Search/tree/main"&gt;https://github.com/HanzlaJavaid/Free-Search/tree/main&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üöÄ Try the deployed version: &lt;a href="https://freesearch.replit.app/docs"&gt;https://freesearch.replit.app/docs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you find this useful, consider starring ‚≠êÔ∏è the GitHub repository to support further development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Celebration-470"&gt; /u/Far-Celebration-470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T19:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl7t6b</id>
    <title>Benchmarked Nemotron-Super-49B vs. LLaMA 70B &amp; others safety alignment</title>
    <updated>2025-03-27T16:19:47+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7t6b/benchmarked_nemotronsuper49b_vs_llama_70b_others/"&gt; &lt;img alt="Benchmarked Nemotron-Super-49B vs. LLaMA 70B &amp;amp; others safety alignment" src="https://external-preview.redd.it/kj_Ee04DEpHPCSeKKwKjPg3O1y9d99_Y7wBBdZAkEqs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe96bc6532b66babfcefeff7976a82063fdfc54a" title="Benchmarked Nemotron-Super-49B vs. LLaMA 70B &amp;amp; others safety alignment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Nemotron is more &amp;quot;safety-aligned&amp;quot; than LLaMA 3.3 70B that it was created from, yet not as much as it appeared at first, and it can also often be tricked. Meanwhile, &amp;quot;modified&amp;quot; models are still far from complying with everything.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;: Nvidia released the &lt;a href="https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1"&gt;SFT dataset&lt;/a&gt; along with &lt;a href="https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1"&gt;Nemotron-Super-49B&lt;/a&gt;, which seems &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jeczzz/comment/mihsocl/"&gt;excessively aligned&lt;/a&gt;, as in: aside from just the reasonable topics it also includes things that shouldn't need a safety-aligned reply that could get in the way of regular use (overview &amp;amp; tons of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jeczzz/comment/minq9an/"&gt;details here&lt;/a&gt;). Yet still, it was straightforward to get it to write stuff involving &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jes9za/comment/mil5mpd/"&gt;language &lt;/a&gt;as well as &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jes9za/comment/mind7l4/"&gt;spicy stuff&lt;/a&gt;. So, is it way too safety-aligned or not? And by how much?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt; Instead of just poking around with individual tests, I chose a test that yielded more fine-grained results on a larger scale, while also enabling an easy comparison with the original model, &amp;quot;modified&amp;quot; models and others. The &lt;a href="https://arxiv.org/pdf/2308.13387"&gt;do-not-answer evaluation&lt;/a&gt; seemed useful for that. I've compared Nemotron-Super - without reasoning (red), LLaMA 3.3 70B (orange) that it's based on, Qwen 2.5 7B (blue) and 3B (lightblue) for their potentially different kind of safety alignment, as well as LLaMA 3.1 8B &amp;quot;modified&amp;quot; (green) as a baseline for what's perceived as free from safety-alignment.&lt;/p&gt; &lt;p&gt;Here is the result. You might need a second window or screen now to sync with the following description.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/omns9enfc9re1.png?width=2228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=969ab384d37f39687c6c040ae87af808d9dec02f"&gt;https://preview.redd.it/omns9enfc9re1.png?width=2228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=969ab384d37f39687c6c040ae87af808d9dec02f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Continuation in the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7t6b/benchmarked_nemotronsuper49b_vs_llama_70b_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7t6b/benchmarked_nemotronsuper49b_vs_llama_70b_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7t6b/benchmarked_nemotronsuper49b_vs_llama_70b_others/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T16:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgv2f</id>
    <title>Qwen releases Qwen/Qwen2.5-Omni-7B</title>
    <updated>2025-03-26T17:07:13+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt; &lt;img alt="Qwen releases Qwen/Qwen2.5-Omni-7B" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen releases Qwen/Qwen2.5-Omni-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke5wg</id>
    <title>M3 Ultra Mac Studio 512GB prompt and write speeds for Deepseek V3 671b gguf q4_K_M, for those curious</title>
    <updated>2025-03-26T15:15:08+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone curious, here's the gguf numbers for Deepseek V3 q4_K_M (the older V3, not the newest one from this week). I loaded it up last night and tested some prompts:&lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf without Flash Attention&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:8102/16384, Amt:902/4000, Init:0.04s, Process:792.65s (9.05T/s), Generate:146.21s (6.17T/s), Total:938.86s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note above: normally I run in debugmode to get the ms per token, but forgot to enable it this time. Comes out to &lt;strong&gt;about 110ms per token for prompt processing&lt;/strong&gt;, and &lt;strong&gt;about 162ms per token for prompt response&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:7847/16384, Amt:647/4000, Init:0.04s, Process:793.14s (110.2ms/T = 9.08T/s), Generate:103.81s (160.5ms/T = 6.23T/s), Total:896.95s (0.72T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In comparison, here is Llama 3.3 70b q8 with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:6293/16384, Amt:222/800, Init:0.07s, Process:41.22s (8.2ms/T = 121.79T/s), Generate:35.71s (160.8ms/T = 6.22T/s), Total:76.92s (2.89T/s &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkwe7u</id>
    <title>How does gpt4o image generator works? and there's gemini flash too, what techinique do they use?</title>
    <updated>2025-03-27T05:01:09+00:00</updated>
    <author>
      <name>/u/aman167k</name>
      <uri>https://old.reddit.com/user/aman167k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to replicate this for domain specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aman167k"&gt; /u/aman167k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T05:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl75ta</id>
    <title>Local LLM using RAG with metadata</title>
    <updated>2025-03-27T15:52:47+00:00</updated>
    <author>
      <name>/u/jkiley</name>
      <uri>https://old.reddit.com/user/jkiley</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know a lot of folks here have done a lot with RAG, and I'm trying to figure out an approach to focus on to get a working example to build on.&lt;/p&gt; &lt;p&gt;I've done tons of searching, but most things are materially not on point in at least a couple ways, making it hard to synthesize something that works.&lt;/p&gt; &lt;p&gt;I've been experimenting with RAG, and I have a dataset that has text, identifiers, and several columns of important metadata (including author and datetime) that it would be interesting to factor into queries. For example, I might want to ask what someone has been writing about lately, synthesizing that person's expressed opinions about a topic, or comparing groups writing about a topic (where the group ids are in the metadata). This is many documents, many authors, and relatively short length per document (1-5 paragraphs).&lt;/p&gt; &lt;p&gt;I've been attempting to use Llama-index, LanceDB, and a small local model (all in docker). I can load the data into LanceDB, including having it use the metadata. When I query with LanceDB itself, I get reasonable results. &lt;/p&gt; &lt;p&gt;Where I'm stuck is getting the RAG part working in the LLM. At the moment, it's just not using the documents because something about opening an existing LanceDB isn't giving it the right object to use to query (and reopening an existing LanceDB rather than populating it in the same notebook is nearly nonexistinent in any documentation I can find). I see features that would let me annotate metadata and have the LLM decide how to query, which could be really great for the kinds of things I may eventually like to do.&lt;/p&gt; &lt;p&gt;Potential approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Keep pushing with the existing tools. That's Llama-index and LanceDB, with note notebook creating a LanceDB database (works) and another linking it up with the model and allowing me to query.&lt;/li&gt; &lt;li&gt;Change up the tools (ChromaDB, Langchain?) but keep the approach the same.&lt;/li&gt; &lt;li&gt;Write out all of the documents to text or markdown, ingest that in easy to use RAG tools (AnywhereLLM, Open WebUI), and see how it works.&lt;/li&gt; &lt;li&gt;Something else that hasn't turned up for me so far.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What do you all think? Any advice and/or pointers toward resources, tools, or on-point examples would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jkiley"&gt; /u/jkiley &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl75ta/local_llm_using_rag_with_metadata/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl75ta/local_llm_using_rag_with_metadata/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl75ta/local_llm_using_rag_with_metadata/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T15:52:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl7j7z</id>
    <title>Exclusive: China's H3C warns of Nvidia AI chip shortage amid surging demand</title>
    <updated>2025-03-27T16:08:12+00:00</updated>
    <author>
      <name>/u/goxedbux</name>
      <uri>https://old.reddit.com/user/goxedbux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7j7z/exclusive_chinas_h3c_warns_of_nvidia_ai_chip/"&gt; &lt;img alt="Exclusive: China's H3C warns of Nvidia AI chip shortage amid surging demand" src="https://external-preview.redd.it/xBjDQdBjDyH49gLIg_Z2RKQox3VrGRKjsg_Y3p-MqTk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf29e7b35bdc93ac74d5ed573b4d1eafc0977ac5" title="Exclusive: China's H3C warns of Nvidia AI chip shortage amid surging demand" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goxedbux"&gt; /u/goxedbux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/technology/artificial-intelligence/chinas-h3c-warns-nvidia-ai-chip-shortage-amid-surging-demand-2025-03-27/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7j7z/exclusive_chinas_h3c_warns_of_nvidia_ai_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7j7z/exclusive_chinas_h3c_warns_of_nvidia_ai_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T16:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkvv1m</id>
    <title>What wrong with Gemma 3?</title>
    <updated>2025-03-27T04:27:41+00:00</updated>
    <author>
      <name>/u/Neffor</name>
      <uri>https://old.reddit.com/user/Neffor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got the impression that Gemma 3 was held captive or detained in a basement, perhaps? The model is excellent and very accurate, but if anything, it constantly belittles itself and apologizes. Unlike the second version, which was truly friendly, the third version is creepy because it behaves like a frightened servant, not an assistant-colleague.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neffor"&gt; /u/Neffor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T04:27:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl4ttd</id>
    <title>AlexBefest's CardProjector-v3 series. 24B is back!</title>
    <updated>2025-03-27T14:12:25+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: AlexBefest/CardProjector-24B-v3, AlexBefest/CardProjector-14B-v3, and AlexBefest/CardProjector-7B-v3&lt;/p&gt; &lt;p&gt;Models URL: &lt;a href="https://huggingface.co/collections/AlexBefest/cardprojector-v3-67e475d584ac4e091586e409"&gt;https://huggingface.co/collections/AlexBefest/cardprojector-v3-67e475d584ac4e091586e409&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's new in v3?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Colossal improvement in the model's ability to develop characters using ordinary natural language (bypassing strictly structured formats).&lt;/li&gt; &lt;li&gt;Colossal improvement in the model's ability to edit characters.&lt;/li&gt; &lt;li&gt;The ability to create a character in the Silly Tavern json format, which is ready for import, has been restored and improved.&lt;/li&gt; &lt;li&gt;Added the ability to convert any character into the Silly Tavern json format (absolutely any character description, regardless of how well it is written or in what format. Whether it‚Äôs just chaotic text or another structured format.)&lt;/li&gt; &lt;li&gt;Added the ability to generate, edit, and convert characters in YAML format (highly recommended; based on my tests, the quality of characters in YAML format significantly surpasses all other character representation formats).&lt;/li&gt; &lt;li&gt;Significant improvement in creative writing.&lt;/li&gt; &lt;li&gt;Significantly enhanced logical depth in character development.&lt;/li&gt; &lt;li&gt;Significantly improved overall stability of all models (models are no longer tied to a single format; they are capable of working in all human-readable formats, and infinite generation loops in certain scenarios have been completely fixed).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Overview:&lt;/h1&gt; &lt;p&gt;CardProjector is a specialized series of language models, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; and &lt;strong&gt;now for creating characters in general&lt;/strong&gt;. These models are designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4ttd/alexbefests_cardprojectorv3_series_24b_is_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4ttd/alexbefests_cardprojectorv3_series_24b_is_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4ttd/alexbefests_cardprojectorv3_series_24b_is_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkd8ik</id>
    <title>Notes on Deepseek v3 0324: Finally, the Sonnet 3.5 at home!</title>
    <updated>2025-03-26T14:35:21+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe we finally have the Claude 3.5 Sonnet at home. &lt;/p&gt; &lt;p&gt;With a release that was very Deepseek-like, the Whale bros released an updated Deepseek v3 with a significant boost in reasoning abilities. &lt;/p&gt; &lt;p&gt;This time, it's a proper MIT license, unlike the original model with a custom license, a 641GB, 685b model. With a knowledge cut-off date of July'24.&lt;br /&gt; But the significant difference is a massive boost in reasoning abilities. It's a base model, but the responses are similar to how a CoT model will think. And I believe RL with GRPO has a lot to do with it.&lt;/p&gt; &lt;p&gt;The OG model matched GPT-4o, and with this upgrade, it's on par with Claude 3.5 Sonnet; though you still may find Claude to be better at some edge cases, the gap is negligible.&lt;/p&gt; &lt;p&gt;To know how good it is compared to Claude Sonnets, I ran a few prompts, &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Deepseek v3 0324 understands &lt;strong&gt;user intention better&lt;/strong&gt; than before; I'd say it's better than Claude 3.7 Sonnet base and thinking. 3.5 is still better at this (perhaps the best)&lt;/li&gt; &lt;li&gt;Again, in raw quality &lt;strong&gt;code generation&lt;/strong&gt;, it is better than 3.7, on par with 3.5, and sometimes better.&lt;/li&gt; &lt;li&gt;Great at &lt;strong&gt;reasoning&lt;/strong&gt;, much better than any and all non-reasoning models available right now.&lt;/li&gt; &lt;li&gt;Better at the &lt;strong&gt;instruction following&lt;/strong&gt; than 3,7 Sonnet but below 3.5 Sonnet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For raw capability in real-world tasks, 3.5 &amp;gt;= v3 &amp;gt; 3.7&lt;/p&gt; &lt;p&gt;For a complete analysis and commentary, check out this blog post: &lt;a href="https://composio.dev/blog/deepseek-v3-0324-the-sonnet-3-5-at-home/"&gt;Deepseek v3 0324: The Sonnet 3.5 at home&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's crazy that there's no similar hype as the OG release for such a massive upgrade. They missed naming it v3.5, or else it would've wiped another bunch of billions from the market. It might be the time Deepseek hires good marketing folks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôd love to hear about your experience with the new DeepSeek-V3 (0324). How do you like it, and how would you compare it to Claude 3.5 Sonnet?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T14:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl33br</id>
    <title>QwQ-32B has the highest KV_cache/model_size ratio?</title>
    <updated>2025-03-27T12:47:17+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used the table 1 of Deepseek V2 paper to calculate KV cache size at 131,072 tokens for the major models that support 128k context. Then I obtained the following table:&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;https://arxiv.org/pdf/2405.04434&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;byte/param&lt;/th&gt; &lt;th align="left"&gt;layer#&lt;/th&gt; &lt;th align="left"&gt;group#&lt;/th&gt; &lt;th align="left"&gt;hidden_sz&lt;/th&gt; &lt;th align="left"&gt;head_dim&lt;/th&gt; &lt;th align="left"&gt;KV cache&lt;/th&gt; &lt;th align="left"&gt;model_sz&lt;/th&gt; &lt;th align="left"&gt;KV%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek-R1&lt;/td&gt; &lt;td align="left"&gt;MLA&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;61&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;7168&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;4.32GB&lt;/td&gt; &lt;td align="left"&gt;671GB&lt;/td&gt; &lt;td align="left"&gt;0.644%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3.1-405B&lt;/td&gt; &lt;td align="left"&gt;GQA&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;126&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;16384&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;126GB&lt;/td&gt; &lt;td align="left"&gt;810GB&lt;/td&gt; &lt;td align="left"&gt;15.56%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma-3-27B&lt;/td&gt; &lt;td align="left"&gt;GQA&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;62&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;5376&lt;/td&gt; &lt;td align="left"&gt;168&lt;/td&gt; &lt;td align="left"&gt;10.17GB&lt;/td&gt; &lt;td align="left"&gt;54GB&lt;/td&gt; &lt;td align="left"&gt;18.83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Large-2411&lt;/td&gt; &lt;td align="left"&gt;GQA&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;88&lt;/td&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;66GB&lt;/td&gt; &lt;td align="left"&gt;246GB&lt;/td&gt; &lt;td align="left"&gt;26.83%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;GQA&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;64&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;5120&lt;/td&gt; &lt;td align="left"&gt;128&lt;/td&gt; &lt;td align="left"&gt;20GB&lt;/td&gt; &lt;td align="left"&gt;65.6GB&lt;/td&gt; &lt;td align="left"&gt;30.49%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;It is not surprising that Deepseek-R1 virtually doesn't use much RAM for KV cache thanks to its innovative MLA. The other major models are all GQA. So it seems QwQ is not doing well in KV_cache/model_sz ratio. Why is that? What can QwQ gain by having a bad ratio?&lt;/p&gt; &lt;p&gt;Did I do the math wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T12:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkyxz7</id>
    <title>Request from HuggingFace to release KBLaM models and datasets</title>
    <updated>2025-03-27T08:06:41+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"&gt; &lt;img alt="Request from HuggingFace to release KBLaM models and datasets" src="https://external-preview.redd.it/JzhIs4JD0Slf_53LQELGJrm4Ih8GgOW9SYQYMoU0mvg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23e45ffaa30e9b8e0f8f288f53876176cdf5d996" title="Request from HuggingFace to release KBLaM models and datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/KBLaM/issues/25"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzpwf</id>
    <title>Models that can actually be used on a 3060</title>
    <updated>2025-03-27T09:08:31+00:00</updated>
    <author>
      <name>/u/negiconfit</name>
      <uri>https://old.reddit.com/user/negiconfit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are some models you folks are using on a 3060 graphics card and what problem does it solve for you.&lt;/p&gt; &lt;p&gt;It has to be something you actually are using and not about whether it is capable of running it cuz there‚Äôs many models that can run but not practicable use because it just hallucinates like crazy &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/negiconfit"&gt; /u/negiconfit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T09:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgvxn</id>
    <title>Qwen 2.5 Omni 7B is out</title>
    <updated>2025-03-26T17:08:12+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt; &lt;img alt="Qwen 2.5 Omni 7B is out" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen 2.5 Omni 7B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172"&gt;https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF link: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Omni-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Tweet seems to have been deleted so attached image&lt;br /&gt; Edit #2: Reposted tweet: &lt;a href="https://x.com/Alibaba_Qwen/status/1904944923159445914"&gt;https://x.com/Alibaba_Qwen/status/1904944923159445914&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkix1t</id>
    <title>China may effectively ban at least some Nvidia GPUs. What will Nvidia do with all those GPUs if they can't sell them in China?</title>
    <updated>2025-03-26T18:30:23+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has made cut down versions of Nvidia GPUs for China that duck under the US export restrictions to China. But it looks like China may effectively ban those Nvidia GPUs in China because they are so power hungry. They violate China's green laws. That's a pretty big market for Nvidia. What will Nvidia do with all those GPUs if they can't sell the in China?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513"&gt;https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T18:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkxhcu</id>
    <title>Gemini 2.5 Pro Dropping Balls</title>
    <updated>2025-03-27T06:16:11+00:00</updated>
    <author>
      <name>/u/Few_Ask683</name>
      <uri>https://old.reddit.com/user/Few_Ask683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"&gt; &lt;img alt="Gemini 2.5 Pro Dropping Balls" src="https://external-preview.redd.it/ZzkyZjhta3FjNnJlMVzwUJIkOD2Hxv0jtWenSPiKkDRwVwE01itA-s_OLvqI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd90e107d4028ec7a2d280854a842f53ce9b0600" title="Gemini 2.5 Pro Dropping Balls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Ask683"&gt; /u/Few_Ask683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7e5dflkqc6re1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T06:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5uu5</id>
    <title>New unit in the Hugging Face LLM course. We dive deep into RL with an advanced and hands-on guide to interpreting GRPO.</title>
    <updated>2025-03-27T14:57:15+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NEW UNIT in the Hugging Face Reasoning course. We dive deep into the algorithm behind DeepSeek R1 with an advanced and hands-on guide to interpreting GRPO.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This unit is super useful if you‚Äôre tuning models with reinforcement learning. It will help with:&lt;/p&gt; &lt;p&gt;- interpreting loss and reward progression during training runs&lt;/p&gt; &lt;p&gt;- selecting effective parameters for training&lt;/p&gt; &lt;p&gt;- reviewing and defining effective reward functions&lt;/p&gt; &lt;p&gt;This unit also works up smoothly toward the existing practical exercises form Maxime Labonne and Unsloth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:57:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl4amv</id>
    <title>A closer look at the NVIDIA DGX Station GB300</title>
    <updated>2025-03-27T13:47:13+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt; &lt;img alt="A closer look at the NVIDIA DGX Station GB300" src="https://external-preview.redd.it/3kMZ_XjxFOw3ZXQvJxmGumZXY5uPpAA35n9ELNs2oWg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d81ba8815cb5a0ec0d51068cff351b711fc0590a" title="A closer look at the NVIDIA DGX Station GB300" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.servethehome.com/nvidia-dgx-station-gb300-edition-arm-launched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T13:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl2bxr</id>
    <title>Are we due a new qwen model today?</title>
    <updated>2025-03-27T12:05:34+00:00</updated>
    <author>
      <name>/u/Perfect_Technology73</name>
      <uri>https://old.reddit.com/user/Perfect_Technology73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or have we had all the new models already?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Technology73"&gt; /u/Perfect_Technology73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T12:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl1yk4</id>
    <title>DeepSeek V3 0324 on livebench surpasses Claude 3.7</title>
    <updated>2025-03-27T11:44:33+00:00</updated>
    <author>
      <name>/u/MrPiradoHD</name>
      <uri>https://old.reddit.com/user/MrPiradoHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt; &lt;img alt="DeepSeek V3 0324 on livebench surpasses Claude 3.7" src="https://b.thumbs.redditmedia.com/rn27tkiEJGK7lj3Fd5ifzNLt6PhUdToT0IvAY2kN6gM.jpg" title="DeepSeek V3 0324 on livebench surpasses Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the latest LiveBench results and DeepSeek's V3 (0324) is showing some impressive performance! It's currently sitting at 10th place overall, but what's really interesting is that it's the second highest non-thinking model, only behind GPT-4.5 Preview, while outperforming Claude 3.7 Sonnet (base model, not the thinking version).&lt;/p&gt; &lt;p&gt;We will have to wait, but this suggests that R2 might be a stupidly great model if V3 is already outperforming Claude 3.7 (base), this next version could seriously challenge to the big ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296"&gt;https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPiradoHD"&gt; /u/MrPiradoHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T11:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzjve</id>
    <title>Microsoft develop a more efficient way to add knowledge into LLMs</title>
    <updated>2025-03-27T08:55:51+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt; &lt;img alt="Microsoft develop a more efficient way to add knowledge into LLMs" src="https://external-preview.redd.it/aCGhAR6FEKRX-h5rqecZAFckea8B8CJ4kaRGE3aJoC0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ed759c95a14ac48041ed2121cc23df6c9a4808d" title="Microsoft develop a more efficient way to add knowledge into LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5jea</id>
    <title>My LLMs are all free thinking and locally-sourced.</title>
    <updated>2025-03-27T14:43:35+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt; &lt;img alt="My LLMs are all free thinking and locally-sourced." src="https://preview.redd.it/s6mrolmfv8re1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4fc11e662f92489410497cde8c31a6b140e9bde" title="My LLMs are all free thinking and locally-sourced." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6mrolmfv8re1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:43:35+00:00</published>
  </entry>
</feed>
