<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-09T16:24:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikn5fg</id>
    <title>Glyphstral-24b: Symbolic Deductive Reasoning Model</title>
    <updated>2025-02-08T13:22:41+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt; &lt;img alt="Glyphstral-24b: Symbolic Deductive Reasoning Model" src="https://external-preview.redd.it/DghrhJAW-NKneHTJvXZ7IAcBmIpZ_fU36ahUXITL0bM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad5bc295749b594e323b350912e29031d02d474" title="Glyphstral-24b: Symbolic Deductive Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone! &lt;/p&gt; &lt;p&gt;So I've been really obsessed lately with symbolic AI and the potential to improve reasoning and multi-dimensional thinking. I decided to go ahead and see if I could train a model to use a framework I am calling &amp;quot;Glyph Code Logic Flow&amp;quot;. &lt;/p&gt; &lt;p&gt;Essentially, it is a method of structured reasoning using deductive symbolic logic. You can learn more about it here &lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I first tried training Deepeek R1-Qwen-14 and QWQ-32 but their heavily pre-trained reasoning data seemed to conflict with my approach, which makes sense given the different concepts and ways of breaking down the problem.&lt;/p&gt; &lt;p&gt;I opted for Mistral-Small-24b to see the results, and after 7 days of pure training 24hrs a day (all locally using MLX-Dora at 4bit on my Mac M2 128GB). In all, the model trained on about 27mil tokens of my custom GCLF dataset (each example was around 30k tokens, with a total of 4500 examples)&lt;/p&gt; &lt;p&gt;I still need to get the docs and repo together, as I will be releasing it this weekend, but I felt like sharing a quick preview since this unexpectedly worked out awesomely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player"&gt;https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilghsu</id>
    <title>Llama instruction following game</title>
    <updated>2025-02-09T14:58:47+00:00</updated>
    <author>
      <name>/u/el_isma</name>
      <uri>https://old.reddit.com/user/el_isma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back there was a game in which you had to give instructions to a bot to walk through a maze. Each turn your bot would get a view of what's around it and you had to reply where to step. Invalid moves would kill you. There was a leaderboard of how many steps your prompt managed to get. &lt;/p&gt; &lt;p&gt;I can't find the game. Does anybody remember? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el_isma"&gt; /u/el_isma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilghsu/llama_instruction_following_game/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilghsu/llama_instruction_following_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilghsu/llama_instruction_following_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T14:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1il6x8c</id>
    <title>Bringing reasoning to Granite - Granite-3.2-8b-instruct-preview</title>
    <updated>2025-02-09T04:45:09+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-3.2-8b-instruct-preview"&gt;https://huggingface.co/ibm-granite/granite-3.2-8b-instruct-preview&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Granite-3.2-8B-Instruct-Preview is an early release of an 8B long-context model fine-tuned for enhanced reasoning (thinking) capabilities. Built on top of &lt;a href="https://huggingface.co/ibm-granite/granite-3.1-8b-instruct"&gt;Granite-3.1-8B-Instruct&lt;/a&gt;, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The model allows controllability of its thinking capability, ensuring it is applied only when required.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.ibm.com/new/announcements/bringing-reasoning-to-granite"&gt;https://www.ibm.com/new/announcements/bringing-reasoning-to-granite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Here is the &lt;code&gt;thinking system prompt&lt;/code&gt; from &lt;a href="https://huggingface.co/ibm-granite/granite-3.2-8b-instruct-preview/blob/main/tokenizer_config.json"&gt;tokenizer_config.json&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are a helpful AI assistant. Respond to every user query in a comprehensive and detailed way. You can write down your thoughts and reasoning process before responding. In the thought process, engage in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. In the response section, based on various attempts, explorations, and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process. Write your thoughts after 'Here is my thought process:' and write your response after 'Here is my response:' for each user query. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il6x8c/bringing_reasoning_to_granite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il6x8c/bringing_reasoning_to_granite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il6x8c/bringing_reasoning_to_granite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T04:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ildq4k</id>
    <title>Almawave/Velvet-14B</title>
    <updated>2025-02-09T12:31:53+00:00</updated>
    <author>
      <name>/u/frivolousfidget</name>
      <uri>https://old.reddit.com/user/frivolousfidget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone using it? I gave it a try today and got interesting results. &lt;/p&gt; &lt;p&gt;Really interesting for a new model from scratch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frivolousfidget"&gt; /u/frivolousfidget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ildq4k/almawavevelvet14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ildq4k/almawavevelvet14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ildq4k/almawavevelvet14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T12:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikt9an</id>
    <title>I Built lfind: A Natural Language File Finder Using LLMs</title>
    <updated>2025-02-08T17:58:53+00:00</updated>
    <author>
      <name>/u/Mahrkeenerh1</name>
      <uri>https://old.reddit.com/user/Mahrkeenerh1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt; &lt;img alt="I Built lfind: A Natural Language File Finder Using LLMs" src="https://preview.redd.it/rwb26a0yeyhe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=653ea170e4f0049c8cfde86a3d70e9eb14484f48" title="I Built lfind: A Natural Language File Finder Using LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mahrkeenerh1"&gt; /u/Mahrkeenerh1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwb26a0yeyhe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikt9an/i_built_lfind_a_natural_language_file_finder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T17:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikp5ko</id>
    <title>GeForce RTX 5090 fails to topple RTX 4090 in GPU compute benchmark.</title>
    <updated>2025-02-08T15:02:14+00:00</updated>
    <author>
      <name>/u/el0_0le</name>
      <uri>https://old.reddit.com/user/el0_0le</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So uh. Anyone have a good reason to upgrade from 4090 to 5090? &lt;/p&gt; &lt;p&gt;VRAM? Power? Paper specs? Future updates? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el0_0le"&gt; /u/el0_0le &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/GeForce-RTX-5090-fails-to-topple-RTX-4090-in-GPU-compute-benchmark-while-RTX-5080-struggles-against-RTX-4070-Ti.958334.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikp5ko/geforce_rtx_5090_fails_to_topple_rtx_4090_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T15:02:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1il720d</id>
    <title>Granite-Vision-3.1-2b-preview</title>
    <updated>2025-02-09T04:53:07+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview"&gt;https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Model Summary:&lt;/strong&gt; granite-vision-3.1-2b-preview is a compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more. The model was trained on a meticulously curated instruction-following dataset, comprising diverse public datasets and synthetic datasets tailored to support a wide range of document understanding and general image tasks. It was trained by fine-tuning a Granite large language model (&lt;a href="https://huggingface.co/ibm-granite/granite-3.1-2b-instruct"&gt;https://huggingface.co/ibm-granite/granite-3.1-2b-instruct&lt;/a&gt;) with both image and text modalities.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il720d/granitevision312bpreview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il720d/granitevision312bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il720d/granitevision312bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T04:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilg4s5</id>
    <title>What happened to the Truffle 1 inference engine</title>
    <updated>2025-02-09T14:41:24+00:00</updated>
    <author>
      <name>/u/NTXL</name>
      <uri>https://old.reddit.com/user/NTXL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like no one talks about it at all despite how much hype it had when it first got announced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NTXL"&gt; /u/NTXL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg4s5/what_happened_to_the_truffle_1_inference_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg4s5/what_happened_to_the_truffle_1_inference_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg4s5/what_happened_to_the_truffle_1_inference_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T14:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilg5rw</id>
    <title>GitHub - deepseek-ai/awesome-deepseek-integration</title>
    <updated>2025-02-09T14:42:43+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg5rw/github_deepseekaiawesomedeepseekintegration/"&gt; &lt;img alt="GitHub - deepseek-ai/awesome-deepseek-integration" src="https://external-preview.redd.it/DJRRAw9ne6pXGWX8w2SCrcSQdL1OnLuCppwiQbC18zU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b09cc5c5e4d2dc598666d951cc942d1352df3ae" title="GitHub - deepseek-ai/awesome-deepseek-integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/awesome-deepseek-integration"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg5rw/github_deepseekaiawesomedeepseekintegration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilg5rw/github_deepseekaiawesomedeepseekintegration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T14:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilh46m</id>
    <title>Training a non-English reasoning model using GRPO and Unsloth</title>
    <updated>2025-02-09T15:26:28+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with training reasoning models in languages other than English/Chinese using the GRPO trainer and Unsloth.AI.&lt;/p&gt; &lt;p&gt;While most reasoning models (like DeepSeek-R1) &amp;quot;think&amp;quot; on English/Chinese, I wanted to validate if we could get decent results in other languages without massive compute.&lt;/p&gt; &lt;p&gt;Using Llama 3.1 8B as the base model, the GRPO trainer from trl, and Unsloth, I managed to get a working prototype in Bulgarian after ~5 hours of training on an L40S GPU.&lt;/p&gt; &lt;p&gt;The approach should work for any language where the base model has some pre-training coverage.&lt;/p&gt; &lt;p&gt;Link to the model: &lt;a href="https://huggingface.co/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1"&gt;https://huggingface.co/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post about the training, dataset, etc: &lt;a href="https://unfoldai.com/reasoning-in-a-non-english-language/"&gt;https://unfoldai.com/reasoning-in-a-non-english-language/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notebooks and training logs: &lt;a href="https://github.com/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1"&gt;https://github.com/s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this helps others working on multilingual reasoning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilh46m/training_a_nonenglish_reasoning_model_using_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T15:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1il9k56</id>
    <title>I built a Spotify agent with 50 lines of YAML and an open source model.</title>
    <updated>2025-02-09T07:36:06+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il9k56/i_built_a_spotify_agent_with_50_lines_of_yaml_and/"&gt; &lt;img alt="I built a Spotify agent with 50 lines of YAML and an open source model." src="https://external-preview.redd.it/c2NlMmhnMTZoMmllMceFbxA9VETScbYkQkd_Y6Vr5Y0XlSdRTwjLPPGK61FC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e18ae5755cc54606e30fa376952236e956b524d3" title="I built a Spotify agent with 50 lines of YAML and an open source model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The second most requested feature for Arch Gateway was bearer authorization for function calling scenarios to secure business APIs.&lt;/p&gt; &lt;p&gt;So when we added support for bearer authorization it opened up new possibilities- including connecting to third-party APIs so that user queries can be fulfilled via existing SaaS tools. Or consumer apps like Spotify. &lt;/p&gt; &lt;p&gt;For those not familiar with the project - Arch is an intelligent (edge and LLM) proxy designed for agentic apps and prompts - it handles the pesky stuff in handling, processing and routing prompts so that you can focus on the core business objectives is your AI app. You can read more here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gxp8r646h2ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il9k56/i_built_a_spotify_agent_with_50_lines_of_yaml_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il9k56/i_built_a_spotify_agent_with_50_lines_of_yaml_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T07:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ila7j9</id>
    <title>Which open source image generation model is the best? Flux, Stable diffusion, Janus-pro or something else? What do you suggest guys?</title>
    <updated>2025-02-09T08:23:15+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can these models generate 4K resolution images?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ila7j9/which_open_source_image_generation_model_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ila7j9/which_open_source_image_generation_model_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ila7j9/which_open_source_image_generation_model_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T08:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1il188r</id>
    <title>How Mistral, ChatGPT and DeepSeek handle sensitive topics</title>
    <updated>2025-02-08T23:46:11+00:00</updated>
    <author>
      <name>/u/Touch105</name>
      <uri>https://old.reddit.com/user/Touch105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"&gt; &lt;img alt="How Mistral, ChatGPT and DeepSeek handle sensitive topics" src="https://external-preview.redd.it/Mzl3Zm8xc2Q1MGllMa1zjpINaNlAwSPaIvApdB3LTGQP8LNtAbklarHk8uDT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce0df316532ed266983f35b08f4ad35ba2291539" title="How Mistral, ChatGPT and DeepSeek handle sensitive topics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Touch105"&gt; /u/Touch105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qkdra4wd50ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il188r/how_mistral_chatgpt_and_deepseek_handle_sensitive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T23:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyq45</id>
    <title>DeepSeek Gained over 100+ Millions Users in 20 days.</title>
    <updated>2025-02-08T21:52:58+00:00</updated>
    <author>
      <name>/u/blacktiger3654</name>
      <uri>https://old.reddit.com/user/blacktiger3654</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since launching DeepSeek R1 on January 20, DeepSeek has gained over 100 million users, with $0 advertising or marketing cost. By February 1, its daily active users surpassed 30 million, making it the fastest application in history to reach this milestone. &lt;/p&gt; &lt;p&gt;Why? I also spend so much time chat with it, the profound answer, is the key reason for me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blacktiger3654"&gt; /u/blacktiger3654 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyq45/deepseek_gained_over_100_millions_users_in_20_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilc325</id>
    <title>Updated "Misguided Attention" eval to v0.3 - 4x longer dataset</title>
    <updated>2025-02-09T10:41:04+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilc325/updated_misguided_attention_eval_to_v03_4x_longer/"&gt; &lt;img alt="Updated &amp;quot;Misguided Attention&amp;quot; eval to v0.3 - 4x longer dataset" src="https://external-preview.redd.it/Ok0Yo0DomGoudXAZXfSGBpNe4tdjQGIC8Lv4hX8A3Vw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33875bc949c146a512b07616855bb57b544e298f" title="Updated &amp;quot;Misguided Attention&amp;quot; eval to v0.3 - 4x longer dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information. &lt;/p&gt; &lt;p&gt;Thanks to numerous community contributions I was able to to increase the number of prompts to 52. Thanks a lot to all contributors! More contributions are always valuable to fight saturation of the benchmark. &lt;/p&gt; &lt;p&gt;In addition, I improved the automatic evaluation so that fewer manual interventions ware required.&lt;/p&gt; &lt;p&gt;Below, you can see the first results from the long dataset evaluation - more will be added over time. R1 took the lead here and we can also see the impressive improvement that finetuning llama-3.3 with deepseek traces brought. I expect that o1 would beat r1 based on the results from the small eval. Currently no o1 long eval is planned due to excessive API costs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5kfepb2ed3ie1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9e4272a4e2012d89ae2afc672da2e71f9c7f056"&gt;https://preview.redd.it/5kfepb2ed3ie1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9e4272a4e2012d89ae2afc672da2e71f9c7f056&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is summary of older results based on the short benchmark. Reasoning models are clearly in the lead as they can recover from initial misinterpretation of the prompts that the &amp;quot;non-reasoning&amp;quot; models fall prey to. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jyy5oaztd3ie1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=732cd7d6f4b22db9b4133b8f1271b47403fd8212"&gt;https://preview.redd.it/jyy5oaztd3ie1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=732cd7d6f4b22db9b4133b8f1271b47403fd8212&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find further details in the &lt;a href="https://github.com/cpldcpu/MisguidedAttention/tree/main/eval"&gt;eval folder of the repository&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilc325/updated_misguided_attention_eval_to_v03_4x_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilc325/updated_misguided_attention_eval_to_v03_4x_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilc325/updated_misguided_attention_eval_to_v03_4x_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T10:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikyclh</id>
    <title>My little setup grows</title>
    <updated>2025-02-08T21:36:13+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt; &lt;img alt="My little setup grows" src="https://preview.redd.it/ivuoew07izhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805b6dbac5a45f4acb9b1e6962f74ce91e1b6aaa" title="My little setup grows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ivuoew07izhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikyclh/my_little_setup_grows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T21:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1il27em</id>
    <title>DeepSeek-R1 (official website) is busy 90% of the time. It's near unusable. Is there away to use it without worrying about that, even if paid?</title>
    <updated>2025-02-09T00:33:42+00:00</updated>
    <author>
      <name>/u/GrayPsyche</name>
      <uri>https://old.reddit.com/user/GrayPsyche</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find DeepSeek-R1 (reasoning) to be the single best model I have ever used for coding. The problem, however, is that I can barely use it. Their website always tells me &amp;quot;The server is busy. Please try again later.&amp;quot;&lt;/p&gt; &lt;p&gt;I wonder why they don't offer paid tiers or servers to help with the traffic? I don't mind paying as long as it's reasonably priced. The free servers will always be there for those who can't or won't pay. And paid servers for those who are willing to pay will ensure stability and uptime.&lt;/p&gt; &lt;p&gt;In the meantime, are there other AI services/wesbites that host the DeepSeek-R1 model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrayPsyche"&gt; /u/GrayPsyche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il27em/deepseekr1_official_website_is_busy_90_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T00:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvnfx</id>
    <title>I really need to upgrade</title>
    <updated>2025-02-08T19:38:41+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt; &lt;img alt="I really need to upgrade" src="https://preview.redd.it/eto6oiq8xyhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be19e4af4ac7edf93efb804fef99881270152ec" title="I really need to upgrade" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eto6oiq8xyhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvnfx/i_really_need_to_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:38:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilegz0</id>
    <title>Anyone else feel like mistral is perfectly set up for maximizing consumer appeal through design? I’ve always felt that out of all the open source AI companies mistral sticks out. Now with their new app it’s really showing. Yet they seem to be behind the curve in actual capabilities.</title>
    <updated>2025-02-09T13:16:07+00:00</updated>
    <author>
      <name>/u/Euphoric_Ad9500</name>
      <uri>https://old.reddit.com/user/Euphoric_Ad9500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t have anything against Chinese companies or anything but could you imagine if mistral pulled of what deepseek did instead? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Ad9500"&gt; /u/Euphoric_Ad9500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilegz0/anyone_else_feel_like_mistral_is_perfectly_set_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilegz0/anyone_else_feel_like_mistral_is_perfectly_set_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilegz0/anyone_else_feel_like_mistral_is_perfectly_set_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilbrxo</id>
    <title>LynxHub: Now support Open-WebUI with full configurations</title>
    <updated>2025-02-09T10:18:31+00:00</updated>
    <author>
      <name>/u/Kinda-Brazy</name>
      <uri>https://old.reddit.com/user/Kinda-Brazy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilbrxo/lynxhub_now_support_openwebui_with_full/"&gt; &lt;img alt="LynxHub: Now support Open-WebUI with full configurations" src="https://external-preview.redd.it/YW9haTI1ODBhM2llMb_Sc7kZVBj08vaxzFa9wBkww8FgNRJMoFQbxur0aceR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d683ba98795a81ca647f1b9bfe9195fa1237470" title="LynxHub: Now support Open-WebUI with full configurations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kinda-Brazy"&gt; /u/Kinda-Brazy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uadi4480a3ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilbrxo/lynxhub_now_support_openwebui_with_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilbrxo/lynxhub_now_support_openwebui_with_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T10:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1il2vwi</id>
    <title>AI.com Now Redirects to DeepSeek</title>
    <updated>2025-02-09T01:07:28+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like AI.com is now redirecting to DeepSeek instead of ChatGPT. This is a surprising move, considering that AI.com had been pointing to OpenAI’s ChatGPT for quite some time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il2vwi/aicom_now_redirects_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T01:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilhmjh</id>
    <title>Are o1 and r1 like models "pure" llms?</title>
    <updated>2025-02-09T15:49:01+00:00</updated>
    <author>
      <name>/u/Independent_Key1940</name>
      <uri>https://old.reddit.com/user/Independent_Key1940</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"&gt; &lt;img alt="Are o1 and r1 like models &amp;quot;pure&amp;quot; llms?" src="https://preview.redd.it/5bo3l9c6x4ie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02869ca10b8a6483466c006c5295d4c533b8587" title="Are o1 and r1 like models &amp;quot;pure&amp;quot; llms?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ofcourse they are! RL has been used in LLM since gpt 3.5 it's just now we've scaled the RL to play a larger part but that doesn't mean the core architecture of llm is changed.&lt;/p&gt; &lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Key1940"&gt; /u/Independent_Key1940 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5bo3l9c6x4ie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilhmjh/are_o1_and_r1_like_models_pure_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T15:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1il9h73</id>
    <title>R1 (1.73bit) on 96GB of VRAM and 128GB DDR4</title>
    <updated>2025-02-09T07:30:27+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il9h73/r1_173bit_on_96gb_of_vram_and_128gb_ddr4/"&gt; &lt;img alt="R1 (1.73bit) on 96GB of VRAM and 128GB DDR4" src="https://external-preview.redd.it/bGV0cHg3cnRmMmllMb2bZhgZ0r-tW17lYRAI6B-ZTFrIXAM2foTrOVbn9AJn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e94883a4c111dd102dfcd3e55452a8c8c341fdae" title="R1 (1.73bit) on 96GB of VRAM and 128GB DDR4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/04p2c5rtf2ie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1il9h73/r1_173bit_on_96gb_of_vram_and_128gb_ddr4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1il9h73/r1_173bit_on_96gb_of_vram_and_128gb_ddr4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T07:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikvo8a</id>
    <title>Your next home lab might have 48GB Chinese card😅</title>
    <updated>2025-02-08T19:39:39+00:00</updated>
    <author>
      <name>/u/Redinaj</name>
      <uri>https://old.reddit.com/user/Redinaj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/"&gt;https://wccftech.com/chinese-gpu-manufacturers-push-out-support-for-running-deepseek-ai-models-on-local-systems/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Things are accelerating. China might give us all the VRAM we want. 😅😅👍🏼 Hope they don't make it illegal to import. For security sake, of course &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Redinaj"&gt; /u/Redinaj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikvo8a/your_next_home_lab_might_have_48gb_chinese_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T19:39:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ilfhyl</id>
    <title>Is Nvidia Becoming a Bottleneck for AI Advancement?</title>
    <updated>2025-02-09T14:10:15+00:00</updated>
    <author>
      <name>/u/TheArchivist314</name>
      <uri>https://old.reddit.com/user/TheArchivist314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking about this this morning and wondering if Nvidia might be a bottleneck on AI advancement which led to me reading about recent developments and debates around AI and gpu hardware—and with Nvidia being at the center of it all. Given its dominant role in powering both the training and inference of AI models, I’m curious about whether Nvidia’s current position might actually be holding back AI progress in some ways.&lt;/p&gt; &lt;p&gt;Here are a few points that have caught my attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Supply Constraints:&lt;/strong&gt;&lt;br /&gt; Recent reports indicate that there are serious concerns about the supply of Nvidia’s AI chips. For example, EU competition chief Margrethe Vestager recently warned about a “huge bottleneck” in Nvidia’s chip supply, suggesting that shortages might slow down the rollout of AI technologies across industries 0.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Scaling Challenges:&lt;/strong&gt;&lt;br /&gt; There’s also discussion around the “scaling law” in AI. Nvidia’s GPUs have been the workhorse behind the rapid advances in large language models and other AI systems. However, as models get larger and inference demands increase, some argue that relying heavily on Nvidia’s architecture (even with innovations like the Blackwell and Hopper series) might hit physical and economic limits. The Financial Times recently discussed how these scaling challenges might be a limiting factor, implying that more chips (and perhaps different chip architectures) will be needed to sustain AI progress 1.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Emerging Alternatives:&lt;/strong&gt;&lt;br /&gt; On the flip side, a number of new players—like Cerebras, Groq, and even competitors from AMD and Intel—are developing specialized hardware for AI inference. These alternatives could potentially ease the pressure on Nvidia if they prove to be more efficient or cost-effective for certain tasks. This makes me wonder: Is the industry’s heavy reliance on Nvidia’s GPUs really sustainable in the long run, or will these emerging solutions shift the balance?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given all this, I’m trying to figure out: - Are Nvidia’s supply and architectural limitations currently acting as a bottleneck to further AI innovation?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Or is the situation more about a temporary growing pain in a rapidly evolving market, where Nvidia’s advancements (and their ability to innovate continuously) will keep pace with demand?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love to hear your thoughts &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheArchivist314"&gt; /u/TheArchivist314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ilfhyl/is_nvidia_becoming_a_bottleneck_for_ai_advancement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-09T14:10:15+00:00</published>
  </entry>
</feed>
