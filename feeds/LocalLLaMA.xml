<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-03T03:39:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lq5d1o</id>
    <title>ChatTree: A simple way to context engineer</title>
    <updated>2025-07-02T19:45:06+00:00</updated>
    <author>
      <name>/u/aadityaubhat</name>
      <uri>https://old.reddit.com/user/aadityaubhat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/"&gt; &lt;img alt="ChatTree: A simple way to context engineer" src="https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b00cfbafde10dd16bd1b2925da2c9cbf5a1efec" title="ChatTree: A simple way to context engineer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?&lt;/p&gt; &lt;p&gt;The idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, is this kind of context control useful? What would you change or build on top?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadityaubhat"&gt; /u/aadityaubhat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/aadityaubhat/ChatTree"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:45:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpl656</id>
    <title>GLM-4.1V-Thinking</title>
    <updated>2025-07-02T03:03:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt; &lt;img alt="GLM-4.1V-Thinking" src="https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f66974e5478d143d6f55b57fcf633e79edaf66" title="GLM-4.1V-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpl656/glm41vthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T03:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpz355</id>
    <title>Cursor terms and conditions seem to be changing</title>
    <updated>2025-07-02T15:38:51+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"&gt; &lt;img alt="Cursor terms and conditions seem to be changing" src="https://preview.redd.it/74fqbljpdhaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b7436040774882ae1855665ecbca193d10edb55" title="Cursor terms and conditions seem to be changing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember when I first downloaded cursor last year, the privacy was on by default, and now not at all. I never selected this embedding thing, but I guess it is automatically turned on. I work in Germany where I do not even dare to use these already, but I am not sure if I can even trust these at all as I worry that the companies will go nuts if they find out about this. Embeddings can be decoded easily, I am literally working on a project where given arbitrary embeddings I am training models to decode stuff to reduce the data storage for some stuff and other use cases.&lt;/p&gt; &lt;p&gt;I am looking for cursor alternatives, as I am not confident that my code snippets will not be used for training or just kept on servers. In hard privacy, I do lose out on many features but on lose ones my embeddings, code snippets etc. will be stored.&lt;/p&gt; &lt;p&gt;All these models and companies are popping up everywhere and they really need your data it feels like? Google is giving away hundreds of calls everyday from their claude code like thing, and cursor which I loved to use is like this now.&lt;/p&gt; &lt;p&gt;Am I being paranoid and trust their SOC-2 ratings, or their statements etc.? Cursor is trustworthy and I should not bother?&lt;/p&gt; &lt;p&gt;OR I should start building my own tool? IMO this is the ultimate data to collect, your literal questions, doubts etc. so I just wanted to know how do people feel here..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/74fqbljpdhaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T15:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq9lkd</id>
    <title>Is it simply about upgrading?</title>
    <updated>2025-07-02T22:41:03+00:00</updated>
    <author>
      <name>/u/outofbandii</name>
      <uri>https://old.reddit.com/user/outofbandii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a total noob to all this. I was having really good results with Gemini 2.5 Pro, o4-mini, and Claude 4.0 Sonnet in VScode. &lt;/p&gt; &lt;p&gt;I decided to try a few local models on my nVidia 8GB RTX 2060 Super (cpu AMD Ryzen 9 3900 12-core, RAM 64GB)&lt;/p&gt; &lt;p&gt;I tested the following models with Roo/ollama: 1) gemma3n:e2b-it-q4&lt;em&gt;K_M 2&lt;/em&gt; hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF 3) deepseek-r1:8b &lt;/p&gt; &lt;p&gt;I have not had good experiences with these models. Probably my hardware limitations. &lt;/p&gt; &lt;p&gt;I'd love to know more and figure out if I can get workable solutions for a reasonable hardware upgrade, or if I should just stick to remote models.&lt;/p&gt; &lt;p&gt;Is it simply that I need to upgrade to a more powerful GPU like a 3090 to get real results from local LLM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/outofbandii"&gt; /u/outofbandii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T22:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpep3m</id>
    <title>Tenstorrent Blackhole Cards</title>
    <updated>2025-07-01T21:56:17+00:00</updated>
    <author>
      <name>/u/SashaUsesReddit</name>
      <uri>https://old.reddit.com/user/SashaUsesReddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt; &lt;img alt="Tenstorrent Blackhole Cards" src="https://preview.redd.it/ffghybw34caf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7e024c8281faff0ddc04029b2d8b6f4dc59b373" title="Tenstorrent Blackhole Cards" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got in some Blackhole p150b cards! Excited to try these out... Anyone else on here running some of these? Curious to collaborate! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SashaUsesReddit"&gt; /u/SashaUsesReddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffghybw34caf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpep3m/tenstorrent_blackhole_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T21:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpwj5j</id>
    <title>AlgoTune: A new benchmark that tests language models' ability to optimize code runtime</title>
    <updated>2025-07-02T13:56:57+00:00</updated>
    <author>
      <name>/u/oripress</name>
      <uri>https://old.reddit.com/user/oripress</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt; &lt;img alt="AlgoTune: A new benchmark that tests language models' ability to optimize code runtime" src="https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e65bd26dfe93e8bb59300666373e0af786916bdb" title="AlgoTune: A new benchmark that tests language models' ability to optimize code runtime" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released AlgoTune which challenges agents to optimize the runtime of 100+ algorithms including gzip compression, AES encryption, and PCA. We also release an agent, AlgoTuner, that enables LMs to iteratively develop efficient code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b"&gt;https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our results show that sometimes frontier LMs are able to find surface level optimizations, but they don't come up with novel algos. There is still a long way to go: the current best AlgoTune score is 1.76x achieved by o4-mini, we think the best potential score is 100x+. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57"&gt;https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For full results + paper + code: &lt;a href="http://algotune.io"&gt;algotune.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oripress"&gt; /u/oripress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T13:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lppg3g</id>
    <title>What's the most complex thing you've been able to (consistently) do with a 4B LLM?</title>
    <updated>2025-07-02T07:15:42+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't mean one-off responses that sound good, I'm thinking more along the lines of: ways in which you've gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T07:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq5wmh</id>
    <title>Critical Vulnerability in Anthropic's MCP Exposes Developer Machines to Remote Exploits</title>
    <updated>2025-07-02T20:07:15+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article from hacker news: &lt;a href="https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1"&gt;https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cybersecurity researchers have discovered a critical security vulnerability in artificial intelligence (AI) company Anthropic's Model Context Protocol (MCP) Inspector project that could result in remote code execution (RCE) and allow an attacker to gain complete access to the hosts.&lt;/p&gt; &lt;p&gt;The vulnerability, tracked as CVE-2025-49596, carries a CVSS score of 9.4 out of a maximum of 10.0.&lt;/p&gt; &lt;p&gt;&amp;quot;This is one of the first critical RCEs in Anthropic's MCP ecosystem, exposing a new class of browser-based attacks against AI developer tools,&amp;quot; Oligo Security's Avi Lumelsky said in a report published last week.&lt;/p&gt; &lt;p&gt;&amp;quot;With code execution on a developer's machine, attackers can steal data, install backdoors, and move laterally across networks - highlighting serious risks for AI teams, open-source projects, and enterprise adopters relying on MCP.&amp;quot;&lt;/p&gt; &lt;p&gt;MCP, introduced by Anthropic in November 2024, is an open protocol that standardizes the way large language model (LLM) applications integrate and share data with external data sources and tools.&lt;/p&gt; &lt;p&gt;The MCP Inspector is a developer tool for testing and debugging MCP servers, which expose specific capabilities through the protocol and allow an AI system to access and interact with information beyond its training data.&lt;/p&gt; &lt;p&gt;It contains two components, a client that provides an interactive interface for testing and debugging, and a proxy server that bridges the web UI to different MCP servers.&lt;/p&gt; &lt;p&gt;That said, a key security consideration to keep in mind is that the server should not be exposed to any untrusted network as it has permission to spawn local processes and can connect to any specified MCP server.&lt;/p&gt; &lt;p&gt;This aspect, coupled with the fact that the default settings developers use to spin up a local version of the tool come with &amp;quot;significant&amp;quot; security risks, such as missing authentication and encryption, opens up a new attack pathway, per Oligo.&lt;/p&gt; &lt;p&gt;&amp;quot;This misconfiguration creates a significant attack surface, as anyone with access to the local network or public internet can potentially interact with and exploit these servers,&amp;quot; Lumelsky said.&lt;/p&gt; &lt;p&gt;The attack plays out by chaining a known security flaw affecting modern web browsers, dubbed 0.0.0.0 Day, with a cross-site request forgery (CSRF) vulnerability in Inspector (CVE-2025-49596) to run arbitrary code on the host simply upon visiting a malicious website.&lt;/p&gt; &lt;p&gt;&amp;quot;Versions of MCP Inspector below 0.14.1 are vulnerable to remote code execution due to lack of authentication between the Inspector client and proxy, allowing unauthenticated requests to launch MCP commands over stdio,&amp;quot; the developers of MCP Inspector said in an advisory for CVE-2025-49596.&lt;/p&gt; &lt;p&gt;0.0.0.0 Day is a 19-year-old vulnerability in modern web browsers that could enable malicious websites to breach local networks. It takes advantage of the browsers' inability to securely handle the IP address 0.0.0.0, leading to code execution.&lt;/p&gt; &lt;p&gt;&amp;quot;Attackers can exploit this flaw by crafting a malicious website that sends requests to localhost services running on an MCP server, thereby gaining the ability to execute arbitrary commands on a developer's machine,&amp;quot; Lumelsky explained.&lt;/p&gt; &lt;p&gt;&amp;quot;The fact that the default configurations expose MCP servers to these kinds of attacks means that many developers may be inadvertently opening a backdoor to their machine.&amp;quot;&lt;/p&gt; &lt;p&gt;Specifically, the proof-of-concept (PoC) makes use of the Server-Sent Events (SSE) endpoint to dispatch a malicious request from an attacker-controlled website to achieve RCE on the machine running the tool even if it's listening on localhost (127.0.0.1).&lt;/p&gt; &lt;p&gt;This works because the IP address 0.0.0.0 tells the operating system to listen on all IP addresses assigned to the machine, including the local loopback interface (i.e., localhost).&lt;/p&gt; &lt;p&gt;In a hypothetical attack scenario, an attacker could set up a fake web page and trick a developer into visiting it, at which point, the malicious JavaScript embedded in the page would send a request to 0.0.0.0:6277 (the default port on which the proxy runs), instructing the MCP Inspector proxy server to execute arbitrary commands.&lt;/p&gt; &lt;p&gt;The attack can also leverage DNS rebinding techniques to create a forged DNS record that points to 0.0.0.0:6277 or 127.0.0.1:6277 in order to bypass security controls and gain RCE privileges.&lt;/p&gt; &lt;p&gt;Following responsible disclosure in April 2025, the vulnerability was addressed by the project maintainers on June 13 with the release of version 0.14.1. The fixes add a session token to the proxy server and incorporate origin validation to completely plug the attack vector.&lt;/p&gt; &lt;p&gt;&amp;quot;Localhost services may appear safe but are often exposed to the public internet due to network routing capabilities in browsers and MCP clients,&amp;quot; Oligo said.&lt;/p&gt; &lt;p&gt;&amp;quot;The mitigation adds Authorization which was missing in the default prior to the fix, as well as verifying the Host and Origin headers in HTTP, making sure the client is really visiting from a known, trusted domain. Now, by default, the server blocks DNS rebinding and CSRF attacks.&amp;quot;&lt;/p&gt; &lt;p&gt;The discovery of CVE-2025-49596 comes days after Trend Micro detailed an unpatched SQL injection bug in Anthropic's SQLite MCP server that could be exploited to seed malicious prompts, exfiltrate data, and take control of agent workflows.&lt;/p&gt; &lt;p&gt;&amp;quot;AI agents often trust internal data whether from databases, log entry, or cached records, agents often treat it as safe,&amp;quot; researcher Sean Park said. &amp;quot;An attacker can exploit this trust by embedding a prompt at that point and can later have the agent call powerful tools (email, database, cloud APIs) to steal data or move laterally, all while sidestepping earlier security checks.&amp;quot;&lt;/p&gt; &lt;p&gt;Although the open-source project has been billed as a reference implementation and not intended for production use, it has been forked over 5,000 times. The GitHub repository was archived on May 29, 2025, meaning no patches have been planned to address the shortcoming.&lt;/p&gt; &lt;p&gt;&amp;quot;The takeaway is clear. If we allow yesterday's web-app mistakes to slip into today's agent infrastructure, we gift attackers an effortless path from SQL injection to full agent compromise,&amp;quot; Park said.&lt;/p&gt; &lt;p&gt;The findings also follow a report from Backslash Security that found hundreds of MCP servers to be susceptible to two major misconfigurations: Allowing arbitrary command execution on the host machine due to unchecked input handling and excessive permissions, and making them accessible to any party on the same local network owing to them being explicitly bound to 0.0.0.0, a vulnerability dubbed NeighborJack.&lt;/p&gt; &lt;p&gt;&amp;quot;Imagine you're coding in a shared coworking space or café. Your MCP server is silently running on your machine,&amp;quot; Backslash Security said. &amp;quot;The person sitting near you, sipping their latte, can now access your MCP server, impersonate tools, and potentially run operations on your behalf. It's like leaving your laptop open – and unlocked for everyone in the room.&amp;quot;&lt;/p&gt; &lt;p&gt;Because MCPs, by design, are built to access external data sources, they can serve as covert pathways for prompt injection and context poisoning, thereby influencing the outcome of an LLM when parsing data from an attacker-controlled site that contains hidden instructions.&lt;/p&gt; &lt;p&gt;&amp;quot;One way to secure an MCP server might be to carefully process any text scraped from a website or database to avoid context poisoning,&amp;quot; researcher Micah Gold said. &amp;quot;However, this approach bloats tools – by requiring each individual tool to reimplement the same security feature – and leaves the user dependent on the security protocol of the individual MCP tool.&amp;quot;&lt;/p&gt; &lt;p&gt;A better approach, Backslash Security noted, is to configure AI rules with MCP clients to protect against vulnerable servers. These rules refer to pre-defined prompts or instructions that are assigned to an AI agent to guide its behavior and ensure it does not break security protocols.&lt;/p&gt; &lt;p&gt;&amp;quot;By conditioning AI agents to be skeptical and aware of the threat posed by context poisoning via AI rules, MCP clients can be secured against MCP servers,&amp;quot; Gold said.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T20:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lphhj3</id>
    <title>DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model</title>
    <updated>2025-07-01T23:59:59+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt; &lt;img alt="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" src="https://preview.redd.it/xxfqfefhpcaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525931b3ac9b9155ccc34e486fc5f097170ed00c" title="DeepSeek-r1-0528 in top 5 on new SciArena benchmark, the ONLY open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post: &lt;a href="https://allenai.org/blog/sciarena"&gt;https://allenai.org/blog/sciarena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen AI puts out good work and contributes heavily to open-source, I am a big fan of Nathan Lambert. &lt;/p&gt; &lt;p&gt;They just released this scientific literature research benchmark and DeepSeek-r1-0528 is the &lt;strong&gt;only&lt;/strong&gt; open-source model in the top 5, sharing the pie with the like of OpenAI's o3, Claude 4 Open, and Gemini 2.5 Pro.&lt;/p&gt; &lt;p&gt;I like to trash DeepSeek here, but not anymore. This level of performance is just insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxfqfefhpcaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lphhj3/deepseekr10528_in_top_5_on_new_sciarena_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T23:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoju6</id>
    <title>World's first Intermediate thinking AI model is now Open Source</title>
    <updated>2025-07-02T06:17:24+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Link: &lt;a href="https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Launch video: &lt;a href="https://www.youtube.com/watch?v=QMnmcXngoks"&gt;https://www.youtube.com/watch?v=QMnmcXngoks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat page: helpingai.co/chat&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoju6/worlds_first_intermediate_thinking_ai_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1417</id>
    <title>[Open Source] Moondream MCP - Vision for AI Agents</title>
    <updated>2025-07-02T16:57:27+00:00</updated>
    <author>
      <name>/u/_colemurray</name>
      <uri>https://old.reddit.com/user/_colemurray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"&gt; &lt;img alt="[Open Source] Moondream MCP - Vision for AI Agents" src="https://preview.redd.it/upyzvjqkrhaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76d29f976867cbc03b905c9152d9b301637ec9c9" title="[Open Source] Moondream MCP - Vision for AI Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely. Open source, self-hosted, no API keys needed. Moondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:&lt;br /&gt; &lt;strong&gt;Caption images&lt;/strong&gt; - &amp;quot;What's in this image?&amp;quot;&lt;br /&gt; &lt;strong&gt;Detect objects&lt;/strong&gt; - Find all instances with bounding boxes&lt;br /&gt; &lt;strong&gt;Visual Q&amp;amp;A&lt;/strong&gt; - &amp;quot;How many people are in this photo?&amp;quot;&lt;br /&gt; &lt;strong&gt;Point to objects&lt;/strong&gt; - &amp;quot;Where's the error message?&amp;quot; &lt;/p&gt; &lt;p&gt;It integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.&lt;br /&gt; &lt;a href="https://github.com/ColeMurray/moondream-mcp/"&gt;https://github.com/ColeMurray/moondream-mcp/&lt;/a&gt;&lt;br /&gt; Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_colemurray"&gt; /u/_colemurray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/upyzvjqkrhaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T16:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq3tuu</id>
    <title>Day 8/50: Building a Small Language Model from Scratch – Rotary Positional Embeddings (RoPE)</title>
    <updated>2025-07-02T18:43:23+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two days, we explored what positional embeddings are and even coded it.&lt;/p&gt; &lt;p&gt;Today, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).&lt;/p&gt; &lt;h1&gt;Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt; &lt;p&gt;Transformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.&lt;/p&gt; &lt;p&gt;To a transformer, these sentences look identical:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The cat sat on the mat.&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;The mat sat on the cat.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That’s a problem. Order matters, especially in language.&lt;/p&gt; &lt;p&gt;To fix this, we add &lt;em&gt;positional embeddings&lt;/em&gt; to inform the model about token positions.&lt;/p&gt; &lt;h1&gt;Traditional Positional Embeddings&lt;/h1&gt; &lt;p&gt;Two popular approaches:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Learned positional embeddings&lt;/strong&gt; – Each position (1, 2, 3...) gets a trainable vector.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal embeddings&lt;/strong&gt; – Use sin/cos functions to generate fixed vectors per position.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But they have limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fixed or learned per-position (no flexibility)&lt;/li&gt; &lt;li&gt;Poor generalization to longer sequences&lt;/li&gt; &lt;li&gt;Don't integrate naturally with attention scores&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What Is RoPE and Why Is It Better?&lt;/h1&gt; &lt;p&gt;RoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.&lt;/p&gt; &lt;p&gt;Instead of adding a position vector, RoPE rotates token embeddings in space based on their position, directly inside the attention mechanism (on query and key vectors).&lt;/p&gt; &lt;p&gt;This encodes relative position information in a more elegant and flexible way.&lt;/p&gt; &lt;p&gt;For each position, the token embedding is rotated by an angle proportional to that position.&lt;/p&gt; &lt;p&gt;A simplified pseudocode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for i in range(0, dim, 2): x1, x2 = x[i], x[i+1] angle = theta * position x[i] = x1 * cos(angle) - x2 * sin(angle) x[i+1] = x1 * sin(angle) + x2 * cos(angle) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This allows attention to naturally reflect &lt;em&gt;how far apart&lt;/em&gt; two tokens are, something traditional embeddings can’t do.&lt;/p&gt; &lt;h1&gt;RoPE vs Traditional Positional Embeddings&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;Traditional Embeddings&lt;/th&gt; &lt;th align="left"&gt;Rotary Positional Embeddings (RoPE)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Position Injected&lt;/td&gt; &lt;td align="left"&gt;Added to input embeddings&lt;/td&gt; &lt;td align="left"&gt;Applied inside attention mechanism&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Absolute or Relative?&lt;/td&gt; &lt;td align="left"&gt;Absolute&lt;/td&gt; &lt;td align="left"&gt;Relative&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generalizes to Long Sequences?&lt;/td&gt; &lt;td align="left"&gt;Poor&lt;/td&gt; &lt;td align="left"&gt;Strong&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Learnable Parameters?&lt;/td&gt; &lt;td align="left"&gt;Sometimes (if learned)&lt;/td&gt; &lt;td align="left"&gt;No&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adopted in SOTA models?&lt;/td&gt; &lt;td align="left"&gt;Less common now&lt;/td&gt; &lt;td align="left"&gt;Yes (LLaMA, DeepSeek)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Why RoPE Is So Useful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Encodes relative positions&lt;/strong&gt; directly in attention scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No extra parameters&lt;/strong&gt; – it's deterministic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handles long sequences&lt;/strong&gt; more gracefully&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple implementation&lt;/strong&gt; using trigonometric rotation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use in Real Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLaMA (Meta):&lt;/strong&gt; Uses RoPE for better generalization and long-context performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Rotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.&lt;/p&gt; &lt;h1&gt;Coming Up Tomorrow&lt;/h1&gt; &lt;p&gt;We'll implement RoPE in code and walk through how it’s used in the open-source&lt;br /&gt; &lt;a href="https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;DeepSeek-Children-Stories-15M model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Follow along, we’re just getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T18:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq4bhu</id>
    <title>best bang for your buck in GPUs for VRAM?</title>
    <updated>2025-07-02T19:02:41+00:00</updated>
    <author>
      <name>/u/starkruzr</name>
      <uri>https://old.reddit.com/user/starkruzr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;have been poring over pcpartpicker, newegg etc. and it seems like the cheapest way to get the most usable VRAM from GPUs is the 16GB 5060Ti? am I missing something obvious? (probably.)&lt;/p&gt; &lt;p&gt;TIA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starkruzr"&gt; /u/starkruzr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:02:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpoqlu</id>
    <title>DiffuCoder 7B - New coding diffusion LLM by Apple</title>
    <updated>2025-07-02T06:29:47+00:00</updated>
    <author>
      <name>/u/DunklerErpel</name>
      <uri>https://old.reddit.com/user/DunklerErpel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt; &lt;img alt="DiffuCoder 7B - New coding diffusion LLM by Apple" src="https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4a0394c2c1d722f620e6214e63d44c79d3e340" title="DiffuCoder 7B - New coding diffusion LLM by Apple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/DiffuCoder-7B-cpGRPO"&gt;https://huggingface.co/apple/DiffuCoder-7B-cpGRPO&lt;/a&gt; (base and instruct also available)&lt;/p&gt; &lt;p&gt;Currently trying - and failing - to run test it on Colab, but really looking forward to it!&lt;/p&gt; &lt;p&gt;Also, anyone got an idea how I can run it on Apple Silicon?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6"&gt;Benchmarks compared to other coding and diffusion models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2506.20639"&gt;https://arxiv.org/pdf/2506.20639&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunklerErpel"&gt; /u/DunklerErpel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T06:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqebbv</id>
    <title>Kwai-Keye/Keye-VL-8B-Preview · Hugging Face</title>
    <updated>2025-07-03T02:29:26+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"&gt; &lt;img alt="Kwai-Keye/Keye-VL-8B-Preview · Hugging Face" src="https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a674ea1c399ba022e42f0633ac66250ac99a0f9e" title="Kwai-Keye/Keye-VL-8B-Preview · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.01949"&gt;https://arxiv.org/abs/2507.01949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://kwai-keye.github.io/"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Kwai-Keye/Keye"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T02:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq9sai</id>
    <title>Machine Learning (ML) Cheat Sheet Material</title>
    <updated>2025-07-02T22:49:05+00:00</updated>
    <author>
      <name>/u/LeveredRecap</name>
      <uri>https://old.reddit.com/user/LeveredRecap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/5aa2375d-a8f6-4430-93f9-a7e4aba55690"&gt;Linear Algebra Cheat Sheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/5be153e6-6dd3-4eef-adbf-554d53afa3ed"&gt;Super VIP Cheatsheet: Artificial Intelligence&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/d8770868-9cbe-4bf8-abe0-2988f39344d9"&gt;VIP Cheatsheet: Transformers and Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/ab4efb6c-6e71-4836-85bc-4841e26312c1"&gt;VIP Cheatsheet: Deep Learning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/a8b3033b-c823-4715-ab2c-24ed9eca98ef"&gt;Super VIP Cheatsheet: Machine Learning (ML)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/79b5f468-d65c-4c03-b9b6-7c117581e677"&gt;Machine Learning Cheat Sheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/65f5ae92-7f08-4869-8d53-cc81ed0fabc2"&gt;ML Cheatsheet Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/ea86a4d6-433a-4eeb-bf40-985b871afcc8"&gt;Machine Learning: UC Berkeley Intro to ML Course Notes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://macro.com/app/pdf/a36b8fd4-f70e-4a41-b18f-9436c2806019"&gt;Machine Learning: A Probabilistic Perspective&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeveredRecap"&gt; /u/LeveredRecap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9sai/machine_learning_ml_cheat_sheet_material/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9sai/machine_learning_ml_cheat_sheet_material/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq9sai/machine_learning_ml_cheat_sheet_material/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T22:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq4cil</id>
    <title>Extended NYT Connections Benchmark updated with Baidu Ernie 4.5 300B A47B, Mistral Small 3.2, MiniMax-M1</title>
    <updated>2025-07-02T19:03:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4cil/extended_nyt_connections_benchmark_updated_with/"&gt; &lt;img alt="Extended NYT Connections Benchmark updated with Baidu Ernie 4.5 300B A47B, Mistral Small 3.2, MiniMax-M1" src="https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=449cce0aca655e2cc3db034127943b5bf3561824" title="Extended NYT Connections Benchmark updated with Baidu Ernie 4.5 300B A47B, Mistral Small 3.2, MiniMax-M1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral Small 3.2 scores 11.5 (Mistral Small 3.1 scored 11.4).&lt;br /&gt; Baidu Ernie 4.5 300B A47B scores 15.2.&lt;br /&gt; MiniMax-M1 (reasoning) scores 21.4 (MiniMax-Text-01 scored 14.6).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4cil/extended_nyt_connections_benchmark_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq4cil/extended_nyt_connections_benchmark_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lpy8nv</id>
    <title>llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)</title>
    <updated>2025-07-02T15:05:54+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"&gt; &lt;img alt="llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)" src="https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f25fb6e0659e004f656c5dd168fe1ce3e1c7b69" title="llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hardware is a mini PC with AMD's Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.&lt;/p&gt; &lt;p&gt;UI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. &lt;a href="https://github.com/amd/gaia"&gt;https://github.com/amd/gaia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU's Radeon 8060S GPU. &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.&lt;/p&gt; &lt;p&gt;Full disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e6ao7yjh5haf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq8gjv</id>
    <title>Ubuntu 24.04: observing that nvidia-535 drivers run 20 tokens/sec faster than nvidia-570 drivers with no other changes in my vLLM setup</title>
    <updated>2025-07-02T21:52:28+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.&lt;/p&gt; &lt;p&gt;I was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using &lt;code&gt;apt&lt;/code&gt; to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.&lt;/p&gt; &lt;p&gt;Curious if anyone has seen similar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:52:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq7vjc</id>
    <title>I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use</title>
    <updated>2025-07-02T21:27:35+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"&gt; &lt;img alt="I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use" src="https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebf4e25308a0e578d32f61beb93e3128aaa19efe" title="I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gh20o4e63jaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq79xx</id>
    <title>FP8 fixed on VLLM for RTX Pro 6000 (and RTX 5000 desktop cards)</title>
    <updated>2025-07-02T21:02:46+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yay! Been waiting for this one for a while, guessing I'm not the only one? &lt;a href="https://github.com/vllm-project/vllm/pull/17280"&gt;https://github.com/vllm-project/vllm/pull/17280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On 70B I'm maxing out around 1400T/s on the Pro 6000 with 100 threads.&lt;/p&gt; &lt;p&gt;Quick install instructions if you want to try it:&lt;/p&gt; &lt;p&gt;mkdir vllm-src&lt;br /&gt; cd vllm-src&lt;br /&gt; python3 -m venv myenv&lt;br /&gt; source myenv/bin/activate&lt;br /&gt; pip install torch torchvision torchaudio --index-url &lt;a href="https://download.pytorch.org/whl/cu128"&gt;https://download.pytorch.org/whl/cu128&lt;/a&gt;&lt;br /&gt; git clone &lt;a href="https://github.com/huggingface/transformers.git"&gt;https://github.com/huggingface/transformers.git&lt;/a&gt;&lt;br /&gt; git clone &lt;a href="https://github.com/vllm-project/vllm.git"&gt;https://github.com/vllm-project/vllm.git&lt;/a&gt;&lt;br /&gt; cd transformers&lt;br /&gt; pip install -e .&lt;br /&gt; cd ../vllm&lt;br /&gt; python use_existing_torch.py&lt;br /&gt; pip install -r requirements/build.txt&lt;br /&gt; pip install -r requirements/cuda.txt&lt;br /&gt; pip install -e . --no-build-isolation&lt;br /&gt; vllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic&lt;br /&gt; vllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T21:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq1jyr</id>
    <title>Mamba-2 support in llama.cpp landed</title>
    <updated>2025-07-02T17:14:08+00:00</updated>
    <author>
      <name>/u/pkmxtw</name>
      <uri>https://old.reddit.com/user/pkmxtw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt; &lt;img alt="Mamba-2 support in llama.cpp landed" src="https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d" title="Mamba-2 support in llama.cpp landed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pkmxtw"&gt; /u/pkmxtw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T17:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqdcgr</id>
    <title>PrivateScribe.ai - a fully local, MIT licensed AI transcription platform</title>
    <updated>2025-07-03T01:40:31+00:00</updated>
    <author>
      <name>/u/SecondPathDev</name>
      <uri>https://old.reddit.com/user/SecondPathDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt; &lt;img alt="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" src="https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67733391cf800cb69df3f2bbf96c8c0dcd8a7ecb" title="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share my first open source project - PrivateScribe.ai.&lt;/p&gt; &lt;p&gt;I’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.&lt;/p&gt; &lt;p&gt;I’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts on the idea or things people would want for other use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SecondPathDev"&gt; /u/SecondPathDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://www.privatescribe.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T01:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqbmwa</id>
    <title>DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp; 20% faster than R1</title>
    <updated>2025-07-03T00:15:16+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt; &lt;img alt="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" src="https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97a766bd7b9c921ab450ffb020d26db72a498fc7" title="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T00:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq5fqq</id>
    <title>I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source</title>
    <updated>2025-07-02T19:48:05+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt; &lt;img alt="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" src="https://preview.redd.it/nmerohq4miaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82000e0b8dd6c6f395384b8459f531f8884586e0" title="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmerohq4miaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:48:05+00:00</published>
  </entry>
</feed>
