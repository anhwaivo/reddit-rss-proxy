<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-14T11:34:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jyjwjl</id>
    <title>Best multimodal for 4gb card?</title>
    <updated>2025-04-13T22:31:22+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wanting to script some photo classification, but haven't messed with local multimodals. I have 32 gb of ram also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jydrnr</id>
    <title>AgenticSeek, one month later</title>
    <updated>2025-04-13T18:01:15+00:00</updated>
    <author>
      <name>/u/fawendeshuo</name>
      <uri>https://old.reddit.com/user/fawendeshuo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, I shared a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;post&lt;/a&gt; on a local-first alternative to ManusAI that I was working on with a friend: &lt;a href="http://github.com/Fosowl/agenticSeek"&gt;AgenticSeek&lt;/a&gt;. Back then I didn’t expect such interest! I saw blogs and even a video pop up about our tool, which was awesome but overwhelming since the project wasn’t quite ready for such success. &lt;/p&gt; &lt;p&gt;Thanks to some community feedback and some helpful contributions, we’ve made big strides in just a few weeks. So I thought it would be nice to share our advancements!&lt;/p&gt; &lt;p&gt;Here’s a quick rundown of the main improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smoother web navigation and note-taking.&lt;/li&gt; &lt;li&gt;Smarter task routing with task complexity estimation.&lt;/li&gt; &lt;li&gt;Added a planner agent to handle complex tasks.&lt;/li&gt; &lt;li&gt;Support for more providers, like LM-Studio and local APIs.&lt;/li&gt; &lt;li&gt;Integrated searxng for free web search.&lt;/li&gt; &lt;li&gt;Ability to use web input forms.&lt;/li&gt; &lt;li&gt;Improved captcha solving and stealthier browser automation.&lt;/li&gt; &lt;li&gt;Agent router now supports multiple languages (previously a prompt in Japanese or French would assign a random agent).&lt;/li&gt; &lt;li&gt;Squashed tons of bugs.&lt;/li&gt; &lt;li&gt;Set up a community server and updates on my X account (see readme).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What’s next?&lt;/strong&gt; I’m focusing on improving the planner agent, handling more type of web inputs, and adding support for MCP, and possibly a finetune of deepseek 👀 &lt;/p&gt; &lt;p&gt;There’s still a lot to do, but it’s delivering solid results compared to a month ago. Can't wait to get more feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fawendeshuo"&gt; /u/fawendeshuo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T18:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytv2q</id>
    <title>Open Sourcing a framework to build SLMs for any regional language</title>
    <updated>2025-04-14T08:25:17+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt; &lt;img alt="Open Sourcing a framework to build SLMs for any regional language" src="https://b.thumbs.redditmedia.com/4kFgQ_rHwmWt-CIQAEZue0vxirSL87ILD-zcoNeRBMM.jpg" title="Open Sourcing a framework to build SLMs for any regional language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jorc5k68grue1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcea88745cbcc03d289cd5f7d7ebd8cb82eaa008"&gt;https://preview.redd.it/jorc5k68grue1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcea88745cbcc03d289cd5f7d7ebd8cb82eaa008&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is our first major contribution towards building foundational LLM capacity for India. &lt;/p&gt; &lt;p&gt;The research paper associated with this work can be found here: &lt;a href="https://arxiv.org/pdf/2504.07989"&gt;https://arxiv.org/pdf/2504.07989&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We believe in open source 100% and have released a Github repository here: &lt;a href="https://github.com/VizuaraAI/Tiny-Stories-Regional"&gt;https://github.com/VizuaraAI/Tiny-Stories-Regional&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyone can use this repository to build a Small Language Model (SLM) for their language of choice.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Here is how we built these models: &lt;/p&gt; &lt;p&gt;(1) We based our methodology on the TinyStories Paper which Microsoft released in 2023: &lt;a href="https://arxiv.org/abs/2305.07759"&gt;https://arxiv.org/abs/2305.07759&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(2) We generated the datasets in regional languages. &lt;/p&gt; &lt;p&gt;(3) We built a language model architecture from scratch for pre-training. &lt;/p&gt; &lt;p&gt;(4) During inference, we evaluated the model creativity, completeness, fluency and grammar. &lt;/p&gt; &lt;p&gt;(5) We used this framework as a proxy for comparing regional tokenizers.&lt;/p&gt; &lt;p&gt;I feel the biggest takeaway from this work is that the framework we have outlined can be utilized by the community to create SLMs fro underrepresented, regional languages. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytv2q/open_sourcing_a_framework_to_build_slms_for_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:25:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyvxk4</id>
    <title>Moving from 48 to 64 NVRAM. What could you do extra?</title>
    <updated>2025-04-14T10:51:42+00:00</updated>
    <author>
      <name>/u/Otherwise-Tiger3359</name>
      <uri>https://old.reddit.com/user/Otherwise-Tiger3359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you could replace 2x3090 with 2x5090 are there any models that would make a difference to coding, text generation and processing, writing, etc. &lt;/p&gt; &lt;p&gt;Not asking if worth it, consider this money no object question (reasons). Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise-Tiger3359"&gt; /u/Otherwise-Tiger3359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvxk4/moving_from_48_to_64_nvram_what_could_you_do_extra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvxk4/moving_from_48_to_64_nvram_what_could_you_do_extra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvxk4/moving_from_48_to_64_nvram_what_could_you_do_extra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T10:51:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy9g9y</id>
    <title>Waifu GPU for AI GF?</title>
    <updated>2025-04-13T14:53:23+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt; &lt;img alt="Waifu GPU for AI GF?" src="https://external-preview.redd.it/BnNrd2081VNoP-o3smbLrBMM4J_6XEXvavemnoJv_qM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea747f555a0c92c80b42a36a37e0db735083b1b2" title="Waifu GPU for AI GF?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lpqhvyq68mue1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad31ac4af8529144b8d1be6323d09048cbf4d8b4"&gt;https://videocardz.com/newz/asus-officially-reveals-first-geforce-rtx-5060-ti-ahead-of-launch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I dont know these characters, but is this the future of mankind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyr38c</id>
    <title>It's been a while since Zhipu AI released a new GLM model</title>
    <updated>2025-04-14T05:06:41+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt; &lt;img alt="It's been a while since Zhipu AI released a new GLM model" src="https://external-preview.redd.it/rUMQwGzzv049_AQ65R_I2zx8r9Fk1GPQDozFx082Elc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71971286a2d2292f2a0a2b67094dc5e3c3a4b46e" title="It's been a while since Zhipu AI released a new GLM model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...but seriously, I'm hyped by the new glm-4 32b coming today&lt;/p&gt; &lt;p&gt;EDIT: so we are getting 6 new models. There is also a Z1-rumination-32B which should be a reasoning-overthinking model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/zRzRzRzRzRzRzR/GLM-4"&gt;https://github.com/zRzRzRzRzRzRzR/GLM-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/upyrr5tr9sue1.png?width=1001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=450e13a12ca31a5f0a50a5740c21bea16f232dc5"&gt;They compare to qwen in benchmarks!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qxmepy0asue1.png?width=505&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=723c7bf57447452c1b1cf8ad3387342c93c9cc16"&gt;Rumination!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T05:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jycfvf</id>
    <title>You can preview quantizations of Llama 4 Maverick 17Bx128E at acceptable speeds even without the necessary memory</title>
    <updated>2025-04-13T17:04:14+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably many already know this, but with llama.cpp it's possible to perform inference off models larger than the available total physical memory; this is thanks to the magic of &lt;code&gt;mmap&lt;/code&gt;. Inference speed might be surprisingly faster than you'd think.&lt;/p&gt; &lt;p&gt;I tested this with &lt;a href="https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ2_M"&gt;Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M&lt;/a&gt;, which is about 143 GB in total and shouldn't fit within my 64GB of DDR4 memory + one RTX3090 (24GB).&lt;/p&gt; &lt;p&gt;It takes a while for prompt processing to occur (admittedly at a fairly slow rate compared to normal), during which NVMe reads appear to be intense (5-6 GiB/s), which can be tracked on Linux with &lt;code&gt;iostat -s 1&lt;/code&gt;, but once that is done, inference speed is fairly decent.&lt;/p&gt; &lt;p&gt;Here's a benchmark with &lt;code&gt;llama-bench&lt;/code&gt; (I couldn't load more than 3 model layers on the GPU):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ./build/bin/llama-bench -m ~/models/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M.gguf -ngl 3 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | test | t/s | | ------------------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | pp512 | 16.43 ± 0.25 | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | tg128 | 3.45 ± 0.26 | build: 06bb53ad (5115) # free total used free shared buff/cache available Mem: 65523176 8262924 600336 184900 57572992 57260252 Swap: 65523172 14129384 51393788 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details for the flag that would prevent this behavior (disabling &lt;code&gt;mmap&lt;/code&gt;): &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/1876"&gt;https://github.com/ggml-org/llama.cpp/discussions/1876&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;--no-mmap&lt;/code&gt;: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using &lt;code&gt;--mlock&lt;/code&gt;. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: from a suggestion in the comments below by PhoenixModBot, starting Llama.cpp with &lt;code&gt;-ngl 999 -ot \\d+.ffn_.*_exps.=CPU&lt;/code&gt; can increase inference speed to &lt;strong&gt;8~18 tokens/s&lt;/strong&gt; (depending on which experts get cached on RAM). What this does is loading the shared model parameters on the GPU, while keeping the FFN layers (the routed experts) on the CPU (RAM). This is documented here: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11397"&gt;https://github.com/ggml-org/llama.cpp/pull/11397&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Additionally, in my own tests I've observed better prompt processing speeds by configuring both the physical and logical batch size to the same value of 2048. This can increase memory usage, though. &lt;code&gt;-b 2048 -ub 2048&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxy26m</id>
    <title>Sam Altman: "We're going to do a very powerful open source model... better than any current open source model out there."</title>
    <updated>2025-04-13T02:55:45+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt; &lt;img alt="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" src="https://external-preview.redd.it/eDJobnVwZ3luaXVlMdXj0QNvtvvTvdLhyylbR9Y6PzQjPjUyfN1eoWAw2jEe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a5f48835aebe28a468ef3c09a1d306d926d0876" title="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wzjs6qgyniue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T02:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyquyo</id>
    <title>AlexBefest's CardProjector-v4 series</title>
    <updated>2025-04-14T04:52:05+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: AlexBefest/CardProjector-27B-v4&lt;/p&gt; &lt;p&gt;Model URL: &lt;a href="https://huggingface.co/AlexBefest/CardProjector-27B-v4"&gt;https://huggingface.co/AlexBefest/CardProjector-27B-v4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's new in v4?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Absolute focus on personality development! This version places an absolute emphasis on designing character personalities, focusing on depth and realism. Eight (!) large datasets were collected, oriented towards all aspects of in-depth personality development. Extensive training was also conducted on a dataset of MBTI profiles with Enneagrams from psychology. The model was carefully trained to select the correct personality type according to both the MBTI and Enneagram systems. I highly recommend using these systems (see Usage recommendations); they provide an incredible boost to character realism. I conducted numerous tests with many RP models ranging from 24-70B parameters, and the MBTI profile system significantly impacts the understanding of the character's personality (especially on 70B models), making the role-playing performance much more realistic. You can see an example of a character's MBTI profile &lt;a href="https://www.personality-database.com/profile/7610/muffins-derpy-hooves-ditzy-doo-my-little-pony-friendship-is-magic-2010-mbti-personality-type"&gt;here&lt;/a&gt;. Currently, version V4 yields the deepest and most realistic characters.&lt;/li&gt; &lt;li&gt;Reduced likelihood of positive bias! I collected a large toxic dataset focused on creating and editing aggressive, extremely cruel, and hypersexualized characters, as well as transforming already &amp;quot;good harmless&amp;quot; characters into extremely cruel anti-versions of the original. Thanks to this, it was possible to significantly reduce the overall positive bias (especially in Gemma 3, where it is quite pronounced in its vanilla state), and make the model more balanced and realistic in terms of creating negative characters. It will no longer strive at all costs to create a cute, kind, ideal character, unless specifically asked to do so. All you need to do is just ask the model to &amp;quot;not make a positive character, but create a realistic one,&amp;quot; and with that one phrase, the entire positive bias goes away.&lt;/li&gt; &lt;li&gt;Moving to Gemma 3! After a series of experiments, it turned out that this model is ideally suited for the task of character design, as it possesses much more developed creative writing skills and higher general knowledge compared to Mistral 2501 in its vanilla state. Gemma 3 also seemed much more logical than its French competitor.&lt;/li&gt; &lt;li&gt;Vision ability! Due to the reason mentioned in the point above, you can freely use vision in this version. If you are using GGUF, you can download the mmproj model for the 27B version from bartowski (a vanilla mmproj will suffice, as I didn't perform vision tuning).&lt;/li&gt; &lt;li&gt;The overall quality of character generation has been significantly increased by expanding the dataset approximately 5 times compared to version V3.&lt;/li&gt; &lt;li&gt;This model is EXTREMELY sensitive to the user's prompt. So you should give instructions with caution, carefully considering.&lt;/li&gt; &lt;li&gt;In version V4, I concentrated only on one model size, 27B. Unfortunately, training multiple models at once is extremely expensive and consumes too much effort and time, so I decided it would be better to direct all my resources into just one model to avoid scattering focus. I hope you understand 🙏&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Overview:&lt;/h1&gt; &lt;p&gt;CardProjector is a specialized series of language models, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; and &lt;strong&gt;now for creating characters in general&lt;/strong&gt;. These models are designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T04:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyhd6i</id>
    <title>[2503.23817] MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration</title>
    <updated>2025-04-13T20:35:30+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;https://arxiv.org/abs/2503.23817&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads before and after in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities. This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29× speedup and 30.5× energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18× and 1.31× throughput improvements, along with 3.04× and 2.35× energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy8h2i</id>
    <title>Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data</title>
    <updated>2025-04-13T14:08:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt; &lt;img alt="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" src="https://b.thumbs.redditmedia.com/Xv5xeh-T-_FI_nUFqM0bvDA9nB-t2tXuTjAVsmlXjdE.jpg" title="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;github repo: &lt;a href="https://github.com/SkyworkAI/Skywork-OR1"&gt;https://github.com/SkyworkAI/Skywork-OR1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680"&gt;https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680&lt;/a&gt;&lt;/p&gt; &lt;p&gt;huggingface: &lt;a href="https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9"&gt;https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e"&gt;https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy813d</id>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
    <updated>2025-04-13T13:47:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T13:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyw3g0</id>
    <title>Zhipu AI Set to Release New GLM Models Today</title>
    <updated>2025-04-14T11:01:50+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone in the community posted today saying &amp;quot;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jyr38c/its_been_a_while_since_zhipu_ai_released_a_new/"&gt;It's been a while since Zhipu AI released a new GLM model.&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;It appears that Zhipu AI is launching a new series of GLM-4 models today after quite some time since their last release. According to information I've seen in the &lt;a href="https://github.com/zRzRzRzRzRzRzR/GLM-4"&gt;Github&lt;/a&gt;, they're releasing multiple variants including chat and reasoning models in two different sizes: 9B and 32B parameter versions.&lt;/p&gt; &lt;p&gt;The model collection link is already live on &lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;Hugging Face&lt;/a&gt;, so let's stay tuned for the good news!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyw3g0/zhipu_ai_set_to_release_new_glm_models_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyw3g0/zhipu_ai_set_to_release_new_glm_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyw3g0/zhipu_ai_set_to_release_new_glm_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:01:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk399</id>
    <title>Dual 5090 va single 5090</title>
    <updated>2025-04-13T22:40:38+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt; &lt;img alt="Dual 5090 va single 5090" src="https://preview.redd.it/z1xl2ob1koue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a09792ff8b0785b5b36ea4cb15fed716f6a7feaf" title="Dual 5090 va single 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man these dual 5090s are awesome. Went from 4t/s on 29b Gemma 3 to 28t/s when going from 1 to 2. I love these things! Easily runs 70b fast! I only wish they were a little cheaper but can’t wait till the RTX 6000 pro comes out with 96gb because I am totally eyeballing the crap out of it…. Who needs money when u got vram!!!&lt;/p&gt; &lt;p&gt;Btw I got 2 fans right under earn, 5 fans in front, 3 on top and one mac daddy on the back, and bout to put the one that came with the gigabyte 5090 on it too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xl2ob1koue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jywg95</id>
    <title>Why is Qwen 2.5 Omni not being talked about enough?</title>
    <updated>2025-04-14T11:23:03+00:00</updated>
    <author>
      <name>/u/BeetranD</name>
      <uri>https://old.reddit.com/user/BeetranD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the Qwen models are pretty good, I've been using a lot of them locally.&lt;br /&gt; They recently (a week or some ago) released 2.5 Omni, which is a 7B real-time multimodal model, that simultaneously generates text and natural speech. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;Qwen/Qwen2.5-Omni-7B · Hugging Face&lt;/a&gt;&lt;br /&gt; I think It would be great to use for something like a local AI alexa clone. But on youtube there's almost no one testing it, and even here, not a lot of people talking about it.&lt;/p&gt; &lt;p&gt;What is it?? Am I over-expecting from this model? or I'm just not well informed about alternatives, please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeetranD"&gt; /u/BeetranD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyo2ds</id>
    <title>Word Synth - Llama 3.2 tiny LLM with sampling parameters exposed</title>
    <updated>2025-04-14T02:09:01+00:00</updated>
    <author>
      <name>/u/Brave_Variety6275</name>
      <uri>https://old.reddit.com/user/Brave_Variety6275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built this as an intuition builder around LLM sampling--it's a bit rough around the edges but sharing in case its useful to anyone else trying to get it straight which sampling parameters do what. &lt;/p&gt; &lt;p&gt;&lt;a href="http://wordsynth.latenthomer.com/"&gt;http://wordsynth.latenthomer.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your browser will yell at you because I didn't use https. Sorry. &lt;/p&gt; &lt;p&gt;Also apologies if it breaks or is really slow, this was also an experiment to deploy. &lt;/p&gt; &lt;p&gt;Thanks for reading :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave_Variety6275"&gt; /u/Brave_Variety6275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T02:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jys33y</id>
    <title>Finally got Local LLM running on rx 9070 xt using onnx and directml</title>
    <updated>2025-04-14T06:14:33+00:00</updated>
    <author>
      <name>/u/dharayM</name>
      <uri>https://old.reddit.com/user/dharayM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No i am not talking about brainwashed llama that comes with adrenaline app.&lt;/p&gt; &lt;p&gt;With vulkan broken for windows and Linux, rocm not being supported for windows and seemingly broken for linux, directml was my only hope&lt;/p&gt; &lt;p&gt;only directml-onnx models works with my solution which essentially consists of phi models but something is better than nothing&lt;/p&gt; &lt;p&gt;Here is the repo:&lt;br /&gt; &lt;a href="https://github.com/dharay/directml-onnx-local-llm"&gt;https://github.com/dharay/directml-onnx-local-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is a work in progress, will probably abandon once we gets rocm support for rx 9000 series on windows&lt;/p&gt; &lt;p&gt;helpful resources:&lt;br /&gt; &lt;a href="https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html"&gt;https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dharayM"&gt; /u/dharayM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6ns6</id>
    <title>Coming soon…..</title>
    <updated>2025-04-13T12:36:19+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt; &lt;img alt="Coming soon….." src="https://preview.redd.it/1cwv3wz7klue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abbae222e535c2c110583987226650f6391ac918" title="Coming soon….." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1cwv3wz7klue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jygxmu</id>
    <title>Open-Weights Model next week?</title>
    <updated>2025-04-13T20:16:32+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt; &lt;img alt="Open-Weights Model next week?" src="https://preview.redd.it/iph04cputnue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f58d7addbdbe94c34055c810ba04a1042cb757a3" title="Open-Weights Model next week?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iph04cputnue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyvzqg</id>
    <title>GLM-4-0414 (9B/32B) (w. &amp; wo. reasoning) Ready to Release</title>
    <updated>2025-04-14T10:55:40+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt; &lt;img alt="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" src="https://external-preview.redd.it/rUMQwGzzv049_AQ65R_I2zx8r9Fk1GPQDozFx082Elc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71971286a2d2292f2a0a2b67094dc5e3c3a4b46e" title="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the developer is making final preparations : &lt;a href="https://github.com/zRzRzRzRzRzRzR/GLM-4"&gt;https://github.com/zRzRzRzRzRzRzR/GLM-4&lt;/a&gt; (note this is developer's fork, only for reference. Also note: some benchmarks in the page are from old versions of GLM model)&lt;/p&gt; &lt;p&gt;Huggingface collection is created (but empty for now): &lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The release contains following models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395"&gt;https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T10:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk213</id>
    <title>Still true 3 months later</title>
    <updated>2025-04-13T22:38:59+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt; &lt;img alt="Still true 3 months later" src="https://preview.redd.it/7644n1vqjoue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0b79a5e35c4e594b33dc646534a2248d3db9159" title="Still true 3 months later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They rushed the release so hard it's been full of implementation bugs. And let's not get started on the custom model to hill climb lmarena alop&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7644n1vqjoue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyp2no</id>
    <title>If we had models like QwQ-32B and Gemma-3-27B two years ago, people would have gone crazy.</title>
    <updated>2025-04-14T03:04:47+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine if we had QwQ-32B or Gemma-3-27B or some of the smaller models, 18-24 months ago. It would have been the craziest thing.&lt;/p&gt; &lt;p&gt;24 months ago, GPT-4 was released. GPT-4o was released 11 months ago. Sometimes we not only forgot how quick things have been moving, but we also forget how good these small models actually are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T03:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jysiwc</id>
    <title>DeepSeek will open-source parts of its inference engine — sharing standalone features and optimizations instead of the full stack</title>
    <updated>2025-04-14T06:45:54+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt; &lt;img alt="DeepSeek will open-source parts of its inference engine — sharing standalone features and optimizations instead of the full stack" src="https://external-preview.redd.it/-j5EXG21mJ1IrGfaacZfdTPmLfMidR-DBjShQEW0nM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8674fe0d9158595daad240e374a62be90da4c4d6" title="DeepSeek will open-source parts of its inference engine — sharing standalone features and optimizations instead of the full stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyu06v</id>
    <title>llama was so deep that now ex employee saying that we r not involved in that project</title>
    <updated>2025-04-14T08:36:06+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt; &lt;img alt="llama was so deep that now ex employee saying that we r not involved in that project" src="https://preview.redd.it/49mfsia3irue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3266a093713e9cb503b3634a7a8b1f7fb0852f0" title="llama was so deep that now ex employee saying that we r not involved in that project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49mfsia3irue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytw62</id>
    <title>DeepSeek is about to open-source their inference engine</title>
    <updated>2025-04-14T08:27:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt; &lt;img alt="DeepSeek is about to open-source their inference engine" src="https://preview.redd.it/1am95yongrue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967ad74640babe443b3c9a2867547f568219bda6" title="DeepSeek is about to open-source their inference engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is about to open-source their inference engine, which is a modified version based on vLLM. Now, DeepSeek is preparing to contribute these modifications back to the community.&lt;/p&gt; &lt;p&gt;I really like the last sentence: 'with the goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0.'&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine"&gt;https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1am95yongrue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:27:29+00:00</published>
  </entry>
</feed>
