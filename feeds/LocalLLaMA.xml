<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-27T22:05:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ibhddu</id>
    <title>Qwen2.5-VL has been released</title>
    <updated>2025-01-27T19:15:23+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhddu/qwen25vl_has_been_released/"&gt; &lt;img alt="Qwen2.5-VL has been released" src="https://external-preview.redd.it/LjxBX3bbxEkw4lfVvB3TrDarOHkoRu3DtMCBIqst5AU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41dfc9ad45f7c4bfaafe697e0177455084353473" title="Qwen2.5-VL has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhddu/qwen25vl_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhddu/qwen25vl_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibiki9</id>
    <title>Qwen2.5-VL released: 3B, 7B, 72B</title>
    <updated>2025-01-27T20:03:59+00:00</updated>
    <author>
      <name>/u/Dry_Rabbit_1123</name>
      <uri>https://old.reddit.com/user/Dry_Rabbit_1123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From the README:&lt;/p&gt; &lt;p&gt;Key Enhancements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images. * **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use. * **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments. * **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes. * **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Rabbit_1123"&gt; /u/Dry_Rabbit_1123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibiki9/qwen25vl_released_3b_7b_72b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibiki9/qwen25vl_released_3b_7b_72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibiki9/qwen25vl_released_3b_7b_72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T20:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhn47</id>
    <title>Qwen 2.5 VL</title>
    <updated>2025-01-27T19:26:19+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhn47/qwen_25_vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhn47/qwen_25_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhn47/qwen_25_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib7mg4</id>
    <title>I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446</title>
    <updated>2025-01-27T12:25:45+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"&gt; &lt;img alt="I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446" src="https://b.thumbs.redditmedia.com/alSIx8B8I53oq9CVNU61AjABH8VeEa2cyNOSjfEiIDc.jpg" title="I spent the last weekend optimizing the DeepSeek V2/V3 llama.cpp implementation - PR #11446" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/11446"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib7mg4/i_spent_the_last_weekend_optimizing_the_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T12:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibelba</id>
    <title>Guess who's jealous</title>
    <updated>2025-01-27T17:25:33+00:00</updated>
    <author>
      <name>/u/No-Point-6492</name>
      <uri>https://old.reddit.com/user/No-Point-6492</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"&gt; &lt;img alt="Guess who's jealous" src="https://preview.redd.it/pfhlaz5jmkfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ca8fb4c59047cde6f8c3383a1d8424c6f63e76" title="Guess who's jealous" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Point-6492"&gt; /u/No-Point-6492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfhlaz5jmkfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibelba/guess_whos_jealous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhcsy</id>
    <title>Janus-Pro-7B first tests</title>
    <updated>2025-01-27T19:14:43+00:00</updated>
    <author>
      <name>/u/HugoDzz</name>
      <uri>https://old.reddit.com/user/HugoDzz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"&gt; &lt;img alt="Janus-Pro-7B first tests" src="https://preview.redd.it/24o1wy1z5lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dedf99adb87d986aed80bf3ef760307490ebb0a4" title="Janus-Pro-7B first tests" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HugoDzz"&gt; /u/HugoDzz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/24o1wy1z5lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhcsy/januspro7b_first_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4qrg</id>
    <title>It was fun while it lasted.</title>
    <updated>2025-01-27T09:50:11+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt; &lt;img alt="It was fun while it lasted." src="https://preview.redd.it/f4z3rtg5dife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2e9e389c105a5198f86f71e16e2dc7186ad1daa" title="It was fun while it lasted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f4z3rtg5dife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibffe6</id>
    <title>DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B</title>
    <updated>2025-01-27T17:59:06+00:00</updated>
    <author>
      <name>/u/aichiusagi</name>
      <uri>https://old.reddit.com/user/aichiusagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"&gt; &lt;img alt="DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B" src="https://external-preview.redd.it/A2EEjszaCCukFYACldeB-sC0pfmrpnJQLkGODk-spiI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55b85aef606aeca155936b8d61992b31b38c1d84" title="DeepSeek just dropped a new multimodal understanding and visual generation model Janus-Pro 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aichiusagi"&gt; /u/aichiusagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/Janus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibffe6/deepseek_just_dropped_a_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib2uuz</id>
    <title>I created a "Can you run it" tool for open source LLMs</title>
    <updated>2025-01-27T07:46:52+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Raskoll2/LLMcalc"&gt;https://github.com/Raskoll2/LLMcalc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's extremly simple but tells you a tk/s estimate of all the quants, and how to run them e.g. 80% layer offload, KV offload, all on GPU. &lt;/p&gt; &lt;p&gt;I have no clue if it'll run on anyone else's systems. I've tried with with linux + 1x Nvidia GPU, if anyone on other systems or multi GPU systems could relay some error messages that would be great&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T07:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iasyc3</id>
    <title>Deepseek is #1 on the U.S. App Store</title>
    <updated>2025-01-26T22:52:07+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt; &lt;img alt="Deepseek is #1 on the U.S. App Store" src="https://preview.redd.it/sr4kvvnv3ffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a82ab88b43a6f7f3f1aa6d284ecb8edff2e4630" title="Deepseek is #1 on the U.S. App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr4kvvnv3ffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T22:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib8wfw</id>
    <title>Nvidia pre-market down 12% due Deepseek</title>
    <updated>2025-01-27T13:23:17+00:00</updated>
    <author>
      <name>/u/puffyarizona</name>
      <uri>https://old.reddit.com/user/puffyarizona</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html"&gt;NVidia Prę-market down 12% due Deepseek&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puffyarizona"&gt; /u/puffyarizona &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib8wfw/nvidia_premarket_down_12_due_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T13:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib99ei</id>
    <title>Same size as the old gpt2 model. Insane.</title>
    <updated>2025-01-27T13:38:32+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"&gt; &lt;img alt="Same size as the old gpt2 model. Insane." src="https://preview.redd.it/cotf3wm1ijfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba2f6e3e7a8a1708887fa7db558a92d1d0c83b4e" title="Same size as the old gpt2 model. Insane." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cotf3wm1ijfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib99ei/same_size_as_the_old_gpt2_model_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T13:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhco4</id>
    <title>Qwen2.5-VL are here</title>
    <updated>2025-01-27T19:14:36+00:00</updated>
    <author>
      <name>/u/x0wl</name>
      <uri>https://old.reddit.com/user/x0wl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"&gt; &lt;img alt="Qwen2.5-VL are here" src="https://preview.redd.it/4x3qcn4y5lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e30cac9b913dab31060cd7e7517ab0833399410" title="Qwen2.5-VL are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0wl"&gt; /u/x0wl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4x3qcn4y5lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhco4/qwen25vl_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib5yuk</id>
    <title>deepseek r1 tops the creative writing rankings</title>
    <updated>2025-01-27T11:00:18+00:00</updated>
    <author>
      <name>/u/Still_Potato_415</name>
      <uri>https://old.reddit.com/user/Still_Potato_415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"&gt; &lt;img alt="deepseek r1 tops the creative writing rankings" src="https://preview.redd.it/yslsnd6fpife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3efe198ef9c53da2bdad4be41459ab90d46ec7" title="deepseek r1 tops the creative writing rankings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Still_Potato_415"&gt; /u/Still_Potato_415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yslsnd6fpife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib5yuk/deepseek_r1_tops_the_creative_writing_rankings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T11:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibc2vx</id>
    <title>Deepseek currently restricts new registrations to Chinese phone numbers only</title>
    <updated>2025-01-27T15:43:22+00:00</updated>
    <author>
      <name>/u/SysPsych</name>
      <uri>https://old.reddit.com/user/SysPsych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See: &lt;a href="https://i.imgur.com/9WLAnko.png"&gt;https://i.imgur.com/9WLAnko.png&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SysPsych"&gt; /u/SysPsych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibc2vx/deepseek_currently_restricts_new_registrations_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4ksj</id>
    <title>How *exactly* is Deepseek so cheap?</title>
    <updated>2025-01-27T09:40:04+00:00</updated>
    <author>
      <name>/u/micamecava</name>
      <uri>https://old.reddit.com/user/micamecava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek's all the rage. I get it, 95-97% reduction in costs. &lt;/p&gt; &lt;p&gt;How *exactly*? &lt;/p&gt; &lt;p&gt;Aside from cheaper training (not doing RLHF), quantization, and caching (semantic input HTTP caching I guess?), where's the reduction coming from? &lt;/p&gt; &lt;p&gt;This can't be all, because supposedly R1 isn't quantized. Right?&lt;/p&gt; &lt;p&gt;Is it subsidized? Is OpenAI/Anthropic just...charging too much? What's the deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/micamecava"&gt; /u/micamecava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe7dn</id>
    <title>Nvidia faces $465 billion loss as DeepSeek disrupts AI market, largest in US market history</title>
    <updated>2025-01-27T17:09:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.financialexpress.com/business/investing-abroad-nvidia-faces-465-billion-loss-as-deepseek-disrupts-ai-market-3728093/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibb8rr</id>
    <title>Qwen3.0 MOE? New Reasoning Model?</title>
    <updated>2025-01-27T15:09:24+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt; &lt;img alt="Qwen3.0 MOE? New Reasoning Model?" src="https://preview.redd.it/0vnua5vqxjfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12861d7e6664e9cd7e45dd0710b87280d3a92aff" title="Qwen3.0 MOE? New Reasoning Model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0vnua5vqxjfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhew9</id>
    <title>Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights.</title>
    <updated>2025-01-27T19:17:00+00:00</updated>
    <author>
      <name>/u/brawll66</name>
      <uri>https://old.reddit.com/user/brawll66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt; &lt;img alt="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." src="https://preview.redd.it/8811npnd6lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1491fe046f7dfb7fa4f728b0b182b4fae6b44b" title="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brawll66"&gt; /u/brawll66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8811npnd6lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibk9us</id>
    <title>Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price</title>
    <updated>2025-01-27T21:13:50+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt; &lt;img alt="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" src="https://external-preview.redd.it/Brnl3ltRvrwiYwAXRD8-9ZQzXA_EE-2JvCrM0Zi5k8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046aa83b70828043ace549a5075989da27f1ff4" title="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the article: &amp;quot;Of the four war rooms Meta has created to respond to DeepSeek’s potential breakthrough, two teams will try to decipher how High-Flyer lowered the cost of training and running DeepSeek with the goal of using those tactics for Llama, the outlet reported citing one anonymous Meta employee. &lt;/p&gt; &lt;p&gt;Among the remaining two teams, one will try to find out which data DeepSeek used to train its model, and the other will consider how Llama can restructure its models based on attributes of the DeepSeek models, The Information reported.&amp;quot;&lt;/p&gt; &lt;p&gt;I am actually excited by this. If Meta can figure it out, it means Llama 4 or 4.x will be substantially better. Hopefully we'll get a 70B dense model that's on part with DeepSeek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibeub5</id>
    <title>llama.cpp PR with 99% of code written by Deepseek-R1</title>
    <updated>2025-01-27T17:35:22+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt; &lt;img alt="llama.cpp PR with 99% of code written by Deepseek-R1" src="https://preview.redd.it/pfm0xpbaokfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3147a5b22a1d22f5ba48a7737734a1af6de28d53" title="llama.cpp PR with 99% of code written by Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfm0xpbaokfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd5x0</id>
    <title>DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model).</title>
    <updated>2025-01-27T16:28:21+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt; &lt;img alt="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." src="https://external-preview.redd.it/n5r1wVoNriwXCNjXkrw2Ab2zRN5UbL6aXFXA0wRQWRU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef80d96659edc7101cd569ddae687c24437596ba" title="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe1ro</id>
    <title>Thoughts? I kinda feel happy about this...</title>
    <updated>2025-01-27T17:03:47+00:00</updated>
    <author>
      <name>/u/Butefluko</name>
      <uri>https://old.reddit.com/user/Butefluko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt; &lt;img alt="Thoughts? I kinda feel happy about this..." src="https://preview.redd.it/6b78kpulikfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f14a6edf107ecbf10ce8c437a2e0826bef5af67" title="Thoughts? I kinda feel happy about this..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Butefluko"&gt; /u/Butefluko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6b78kpulikfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbloy</id>
    <title>1.58bit DeepSeek R1 - 131GB Dynamic GGUF</title>
    <updated>2025-01-27T15:24:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt; &lt;img alt="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" src="https://external-preview.redd.it/uHOmNdCTHW-Q1CBdw01aifeSpeyvgfhjJI_lcC-SH5c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc7cd6ab7b35a273b107dce1a4113ba2c9dcca51" title="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I managed to &lt;strong&gt;dynamically quantize&lt;/strong&gt; the full DeepSeek R1 671B MoE to 1.58bits in GGUF format. The trick is &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but quantize only the MoE layers to 1.5bit, and leave attention and other layers in 4 or 6bit.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;131GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;158GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;183GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;212GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can get &lt;strong&gt;140 tokens / s&lt;/strong&gt; on 2x H100 80GB GPUs with all layers offloaded. A 24GB GPU like RTX 4090 should be able to get at least 1 to 3 tokens / s.&lt;/p&gt; &lt;p&gt;If we naively quantize all layers to 1.5bit (-1, 0, 1), the model will fail dramatically, since it'll produce &lt;strong&gt;gibberish&lt;/strong&gt; and &lt;strong&gt;infinite repetitions&lt;/strong&gt;. I selectively leave all attention layers in 4/6bit, and leave the first 3 transformer dense layers in 4/6bit. The MoE layers take up 88% of all space, so we can leave them in 1.5bit. We get in total a weighted sum of 1.58bits!&lt;/p&gt; &lt;p&gt;I asked it the 1.58bit model to create Flappy Bird with 10 conditions (like random colors, a best score etc), and it did pretty well! Using a generic non dynamically quantized model will fail miserably - there will be no output at all!&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k8nfun2ezjfe1.gif"&gt;Flappy Bird game made by 1.58bit R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's more details in the blog here: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt; The link to the 1.58bit GGUF is here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S&lt;/a&gt; You should be able to run it in your favorite inference tool if it supports i matrix quants. No need to re-update llama.cpp.&lt;/p&gt; &lt;p&gt;A reminder on DeepSeek's chat template (for distilled versions as well) - it auto adds a BOS - do not add it manually!&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;｜begin▁of▁sentence｜&amp;gt;&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;It's 2.&amp;lt;｜end▁of▁sentence｜&amp;gt;&amp;lt;｜User｜&amp;gt;Explain more!&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To know how many layers to offload to the GPU, I approximately calculated it as below:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;File Size&lt;/th&gt; &lt;th align="left"&gt;24GB GPU&lt;/th&gt; &lt;th align="left"&gt;80GB GPU&lt;/th&gt; &lt;th align="left"&gt;2x80GB GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;131GB&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;All layers 61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;158GB&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;183GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;212GB&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All other GGUFs for R1 are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF&lt;/a&gt; There's also GGUFs and dynamic 4bit bitsandbytes quants and others for all other distilled versions (Qwen, Llama etc) at &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employee’s reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employee’s reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employee’s reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
</feed>
