<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-03T20:37:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lqmmv2</id>
    <title>Best way to get an LLM to sound like me? Prompt eng or Finetune?</title>
    <updated>2025-07-03T10:57:32+00:00</updated>
    <author>
      <name>/u/RelevantPractice2074</name>
      <uri>https://old.reddit.com/user/RelevantPractice2074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.&lt;/p&gt; &lt;p&gt;My use case: Creating social content which sounds like me, not AI slop.&lt;/p&gt; &lt;p&gt;What's the best way to do this nowadays? Would appreciate any direction&lt;/p&gt; &lt;p&gt;Edit for more context: Right now I'm generating content with a powerful model, then I'm aiming to do the 'styling' in a final call.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelevantPractice2074"&gt; /u/RelevantPractice2074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:57:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqmbh3</id>
    <title>Anyone here run llama4 scout/Maverick with 1 million to 10 million context?</title>
    <updated>2025-07-03T10:38:22+00:00</updated>
    <author>
      <name>/u/night0x63</name>
      <uri>https://old.reddit.com/user/night0x63</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Anyone here run llama4 with 1 million to 10 million context?&lt;/h3&gt; &lt;p&gt;Just curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.&lt;/p&gt; &lt;p&gt;What are vram/ram requirements for 1m context? 10m context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/night0x63"&gt; /u/night0x63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqj3eq</id>
    <title>Sharing new inference engines I got to know recently</title>
    <updated>2025-07-03T07:04:38+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/cactus-compute/cactus"&gt;https://github.com/cactus-compute/cactus&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/jafioti/luminal"&gt;https://github.com/jafioti/luminal&lt;/a&gt; ( Rust )&lt;/p&gt; &lt;p&gt;Catus seems to start from fork of llama.cpp. (similar to Ollama)&lt;/p&gt; &lt;p&gt;Luminal is more interesting since it rebuild everything.&lt;br /&gt; GeoHot from Tinygrad is quite active in Luminal's Discord too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T07:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqlsyb</id>
    <title>Yappp - Yet Another Poor Peasent Post</title>
    <updated>2025-07-03T10:06:40+00:00</updated>
    <author>
      <name>/u/needthosepylons</name>
      <uri>https://old.reddit.com/user/needthosepylons</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I wanted to share my experience and hear about yours.&lt;/p&gt; &lt;p&gt;Hardware : &lt;/p&gt; &lt;p&gt;GPU : 3060 12GB CPU : i5-3060 RAM : 32GB&lt;/p&gt; &lt;p&gt;Front-end : Koboldcpp + open-webui&lt;/p&gt; &lt;p&gt;Use cases : General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt; &lt;p&gt;I've been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. &lt;/p&gt; &lt;p&gt;GEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they're quite fast, and have a good ability to stick to the prompt. &lt;/p&gt; &lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it's variants. &lt;/p&gt; &lt;p&gt;What are your experiences? Do you use other models of the same range? &lt;/p&gt; &lt;p&gt;Good day everyone! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/needthosepylons"&gt; /u/needthosepylons &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T10:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqxs6n</id>
    <title>Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen</title>
    <updated>2025-07-03T18:58:08+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt; &lt;img alt="Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen" src="https://b.thumbs.redditmedia.com/b-h03xyvoled26aXh7C0uCGNcWCl9E1UZV5LDMiLKeM.jpg" title="Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a"&gt;https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;9.8t/s on a 235b model with just a 16GB card? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-server.exe -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot exps=CPU -c 30000 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA0 -md Qwen3-0.6B-BF16.gguf -devd CUDA0 -ngld 99&lt;/p&gt; &lt;p&gt;prompt eval time = 10924.78 ms / 214 tokens ( 51.05 ms per token, 19.59 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 594651.64 ms / 5826 tokens ( 102.07 ms per token, 9.80 tokens per second)&lt;/p&gt; &lt;p&gt;total time = 605576.42 ms / 6040 tokens&lt;/p&gt; &lt;p&gt;slot print_timing: id 0 | task 0 |&lt;/p&gt; &lt;p&gt;draft acceptance rate = 0.86070 ( 4430 accepted / 5147 generated)&lt;/p&gt; &lt;p&gt;I've now tried quite a few Qwen 0.6b draft models. TLDR, Q80 is marginally faster BUT FOR SOME REASON the bf16 draft model produces better outputs than all the others. Also, look at that acceptance rate. 86%!&lt;/p&gt; &lt;p&gt;This was the classic flappy bird test and here's the code it produced:&lt;/p&gt; &lt;p&gt;&lt;code&gt;import pygame&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;import random&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;import sys&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Initialize pygame&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.init()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Set up display&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;width, height = 400, 600&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;screen = pygame.display.set_mode((width, height))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.display.set_caption(&amp;quot;Flappy Bird&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Set up game clock&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;clock = pygame.time.Clock()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Bird parameters&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_x = width // 4&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_y = height // 2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_velocity = 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;gravity = 0.5&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;acceleration = -8&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_size = 30&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_shape = random.choice(['square', 'circle', 'triangle'])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Land parameters&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;land_height = random.choice([50, 100])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;land_color = random.choice([(139, 69, 19), (255, 255, 0)])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Pipe parameters&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe_width = 60&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe_gap = 150&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe_velocity = 3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipes = []&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe_colors = [(0, 100, 0), (165, 105, 55), (60, 60, 60)]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Score&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;score = 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;best_score = 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;font = pygame.font.Font(None, 36)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Background&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;background_color = (173, 216, 230) # light blue&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Game state&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;game_active = True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def create_pipe():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe_height = random.randint(100, height - pipe_gap - land_height - 50)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;top_pipe = pygame.Rect(width, 0, pipe_width, pipe_height)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bottom_pipe = pygame.Rect(width, pipe_height + pipe_gap, pipe_width, height - pipe_height - pipe_gap)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;color = random.choice(pipe_colors)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return [top_pipe, bottom_pipe, color, False] # False for scored status&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def draw_bird():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if bird_shape == 'square':&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.rect(screen, bird_color, (bird_x, bird_y, bird_size, bird_size))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;elif bird_shape == 'circle':&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.circle(screen, bird_color, (bird_x + bird_size//2, bird_y + bird_size//2), bird_size//2)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;elif bird_shape == 'triangle':&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;points = [(bird_x, bird_y + bird_size),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(bird_x + bird_size//2, bird_y),&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;(bird_x + bird_size, bird_y + bird_size)]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.polygon(screen, bird_color, points)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;def check_collision():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Create bird rect&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_rect = pygame.Rect(bird_x, bird_y, bird_size, bird_size)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Check collision with pipes&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for pipe in pipes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if pipe[0].colliderect(bird_rect) or pipe[1].colliderect(bird_rect):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Check collision with ground or ceiling&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if bird_y &amp;gt;= height - land_height or bird_y &amp;lt;= 0:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;return False&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Initial pipe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipes.append(create_pipe())&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Main game loop&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;while True:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for event in pygame.event.get():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if event.type == pygame.QUIT:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.quit()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sys.exit()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if event.type == pygame.KEYDOWN:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if event.key == pygame.K_SPACE:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if game_active:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_velocity = acceleration&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;else:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Restart game&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_y = height // 2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_velocity = 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipes = [create_pipe()]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;score = 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;game_active = True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if event.key == pygame.K_q or event.key == pygame.K_ESCAPE:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.quit()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sys.exit()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if game_active:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Update bird position&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_velocity += gravity&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bird_y += bird_velocity&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Update pipes&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if not pipes or pipes[-1][0].x &amp;lt; width - 200:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipes.append(create_pipe())&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for pipe in pipes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe[0].x -= pipe_velocity&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe[1].x -= pipe_velocity&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Remove off-screen pipes&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipes = [pipe for pipe in pipes if pipe[0].x + pipe_width &amp;gt; 0]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Check for collision&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if check_collision():&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;game_active = False&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;best_score = max(score, best_score)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Check for score update&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for pipe in pipes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if not pipe[3]: # If not scored yet&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if pipe[0].x + pipe_width &amp;lt; bird_x:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;score += 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pipe[3] = True&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Draw everything&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;screen.fill(background_color)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Draw pipes&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for pipe in pipes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.rect(screen, pipe[2], pipe[0])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.rect(screen, pipe[2], pipe[1])&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Draw bird&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;draw_bird()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Draw land&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.draw.rect(screen, land_color, (0, height - land_height, width, land_height))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Draw score&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;score_text = font.render(f&amp;quot;Score: {score}&amp;quot;, True, (0, 0, 0))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;best_score_text = font.render(f&amp;quot;Best: {best_score}&amp;quot;, True, (0, 0, 0))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;screen.blit(score_text, (width - 150, 20))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;screen.blit(best_score_text, (width - 150, 50))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;if not game_active:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;game_over_text = font.render(&amp;quot;Game Over! Press SPACE to restart&amp;quot;, True, (0, 0, 0))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;screen.blit(game_over_text, (width//2 - 150, height//2 - 50))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pygame.display.flip()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;clock.tick(60)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I had no intention of using this model, I was just trying to see how badly it would run however, I'm starting to think there may be some sort of synergy between Unsloth's Q2K 235b and their BF16 0.6b as a draft model.&lt;/p&gt; &lt;p&gt;The game seems to run and play fine, too:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e"&gt;https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T18:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqdcgr</id>
    <title>PrivateScribe.ai - a fully local, MIT licensed AI transcription platform</title>
    <updated>2025-07-03T01:40:31+00:00</updated>
    <author>
      <name>/u/SecondPathDev</name>
      <uri>https://old.reddit.com/user/SecondPathDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt; &lt;img alt="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" src="https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67733391cf800cb69df3f2bbf96c8c0dcd8a7ecb" title="PrivateScribe.ai - a fully local, MIT licensed AI transcription platform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share my first open source project - PrivateScribe.ai.&lt;/p&gt; &lt;p&gt;I’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.&lt;/p&gt; &lt;p&gt;I’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts on the idea or things people would want for other use cases. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SecondPathDev"&gt; /u/SecondPathDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://www.privatescribe.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T01:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqyabt</id>
    <title>Anybody using local LLM to augment in-camera person-detection for people counting?</title>
    <updated>2025-07-03T19:18:15+00:00</updated>
    <author>
      <name>/u/MHTMakerspace</name>
      <uri>https://old.reddit.com/user/MHTMakerspace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have a dozen rooms in our makerspace, are trying to calculate occupancy heatmaps and collect general &amp;quot;is this space being utilized&amp;quot; data. Has anybody used TensorFlow Lite or a &amp;quot;vision&amp;quot; LLM running locally to get an (approximate) count of people in a room using snapshots?&lt;/p&gt; &lt;p&gt;We have mostly Amcrest &amp;quot;AI&amp;quot; cameras along with Seeed's 24Ghz mmwave &amp;quot;Human Static Presence&amp;quot; sensors. In combination these are fairly accurate at binary yes/no detection of human occupancy, but do not offer people counting. We have looked at other mmWave sensors, but they're expensive, and mostly can only count accurately to 3. We can however set things up so a snapshot is captured from each AI camera anytime it sees an object that it identifies as a person.&lt;/p&gt; &lt;p&gt;Using 5mp full-resolution snapshots we've found that the following prompt gives a fairly accurate (+/-1) count, including sitting and standing persons, without custom tuning of the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ollama run gemma3:4b &amp;quot;Return as an integer the number of people in this image: ./snapshot-1234.jpg&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using a cloud-based AI such as google Vision, Azure, or NVIDIA cloud is about as accurate, but faster than our local RTX4060 GPU. Worst case response time for any of these options is ~7 seconds per frame analyzed, which is acceptable for our purpose (a dozen rooms, snapshots at most once every 5 minutes or so, only captured when a sensor or camera reports a room is not empty).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any other recommended approaches&lt;/strong&gt;? I assume a Coral Edge TPU would give an answer faster, but would TensorFlow Lite also be more accurate out-of-the box, or would we need to invest time and effort in tuning for each camera/scene?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MHTMakerspace"&gt; /u/MHTMakerspace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyabt/anybody_using_local_llm_to_augment_incamera/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyabt/anybody_using_local_llm_to_augment_incamera/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyabt/anybody_using_local_llm_to_augment_incamera/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T19:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqndyy</id>
    <title>Hey r/LocalLLaMA! We made evolutionary model merging feasible on consumer GPUs – meet Mergenetic 🧬</title>
    <updated>2025-07-03T11:40:36+00:00</updated>
    <author>
      <name>/u/leviatan0</name>
      <uri>https://old.reddit.com/user/leviatan0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with &lt;strong&gt;Mergenetic&lt;/strong&gt;, an open-source library that makes &lt;em&gt;evolutionary&lt;/em&gt; merging practical without needing big hardware.&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evolves high-quality LLM merges using evolutionary algorithms&lt;/li&gt; &lt;li&gt;Supports SLERP, TIES, DARE, Task Arithmetic, and more&lt;/li&gt; &lt;li&gt;Efficient: search happens in parameter space, not gradient needed&lt;/li&gt; &lt;li&gt;Modular, hackable, and built on familiar tools (&lt;code&gt;mergekit&lt;/code&gt;, &lt;code&gt;pymoo&lt;/code&gt;, &lt;code&gt;lm-eval-harness&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.&lt;/p&gt; &lt;p&gt;For details, check out our papers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ACL 2025 Demo: &lt;a href="https://arxiv.org/pdf/2505.11427"&gt;arxiv.org/abs/2505.11427&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ICML 2025: &lt;a href="https://arxiv.org/pdf/2502.10436"&gt;arxiv.org/abs/2502.10436&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/tommasomncttn/mergenetic"&gt;GitHub: tommasomncttn/mergenetic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or contributions — hope it’s useful to some of you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leviatan0"&gt; /u/leviatan0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T11:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqbmwa</id>
    <title>DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp; 20% faster than R1</title>
    <updated>2025-07-03T00:15:16+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt; &lt;img alt="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" src="https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97a766bd7b9c921ab450ffb020d26db72a498fc7" title="DeepSeek-TNG-R1T2-Chimera - 200% faster than R1-0528 &amp;amp; 20% faster than R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T00:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqu1om</id>
    <title>What are some of the most mammoth homebuilds here? What have you done with them?</title>
    <updated>2025-07-03T16:30:37+00:00</updated>
    <author>
      <name>/u/Gary5Host9</name>
      <uri>https://old.reddit.com/user/Gary5Host9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious to see how far the most hardcore home builds have gone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gary5Host9"&gt; /u/Gary5Host9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqu1om/what_are_some_of_the_most_mammoth_homebuilds_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqu1om/what_are_some_of_the_most_mammoth_homebuilds_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqu1om/what_are_some_of_the_most_mammoth_homebuilds_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T16:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqsvmf</id>
    <title>Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)</title>
    <updated>2025-07-03T15:43:53+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/"&gt; &lt;img alt="Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)" src="https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=438b67c1d9deb5dd0f97a1c609b743a0ba02da9a" title="Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7"&gt;https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Quick Recap: What is RoPE?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;This provides several advantages:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Relative Position Awareness&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Understands the distance between tokens&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Extrapolation&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles sequences longer than seen during training&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficiency&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Doesn’t require additional embeddings — just math inside attention&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt; Code Walkthrough&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model&lt;/em&gt; &lt;a href="https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt; &lt;em&gt;codebase.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1: Implementation: RoPEPositionalEncoding&lt;/h1&gt; &lt;p&gt;&lt;em&gt;In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;This class:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Precomputes rotation frequencies&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Provides an apply_rope method&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Applies RoPE to input tensors, usually the query and key vectors&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# deepseek.py class RoPEPositionalEncoding(nn.Module): def __init__(self, dim, max_len=2048): super().__init__() inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim)) t = torch.arange(max_len, dtype=torch.float) freqs = torch.einsum(&amp;quot;i,j-&amp;gt;ij&amp;quot;, t, inv_freq) emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1) self.register_buffer(&amp;quot;positional_encoding&amp;quot;, emb) def apply_rope(self, x, position_ids): rope = self.positional_encoding[position_ids] x1, x2 = x[..., ::2], x[..., 1::2] rope1, rope2 = rope[..., ::2], rope[..., 1::2] return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1) &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;2: Usage: Integrating RoPE into Attention&lt;/h1&gt; &lt;p&gt;&lt;em&gt;The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# deepseek.py q = self.q_proj(x) k = self.k_proj(x) q = self.rope.apply_rope(q, position_ids) k = self.rope.apply_rope(k, position_ids) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;What’s happening?&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;x&lt;/code&gt; &lt;em&gt;is projected into query (&lt;/em&gt;&lt;code&gt;q&lt;/code&gt;&lt;em&gt;) and key (&lt;/em&gt;&lt;code&gt;k&lt;/code&gt;&lt;em&gt;) vectors.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;RoPE is applied to both using apply_rope, injecting position awareness.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Attention proceeds as usual — except now the queries and keys are aware of their relative positions.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3: Where RoPE is Used&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Every Transformer Block&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Each block in the DeepSeek model uses MLA and applies RoPE.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;During Both Training and Inference&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: RoPE is always on, helping the model understand the token sequence no matter the mode.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why RoPE is Perfect for Story Generation&lt;/h1&gt; &lt;p&gt;&lt;em&gt;In story generation, especially for children’s stories, context is everything.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;RoPE enables the model to:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Track who did what across paragraphs&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Maintain chronological consistency&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Preserve narrative flow even in long outputs&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Codebase:&lt;/em&gt; &lt;a href="https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T15:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lq5fqq</id>
    <title>I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source</title>
    <updated>2025-07-02T19:48:05+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt; &lt;img alt="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" src="https://preview.redd.it/nmerohq4miaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82000e0b8dd6c6f395384b8459f531f8884586e0" title="I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmerohq4miaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-02T19:48:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqyd4l</id>
    <title>Local vs Cloud AI in my time tracking app - the struggle is real</title>
    <updated>2025-07-03T19:21:28+00:00</updated>
    <author>
      <name>/u/tuanvuvn007</name>
      <uri>https://old.reddit.com/user/tuanvuvn007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyd4l/local_vs_cloud_ai_in_my_time_tracking_app_the/"&gt; &lt;img alt="Local vs Cloud AI in my time tracking app - the struggle is real" src="https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003ce18822c2f2ae159b9c1f8e471e951d30be2" title="Local vs Cloud AI in my time tracking app - the struggle is real" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I am building a time tracking app for mac that can automatically assign activities to the project without any manual assignment (at least that my goal).&lt;/p&gt; &lt;p&gt;Here the data that I track:&lt;br /&gt; - Window title&lt;br /&gt; - File path&lt;br /&gt; - URL (browser)&lt;br /&gt; - App name&lt;/p&gt; &lt;p&gt;From my experience with that limited data it very hard for the local LLM model to figure out which project that activities should belongs to. &lt;/p&gt; &lt;p&gt;I have tried to add more context to the prompt like most recent assignment but local LLM is still reliable enough.&lt;/p&gt; &lt;p&gt;I am using 3B up to 12B model (Gemma3 12B)&lt;/p&gt; &lt;p&gt;In the end I changed to use fastText (&lt;a href="https://fasttext.cc/"&gt;https://fasttext.cc/&lt;/a&gt;) to do the classification, the result is not that good compare to LLM but it way faster, I mean under 1 second prediction.&lt;/p&gt; &lt;p&gt;If anyone have any ideas to solve this problem, please let me know, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuanvuvn007"&gt; /u/tuanvuvn007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p91ir3elkpaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyd4l/local_vs_cloud_ai_in_my_time_tracking_app_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqyd4l/local_vs_cloud_ai_in_my_time_tracking_app_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T19:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqpm60</id>
    <title>[Upcoming Release &amp; Feedback] A new 4B &amp; 20B model, building on our SmallThinker work. Plus, a new hardware device to run them locally.</title>
    <updated>2025-07-03T13:28:43+00:00</updated>
    <author>
      <name>/u/yzmizeyu</name>
      <uri>https://old.reddit.com/user/yzmizeyu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;We're the startup team behind some of the projects you might be familiar with, including &lt;strong&gt;PowerInfer (&lt;a href="https://github.com/SJTU-IPADS/PowerInfer"&gt;https://github.com/SJTU-IPADS/PowerInfer&lt;/a&gt;)&lt;/strong&gt; and &lt;strong&gt;SmallThinker (&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-3B-Preview"&gt;https://huggingface.co/PowerInfer/SmallThinker-3B-Preview&lt;/a&gt;)&lt;/strong&gt;. The feedback from this community has been crucial, and we're excited to give you a heads-up on our next open-source release coming in &lt;strong&gt;late July&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We're releasing two new MoE models, both of which we have &lt;strong&gt;pre-trained from scratch&lt;/strong&gt; with a structure specifically optimized for efficient inference on edge devices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A new 4B Reasoning Model:&lt;/strong&gt; An evolution of SmallThinker with significantly improved logic capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 20B Model:&lt;/strong&gt; Designed for high performance in a local-first environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We'll be releasing the &lt;strong&gt;full weights, a technical report, and parts of the training dataset&lt;/strong&gt; for both.&lt;/p&gt; &lt;p&gt;Our core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we've also been developing a dedicated edge device. It's a small, self-contained unit (&lt;strong&gt;around 10x7x1.5 cm&lt;/strong&gt;) capable of running the 20B model completely offline with a power draw of &lt;strong&gt;around 30W&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is still a work in progress, but it proves what's possible with full-stack optimization. We'd love to get your feedback on this direction:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;For a compact, private device like this, what are the most compelling use cases you can imagine?&lt;/li&gt; &lt;li&gt;For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?&lt;/li&gt; &lt;li&gt;Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We'll be in the comments to answer questions. We're incredibly excited to share our work and believe local AI is the future we're all building together&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yzmizeyu"&gt; /u/yzmizeyu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T13:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqi863</id>
    <title>DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling</title>
    <updated>2025-07-03T06:08:33+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"&gt; &lt;img alt="DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling" src="https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01af29ebca7d21fb21f6786fc5df5242a0853781" title="DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33"&gt;https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/agentica-org/DeepSWE-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T06:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnczx</id>
    <title>AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework</title>
    <updated>2025-07-03T11:39:08+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"&gt; &lt;img alt="AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework" src="https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b7cac54a1763a205a826aebda0fe3bb69d0bd91" title="AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AIDC-AI/Ovis-U1-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T11:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqkknh</id>
    <title>Jan now supports MCP servers as an experimental feature</title>
    <updated>2025-07-03T08:44:41+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"&gt; &lt;img alt="Jan now supports MCP servers as an experimental feature" src="https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413b9cacad71e68f248ebbfebbea2dd7080cf113" title="Jan now supports MCP servers as an experimental feature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, this is Emre from the Jan team. &lt;/p&gt; &lt;p&gt;We've been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.&lt;/p&gt; &lt;p&gt;How to try MCP in Jan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Settings -&amp;gt; General -&amp;gt; toggle &amp;quot;Experimental Features&amp;quot;&lt;/li&gt; &lt;li&gt;A new &amp;quot;MCP Servers&amp;quot; tab appears -&amp;gt; add or enable your server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quick tip: To use MCP servers, make sure the model's Tools capability is enabled.&lt;/p&gt; &lt;p&gt;Full doc with screenshots: &lt;a href="https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan"&gt;https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.&lt;/p&gt; &lt;p&gt;Plus, since then we've pushed a few hot-fixes to smooth out model loading and MCP performance.&lt;/p&gt; &lt;p&gt;Other recent fixes &amp;amp; tweaks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).&lt;/li&gt; &lt;li&gt;We fixed a bug that caused some GGUF models to get stuck while loading.&lt;/li&gt; &lt;li&gt;Lighter UI polish and clearer error messages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this update, Jan now supports &lt;a href="https://huggingface.co/Menlo/Jan-nano-gguf"&gt;Jan-nano 4B &lt;/a&gt;as well, it's available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.&lt;/p&gt; &lt;p&gt;For the latest build, please update your Jan or &lt;a href="https://jan.ai/"&gt;download the latest&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8sdnjxd6emaf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T08:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqh55j</id>
    <title>No love for these new models?</title>
    <updated>2025-07-03T05:03:21+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dots&lt;/p&gt; &lt;p&gt;Minimax&lt;/p&gt; &lt;p&gt;Hunyuan&lt;/p&gt; &lt;p&gt;Ernie&lt;/p&gt; &lt;p&gt;I’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.&lt;/p&gt; &lt;p&gt;Sorry, just wanted to put this out here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T05:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqw2yg</id>
    <title>Deep Dive into Deep Research with Qwen3-30b-a3b</title>
    <updated>2025-07-03T17:50:31+00:00</updated>
    <author>
      <name>/u/charlie-woodworking</name>
      <uri>https://old.reddit.com/user/charlie-woodworking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"&gt; &lt;img alt="Deep Dive into Deep Research with Qwen3-30b-a3b" src="https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893874a6c9a7feb3582a1c15d40e9a7e7c407abe" title="Deep Dive into Deep Research with Qwen3-30b-a3b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recorded an explanation of how I architected, experimented with, and iterated on a custom deep research application using Qwen3-30b-a3b as the base model for a multi-agent orchestrated flow. Sprinkled in there are a few lessons I learned along the way.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=PCuBNUyS8Bc"&gt;https://www.youtube.com/watch?v=PCuBNUyS8Bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to hit me up with questions or discussions. This is the primary demo I'm giving at a tech conference in a few weeks so definitely open to improving it based on what folks want to know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charlie-woodworking"&gt; /u/charlie-woodworking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=PCuBNUyS8Bc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqxm89</id>
    <title>We Built an Open Source Clone of Lovable</title>
    <updated>2025-07-03T18:51:34+00:00</updated>
    <author>
      <name>/u/velobro</name>
      <uri>https://old.reddit.com/user/velobro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI-coding agents like Lovable and Bolt are taking off, but it's still not widely known how they actually work.&lt;/p&gt; &lt;p&gt;We decided to build an open-source Lovable clone that includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Structured prompts using BAML (like RPCs for LLMs)&lt;/li&gt; &lt;li&gt;Secure sandboxing for generated code&lt;/li&gt; &lt;li&gt;Real-time previews with WebSockets and FastAPI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're curious about how agentic apps work under the hood or want to build your own, this might help. Everything we learned is in the blog post below, and you can see all the code on Github.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog Post&lt;/strong&gt;: &lt;a href="https://www.beam.cloud/blog/agentic-apps"&gt;https://www.beam.cloud/blog/agentic-apps&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/beam-cloud/lovable-clone"&gt;https://github.com/beam-cloud/lovable-clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know if you have feedback or if there's anything we missed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/velobro"&gt; /u/velobro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T18:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqx16</id>
    <title>Kyutai Unmute (incl. TTS) released</title>
    <updated>2025-07-03T14:25:11+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmute github: &lt;a href="https://github.com/kyutai-labs/unmute"&gt;https://github.com/kyutai-labs/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unmute blog: &lt;a href="https://kyutai.org/next/unmute"&gt;https://kyutai.org/next/unmute&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS blog with a demo: &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS weights: &lt;a href="https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29"&gt;https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;STT was released earlier so the whole component stack is now out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqycp0</id>
    <title>Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation</title>
    <updated>2025-07-03T19:20:57+00:00</updated>
    <author>
      <name>/u/pheonis2</name>
      <uri>https://old.reddit.com/user/pheonis2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt; &lt;img alt="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" src="https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb529167e09a3692724b342df0121216749b7bd" title="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd"&gt;https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.&lt;/p&gt; &lt;p&gt;It’s super fast, starting to generate audio in just ~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.&lt;/p&gt; &lt;p&gt;You can also clone voices with just 10 seconds of audio.&lt;/p&gt; &lt;p&gt;And yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/|&lt;/a&gt;&lt;br /&gt; Huggingface: &lt;a href="https://huggingface.co/kyutai/tts-1.6b-en_fr"&gt;https://huggingface.co/kyutai/tts-1.6b-en_fr&lt;/a&gt;&lt;br /&gt; &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pheonis2"&gt; /u/pheonis2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T19:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqxhq</id>
    <title>I have made a True Reasoning LLM</title>
    <updated>2025-07-03T14:25:42+00:00</updated>
    <author>
      <name>/u/moilanopyzedev</name>
      <uri>https://old.reddit.com/user/moilanopyzedev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt; &lt;p&gt;You can get it here&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/moelanoby/phi-3-M3-coder"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moilanopyzedev"&gt; /u/moilanopyzedev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqnwih</id>
    <title>I can't believe it actually runs - Qwen 235b @ 16GB VRAM</title>
    <updated>2025-07-03T12:07:58+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by this post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I decided to try my luck with Qwen 235b so downloaded Unsloth's Q2XL. I've got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).&lt;/p&gt; &lt;p&gt;My runtime args:&lt;/p&gt; &lt;p&gt;llama-cli -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;/p&gt; &lt;p&gt;Super simple user prompt because I wasn't expecting miracles:&lt;/p&gt; &lt;p&gt;tell me a joke&lt;/p&gt; &lt;p&gt;Result:&lt;br /&gt; 8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven't tried any actual work on it yet.&lt;/p&gt; &lt;p&gt;cli output blurb:&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 24.81 ms / 476 runs ( 0.05 ms per token, 19183.49 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: load time = 16979.96 ms&lt;/p&gt; &lt;p&gt;llama_perf_context_print: prompt eval time = 1497.01 ms / 12 tokens ( 124.75 ms per token, 8.02 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: eval time = 85040.21 ms / 463 runs ( 183.67 ms per token, 5.44 tokens per second)&lt;/p&gt; &lt;p&gt;llama_perf_context_print: total time = 100251.11 ms / 475 tokens&lt;/p&gt; &lt;p&gt;Question:&lt;/p&gt; &lt;p&gt;It looks like I'm only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Managed to fill out the rest of the VRAM with a draft model. &lt;/p&gt; &lt;p&gt;Generation went up to 9.8t/s:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T12:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqvovt</id>
    <title>A project to bring CUDA to non-Nvidia GPUs is making major progress</title>
    <updated>2025-07-03T17:35:16+00:00</updated>
    <author>
      <name>/u/OwnWitness2836</name>
      <uri>https://old.reddit.com/user/OwnWitness2836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt; &lt;img alt="A project to bring CUDA to non-Nvidia GPUs is making major progress" src="https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f" title="A project to bring CUDA to non-Nvidia GPUs is making major progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnWitness2836"&gt; /u/OwnWitness2836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T17:35:16+00:00</published>
  </entry>
</feed>
