<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-14T09:34:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ip2ze3</id>
    <title>Is there any model that can run suficiently well on the avarege smartphone?</title>
    <updated>2025-02-14T04:56:38+00:00</updated>
    <author>
      <name>/u/Donshio</name>
      <uri>https://old.reddit.com/user/Donshio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My idea is to make an app that can extract meaningfull details from a conversation with a human user, and fill in a template with specific key details for that conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Donshio"&gt; /u/Donshio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2ze3/is_there_any_model_that_can_run_suficiently_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2ze3/is_there_any_model_that_can_run_suficiently_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2ze3/is_there_any_model_that_can_run_suficiently_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T04:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iozmns</id>
    <title>Best small model for function calling?</title>
    <updated>2025-02-14T01:52:00+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best small model for function calling without having to fine tune on my own. &lt;/p&gt; &lt;p&gt;&amp;lt; 5b model with computer use would be awesome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iozmns/best_small_model_for_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iozmns/best_small_model_for_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iozmns/best_small_model_for_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T01:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iouhbo</id>
    <title>Running Deepseek R1 discussion on level 1 techs</title>
    <updated>2025-02-13T21:45:39+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's a decent discussion on level 1 techs on running Deepseek R1 for anyone who's interested&lt;/p&gt; &lt;p&gt;It dives into some thorough benchmarks for your system to understand how your memory and disks are performing&lt;/p&gt; &lt;p&gt;&lt;a href="https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4"&gt;https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iouhbo/running_deepseek_r1_discussion_on_level_1_techs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioxxb5</id>
    <title>LM2: Large Memeory Models</title>
    <updated>2025-02-14T00:24:03+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.06049"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioxxb5/lm2_large_memeory_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioxxb5/lm2_large_memeory_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T00:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can’t actually see or look close. It can’t zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ion5at</id>
    <title>i built a free, open-source video transcription tool alternative to happyscribe</title>
    <updated>2025-02-13T16:35:41+00:00</updated>
    <author>
      <name>/u/ShakaLaka_Around</name>
      <uri>https://old.reddit.com/user/ShakaLaka_Around</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks,&lt;/p&gt; &lt;p&gt;after spending months building a video transcription service and failing to turn it into a viable business, I decided to open-source the entire thing. It's called halfway, and it might be useful for anyone needing reliable video/audio transcription.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast transcription of any audio/video file&lt;/li&gt; &lt;li&gt;Speaker detection/diarization&lt;/li&gt; &lt;li&gt;Clean, minimal editor interface&lt;/li&gt; &lt;li&gt;Export to SRT, VTT, CSV, TXT, JSON, PDF&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you'll need your own AssemblyAI API key to run it, but they offer a free tier with 50$ of transcription. more models &amp;amp; ollama will be supported in the near future.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="http://github.com/moaljumaa/halfwayml_open"&gt;github.com/moaljumaa/halfwayml_open&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hope it solves a problem for any of you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShakaLaka_Around"&gt; /u/ShakaLaka_Around &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ion5at/i_built_a_free_opensource_video_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T16:35:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip6w30</id>
    <title>Did any of you try to RAG or train on an existing codebase?</title>
    <updated>2025-02-14T09:28:42+00:00</updated>
    <author>
      <name>/u/hugthemachines</name>
      <uri>https://old.reddit.com/user/hugthemachines</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I thought a bit about training a model, perhaps qwen2.5 coder on a codebase so that a developer later could ask for suggestions on how to code a planned, added feature of that codebase.&lt;/p&gt; &lt;p&gt;Have any of you tried it, and how well did you think it worked?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hugthemachines"&gt; /u/hugthemachines &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6w30/did_any_of_you_try_to_rag_or_train_on_an_existing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6w30/did_any_of_you_try_to_rag_or_train_on_an_existing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6w30/did_any_of_you_try_to_rag_or_train_on_an_existing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T09:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip5n0u</id>
    <title>How to disable small concepts in the model?</title>
    <updated>2025-02-14T07:54:42+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I have heard about obliteration. But it probably hurts LLM's performance, is it? I need something where I can disable certain concepts in LLM. For example, it will always think that it is a human and will never talk about AI, or instead of liking green color like blue color.&lt;/p&gt; &lt;p&gt;As you can see, it is not about enabling NSFW, it's just small adjustments. Do you know how to do that? Any code would be appreciated! For LLaMA 3.1 8B running on Google Colab A100. Better raw text. I tried to do it with just fine-tuning and DPO, but it still sometimes fails!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip5n0u/how_to_disable_small_concepts_in_the_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip5n0u/how_to_disable_small_concepts_in_the_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip5n0u/how_to_disable_small_concepts_in_the_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T07:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iolo5o</id>
    <title>SWE-agent is the new open-source SOTA on SWE-bench Lite. It run locally as well!</title>
    <updated>2025-02-13T15:32:53+00:00</updated>
    <author>
      <name>/u/ofirpress</name>
      <uri>https://old.reddit.com/user/ofirpress</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SWE-agent is an open source software engineering agent that works with any kind of model. Our 1.0 release adds tons of new features: massively parallel runs; cloud-based deployment; extensive configurability with tool bundles; new command line interface &amp;amp; utilities. Completely open-source (MIT), extensive configuration, easy to hack. Since it uses LiteLLM for LM interfacing, you can use it with a local LM: we've used it with Qwen and other community members have used it with Llama. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/swe-agent/swe-agent"&gt;https://github.com/swe-agent/swe-agent&lt;/a&gt; &lt;/p&gt; &lt;p&gt;SWE-agent is now powered by our new SWE-ReX package, a lightweight, general purpose sandboxed code execution engine that supports local Docker, AWS, Modal deployments &lt;a href="https://github.com/SWE-agent/swe-rex"&gt;https://github.com/SWE-agent/swe-rex&lt;/a&gt;. You can use it to easily build your own agent with code execution from scratch without the hassle of figuring out how to communicate with running docker containers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ofirpress"&gt; /u/ofirpress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iolo5o/sweagent_is_the_new_opensource_sota_on_swebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T15:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip64n6</id>
    <title>AirLLM vs CPU inference ?</title>
    <updated>2025-02-14T08:30:44+00:00</updated>
    <author>
      <name>/u/BraceletGrolf</name>
      <uri>https://old.reddit.com/user/BraceletGrolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone !&lt;/p&gt; &lt;p&gt;I'm considering trying out some AI tools that require big models, but I don't have the VRAM to run the whole model, and I wonder if it wouldn't be faster to run something like LLama 90B with AirLLM instead of just offloading some layers.&lt;/p&gt; &lt;p&gt;So has anyone tried AirLLM to compare ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BraceletGrolf"&gt; /u/BraceletGrolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip64n6/airllm_vs_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip64n6/airllm_vs_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip64n6/airllm_vs_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T08:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip6c9e</id>
    <title>Looking to buy two arc a770 16gb for llm inference. Good idea?</title>
    <updated>2025-02-14T08:47:23+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that some of you use single Intel GPUs successfully. But has someone had success using a couple of them to take advantage of combined VRAM? Is it doable without extreme speed losses? ☺️&lt;/p&gt; &lt;p&gt;I'm mainly interested in arc a700 because they're very cheap and new where I am at. And used 3090s are of very dubious quality here, and its prices are going up and up... 😭&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6c9e/looking_to_buy_two_arc_a770_16gb_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6c9e/looking_to_buy_two_arc_a770_16gb_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip6c9e/looking_to_buy_two_arc_a770_16gb_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T08:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioikl0</id>
    <title>Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445</title>
    <updated>2025-02-13T13:03:22+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt; &lt;img alt="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" src="https://preview.redd.it/8u7jixwzmwie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f7ab3742f3fd3c2245bf8eadfbaad2fecacd6ac" title="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8u7jixwzmwie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip2rk5</id>
    <title>I noticed a couple discussions surrounding the w7900 gpu. Is ROCm getting to the point where it’s usable for local ai?</title>
    <updated>2025-02-14T04:43:48+00:00</updated>
    <author>
      <name>/u/Euphoric_Ad9500</name>
      <uri>https://old.reddit.com/user/Euphoric_Ad9500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve completely dismissed any amd gpu for AI another than the mi300x due to the lack of documentation and support but this was in 2022-2023. How is it looking right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Ad9500"&gt; /u/Euphoric_Ad9500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2rk5/i_noticed_a_couple_discussions_surrounding_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2rk5/i_noticed_a_couple_discussions_surrounding_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip2rk5/i_noticed_a_couple_discussions_surrounding_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T04:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioj9ly</id>
    <title>Hugging Face just open sourced the free agents course!</title>
    <updated>2025-02-13T13:40:53+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt; &lt;img alt="Hugging Face just open sourced the free agents course!" src="https://external-preview.redd.it/dZG5o7Z-X0P3aHLdfTQ585OGGBpbWn7hxgShkrQ_wfw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c62c131ea4a762f597958b6a4288a1baf7a8d965" title="Hugging Face just open sourced the free agents course!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/agents-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohk4o</id>
    <title>Let's build DeepSeek from Scratch | Taught by MIT PhD graduate</title>
    <updated>2025-02-13T12:03:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt; &lt;img alt="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vjwhw6ticwie1.gif"&gt;https://i.redd.it/vjwhw6ticwie1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us for the 6pm Youtube premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ever since DeepSeek was launched, everyone is focused on: &lt;/p&gt; &lt;p&gt;- Flashy headlines&lt;/p&gt; &lt;p&gt;- Company wars&lt;/p&gt; &lt;p&gt;- Building LLM applications powered by DeepSeek&lt;/p&gt; &lt;p&gt;I very strongly think that students, researchers, engineers and working professionals should focus on the foundations. &lt;/p&gt; &lt;p&gt;The real question we should ask ourselves is: &lt;/p&gt; &lt;p&gt;“Can I build the DeepSeek architecture and model myself, from scratch?”&lt;/p&gt; &lt;p&gt;If you ask this question, you will discover that to make DeepSeek work, there are a number of key ingredients which play a role:&lt;/p&gt; &lt;p&gt;(1) Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;(2) Multi-head Latent Attention (MLA)&lt;/p&gt; &lt;p&gt;(3) Rotary Positional Encodings (RoPE)&lt;/p&gt; &lt;p&gt;(4) Multi-token prediction (MTP)&lt;/p&gt; &lt;p&gt;(5) Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;(6) Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;My aim with the “Build DeepSeek from Scratch” playlist is: &lt;/p&gt; &lt;p&gt;- To teach you the mathematical foundations behind all the 6 ingredients above.&lt;/p&gt; &lt;p&gt;- To code all 6 ingredients above, from scratch.&lt;/p&gt; &lt;p&gt;- To assemble these ingredients and to run a “mini Deep-Seek” on your own.&lt;/p&gt; &lt;p&gt;After this, you will among the top 0.1%. of ML/LLM engineers who can build DeepSeek ingredients on their own.&lt;/p&gt; &lt;p&gt;This playlist won’t be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours. &lt;/p&gt; &lt;p&gt;It will be in-depth. No fluff. Solid content. &lt;/p&gt; &lt;p&gt;Join us for the 6pm premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S: Attached is a small GIF showing the notes we have made. This is just 5-10% of the total amount of notes and material we have prepared for this series!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioueat</id>
    <title>MatterGen - eh, let's go ahead and change the world right quick</title>
    <updated>2025-02-13T21:41:58+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Creating novel materials with diffusion models. &lt;/p&gt; &lt;p&gt;Code...&lt;br /&gt; Yes.&lt;br /&gt; &lt;a href="https://github.com/microsoft/mattergen"&gt;https://github.com/microsoft/mattergen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/"&gt;https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioueat/mattergen_eh_lets_go_ahead_and_change_the_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:41:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iolxnb</id>
    <title>A live look at the ReflectionR1 distillation process…</title>
    <updated>2025-02-13T15:44:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"&gt; &lt;img alt="A live look at the ReflectionR1 distillation process…" src="https://preview.redd.it/e851xee0gxie1.gif?width=216&amp;amp;crop=smart&amp;amp;s=148dc8683c793423d50c77fc3ceaf9b8b4b9d303" title="A live look at the ReflectionR1 distillation process…" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e851xee0gxie1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iolxnb/a_live_look_at_the_reflectionr1_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T15:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iozydx</id>
    <title>AIME 2025 scores of the distilled R1 models are really impressive considering how little data was needed for this boost</title>
    <updated>2025-02-14T02:09:14+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iozydx/aime_2025_scores_of_the_distilled_r1_models_are/"&gt; &lt;img alt="AIME 2025 scores of the distilled R1 models are really impressive considering how little data was needed for this boost" src="https://b.thumbs.redditmedia.com/rqia3TBJ7Bj6KPbPWtOLAeRX_1yws7CfT26rzCi0W3w.jpg" title="AIME 2025 scores of the distilled R1 models are really impressive considering how little data was needed for this boost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iozydx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iozydx/aime_2025_scores_of_the_distilled_r1_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iozydx/aime_2025_scores_of_the_distilled_r1_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T02:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioun7d</id>
    <title>TransformerLab - Generate Datasets and FineTune LLMs on them</title>
    <updated>2025-02-13T21:52:59+00:00</updated>
    <author>
      <name>/u/Firm-Development1953</name>
      <uri>https://old.reddit.com/user/Firm-Development1953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"&gt; &lt;img alt="TransformerLab - Generate Datasets and FineTune LLMs on them" src="https://external-preview.redd.it/enAzdmN2MWQ5emllMZpy0iTD7NNvaDxqshMpw7GdO8fY3vqdzTO6gEvuQwaM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33dd2b3f9422dac66a9b82657ae76ab85a0017c1" title="TransformerLab - Generate Datasets and FineTune LLMs on them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm-Development1953"&gt; /u/Firm-Development1953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xyvsqv1d9zie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioun7d/transformerlab_generate_datasets_and_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iou563</id>
    <title>Nous DeepHermes-3 8B</title>
    <updated>2025-02-13T21:30:54+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Introducing DeepHermes-3 Preview, a new LLM that unifies reasoning and intuitive language model capabilities.&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview&lt;/a&gt; GGUF Quants: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DeepHermes 3 is built from the Hermes 3 datamix, with new reasoning data, creating a model that can toggle on and off long chains of thought for improved accuracy at the cost of more test time compute!&lt;/p&gt; &lt;p&gt;This is our first work on reasoning models, and hope our unique approach to user controlled, toggleable reasoning mode furthers our mission of giving those who use DeepHermes more steerability for whatever need they have.&lt;/p&gt; &lt;p&gt;These early benchmarks show extreme improvement in Mathematical reasoning capabilities when enabled, as well as a modest improvement in GPQA (Google Proof Question Answering) benchmarks&lt;/p&gt; &lt;p&gt;As this is an experimental preview, there is much work to discover the full extent of reasoning generalization, quirks or issues, and much more. &lt;/p&gt; &lt;p&gt;We hope the community will help us in exploring the model and new reasoning paradigm on all sorts of tasks and usecases. We looking forward to hearing your feedback on how we can improve the deep reasoning models we make in the future!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;em&gt;FYI, I'm not from Hermes, just copied this message.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iou563/nous_deephermes3_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T21:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioybsf</id>
    <title>I Live-Streamed DeepSeek R-1 671B-q4 Running w/ KTransformers on Epyc 7713, 512GB RAM, and 14x RTX 3090s</title>
    <updated>2025-02-14T00:44:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends, if anyone remembers me, I am the guy with the &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hi24k9/home_server_final_boss_14x_rtx_3090_build/?sort=new"&gt;14x RTX 3090s in his basement&lt;/a&gt;, AKA &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1gjje70/now_i_need_to_explain_this_to_her/lvdk9d1/"&gt;LocalLLaMA Home Server Final Boss&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Last week, seeing the post on &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/"&gt;KTransformers Optimizations for the DeepSeek R-1 671B model&lt;/a&gt; I decided I will try it on my AI Server, which has a single Epyc 7713 CPU w/ 64 Cores/128 Threads, 512GB DDR4 3200MHZ RAM, and 14x RTX 3090s. I &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ilzcwm/671b_deepseekr1v3q4_on_a_single_machine_2_xeon/mc05taq/?context=3"&gt;commented&lt;/a&gt; on that post initially with my plans on doing a test run on my Epyc 7004 Platform CPU given that the KTransformers team benchmarked on an an Intel Dual-Socket DDR5 Xeon Server, which supports more optimized MoE kernels than that of the Epyc 7004 Platform. However, I decided to livestream the entire thing from A-to-Z.&lt;/p&gt; &lt;p&gt;This was my first live stream (please be nice to me :D), so it is actually quite long, and given the sheer number of people that were watching, I decided to showcase different things that I do on my AI Server (vLLM and ExLlamaV2 runs and comparisons w/ OpenWeb-UI). In case you're just interested in the evaluation numbers, I asked the model &lt;code&gt;How many 'r's are in the word &amp;quot;strawberry&amp;quot;?&lt;/code&gt; and the &lt;a href="https://x.com/TheAhmadOsman/status/1889770367033426097"&gt;evaluation numbers can be found here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you wanna watch the model running and offloading a single layer (13GB) on the GPU with 390GB of the weights being offloaded to the CPU, at the &lt;a href="https://www.youtube.com/watch?v=9tQeXB5wiwM&amp;amp;t=6000s"&gt;1:39:59 timestamp of the recording&lt;/a&gt;. I did multiple runs with multiple settings changes (token generation length, number of threads, etc), and I also did multiple llama.cpp runs with the same exact model to see if the reported improvements by the KTransformers team matched it. W/ my llama.cpp runs, I offloaded as many layers to my 14x RTX 3090s first, an then I did 1 layer only offloaded to a single GPU like the test run with KTransformers, and I show and compare the evaluation numbers of these runs with the one using KTransformers starting from the &lt;a href="https://www.youtube.com/watch?v=9tQeXB5wiwM&amp;amp;t=15149s"&gt;4:12:29 timestamp of the recording&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, my cat arrives to claim his designated chair in my office at the &lt;a href="https://www.youtube.com/watch?v=9tQeXB5wiwM&amp;amp;t=10140s"&gt;2:49:00 timestamp of the recording&lt;/a&gt; in case you wanna see something funny :D&lt;/p&gt; &lt;p&gt;Funny enough, last week I wrote a blogbost on &lt;a href="https://ahmadosman.com/blog/do-not-use-llama-cpp-or-ollama-on-multi-gpus-setups-use-vllm-or-exllamav2/"&gt;Multi-GPU Setups With llama.cpp being a waste&lt;/a&gt; and I shared it &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"&gt;here&lt;/a&gt; only for me to end up running llama.cpp on a live stream this week hahaha.&lt;/p&gt; &lt;p&gt;Please let me know your thoughts or if you have any questions. I also wanna stream again, so please let me know if you have any interesting ideas for things to do with an AI server like mine, and I'll do my best to live stream it. Maybe you can even join as a guest, and we can do it live together!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; &lt;a href="https://x.com/TheAhmadOsman/status/1889770367033426097"&gt;Evaluation numbers can be found here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; I ran the &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;v0.3 of KTransformers&lt;/a&gt; by building it from source. In fact, building KTransformers v0.3 from source (and llama.cpp main branch latest) took a big chunk of the stream, but I wanted to just go live and do my usual thing rather than being nervous about what I am going to present.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; Expanding my the TL;DR: The prompt eval is a very important factor here. An identical run configuration with &lt;code&gt;llama.cpp&lt;/code&gt; showed that the prompt evaluation speed pretty much had a 15x speed increase under &lt;code&gt;KTransformers&lt;/code&gt;. The full numbers are below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt Eval:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;prompt eval count&lt;/strong&gt;: 14 token(s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;prompt eval duration&lt;/strong&gt;: 1.5244331359863281s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;prompt eval rate&lt;/strong&gt;: 9.183741595161415 tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Generation Eval:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;eval count&lt;/strong&gt;: 805 token(s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eval duration&lt;/strong&gt;: 97.70413899421692s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eval rate&lt;/strong&gt;: 8.239159653693358 tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Edit 3:&lt;/strong&gt; Just uploaded a &lt;a href="https://www.youtube.com/watch?v=9tQeXB5wiwM&amp;amp;ab_channel=TheAIServerGuy"&gt;YouTube video&lt;/a&gt; and updated the timestamps accordingly. If you're into LLMs and AI, feel free to subscribe—I’ll be streaming regularly with more content!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioybsf/i_livestreamed_deepseek_r1_671bq4_running_w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioybsf/i_livestreamed_deepseek_r1_671bq4_running_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioybsf/i_livestreamed_deepseek_r1_671bq4_running_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T00:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip4jpx</id>
    <title>This is why we need open weights reasoning models (response from o1)</title>
    <updated>2025-02-14T06:36:10+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip4jpx/this_is_why_we_need_open_weights_reasoning_models/"&gt; &lt;img alt="This is why we need open weights reasoning models (response from o1)" src="https://preview.redd.it/avuuy23zu1je1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55dba5831ea62cae9b08fa3e3a446addc3c7eae7" title="This is why we need open weights reasoning models (response from o1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/avuuy23zu1je1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip4jpx/this_is_why_we_need_open_weights_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip4jpx/this_is_why_we_need_open_weights_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T06:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip53bj</id>
    <title>SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency</title>
    <updated>2025-02-14T07:14:04+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip53bj/sambanova_launches_the_fastest_deepseekr1_671b/"&gt; &lt;img alt="SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency" src="https://external-preview.redd.it/8zVSNjnKJ_Ox162z35gXkBL65KEBJ2FOQQvNJjA_uWE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3697c6fbcc2dde718d7194a242cd886f11e06507" title="SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip53bj/sambanova_launches_the_fastest_deepseekr1_671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip53bj/sambanova_launches_the_fastest_deepseekr1_671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T07:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ip33v1</id>
    <title>I am considering buying a Mac Studio for running local LLMs. Going for maximum RAM but does the GPU core count make a difference that justifies the extra $1k?</title>
    <updated>2025-02-14T05:03:31+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip33v1/i_am_considering_buying_a_mac_studio_for_running/"&gt; &lt;img alt="I am considering buying a Mac Studio for running local LLMs. Going for maximum RAM but does the GPU core count make a difference that justifies the extra $1k?" src="https://preview.redd.it/gc5p44pee1je1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ba52862283a2e5a6c93fa8fcb1442fa2fceda20" title="I am considering buying a Mac Studio for running local LLMs. Going for maximum RAM but does the GPU core count make a difference that justifies the extra $1k?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc5p44pee1je1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ip33v1/i_am_considering_buying_a_mac_studio_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ip33v1/i_am_considering_buying_a_mac_studio_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T05:03:31+00:00</published>
  </entry>
</feed>
