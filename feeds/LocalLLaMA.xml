<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-07T20:06:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kgpujo</id>
    <title>ik_llama and ktransformers are fast, but they completely break OpenAI style tool calling and structured responses</title>
    <updated>2025-05-07T05:35:46+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been testing local LLM frameworks like &lt;strong&gt;ik_llama&lt;/strong&gt; and &lt;strong&gt;ktransformers&lt;/strong&gt; because they offer great performance on large moe models like Qwen3-235B and DeepSeek-V3-0324 685billion parameters.&lt;/p&gt; &lt;p&gt;But there’s a serious issue I haven’t seen enough people talk about them breaking OpenAI-compatible features like tool calling and structured JSON responses. Even though they expose a &lt;code&gt;/v1/chat/completions&lt;/code&gt; endpoint and claim OpenAI compatibility, neither &lt;code&gt;ik_llama&lt;/code&gt; nor &lt;code&gt;ktransformers&lt;/code&gt; properly handle: the tools or function field in a request or emitting valid JSON when expected&lt;/p&gt; &lt;p&gt;To work around this, I wrote a local wrapper that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;intercepts chat completions&lt;/li&gt; &lt;li&gt;enriches prompts with tool metadata&lt;/li&gt; &lt;li&gt;parses and transforms the output into OpenAI-compatible responses&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This lets me continue using fast backends while preserving tool calling logic.&lt;br /&gt; If anyone else is hitting this issue: how are you solving it?&lt;/p&gt; &lt;p&gt;I’m curious if others are patching the backend, modifying prompts, or intercepting responses like I am. Happy to share details if people are interested in the wrapper.&lt;/p&gt; &lt;p&gt;If you want to make use of my hack here is the repo for it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Teachings/FastAgentAPI"&gt;https://github.com/Teachings/FastAgentAPI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also did a walkthrough of how to set it up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=JGo9HfkzAmc"&gt;https://www.youtube.com/watch?v=JGo9HfkzAmc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgpujo/ik_llama_and_ktransformers_are_fast_but_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T05:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgqw08</id>
    <title>Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi’s LLM benchmark</title>
    <updated>2025-05-07T06:45:56+00:00</updated>
    <author>
      <name>/u/Shamp0oo</name>
      <uri>https://old.reddit.com/user/Shamp0oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"&gt; &lt;img alt="Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi’s LLM benchmark" src="https://external-preview.redd.it/Hw7kJv_JneYm55PcK7LkdRzAPhO5o4gr4OcKc4__3Nw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad6cf9e97cf2006fdfe2b75e079d91ccb83a824" title="Qwen3-235B-A22B and Qwen3-14B rank 2nd and 4th on Kagi’s LLM benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shamp0oo"&gt; /u/Shamp0oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://help.kagi.com/kagi/ai/llm-benchmark.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgqw08/qwen3235ba22b_and_qwen314b_rank_2nd_and_4th_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T06:45:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzb0c</id>
    <title>What’s Your Current Daily Driver Model and Setup?</title>
    <updated>2025-05-07T14:52:33+00:00</updated>
    <author>
      <name>/u/jedsk</name>
      <uri>https://old.reddit.com/user/jedsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Local gang,&lt;/p&gt; &lt;p&gt;What's your daily driver model these days? Would love to hear about your go to setups, preferred models + quants, and use cases. Just curious to know what's working well for everyone and find some new inspiration!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interface:&lt;/strong&gt; Ollama + OWUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; Gemma3:27b-fp16 and Qwen3:32b-fp16 (12k ctx)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; 4x RTX 3090s + Threadripper 3975WX + 256GB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use Case:&lt;/strong&gt; Enriching scraped data with LLMs for insight extraction and opportunity detection&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for sharing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jedsk"&gt; /u/jedsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzb0c/whats_your_current_daily_driver_model_and_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzb0c/whats_your_current_daily_driver_model_and_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzb0c/whats_your_current_daily_driver_model_and_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T14:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh4lbl</id>
    <title>LLMs play Wikipedia race</title>
    <updated>2025-05-07T18:23:37+00:00</updated>
    <author>
      <name>/u/loubnabnl</name>
      <uri>https://old.reddit.com/user/loubnabnl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh4lbl/llms_play_wikipedia_race/"&gt; &lt;img alt="LLMs play Wikipedia race" src="https://external-preview.redd.it/afJBw1uQ95TJ6CIORq1av9mj6DnDp__NkJYkqs5ol3g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fd8493f4cc69226985b23e4e6a1b44fc2547926" title="LLMs play Wikipedia race" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jorv9exzjeze1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b63addbe645329e368a3086adef59356cd6e1ee1"&gt;https://preview.redd.it/jorv9exzjeze1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b63addbe645329e368a3086adef59356cd6e1ee1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Watch Qwen3 and DeepSeek play Wikipedia game to connect distant pages &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/wikiracing-llms"&gt;https://huggingface.co/spaces/HuggingFaceTB/wikiracing-llms&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loubnabnl"&gt; /u/loubnabnl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh4lbl/llms_play_wikipedia_race/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh4lbl/llms_play_wikipedia_race/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh4lbl/llms_play_wikipedia_race/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgupcy</id>
    <title>Gemini 2.5 Pro 05-06 (IO Edition)</title>
    <updated>2025-05-07T11:12:09+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"&gt; &lt;img alt="Gemini 2.5 Pro 05-06 (IO Edition)" src="https://external-preview.redd.it/AcvY2iQydAkuLrR9B5tut99az-M6OzZYxQpuRtv_6NM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b24b4187cb1c776988e7af64e34ada19e1c9936b" title="Gemini 2.5 Pro 05-06 (IO Edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kgupcy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgupcy/gemini_25_pro_0506_io_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgwxeo</id>
    <title>Faster open webui title generation for Qwen3 models</title>
    <updated>2025-05-07T13:08:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you use Qwen3 in Open WebUI, by default, WebUI will use Qwen3 for title generation with reasoning turned on, which is really unnecessary for this simple task.&lt;/p&gt; &lt;p&gt;Simply adding &amp;quot;/no_think&amp;quot; to the end of the title generation prompt can fix the problem.&lt;/p&gt; &lt;p&gt;Even though they &lt;strong&gt;&lt;em&gt;&amp;quot;hide&amp;quot;&lt;/em&gt;&lt;/strong&gt; the title generation prompt for some reason, you can search their GitHub to find all of their default prompts. Here is the title generation one with &amp;quot;/no_think&amp;quot; added to the end of it:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;By the way are there any good webui alternative to this one? I tried librechat but it's not friendly to local inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;### Task: Generate a concise, 3-5 word title with an emoji summarizing the chat history. ### Guidelines: - The title should clearly represent the main theme or subject of the conversation. - Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting. - Write the title in the chat's primary language; default to English if multilingual. - Prioritize accuracy over excessive creativity; keep it clear and simple. ### Output: JSON format: { &amp;quot;title&amp;quot;: &amp;quot;your concise title here&amp;quot; } ### Examples: - { &amp;quot;title&amp;quot;: &amp;quot;📉 Stock Market Trends&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;🍪 Perfect Chocolate Chip Recipe&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Evolution of Music Streaming&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Remote Work Productivity Tips&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Artificial Intelligence in Healthcare&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;🎮 Video Game Development Insights&amp;quot; } ### Chat History: &amp;lt;chat_history&amp;gt; {{MESSAGES:END:2}} &amp;lt;/chat_history&amp;gt; /no_think &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And here is a faster one with chat history limited to 2k tokens to improve title generation speed:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Task: Generate a concise, 3-5 word title with an emoji summarizing the chat history. ### Guidelines: - The title should clearly represent the main theme or subject of the conversation. - Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting. - Write the title in the chat's primary language; default to English if multilingual. - Prioritize accuracy over excessive creativity; keep it clear and simple. ### Output: JSON format: { &amp;quot;title&amp;quot;: &amp;quot;your concise title here&amp;quot; } ### Examples: - { &amp;quot;title&amp;quot;: &amp;quot;📉 Stock Market Trends&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;🍪 Perfect Chocolate Chip Recipe&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Evolution of Music Streaming&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Remote Work Productivity Tips&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;Artificial Intelligence in Healthcare&amp;quot; }, - { &amp;quot;title&amp;quot;: &amp;quot;🎮 Video Game Development Insights&amp;quot; } ### Chat History: &amp;lt;chat_history&amp;gt; {{prompt:start:1000}} {{prompt:end:1000}} &amp;lt;/chat_history&amp;gt; /no_think &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgwxeo/faster_open_webui_title_generation_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgs1z7</id>
    <title>3090+3060+3060 llama.cpp benchmarks / tips</title>
    <updated>2025-05-07T08:10:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"&gt; &lt;img alt="3090+3060+3060 llama.cpp benchmarks / tips" src="https://a.thumbs.redditmedia.com/DbTGh_0uq0Zr27C4QY6dD_OTncYdCY186kdH1HyuqI8.jpg" title="3090+3060+3060 llama.cpp benchmarks / tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Building LocalLlama Machine – Episode 3: Performance Optimizations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the previous episode, I had all three GPUs mounted directly in the motherboard slots. Now, I’ve &lt;strong&gt;moved one 3090 onto a riser&lt;/strong&gt; to make it a bit happier. Let’s use this setup for benchmarking.&lt;/p&gt; &lt;p&gt;Some people ask whether it's allowed to mix different GPUs, in this tutorial, I’ll explain how to handle that topic.&lt;/p&gt; &lt;p&gt;First, let’s try some smaller models. In the first screenshot, you can see the results for Qwen3 8B and Qwen3 14B. These models are small enough to fit entirely inside a 3090, so the 3060s are not needed. If we disable them, we see a performance boost: from &lt;strong&gt;48 to 82&lt;/strong&gt; tokens per second, and from &lt;strong&gt;28 to 48&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Next, we switch to &lt;strong&gt;Qwen3 32B&lt;/strong&gt;. This model is larger, and to run it in Q8, you need more than a single 3090. However, in &lt;code&gt;llama.cpp&lt;/code&gt;, we can control how the tensors are split. For example, we can allocate more memory on the first card and less on the second and third. These values are discovered experimentally for each model, so your optimal settings may vary. If the values are incorrect, the model won't load, for instance, it might try to allocate 26GB on a 24GB GPU.&lt;/p&gt; &lt;p&gt;We can improve performance from the default &lt;strong&gt;13.0&lt;/strong&gt; tokens per second to &lt;strong&gt;15.6&lt;/strong&gt; by adjusting the tensor split. Furthermore, we can go even higher, to &lt;strong&gt;16.4 tokens per second&lt;/strong&gt;, by using the &amp;quot;row&amp;quot; split mode. This mode was broken in &lt;code&gt;llama.cpp&lt;/code&gt; until recently, so make sure you're using the latest version of the code.&lt;/p&gt; &lt;p&gt;Now let’s try &lt;strong&gt;Nemotron 49B&lt;/strong&gt;. I really like this model, though I can't run it fully in Q8 yet, that’s a good excuse to buy another 3090! For now, let's use Q6. With some tuning, we can go &lt;strong&gt;from 12.4 to 14.1 tokens per second&lt;/strong&gt;. Not bad.&lt;/p&gt; &lt;p&gt;Then we move on to a 70B model. I'm using &lt;strong&gt;DeepSeek-R1-Distill-Llama-70B&lt;/strong&gt; in Q4. We start at &lt;strong&gt;10.3&lt;/strong&gt; tokens per second and improve to &lt;strong&gt;12.1&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma3 27B&lt;/strong&gt; is a different case. With optimized tensor split values, we boost performance from 14.9 to &lt;strong&gt;18.9 tokens per second&lt;/strong&gt;. However, using &lt;code&gt;sm&lt;/code&gt; row mode slightly decreases the speed to 18.5.&lt;/p&gt; &lt;p&gt;Finally, we see similar behavior with &lt;strong&gt;Mistral Small 24B&lt;/strong&gt; (why is it called Llama 13B?). Performance goes from 18.8 to &lt;strong&gt;28.2 tokens per second&lt;/strong&gt; with tensor split, but again, &lt;code&gt;sm&lt;/code&gt; row mode reduces it slightly to 26.1.&lt;/p&gt; &lt;p&gt;So, you’ll need to experiment with your favorite models and your specific setup, but now you know the direction to take on your journey. Good luck!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kgs1z7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgs1z7/309030603060_llamacpp_benchmarks_tips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh5vrx</id>
    <title>Trying out the Ace-Step Song Generation Model</title>
    <updated>2025-05-07T19:15:23+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt; &lt;img alt="Trying out the Ace-Step Song Generation Model" src="https://external-preview.redd.it/bDY3Zm5xNjd0ZXplMfhj0slUHjrXE-UilZ0dRUIJzmh3kn39RuiWSQcdWvp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0b49332e389d6104e19b726896e9784113cd99d" title="Trying out the Ace-Step Song Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I got Gemini to whip up some lyrics for an alphabet song, and then I used ACE-Step-v1-3.5B to generate a rock-style track at 105bpm. &lt;/p&gt; &lt;p&gt;Give it a listen – how does it sound to you?&lt;/p&gt; &lt;p&gt;My feeling is that some of the transitions are still a bit off, and there are issues with the pronunciation of individual lyrics. But on the whole, it's not bad! I reckon it'd be pretty smooth for making those catchy, repetitive tunes (like that &amp;quot;Shawarma Legend&amp;quot; kind of vibe).&lt;br /&gt; This was generated on HuggingFace, took about 50 seconds.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dfm1hq67teze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgdmz6</id>
    <title>The real reason OpenAI bought WindSurf</title>
    <updated>2025-05-06T19:40:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt; &lt;img alt="The real reason OpenAI bought WindSurf" src="https://preview.redd.it/knqgtodvs7ze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b31b8bf514ff9c2407608d699ee65ce7c164f986" title="The real reason OpenAI bought WindSurf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who don’t know, today it was announced that OpenAI bought WindSurf, the AI-assisted IDE, for 3 billion USD. Previously, they tried to buy Cursor, the leading company that offers AI-assisted IDE, but didn’t agree on the details (probably on the price). Therefore, they settled for the second biggest player in terms of market share, WindSurf.&lt;/p&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;A lot of people question whether this is a wise move from OpenAI considering that these companies have limited innovation, since they don’t own the models and their IDE is just a fork of VS code.&lt;/p&gt; &lt;p&gt;Many argued that the reason for this purchase is to acquire the market position, the user base, since these platforms are already established with a big number of users.&lt;/p&gt; &lt;p&gt;I disagree in some degree. It’s not about the users per se, it’s about the training data they create. It doesn’t even matter which model users choose to use inside the IDE, Gemini2.5, Sonnet3.7, doesn’t really matter. There is a huge market that will be created very soon, and that’s coding agents. Some rumours suggest that OpenAI would sell them for 10k USD a month! These kind of agents/models need the exact kind of data that these AI-assisted IDEs collect. &lt;/p&gt; &lt;p&gt;Therefore, they paid the 3 billion to buy the training data they’d need to train their future coding agent models.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knqgtodvs7ze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgdmz6/the_real_reason_openai_bought_windsurf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T19:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgo7d4</id>
    <title>Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M</title>
    <updated>2025-05-07T03:56:08+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt; &lt;img alt="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" src="https://external-preview.redd.it/luDTORHovWSvyyKGyVuQUU_AS82WswbZpoHOp59s5cs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80203fde524d99b74a2b8e4185b0d45043a2a35e" title="Qwen3-30B-A3B GGUFs MMLU-PRO benchmark comparison - Q6_K / Q5_K_M / Q4_K_M / Q3_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q6_K / Q5_K_M / Q4_K_M / Q3_K_M&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;10 hours 32 minutes 19 seconds&lt;/strong&gt;.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I wanted to test unsloth dynamic ggufs as well, but ollama still can't run those ggufs properly, and yes I downloaded v0.6.8, lm studio can run them but doesn't support batching. So I only tested _K_M ggufs&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814"&gt;https://preview.redd.it/n8uisayb8aze1.png?width=445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2ef9b9f7f01091787bc58917ea58a7fe07d814&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038"&gt;https://preview.redd.it/rlopilhc8aze1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972522557abddeafa03ea3033ef2f3e05e396038&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a"&gt;https://preview.redd.it/sqzkrdkd8aze1.png?width=2003&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f19a8a0d4d6ee9552209ff8da9b0f9f3d51923a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d"&gt;https://preview.redd.it/s35vihde8aze1.png?width=1235&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9261ee5820639594e218ed77edff47a3ea4dcb8d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Q8 KV Cache / No kv cache quant&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a"&gt;https://preview.redd.it/te4noxve8aze1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c14e380265fe29f0805bf030dada4d452f5e86a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a"&gt;https://preview.redd.it/gxppkzef8aze1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9acc93a5c253f2a9401c3641322231179c3190a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ggufs: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T03:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kg9jkq</id>
    <title>New SOTA music generation model</title>
    <updated>2025-05-06T16:56:14+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt; &lt;img alt="New SOTA music generation model" src="https://external-preview.redd.it/N2dybzhkY2h6NnplMUATahysLltY5LFjwkyeKdWeoWJNo8-MZQBD68gR6Fn5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54585ce0978377749da79c9379f3519be85eac15" title="New SOTA music generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ace-step is a multilingual 3.5B parameters music generation model. They released training code, LoRa training code and will release more stuff soon.&lt;/p&gt; &lt;p&gt;It supports 19 languages, instrumental styles, vocal techniques, and more.&lt;/p&gt; &lt;p&gt;I’m pretty exited because it’s really good, I never heard anything like it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://ace-step.github.io/"&gt;https://ace-step.github.io/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B"&gt;https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gf0uynfhz6ze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kg9jkq/new_sota_music_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-06T16:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh6kh3</id>
    <title>Qwen3 MMLU-Pro Computer Science LLM Benchmark Results</title>
    <updated>2025-05-07T19:43:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt; &lt;img alt="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" src="https://preview.redd.it/3yuv5m5qxeze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ed0dfa07bd3b2e9b138176b2104e26c7a51e6e4" title="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive &lt;strong&gt;Qwen 3 evaluations&lt;/strong&gt; across a range of formats and quantisations, focusing on &lt;strong&gt;MMLU-Pro&lt;/strong&gt; (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-235B-A22B&lt;/strong&gt; (via Fireworks API) tops the table at &lt;strong&gt;83.66%&lt;/strong&gt; with ~55 tok/s.&lt;/li&gt; &lt;li&gt;But the &lt;strong&gt;30B-A3B Unsloth&lt;/strong&gt; quant delivered &lt;strong&gt;82.20%&lt;/strong&gt; while running locally at ~45 tok/s and with zero API spend.&lt;/li&gt; &lt;li&gt;The same Unsloth build is ~5x faster than Qwen's &lt;strong&gt;Qwen3-32B&lt;/strong&gt;, which scores &lt;strong&gt;82.20%&lt;/strong&gt; as well yet crawls at &amp;lt;10 tok/s.&lt;/li&gt; &lt;li&gt;On Apple silicon, the &lt;strong&gt;30B MLX&lt;/strong&gt; port hits &lt;strong&gt;79.51%&lt;/strong&gt; while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;0.6B&lt;/strong&gt; micro-model races above 180 tok/s but tops out at &lt;strong&gt;37.56%&lt;/strong&gt; - that's why it's not even on the graph (50 % performance cut-off).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, Alibaba/Qwen - you really whipped the llama's ass! And to OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. &lt;em&gt;This&lt;/em&gt; is the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yuv5m5qxeze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:43:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgxhdt</id>
    <title>Ollama vs Llama.cpp on 2x3090 and M3Max using qwen3-30b</title>
    <updated>2025-05-07T13:33:49+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone.&lt;/p&gt; &lt;p&gt;This is a comparison test between Ollama and Llama.cpp on 2 x RTX-3090 and M3-Max with 64GB using qwen3:30b-a3b-q8_0.&lt;/p&gt; &lt;p&gt;Just note that this speed test won't translate to other dense models. It'll be completely different.&lt;/p&gt; &lt;p&gt;For VLLM, SGLang Exllama don't support rtx3090 with this particular Qwen MoE architecture yet.&lt;/p&gt; &lt;p&gt;I ran a separate &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ke26sl/another_attempt_to_measure_speed_for_qwen3_moe_on/"&gt;benchmark with M3Max, rtx-4090 on MLX, Llama.cpp, VLLM SGLang&lt;/a&gt; here. This was primarily to compare Ollama and Llama.cpp.&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;To ensure consistency, I used a custom Python script that sends requests to the server via the OpenAI-compatible API. Metrics were calculated as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time to First Token (TTFT): Measured from the start of the streaming request to the first streaming event received.&lt;/li&gt; &lt;li&gt;Prompt Processing Speed (PP): Number of prompt tokens divided by TTFT.&lt;/li&gt; &lt;li&gt;Token Generation Speed (TG): Number of generated tokens divided by (total duration - TTFT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The displayed results were truncated to two decimal places, but the calculations used full precision. I made the script to prepend 40% new material in the beginning of next longer prompt to avoid caching effect.&lt;/p&gt; &lt;p&gt;Here's my script for anyone interest. &lt;a href="https://github.com/chigkim/prompt-test"&gt;https://github.com/chigkim/prompt-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses OpenAI API, so it should work in variety setup. Also, this tests one request at a time, so multiple parallel requests could result in higher throughput in different tests.&lt;/p&gt; &lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;Both use the same q8_0 model from Ollama library with flash attention. I'm sure you can optimize more, but I copied the flags from Ollama log in order to keep it consistent, so both use the exactly same flags when loading the model.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ~/.ollama/models/blobs/sha256... --ctx-size 36000 --batch-size 512 --n-gpu-layers 49 --verbose --threads 24 --flash-attn --parallel 1 --tensor-split 25,24 --port 11434&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama.cpp: Commit 2f54e34&lt;/li&gt; &lt;li&gt;Ollama: 0.6.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each row in the results represents a test (a specific combination of machine, engine, and prompt length). There are 4 tests per prompt length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup 1: 2xRTX3090, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 2: 2xRTX3090, Ollama&lt;/li&gt; &lt;li&gt;Setup 3: M3Max, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 4: M3Max, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Result&lt;/h3&gt; &lt;p&gt;Please zoom in to see the graph better.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img xcmmuk1bycze1...&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1663.57&lt;/td&gt; &lt;td&gt;0.42&lt;/td&gt; &lt;td&gt;1419&lt;/td&gt; &lt;td&gt;82.19&lt;/td&gt; &lt;td&gt;17.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;1595.04&lt;/td&gt; &lt;td&gt;0.44&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;77.41&lt;/td&gt; &lt;td&gt;18.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;289.53&lt;/td&gt; &lt;td&gt;2.42&lt;/td&gt; &lt;td&gt;1485&lt;/td&gt; &lt;td&gt;55.60&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;702&lt;/td&gt; &lt;td&gt;288.32&lt;/td&gt; &lt;td&gt;2.43&lt;/td&gt; &lt;td&gt;1440&lt;/td&gt; &lt;td&gt;55.78&lt;/td&gt; &lt;td&gt;28.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1768.00&lt;/td&gt; &lt;td&gt;0.54&lt;/td&gt; &lt;td&gt;1210&lt;/td&gt; &lt;td&gt;81.47&lt;/td&gt; &lt;td&gt;15.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;1723.07&lt;/td&gt; &lt;td&gt;0.56&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;74.82&lt;/td&gt; &lt;td&gt;17.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;458.40&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1337&lt;/td&gt; &lt;td&gt;55.28&lt;/td&gt; &lt;td&gt;26.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;959&lt;/td&gt; &lt;td&gt;459.38&lt;/td&gt; &lt;td&gt;2.09&lt;/td&gt; &lt;td&gt;1302&lt;/td&gt; &lt;td&gt;55.44&lt;/td&gt; &lt;td&gt;25.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1752.04&lt;/td&gt; &lt;td&gt;0.75&lt;/td&gt; &lt;td&gt;1108&lt;/td&gt; &lt;td&gt;80.95&lt;/td&gt; &lt;td&gt;14.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;1725.06&lt;/td&gt; &lt;td&gt;0.76&lt;/td&gt; &lt;td&gt;1209&lt;/td&gt; &lt;td&gt;73.83&lt;/td&gt; &lt;td&gt;17.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;455.39&lt;/td&gt; &lt;td&gt;2.87&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.84&lt;/td&gt; &lt;td&gt;24.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1306&lt;/td&gt; &lt;td&gt;458.06&lt;/td&gt; &lt;td&gt;2.85&lt;/td&gt; &lt;td&gt;1213&lt;/td&gt; &lt;td&gt;54.96&lt;/td&gt; &lt;td&gt;24.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1763.32&lt;/td&gt; &lt;td&gt;1.01&lt;/td&gt; &lt;td&gt;1330&lt;/td&gt; &lt;td&gt;80.44&lt;/td&gt; &lt;td&gt;17.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;1823.88&lt;/td&gt; &lt;td&gt;0.97&lt;/td&gt; &lt;td&gt;1370&lt;/td&gt; &lt;td&gt;78.26&lt;/td&gt; &lt;td&gt;18.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;320.44&lt;/td&gt; &lt;td&gt;5.54&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.10&lt;/td&gt; &lt;td&gt;29.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1774&lt;/td&gt; &lt;td&gt;321.45&lt;/td&gt; &lt;td&gt;5.52&lt;/td&gt; &lt;td&gt;1281&lt;/td&gt; &lt;td&gt;54.26&lt;/td&gt; &lt;td&gt;29.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1776.17&lt;/td&gt; &lt;td&gt;1.45&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;79.39&lt;/td&gt; &lt;td&gt;20.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;1851.35&lt;/td&gt; &lt;td&gt;1.40&lt;/td&gt; &lt;td&gt;1118&lt;/td&gt; &lt;td&gt;75.08&lt;/td&gt; &lt;td&gt;16.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;445.47&lt;/td&gt; &lt;td&gt;5.80&lt;/td&gt; &lt;td&gt;1321&lt;/td&gt; &lt;td&gt;52.86&lt;/td&gt; &lt;td&gt;30.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2584&lt;/td&gt; &lt;td&gt;447.47&lt;/td&gt; &lt;td&gt;5.77&lt;/td&gt; &lt;td&gt;1359&lt;/td&gt; &lt;td&gt;53.00&lt;/td&gt; &lt;td&gt;31.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1832.97&lt;/td&gt; &lt;td&gt;1.94&lt;/td&gt; &lt;td&gt;1500&lt;/td&gt; &lt;td&gt;77.61&lt;/td&gt; &lt;td&gt;21.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;1928.76&lt;/td&gt; &lt;td&gt;1.84&lt;/td&gt; &lt;td&gt;1653&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;25.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;444.32&lt;/td&gt; &lt;td&gt;8.01&lt;/td&gt; &lt;td&gt;1481&lt;/td&gt; &lt;td&gt;51.34&lt;/td&gt; &lt;td&gt;36.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;3557&lt;/td&gt; &lt;td&gt;442.89&lt;/td&gt; &lt;td&gt;8.03&lt;/td&gt; &lt;td&gt;1430&lt;/td&gt; &lt;td&gt;51.52&lt;/td&gt; &lt;td&gt;35.79&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1773.28&lt;/td&gt; &lt;td&gt;2.67&lt;/td&gt; &lt;td&gt;1279&lt;/td&gt; &lt;td&gt;76.60&lt;/td&gt; &lt;td&gt;19.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;1910.52&lt;/td&gt; &lt;td&gt;2.48&lt;/td&gt; &lt;td&gt;1877&lt;/td&gt; &lt;td&gt;71.85&lt;/td&gt; &lt;td&gt;28.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;421.06&lt;/td&gt; &lt;td&gt;11.26&lt;/td&gt; &lt;td&gt;1472&lt;/td&gt; &lt;td&gt;49.97&lt;/td&gt; &lt;td&gt;40.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4739&lt;/td&gt; &lt;td&gt;420.51&lt;/td&gt; &lt;td&gt;11.27&lt;/td&gt; &lt;td&gt;1316&lt;/td&gt; &lt;td&gt;50.16&lt;/td&gt; &lt;td&gt;37.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1760.68&lt;/td&gt; &lt;td&gt;3.70&lt;/td&gt; &lt;td&gt;1435&lt;/td&gt; &lt;td&gt;73.77&lt;/td&gt; &lt;td&gt;23.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;1897.12&lt;/td&gt; &lt;td&gt;3.44&lt;/td&gt; &lt;td&gt;1781&lt;/td&gt; &lt;td&gt;68.85&lt;/td&gt; &lt;td&gt;29.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;418.03&lt;/td&gt; &lt;td&gt;15.60&lt;/td&gt; &lt;td&gt;1998&lt;/td&gt; &lt;td&gt;47.56&lt;/td&gt; &lt;td&gt;57.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;6520&lt;/td&gt; &lt;td&gt;417.70&lt;/td&gt; &lt;td&gt;15.61&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;47.81&lt;/td&gt; &lt;td&gt;57.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1714.65&lt;/td&gt; &lt;td&gt;5.31&lt;/td&gt; &lt;td&gt;1528&lt;/td&gt; &lt;td&gt;70.17&lt;/td&gt; &lt;td&gt;27.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;1881.13&lt;/td&gt; &lt;td&gt;4.84&lt;/td&gt; &lt;td&gt;1801&lt;/td&gt; &lt;td&gt;68.09&lt;/td&gt; &lt;td&gt;31.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;250.25&lt;/td&gt; &lt;td&gt;36.37&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;36.29&lt;/td&gt; &lt;td&gt;89.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;9101&lt;/td&gt; &lt;td&gt;244.02&lt;/td&gt; &lt;td&gt;37.30&lt;/td&gt; &lt;td&gt;1941&lt;/td&gt; &lt;td&gt;35.55&lt;/td&gt; &lt;td&gt;91.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1591.33&lt;/td&gt; &lt;td&gt;7.81&lt;/td&gt; &lt;td&gt;1001&lt;/td&gt; &lt;td&gt;66.74&lt;/td&gt; &lt;td&gt;22.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;1805.88&lt;/td&gt; &lt;td&gt;6.88&lt;/td&gt; &lt;td&gt;1284&lt;/td&gt; &lt;td&gt;64.01&lt;/td&gt; &lt;td&gt;26.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;280.46&lt;/td&gt; &lt;td&gt;44.32&lt;/td&gt; &lt;td&gt;1291&lt;/td&gt; &lt;td&gt;39.89&lt;/td&gt; &lt;td&gt;76.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12430&lt;/td&gt; &lt;td&gt;278.79&lt;/td&gt; &lt;td&gt;44.58&lt;/td&gt; &lt;td&gt;1502&lt;/td&gt; &lt;td&gt;39.82&lt;/td&gt; &lt;td&gt;82.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1546.35&lt;/td&gt; &lt;td&gt;11.04&lt;/td&gt; &lt;td&gt;1028&lt;/td&gt; &lt;td&gt;63.55&lt;/td&gt; &lt;td&gt;27.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;1722.15&lt;/td&gt; &lt;td&gt;9.92&lt;/td&gt; &lt;td&gt;1100&lt;/td&gt; &lt;td&gt;59.36&lt;/td&gt; &lt;td&gt;28.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.38&lt;/td&gt; &lt;td&gt;63.16&lt;/td&gt; &lt;td&gt;1461&lt;/td&gt; &lt;td&gt;34.89&lt;/td&gt; &lt;td&gt;105.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;17078&lt;/td&gt; &lt;td&gt;270.49&lt;/td&gt; &lt;td&gt;63.14&lt;/td&gt; &lt;td&gt;1673&lt;/td&gt; &lt;td&gt;34.28&lt;/td&gt; &lt;td&gt;111.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1429.31&lt;/td&gt; &lt;td&gt;16.55&lt;/td&gt; &lt;td&gt;1039&lt;/td&gt; &lt;td&gt;58.46&lt;/td&gt; &lt;td&gt;34.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;1586.04&lt;/td&gt; &lt;td&gt;14.92&lt;/td&gt; &lt;td&gt;1041&lt;/td&gt; &lt;td&gt;53.90&lt;/td&gt; &lt;td&gt;34.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;241.20&lt;/td&gt; &lt;td&gt;98.09&lt;/td&gt; &lt;td&gt;1681&lt;/td&gt; &lt;td&gt;28.04&lt;/td&gt; &lt;td&gt;158.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;23658&lt;/td&gt; &lt;td&gt;240.64&lt;/td&gt; &lt;td&gt;98.31&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;170.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1293.65&lt;/td&gt; &lt;td&gt;25.91&lt;/td&gt; &lt;td&gt;1311&lt;/td&gt; &lt;td&gt;52.92&lt;/td&gt; &lt;td&gt;50.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;1441.12&lt;/td&gt; &lt;td&gt;23.26&lt;/td&gt; &lt;td&gt;1418&lt;/td&gt; &lt;td&gt;49.76&lt;/td&gt; &lt;td&gt;51.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;217.15&lt;/td&gt; &lt;td&gt;154.38&lt;/td&gt; &lt;td&gt;1453&lt;/td&gt; &lt;td&gt;23.91&lt;/td&gt; &lt;td&gt;215.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;33525&lt;/td&gt; &lt;td&gt;219.68&lt;/td&gt; &lt;td&gt;152.61&lt;/td&gt; &lt;td&gt;1522&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;216.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T13:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgu4qg</id>
    <title>Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090</title>
    <updated>2025-05-07T10:37:38+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt; &lt;img alt="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" src="https://preview.redd.it/1ijx9ffv8cze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42da044e5a349add8ff7c3733ec1c4e706676161" title="Qwen3-235B Q6_K ktransformers at 56t/s prefill 4.5t/s decode on Xeon 3175X (384GB DDR4-3400) and RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ijx9ffv8cze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgu4qg/qwen3235b_q6_k_ktransformers_at_56ts_prefill_45ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T10:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh5cyt</id>
    <title>Speeds of LLMs running on an AMD AI Max+ 395 128GB.</title>
    <updated>2025-05-07T18:54:45+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a YouTube video where the creator runs a variety of LLM models on an HP G1A. That has a power limited version of the AMD AI Max+ 395. From the video you can see the GPU uses 70 watts. ETA Prime has shown that the yet to be revealed mini-pc he's using can go up to 120-130 watts. The numbers seen on this video are not memory bandwidth limited, so they must be compute limited. Thus the extra TDP of the mini-pc version of the Max+ should allow it to have more compute and thus the LLMs should have a higher token count.&lt;/p&gt; &lt;p&gt;The tests this person does are less than ideal. He's using ollama and really short prompts and thus short context. But it is what it is. Also, he's seeing that the system RAM use matches the GPU RAM use when he loads a model and thus that's limiting him to 64GB of &amp;quot;VRAM&amp;quot;. I wonder how old the version of llama.cpp is that ollama is using. Since that was a problem with llama.cpp. I've complained about it in the past. But that was months ago and has since been fixed.&lt;/p&gt; &lt;p&gt;Overall, the speeds on this power limited Max+ are comparable to my M1 Max. Which I have to confess, I find slowish. Hopefully the extra TDP of the mini-pc enabled version give it an extra kick. Worse case is that the Max+ 395 is a 128GB M1 Max which isn't the worse thing in the world.&lt;/p&gt; &lt;p&gt;Anyways. Enjoy.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-HJ-VipsuSk"&gt;https://www.youtube.com/watch?v=-HJ-VipsuSk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5cyt/speeds_of_llms_running_on_an_amd_ai_max_395_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5cyt/speeds_of_llms_running_on_an_amd_ai_max_395_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5cyt/speeds_of_llms_running_on_an_amd_ai_max_395_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrab2</id>
    <title>Self-improving AI unlocked?</title>
    <updated>2025-05-07T07:13:24+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Absolute Zero: Reinforced Self-play Reasoning with Zero Data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. &lt;strong&gt;Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision&lt;/strong&gt;, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, &lt;strong&gt;we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples&lt;/strong&gt;. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2505.03335"&gt;Paper&lt;/a&gt; &lt;a href="https://x.com/AndrewZ45732491/status/1919920459748909288"&gt;Thread&lt;/a&gt; &lt;a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner"&gt;GitHub&lt;/a&gt; &lt;a href="https://huggingface.co/papers/2505.03335"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrab2/selfimproving_ai_unlocked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgt8m5</id>
    <title>nanoVLM: A minimal Vision-Language Model with a LLaMA-style decoder — now open source</title>
    <updated>2025-05-07T09:37:59+00:00</updated>
    <author>
      <name>/u/zKingFrist</name>
      <uri>https://old.reddit.com/user/zKingFrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all — we just open-sourced &lt;strong&gt;nanoVLM&lt;/strong&gt;, a lightweight Vision-Language Model (VLM) built from scratch in &lt;strong&gt;pure PyTorch&lt;/strong&gt;, with a &lt;strong&gt;LLaMA-style decoder&lt;/strong&gt;. It's designed to be simple, hackable, and easy to train — the full model is just ~750 lines of code.&lt;/p&gt; &lt;p&gt;Why it's interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves &lt;strong&gt;35.3% on MMStar&lt;/strong&gt; with only &lt;strong&gt;6 hours of training on a single H100,&lt;/strong&gt; matching SmolVLM-256M performance — but using 100x fewer GPU hours.&lt;/li&gt; &lt;li&gt;Can be trained in a &lt;strong&gt;free Google Colab notebook&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Great for learning, prototyping, or building your own VLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision encoder: &lt;strong&gt;SigLiP-ViT&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Language decoder: &lt;strong&gt;LLaMA-style&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Modality projector connecting the two&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inspired by nanoGPT, this is like the VLM version — compact and easy to understand. Would love to see someone try running this on local hardware or mixing it with other projects.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/huggingface/nanoVLM"&gt;https://github.com/huggingface/nanoVLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zKingFrist"&gt; /u/zKingFrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgt8m5/nanovlm_a_minimal_visionlanguage_model_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T09:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh579e</id>
    <title>Qwen 3 evaluations</title>
    <updated>2025-05-07T18:48:14+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt; &lt;img alt="Qwen 3 evaluations" src="https://preview.redd.it/8f8g366goeze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd68bf0ab81adb00446d201fbee1d90070c68389" title="Qwen 3 evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive Qwen 3 evaluations across a range of formats and quantisations, focusing on MMLU-Pro (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;p&gt;1️⃣ Qwen3-235B-A22B (via Fireworks API) tops the table at 83.66% with ~55 tok/s.&lt;/p&gt; &lt;p&gt;2️⃣ But the 30B-A3B Unsloth quant delivered 82.20% while running locally at ~45 tok/s and with zero API spend.&lt;/p&gt; &lt;p&gt;3️⃣ The same Unsloth build is ~5x faster than Qwen's Qwen3-32B, which scores 82.20% as well yet crawls at &amp;lt;10 tok/s.&lt;/p&gt; &lt;p&gt;4️⃣ On Apple silicon, the 30B MLX port hits 79.51% while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/p&gt; &lt;p&gt;5️⃣ The 0.6B micro-model races above 180 tok/s but tops out at 37.56% - that's why it's not even on the graph (50 % performance cut-off).&lt;/p&gt; &lt;p&gt;All local runs were done with @lmstudio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;Conclusion: Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, @Alibaba_Qwen - you really whipped the llama's ass! And to @OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. This is the future!&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/wolframrvnwlf/status/1920186645384478955?s=46"&gt;https://x.com/wolframrvnwlf/status/1920186645384478955?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8f8g366goeze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzskq</id>
    <title>Mistral-Medium 3 (unfortunately no local support so far)</title>
    <updated>2025-05-07T15:12:17+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt; &lt;img alt="Mistral-Medium 3 (unfortunately no local support so far)" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral-Medium 3 (unfortunately no local support so far)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzey8</id>
    <title>Run FLUX.1 losslessly on a GPU with 20GB VRAM</title>
    <updated>2025-05-07T14:57:14+00:00</updated>
    <author>
      <name>/u/arty_photography</name>
      <uri>https://old.reddit.com/user/arty_photography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've released &lt;strong&gt;losslessly compressed versions&lt;/strong&gt; of the &lt;strong&gt;12B FLUX.1-dev&lt;/strong&gt; and &lt;strong&gt;FLUX.1-schnell&lt;/strong&gt; models using &lt;strong&gt;DFloat11&lt;/strong&gt;, a compression method that applies entropy coding to BFloat16 weights. This reduces model size by &lt;strong&gt;~30%&lt;/strong&gt; &lt;em&gt;without changing outputs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This brings the models down from &lt;strong&gt;24GB to ~16.3GB&lt;/strong&gt;, enabling them to run on a &lt;strong&gt;single GPU with 20GB or more of VRAM&lt;/strong&gt;, with only a &lt;strong&gt;few seconds of extra overhead per image&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;🔗 Downloads &amp;amp; Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-dev&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-dev-DF11"&gt;huggingface.co/DFloat11/FLUX.1-dev-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-schnell&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-schnell-DF11"&gt;huggingface.co/DFloat11/FLUX.1-schnell-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Example Code&lt;/strong&gt;: &lt;a href="https://github.com/LeanModels/DFloat11/tree/master/examples/flux.1"&gt;github.com/LeanModels/DFloat11/tree/master/examples/flux.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed LLMs (Qwen 3, Gemma 3, etc.)&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11"&gt;huggingface.co/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feedback welcome&lt;/strong&gt;! Let me know if you try them out or run into any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arty_photography"&gt; /u/arty_photography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh3g7f</id>
    <title>Did anyone try out Mistral Medium 3?</title>
    <updated>2025-05-07T17:38:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt; &lt;img alt="Did anyone try out Mistral Medium 3?" src="https://external-preview.redd.it/Z3k2eW51bjJiZXplMcUDN_ixsC3ErmhxAmaSJ8XxFt_ddYdhD_A2seyNDJhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=380661f2e13d59675a91c29257e8bd38b702503f" title="Did anyone try out Mistral Medium 3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I briefly tried Mistral Medium 3 on OpenRouter, and I feel its performance might not be as good as Mistral's blog claims. (The video shows the best result out of the 5 shots I ran. ) &lt;/p&gt; &lt;p&gt;Additionally, I tested having it recognize and convert the benchmark image from the blog into JSON. However, it felt like it was just randomly converting things, and not a single field matched up. Could it be that its input resolution is very low, causing compression and therefore making it unable to recognize the text in the image? &lt;/p&gt; &lt;p&gt;Also, I don't quite understand why it uses 5-shot in the GPTQ diamond and MMLU Pro benchmarks. Is that the default number of shots for these tests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6w9w0rl2beze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T17:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kguqmd</id>
    <title>Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp; Servicenow)</title>
    <updated>2025-05-07T11:14:13+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt; &lt;img alt="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" src="https://external-preview.redd.it/EuRAXuyDLNOAu2-1ktV_-X31N5aAiqTZTPHiWEhPj-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d4cc697d8897122cd61888ad7f02b892afd49c4" title="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Service now and Nvidia brings a new 15B thinking model with comparable performance with 32B&lt;br /&gt; Model: &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt; (MIT licence)&lt;br /&gt; It looks very promising (resumed by Gemini) : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Claimed to be half the size of some SOTA models (like QWQ-32b, EXAONE-32b) and consumes significantly fewer tokens (~40% less than QWQ-32b) for comparable tasks, directly impacting VRAM requirements and inference costs for local or self-hosted setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning/Enterprise:&lt;/strong&gt; Reports strong performance on benchmarks like MBPP, BFCL, Enterprise RAG, IFEval, and Multi-Challenge. The focus on Enterprise RAG is notable for business-specific applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Competitive results on coding tasks like MBPP and HumanEval, important for development workflows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic:&lt;/strong&gt; Holds competitive scores on academic reasoning benchmarks (AIME, AMC, MATH, GPQA) relative to its parameter count.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; We need to test it&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kguqmd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrjor</id>
    <title>New ""Open-Source"" Video generation model</title>
    <updated>2025-05-07T07:32:32+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt; &lt;img alt="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" src="https://external-preview.redd.it/ZHdlOHlodmQ5YnplMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6db55c8236c9c875dc6ec641feb87d228687bd65" title="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216×704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &lt;p&gt;To be honest, I don't view it as open-source, not even open-weight. The license is weird, not a license we know of, and there's &amp;quot;Use Restrictions&amp;quot;. By doing so, it is NOT open-source.&lt;br /&gt; Yes, the restrictions are honest, and I invite you to read them, &lt;a href="https://static.lightricks.com/legal/LTXV-13b-0.9.7-dev.pdf"&gt;here is an example&lt;/a&gt;, but I think they're just doing this to protect themselves.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;https://github.com/Lightricks/LTX-Video&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;https://huggingface.co/Lightricks/LTX-Video&lt;/a&gt; (FP8 coming soon)&lt;br /&gt; Documentation: &lt;a href="https://www.lightricks.com/ltxv-documentation"&gt;https://www.lightricks.com/ltxv-documentation&lt;/a&gt;&lt;br /&gt; Tweet: &lt;a href="https://x.com/LTXStudio/status/1919751150888239374"&gt;https://x.com/LTXStudio/status/1919751150888239374&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i4ioviud9bze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0hcd</id>
    <title>Cracking 40% on SWE-bench verified with open source models &amp; agents &amp; open-source synth data</title>
    <updated>2025-05-07T15:39:46+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt; &lt;img alt="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" src="https://preview.redd.it/4lwtc2sgpdze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f581dfebc0968cbf87949bad4b08918a6afa989" title="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know that finetuning &amp;amp; RL work great for getting great LMs for agents -- the problem is where to get the training data!&lt;/p&gt; &lt;p&gt;We've generated 50k+ task instances for 128 popular GitHub repositories, then trained our own LM for SWE-agent. The result? We achieve 40% pass@1 on SWE-bench Verified -- a new SoTA among open source models.&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;em&gt;everything&lt;/em&gt;, and we're excited to see what you build with it! This includes the agent (SWE-agent), the framework used to generate synthetic task instances (SWE-smith), and our fine-tuned LM (SWE-agent-LM-32B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4lwtc2sgpdze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzwe9</id>
    <title>New mistral model benchmarks</title>
    <updated>2025-05-07T15:16:25+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt; &lt;img alt="New mistral model benchmarks" src="https://preview.redd.it/hrtrvrvnmdze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a47a4a215c33b3670819e5b09e20d25a73074d7" title="New mistral model benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hrtrvrvnmdze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:16:25+00:00</published>
  </entry>
</feed>
