<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-29T19:21:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jm36yg</id>
    <title>CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU</title>
    <updated>2025-03-28T19:20:28+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"&gt; &lt;img alt="CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU" src="https://external-preview.redd.it/57C9ha6egNvnO5W3Odr1a-BMcv_qWXnF4sDFPfNBsCc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5550e16a355dc853fe379c1f46ba3e7e29f64aa" title="CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=W5X8MEZVqzM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T19:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmf16i</id>
    <title>Are there reliable DeepSeek V3 API providers?</title>
    <updated>2025-03-29T05:04:54+00:00</updated>
    <author>
      <name>/u/kappaappa</name>
      <uri>https://old.reddit.com/user/kappaappa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently the official DeepSeek v3 api has really bad reliability, so I looked on openrouter for alternatives - when I tried fireworks / nebius they performed noticeably worse (than the official API) on our internal evals across several runs (even though they claim to use an un-quantized model).&lt;/p&gt; &lt;p&gt;I used the same temperature, top-p etc. These tests were run on the old v3 (not the recent 0324 model since those aren’t out yet across all providers).&lt;/p&gt; &lt;p&gt;It could be there are some settings or system prompts that each provider injects that I don’t know about which leads to the discrepancy though. Has anybody run into the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kappaappa"&gt; /u/kappaappa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T05:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlw5hb</id>
    <title>New TTS model from bytedance</title>
    <updated>2025-03-28T14:18:47+00:00</updated>
    <author>
      <name>/u/bio_risk</name>
      <uri>https://old.reddit.com/user/bio_risk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt; &lt;img alt="New TTS model from bytedance" src="https://external-preview.redd.it/uKiEmAEQx3Gdvnl2sNzEW0QEbrYYFLxWFMibNhAEifw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=813b2b376a8aba99e7464ee633d5a6f6d97c2749" title="New TTS model from bytedance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bio_risk"&gt; /u/bio_risk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/MegaTTS3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T14:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmi7hp</id>
    <title>Best UI/frontend for story/creative/general writing?</title>
    <updated>2025-03-29T08:59:30+00:00</updated>
    <author>
      <name>/u/Tripel_Meow</name>
      <uri>https://old.reddit.com/user/Tripel_Meow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What I mean is not just prompting the LLM do do one thing and zero shot it, but like create drafts, edit in place, write extra, expand text, verbose, paraphrase and so on. Basically as if you were writing, but leaving the writing to the model. idk I think I'm poorly explaining it but imagine as if you had a code assistant in some IDE, but for creative writing instead of coding? Something like that or something similar, does it exist?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tripel_Meow"&gt; /u/Tripel_Meow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmi7hp/best_uifrontend_for_storycreativegeneral_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmi7hp/best_uifrontend_for_storycreativegeneral_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmi7hp/best_uifrontend_for_storycreativegeneral_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T08:59:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlptqu</id>
    <title>Reverse engineering GPT-4o image gen via Network tab - here's what I found</title>
    <updated>2025-03-28T07:46:37+00:00</updated>
    <author>
      <name>/u/seicaratteri</name>
      <uri>https://old.reddit.com/user/seicaratteri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt; &lt;img alt="Reverse engineering GPT-4o image gen via Network tab - here's what I found" src="https://b.thumbs.redditmedia.com/6ICxPu6L9VzJwgZpZB11Ryf2Jzo9ZxiBnaamsutC34E.jpg" title="Reverse engineering GPT-4o image gen via Network tab - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am very intrigued about this new model; I have been working in the image generation space a lot, and I want to understand what's going on&lt;/p&gt; &lt;p&gt;I found interesting details when opening the network tab to see what the BE was sending - here's what I found. I tried with few different prompts, let's take this as a starter:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;An image of happy dog running on the street, studio ghibli style&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here I got four intermediate images, as follows:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9"&gt;https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The BE is actually returning the image as we see it in the UI&lt;/li&gt; &lt;li&gt;It's not really clear wether the generation is autoregressive or not - we see &lt;em&gt;some&lt;/em&gt; details and a faint global structure of the image, this could mean two things: &lt;ul&gt; &lt;li&gt;Like usual diffusion processes, we first generate the global structure and then add details&lt;/li&gt; &lt;li&gt;OR - The image is &lt;em&gt;actually&lt;/em&gt; generated autoregressively&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If we analyze the 100% zoom of the first and last frame, we can see details are being added to high frequency textures like the trees&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965"&gt;https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what we would typically expect from a diffusion model. This is further accentuated in this other example, where I prompted specifically for a high frequency detail texture (&amp;quot;create the image of a grainy texture, abstract shape, very extremely highly detailed&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf"&gt;https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, I got only three images here from the BE; and the details being added is obvious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728"&gt;https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This could be done of course as a separate post processing step too, for example like SDXL introduced the refiner model back in the days that was specifically trained to add details to the VAE latent representation before decoding it to pixel space.&lt;/p&gt; &lt;p&gt;It's also unclear if I got less images with this prompt due to availability (i.e. the BE could give me more flops), or to some kind of specific optimization (eg: latent caching).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So where I am at now:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's probably a multi step process pipeline&lt;/li&gt; &lt;li&gt;OpenAI in the model card is stating that &amp;quot;Unlike DALL·E, which operates as a diffusion model, 4o image generation is an autoregressive model natively embedded within ChatGPT&amp;quot;&lt;/li&gt; &lt;li&gt;This makes me think of this recent paper: &lt;a href="https://arxiv.org/pdf/2409.11340"&gt;OmniGen&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There they directly connect the VAE of a Latent Diffusion architecture to an LLM and learn to model jointly both text and images; they observe few shot capabilities and emerging properties too which would explain the vast capabilities of GPT4-o, and it makes even more sense if we consider the usual OAI formula:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More / higher quality data&lt;/li&gt; &lt;li&gt;More flops&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture proposed in OmniGen has &lt;em&gt;great&lt;/em&gt; potential to scale given that is purely transformer based - and if we know one thing is surely that transformers scale well, and that OAI is especially good at that&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What do you think?&lt;/strong&gt; would love to take this as a space to investigate together! Thanks for reading and let's get to the bottom of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seicaratteri"&gt; /u/seicaratteri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmh2wj</id>
    <title>Does anyone know about the model code name: 'Spider' in LM arena??</title>
    <updated>2025-03-29T07:30:55+00:00</updated>
    <author>
      <name>/u/Harsh2588</name>
      <uri>https://old.reddit.com/user/Harsh2588</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spider model is somewhat more human-like and its answers are quite different compared to other LLM. It so far told me that it is a GPT-4 model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Harsh2588"&gt; /u/Harsh2588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T07:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmeypf</id>
    <title>What are the current trendings in TTS and STT??</title>
    <updated>2025-03-29T05:00:31+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What models are you sticking with? and why..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeypf/what_are_the_current_trendings_in_tts_and_stt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeypf/what_are_the_current_trendings_in_tts_and_stt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeypf/what_are_the_current_trendings_in_tts_and_stt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T05:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmraxr</id>
    <title>How The ChatGPT Voice and Video Mode Perform So Well</title>
    <updated>2025-03-29T17:22:08+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen this reference in the group a couple of times but honestly I’m surprised it hasn’t been brought up more. &lt;/p&gt; &lt;p&gt;I use ChatGPT’s enhanced voice mode almost daily. I hardly listen to music anymore when I drive (unless Bluey Soundtrack with my son). Instead, I use voice mode as my brainstorming/concept understanding sessions. I will ask about concepts I am struggling to understand or ask about how certain concepts are implemented. &lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;Kyle, &lt;/p&gt; &lt;p&gt;What are some industry standard tools and techniques for secure development lifecycle management?&lt;/p&gt; &lt;p&gt;What are the best ways to secure applications using CloudFlare?&lt;/p&gt; &lt;p&gt;What database platforms are best suited for real time data vs historical data and list some paid vs open source?&lt;/p&gt; &lt;p&gt;This has been incredibly valuable for my own projects and just understanding things “out of band”. It’s another way I can casually digest information without having to read, It’s just a casual conversation with Kyle who is much smarter than I am. But, I want to use this outside OpenAI. &lt;/p&gt; &lt;p&gt;I started asking myself how does ChatGPT seem to be streaming my long winded questions and conversations, but Kyle replies almost immediately as if there was no processing or inference time? Once I looked it up it made complete sense and thought it was such a genius idea! &lt;/p&gt; &lt;p&gt;OpenAI is using WebRTC to stream your audio and video content to and from the LLM. This is why you can interrupt him mid sentence and he will notice right away, why the responses are so immediate, and why there appears to be no processing/inference time.&lt;/p&gt; &lt;p&gt;Many of you may have already known this, but I was oblivious to it and found it such a cool way to use WebRTC which is typically reserved for things like video conferencing (zoom, Teams, etc). They use a company called LiveKit which has some amazing resources and a great SDK to get up and running on a free account fast. Check them out, they are doing a lot of really cool stuff! &lt;/p&gt; &lt;p&gt;Anyways, just sharing in case anyone was oblivious as I was. &lt;/p&gt; &lt;p&gt;&lt;a href="https://livekit.io"&gt;https://livekit.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/livekit"&gt;https://github.com/livekit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmraxr/how_the_chatgpt_voice_and_video_mode_perform_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmraxr/how_the_chatgpt_voice_and_video_mode_perform_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmraxr/how_the_chatgpt_voice_and_video_mode_perform_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T17:22:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmoak6</id>
    <title>Cloud GPU suggestions for a privacy-conscious network engineer?</title>
    <updated>2025-03-29T15:06:37+00:00</updated>
    <author>
      <name>/u/dathtd119</name>
      <uri>https://old.reddit.com/user/dathtd119</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with some local LLMs on my 1660 Super, but I need to step up my game for some real work while keeping my data private (because, you know, telling Claude about our network vulnerabilities probably isn't in the company handbook 💔).&lt;/p&gt; &lt;p&gt;I'm looking to rent a cloud GPU to run models like Gemma 3, DeepSeek R1, and DeepSeek V3 for: - Generating network config files - Coding assistance - Summarizing internal docs&lt;/p&gt; &lt;p&gt;Budget: $100-200/month (planning to schedule on/off to save costs)&lt;/p&gt; &lt;p&gt;Questions: 1. Which cloud GPU providers have worked best for you? 2. Should I focus on specific specs beyond VRAM? (TFLOPs, CPU, etc.) 3. Any gotchas I should watch out for?&lt;/p&gt; &lt;p&gt;My poor 1660 Super is currently making sad GPU noises whenever I ask it to do anything beyond &amp;quot;hello world&amp;quot; with these models. Help a network engineer join the local LLM revolution!&lt;/p&gt; &lt;p&gt;Thanks in advance! 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dathtd119"&gt; /u/dathtd119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmoak6/cloud_gpu_suggestions_for_a_privacyconscious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmoak6/cloud_gpu_suggestions_for_a_privacyconscious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmoak6/cloud_gpu_suggestions_for_a_privacyconscious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T15:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmaauq</id>
    <title>QwenPhi-4-0.5b-Draft</title>
    <updated>2025-03-29T00:41:28+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"&gt; &lt;img alt="QwenPhi-4-0.5b-Draft" src="https://external-preview.redd.it/st6SxzZLy5bw__kD-hkAlJzI_Tjlnzg5MffUlguivIA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9159e9a9e077f42f3fd0ee897f79ccf57e374" title="QwenPhi-4-0.5b-Draft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, inspired on the recently shared here Mistral Small Draft model, I used the same technique to make this &lt;strong&gt;draft model for the Phi 4 model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I also made a MLX 8bit version available of this model. &lt;/p&gt; &lt;p&gt;On my local lmstudio it caused Phi 4 - 4 bit Token generation to increase from 10tk/s to 20tk/s (MLX , mac m4 , low context , coding task)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/rdsm/QwenPhi-4-0.5b-Draft"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmg7td</id>
    <title>Best models to run with 8GB VRAM, 16GB RAM</title>
    <updated>2025-03-29T06:27:04+00:00</updated>
    <author>
      <name>/u/Qxz3</name>
      <uri>https://old.reddit.com/user/Qxz3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with local LLMs on my gaming laptop (RTX 4070 8GB, 16GB of RAM). My use cases have been coding and creative writing. Models that work well and that I like:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma 3 12B&lt;/strong&gt; - low quantization (IQ3_XS), 100% offloaded to GPU, spilling into RAM. ~10t/s. Great at following instructions and general knowledge. &lt;em&gt;This is the sweet spot and my main model&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma 3 4B&lt;/strong&gt; - full quantization (Q8), 100% offloaded to GPU, minimal spill. ~30-40t/s. Still smart and competent but more limited knowledge. &lt;em&gt;This is an amazing model at this performance level&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MN GRAND Gutenburg Lyra4 Lyra 23.5B,&lt;/strong&gt; medium quant (Q4) (lower quants are just too wonky) about 50% offloaded to GPU, 2-3t/s. When quality of prose and writing a captivating story matters. Tends to break down so needs some supervision, but it's in another league entirely - Gemma 3 just cannot write like this &lt;em&gt;whatsoever&lt;/em&gt; (although Gemma follows instructions more closely). Great companion for creative writing. &lt;strong&gt;12B version&lt;/strong&gt; of this is way faster (100% GPU, 15t/s) and still strong stylistically, although its stories aren't nearly as engaging so I tend to be patient and wait for the 23.5B.&lt;/p&gt; &lt;p&gt;I was disappointed with:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 8B&lt;/strong&gt; - runs fast, but responses are short, superficial and uninteresting compared with Gemma 3 4B.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small 3.1&lt;/strong&gt; - Can barely run on my machine, and for the extreme slowness, wasn't impressed with the responses. Would rather run &lt;strong&gt;Gemma 3 27B&lt;/strong&gt; instead.&lt;/p&gt; &lt;p&gt;I wish I could run:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;QWQ 32B&lt;/strong&gt; - doesn't do well at the lower quants that would allow it to run on my system, just too slow.&lt;br /&gt; &lt;strong&gt;Gemma 3 27B&lt;/strong&gt; - it runs but the jump in quality compared to &lt;strong&gt;12B&lt;/strong&gt; hasn't been worth going down to 2t/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qxz3"&gt; /u/Qxz3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgj6z</id>
    <title>is there a future for diffusion language models ?</title>
    <updated>2025-03-29T06:50:17+00:00</updated>
    <author>
      <name>/u/superNova-best</name>
      <uri>https://old.reddit.com/user/superNova-best</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;there's this new shiny type of models that are diffusion based not autoregressive, said to be faster cheaper and better, i've seen one called mercury by inception labs, what you think guys about those ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superNova-best"&gt; /u/superNova-best &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmcdgy</id>
    <title>[Proprietary Model] I "Vibe Coded" An ML model From Scratch Without Any Solid Experience, Gemini-2.5</title>
    <updated>2025-03-29T02:28:43+00:00</updated>
    <author>
      <name>/u/Few_Ask683</name>
      <uri>https://old.reddit.com/user/Few_Ask683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt; &lt;img alt="[Proprietary Model] I &amp;quot;Vibe Coded&amp;quot; An ML model From Scratch Without Any Solid Experience, Gemini-2.5" src="https://b.thumbs.redditmedia.com/YDbfH5XQRfduI0VuE3UVLcuur80RphfxKjdAIanrr-A.jpg" title="[Proprietary Model] I &amp;quot;Vibe Coded&amp;quot; An ML model From Scratch Without Any Solid Experience, Gemini-2.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using the model via Google Studio for a while and I just can't wrap my head around it. I said fuck it, why not push it further, but in a meaningful way. I don't expect it to write Crysis from scratch or spell out the R's in the word STRAWBERRY, but I wonder, what's the limit of pure prompting here?&lt;/p&gt; &lt;p&gt;This was my third rendition of a sloppily engineered prompt after a couple of successful but underperforming results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/urp4vl2lfjre1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97d211afbe14b3f3b40c124665d433dc0b4e30a"&gt;The generated code worked first try.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then, I wanted to improve the logic:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u0l1334ufjre1.png?width=1241&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a1a827c48ba2ed5dc9fc06b281ad41485f61364"&gt;It gave a single error due to huber loss implementation, which was solved by adding a single line of code.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is way too long to share as a screenshot, sorry. But don't worry, I will give you a pastebin link.&lt;/p&gt; &lt;p&gt;At this point I wondered, are we trying to train a model without any meaningful input? Because I did not necessarily specify a certain workflow or method. Just average geek person words.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lhwmovg4gjre1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3fd7d45b2b687e8ac14cb356081a4e6ad08fd800"&gt;It in fact is not random, according to Gemini.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, the model uses pygame to run the simulation, but it's annoying to run pygame on colab, in a cell. So, it saves the best results as a video. There is no way it just works, right?&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jmcdgy/video/0et9mjq1hjre1/player"&gt;Epoch 3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the Epoch 23!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jmcdgy/video/hzl0gofahjre1/player"&gt;https://reddit.com/link/1jmcdgy/video/hzl0gofahjre1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;## Final Thoughts&lt;/p&gt; &lt;p&gt;Please use as much as free Gemini possible and save the outputs. We can create a state of the art dataset together. The pastebin link is in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Ask683"&gt; /u/Few_Ask683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T02:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmttah</id>
    <title>Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak.</title>
    <updated>2025-03-29T19:13:59+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rvhj7wnchore1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmttah/seen_a_lot_of_setups_but_i_had_to_laugh_at_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgum6</id>
    <title>Test results of gemini 2.5 pro exp on ARC AGI 2</title>
    <updated>2025-03-29T07:13:29+00:00</updated>
    <author>
      <name>/u/flysnowbigbig</name>
      <uri>https://old.reddit.com/user/flysnowbigbig</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt; &lt;img alt="Test results of gemini 2.5 pro exp on ARC AGI 2" src="https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12fe1cebb1de52a5010be00250423f821bd40d0a" title="Test results of gemini 2.5 pro exp on ARC AGI 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source:&lt;a href="https://arcprize.org/leaderboard"&gt;https://arcprize.org/leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When it was first launched, I used my own tests to determine that its &lt;strong&gt;generalization reasoning was significantly weaker than that of O3 mini high.&lt;/strong&gt; It seems that ARC AGI is still a things.&lt;/p&gt; &lt;p&gt;Livebench Publicly accessible reasoning problem stays at 2024-10-22&lt;/p&gt; &lt;p&gt;I don't know what they use now&lt;/p&gt; &lt;p&gt;Assuming it still uses the same type of zebra reasoning, web of lies, but just changes the name, number, and other parameters? Then it is easy to target training, so it may not be so reliable anymore&lt;/p&gt; &lt;p&gt;Of all the models Provider, Sam seems to be the only one who is reluctant to provide detailed COT. It seems that there is a reason for this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ppg2vyxn2lre1.png?width=1069&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e8904afd4bd1d45d63eec92481c25b33a2fc70d"&gt;https://preview.redd.it/ppg2vyxn2lre1.png?width=1069&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e8904afd4bd1d45d63eec92481c25b33a2fc70d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flysnowbigbig"&gt; /u/flysnowbigbig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T07:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmpjeu</id>
    <title>SOTA 3d?</title>
    <updated>2025-03-29T16:03:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt; &lt;img alt="SOTA 3d?" src="https://external-preview.redd.it/ErYaOL2J__P1a1nSZoN5VkFh-_pWwoLL-ogamC2v0BM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16974cbb9664a3f501eea6ca32995ed70308e190" title="SOTA 3d?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/VAST-AI/TripoSG"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmpjeu/sota_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm9l6q</id>
    <title>New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp; cozy sample reader</title>
    <updated>2025-03-29T00:05:35+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt; &lt;img alt="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" src="https://b.thumbs.redditmedia.com/1wR0A4z7POWQrz_UIslETNDq8PPe15red_LwAnuSiTU.jpg" title="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Find the leaderboard here: &lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A nice long writeup: &lt;a href="https://eqbench.com/about.html#creative-writing-v3"&gt;https://eqbench.com/about.html#creative-writing-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/EQ-bench/creative-writing-bench"&gt;https://github.com/EQ-bench/creative-writing-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jm9l6q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm4agx</id>
    <title>Qwen-2.5-72b is now the best open source OCR model</title>
    <updated>2025-03-28T20:07:08+00:00</updated>
    <author>
      <name>/u/Tylernator</name>
      <uri>https://old.reddit.com/user/Tylernator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This has been a big week for open source LLMs. In the last few days we got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b)&lt;/li&gt; &lt;li&gt;Gemma-3 (27b)&lt;/li&gt; &lt;li&gt;DeepSeek-v3-0324&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And a couple weeks ago we got the new mistral-ocr model. We updated our OCR benchmark to include the new models.&lt;/p&gt; &lt;p&gt;We evaluated 1,000 documents for JSON extraction accuracy. Major takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b) are by far the most impressive. Both landed right around 75% accuracy (equivalent to GPT-4o’s performance). Qwen 72b was only 0.4% above 32b. Within the margin of error.&lt;/li&gt; &lt;li&gt;Both Qwen models passed mistral-ocr (72.2%), which is specifically trained for OCR.&lt;/li&gt; &lt;li&gt;Gemma-3 (27B) only scored 42.9%. Particularly surprising given that it's architecture is based on Gemini 2.0 which still tops the accuracy chart.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The data set and benchmark runner is fully open source. You can check out the code and reproduction steps here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/getomni-ai/benchmark"&gt;https://github.com/getomni-ai/benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark"&gt;https://huggingface.co/datasets/getomni-ai/ocr-benchmark&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tylernator"&gt; /u/Tylernator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T20:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmruim</id>
    <title>[Build] A Beautiful Contradiction</title>
    <updated>2025-03-29T17:46:01+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt; &lt;img alt="[Build] A Beautiful Contradiction" src="https://external-preview.redd.it/cjdycmFoa3Mxb3JlMXAQIb2sMeAVglK21tTmFYitWWLXfsVRBH8Hkw8Jz_5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e9b78cea2f802b090ea4497620a669bf9afa261" title="[Build] A Beautiful Contradiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing my absolute contradiction of a local LLM rig - I found a 2019 Mac Pro outer shell for sale on eBay for $250 and wanted room to upsize my ITX build so I said fuck it and thus, a monstrosity was born. &lt;/p&gt; &lt;p&gt;Specs in the comments, hate welcomed 🙏 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/irhtg3os1ore1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmruim/build_a_beautiful_contradiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T17:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqlii</id>
    <title>I Made a simple online tokenizer for any Hugging Face model</title>
    <updated>2025-03-29T16:50:57+00:00</updated>
    <author>
      <name>/u/Tweed_Beetle</name>
      <uri>https://old.reddit.com/user/Tweed_Beetle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;When I'm experimenting with different open models from Hugging Face, I often want to know how many tokens my prompts or texts actually are &lt;em&gt;for that specific model's tokenizer&lt;/em&gt;. It felt clunky to do this locally every time, and online tools seemed non-existent apart from OpenAI's tokenizer.&lt;/p&gt; &lt;p&gt;So I built a little web tool to help with this: &lt;strong&gt;Tokiwi&lt;/strong&gt; -&amp;gt; &lt;a href="https://tokiwi.dev"&gt;https://tokiwi.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You just paste text and give it any HF repo ID (like &lt;code&gt;google/gemma-3-27b-it&lt;/code&gt;, &lt;code&gt;deepseek-ai/DeepSeek-V3-0324&lt;/code&gt;, your own fine-tune if it's public, etc.) and it shows the token count and the tokens themselves. It can also handle gated models if you give it an HF access token.&lt;/p&gt; &lt;p&gt;Wondering if this might be useful to others here. Let me know what you think! Any feedback is appreciated.&lt;/p&gt; &lt;p&gt;Thank you for your time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tweed_Beetle"&gt; /u/Tweed_Beetle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:50:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmlopu</id>
    <title>Why is Falcon3-7b so rarely used (or cited) as a model?</title>
    <updated>2025-03-29T12:56:38+00:00</updated>
    <author>
      <name>/u/Prudence-0</name>
      <uri>https://old.reddit.com/user/Prudence-0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a model that adheres well to prompting, its knowledge and responses are relevant, and it supports system/user/assistant prompts very well.&lt;/p&gt; &lt;p&gt;As a &amp;quot;small&amp;quot; model, I use it professionally in conjunction with the RAG system for chat.&lt;/p&gt; &lt;p&gt;I'd like your opinion on this model as well as the alternatives you use (&amp;lt;8b), Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prudence-0"&gt; /u/Prudence-0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmlopu/why_is_falcon37b_so_rarely_used_or_cited_as_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T12:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmtkgo</id>
    <title>4x3090</title>
    <updated>2025-03-29T19:02:48+00:00</updated>
    <author>
      <name>/u/zetan2600</name>
      <uri>https://old.reddit.com/user/zetan2600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt; &lt;img alt="4x3090" src="https://preview.redd.it/zi8ghi2ifore1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaa2ef7723a30f4134fa44b42f76a17aa5ba357" title="4x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the only benefit of multiple GPUs concurrency of requests? I have 4x3090 but still seem limited to small models because it needs to fit in 24G vram. &lt;/p&gt; &lt;p&gt;AMD threadripper pro 5965wx 128 PCIe lanes ASUS ws pro wrx80 256G ddr4 3200 8 channels Primary PSU Corsair i1600 watt Secondary PSU 750watt 4 gigabyte 3090 turbos Phanteks Enthoo Pro II case Noctua industrial fans Artic cpu cooler&lt;/p&gt; &lt;p&gt;I am using vllm with tensor parallism of 4. I see all 4 cards loaded up and utilized evenly but doesn't seem any faster than 2 GPUs. &lt;/p&gt; &lt;p&gt;Currently using Qwen/Qwen2.5-14B-Instruct-AWQ with good success paired with Cline. &lt;/p&gt; &lt;p&gt;Will a nvlink bridge help? How can I run larger models? &lt;/p&gt; &lt;p&gt;14b seems really dumb compared to Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zetan2600"&gt; /u/zetan2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi8ghi2ifore1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jml2w8</id>
    <title>Nemotron-49B uses 70% less KV cache compare to source Llama-70B</title>
    <updated>2025-03-29T12:21:40+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While studying how much KV cache major models uses using formula and empirically running it with llama.cpp if possible, I found that the Nemotron models are not only 30% smaller in model size, KV cache is also 70% less. Overall, it is 38% VRAM saving if you run at 128k context.&lt;/p&gt; &lt;p&gt;This is because the non-self attention layers doesn't have any KV cache at all. For Nemotron-49B, 31 out of 80 layers are non-self attention. For 51B, 26 out of 80 layers.&lt;/p&gt; &lt;p&gt;So if you are into 128k context and have 48GB VRAM, Nemotron can run at Q5_K_M at 128k with unquantized KV cache. On the other hand, QwQ can only run at IQ3_M due to 32GB KV cache.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jl33br/qwq32b_has_the_highest_kv_cachemodel_size_ratio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Other things I learned:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;gemma-3 is pretty bad at KV cache while running with llama.cpp but this is because llama.cpp doesn't implement interleaved sliding window attention that can reduce KV cache to one sixth. (probably HF's transformers is the only one that support iSWA?)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek should make smaller MLA models that fit in 24GB or 48GB VRAM. This will blow the competition out of the water for local long context use.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jml2w8/nemotron49b_uses_70_less_kv_cache_compare_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T12:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmqqxz</id>
    <title>First time testing: Qwen2.5:72b -&gt; Ollama Mac + open-webUI -&gt; M3 Ultra 512 gb</title>
    <updated>2025-03-29T16:57:54+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt; &lt;img alt="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" src="https://b.thumbs.redditmedia.com/GHJGnHixtYfi5hcwQIzYQveJXry9-u0b_5OgRRmDegc.jpg" title="First time testing: Qwen2.5:72b -&amp;gt; Ollama Mac + open-webUI -&amp;gt; M3 Ultra 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First time using it. Tested with the qwen2.5:72b, I add in the gallery the results of the first run. I would appreciate any comment that could help me to improve it. I also, want to thanks the community for the patience answering some doubts I had before buying this machine. I'm just beginning. &lt;/p&gt; &lt;p&gt;Doggo is just a plus!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jmqqxz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmqqxz/first_time_testing_qwen2572b_ollama_mac_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T16:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmjq5h</id>
    <title>Finally someone's making a GPU with expandable memory!</title>
    <updated>2025-03-29T10:54:13+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a RISC-V gpu with SO-DIMM slots, so don't get your hopes up just yet, but it's &lt;em&gt;something&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/"&gt;https://www.servethehome.com/bolt-graphics-zeus-the-new-gpu-architecture-with-up-to-2-25tb-of-memory-and-800gbe/2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://bolt.graphics/"&gt;https://bolt.graphics/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmjq5h/finally_someones_making_a_gpu_with_expandable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T10:54:13+00:00</published>
  </entry>
</feed>
