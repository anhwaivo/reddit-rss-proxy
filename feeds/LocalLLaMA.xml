<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-22T04:38:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jgf07w</id>
    <title>Vulkan 1.4.311 Released With New Extension For BFloat16</title>
    <updated>2025-03-21T11:51:53+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"&gt; &lt;img alt="Vulkan 1.4.311 Released With New Extension For BFloat16" src="https://external-preview.redd.it/2t1oSj_vzvn5bCPIAvxtQL6anEzXfRChmhjaylPaVXw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=738740aa1d4f0c6631304412cf45ecce910e09ab" title="Vulkan 1.4.311 Released With New Extension For BFloat16" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Vulkan-1.4.311-Released"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T11:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jglwqf</id>
    <title>Testing new Moshi voices</title>
    <updated>2025-03-21T17:09:28+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jglwqf/testing_new_moshi_voices/"&gt; &lt;img alt="Testing new Moshi voices" src="https://external-preview.redd.it/ZnhleWs4NnZyMnFlMWDbG0pd8amqSlxiI07cjnjg9xIddGNtIKU5TlRLewT6.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07a7f0e70b4479284c331ada1c4e730bef8081a2" title="Testing new Moshi voices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nq3gcd6vr2qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jglwqf/testing_new_moshi_voices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jglwqf/testing_new_moshi_voices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T17:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg0exn</id>
    <title>Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'â€”Says Jensen Got Lucky and Inferencing Needs a Reality Check</title>
    <updated>2025-03-20T21:38:25+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt; &lt;img alt="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'â€”Says Jensen Got Lucky and Inferencing Needs a Reality Check" src="https://external-preview.redd.it/KOwl-jl-bbq-ggDpuf4_ihRqXydkagmZwZ9bDY3co3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0adc6319f752a232be3be30ea4365da6e1a21d20" title="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'â€”Says Jensen Got Lucky and Inferencing Needs a Reality Check" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick Breakdown (for those who don't want to read the full thing):&lt;/p&gt; &lt;p&gt;Intelâ€™s former CEO, Pat Gelsinger, openly criticized NVIDIA, saying their AI GPUs are massively overpriced (he specifically said they're &amp;quot;10,000 times&amp;quot; too expensive) for AI inferencing tasks.&lt;/p&gt; &lt;p&gt;Gelsinger praised NVIDIA CEO Jensen Huang's early foresight and perseverance but bluntly stated Jensen &amp;quot;got lucky&amp;quot; with AI blowing up when it did.&lt;/p&gt; &lt;p&gt;His main argument: NVIDIA GPUs are optimized for AI training, but they're totally overkill for inferencing workloadsâ€”which don't require the insanely expensive hardware NVIDIA pushes.&lt;/p&gt; &lt;p&gt;Intel itself, though, hasn't delivered on its promise to challenge NVIDIA. They've struggled to launch competitive GPUs (Falcon Shores got canned, Gaudi has underperformed, and Jaguar Shores is still just a future promise).&lt;/p&gt; &lt;p&gt;Gelsinger thinks the next big wave after AI could be quantum computing, potentially hitting the market late this decade.&lt;/p&gt; &lt;p&gt;TL;DR: Even Intelâ€™s former CEO thinks NVIDIA is price-gouging AI inferencing hardwareâ€”but admits Intel hasn't stepped up enough yet. CUDA dominance and lack of competition are keeping NVIDIA comfortable, while many of us just want affordable VRAM-packed alternatives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-former-ceo-claims-nvidia-ai-gpus-are-10000-times-more-expensive-than-what-is-needed-for-ai-inferencing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T21:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgd0e8</id>
    <title>The Hugging Face Agents Course now includes three major agent frameworks (smolagents, langchain, and llamaindex)</title>
    <updated>2025-03-21T09:41:02+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Hugging Face Agents Course now includes three major agent frameworks.&lt;/p&gt; &lt;p&gt;ðŸ”— &lt;a href="https://huggingface.co/agents-course"&gt;https://huggingface.co/agents-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in distinctive ways so that learners can reflect on when and where to use each. &lt;/p&gt; &lt;p&gt;This also means that you can follow the course if you're already familiar with one of these frameworks, and soak up some of the fundamental knowledge in earlier units. &lt;/p&gt; &lt;p&gt;Hopefully, this makes the agents course as open to as many people as possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T09:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgzpdb</id>
    <title>What are you using local LLMs for? How do they compare to the big tech offerings?</title>
    <updated>2025-03-22T03:43:40+00:00</updated>
    <author>
      <name>/u/TedHoliday</name>
      <uri>https://old.reddit.com/user/TedHoliday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m just curious what all people are using local LLMs for. For me personally, I use Claude daily at work I like the idea of running an LLM locally, but I know it would be less accurate than my single PC with one single RTX 4090. &lt;/p&gt; &lt;p&gt;I like the idea of not being subject to the constantly changing pricing models and worrying about how many tokens Iâ€™ve used up, but I feel like even like 5% more accurate code is worth it due to the time it can save.&lt;/p&gt; &lt;p&gt;So Iâ€™m just curious what people are using them for, and how are they now compared to the big players (and with what hardware)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TedHoliday"&gt; /u/TedHoliday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgzpdb/what_are_you_using_local_llms_for_how_do_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T03:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgyj01</id>
    <title>RTX PRO 5000 Laptop 24GB GDDR7 10496 cores 175W</title>
    <updated>2025-03-22T02:38:52+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;256-bit 896GB/s bandwidth. 228TFLOPS Tensor Core F16 (60% faster than 3090).&lt;/p&gt; &lt;p&gt;Should have made a similar desktop card that would be a no-brainer upgrade for the 3090/4090 users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory"&gt;https://videocardz.com/newz/nvidia-announces-rtx-pro-blackwell-laptop-gpus-up-to-10496-cuda-cores-and-24gb-gddr7-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgyj01/rtx_pro_5000_laptop_24gb_gddr7_10496_cores_175w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T02:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgvndh</id>
    <title>Open-Schizo-Leaderboard (The anti-leaderboard)</title>
    <updated>2025-03-22T00:11:39+00:00</updated>
    <author>
      <name>/u/Rombodawg</name>
      <uri>https://old.reddit.com/user/Rombodawg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its fun to see how bonkers model cards can be. Feel free to help me improve the code to better finetune the leaderboard filtering.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/rombodawg/Open-Schizo-Leaderboard"&gt;https://huggingface.co/spaces/rombodawg/Open-Schizo-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rombodawg"&gt; /u/Rombodawg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgvndh/openschizoleaderboard_the_antileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T00:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh01fi</id>
    <title>Can someone ELI5 what makes NVIDIA a monopoly in AI race?</title>
    <updated>2025-03-22T04:02:06+00:00</updated>
    <author>
      <name>/u/Trysem</name>
      <uri>https://old.reddit.com/user/Trysem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard somewhere it's cuda,then why some other companies like AMD is not making something like cuda of their own?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trysem"&gt; /u/Trysem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh01fi/can_someone_eli5_what_makes_nvidia_a_monopoly_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgdvr7</id>
    <title>GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI</title>
    <updated>2025-03-21T10:41:39+00:00</updated>
    <author>
      <name>/u/blazerx</name>
      <uri>https://old.reddit.com/user/blazerx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt; &lt;img alt="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI" src="https://external-preview.redd.it/FfsH6_lD8ZDjWq58F-nI0b3jtl3po6p4fWNK_dOGUbY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=067fd5517027950d0c15b75fbdf4301c41337833" title="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzenâ„¢ AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blazerx"&gt; /u/blazerx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/gaia-an-open-source-project-from-amd-for-running-local-llms-on-ryzen-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgau52</id>
    <title>Gemma 3 27b vs. Mistral 24b vs. QwQ 32b: I tested on personal benchmark, here's what I found out</title>
    <updated>2025-03-21T06:53:42+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for LLMs to use locally; the requirements are good enough reasoning and understanding, coding, and some elementary-level mathematics. I was looking into QwQ 32b, which seemed very promising.&lt;br /&gt; Last week, Google and Mistral released Gemma 3 27b and Mistral small 3.1 24b; from the benchmarks, both seem capable models approximating Deepseek r1 in ELO rating, which is impressive.&lt;/p&gt; &lt;p&gt;But, tbh, I have stopped caring about benchmarks, especially Lmsys; idk. The rankings always seem off when you try the models IRL.&lt;/p&gt; &lt;p&gt;So, I ran a small test to vibe-check which models to pick. I also benchmarked answers with Deepseek r1, as I use it often to get a better picture. &lt;/p&gt; &lt;p&gt;Here's what I found out&lt;/p&gt; &lt;h1&gt;For Coding&lt;/h1&gt; &lt;p&gt;QwQ 32b is just miles ahead in coding among the three. It sometimes does better code than Deepseek r1. They weren't lying in the benchmarks. It feels good to talk to you as well. Gemma is 2nd and does the job for easy tasks. Mistral otoh was bad.&lt;/p&gt; &lt;h1&gt;For Reasoning&lt;/h1&gt; &lt;p&gt;Again, Qwen was better. Well, ofc it's a reasoning model, but Gemma was also excellent. They made a good base model. Mistral was there but not there.&lt;/p&gt; &lt;h1&gt;For Math&lt;/h1&gt; &lt;p&gt;Gemma and QwQ were good enough for simple math tasks. Gemma, being a base model, was faster. I might test more with these two. Mistral was decent but 3rd again.&lt;/p&gt; &lt;h1&gt;What to pick?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;QwQ 32b is no doubt the best available model in its class. Great at coding, reasoning, and math. It's been a long since I used a local model, the last one was Mixtral, a year ago, and I never expected them to be this good. QwQ is promising; I can't wait for their new max model.&lt;/li&gt; &lt;li&gt;Gemma 3 27b is a solid base model. Great vibes. And you wouldn't be missing a lot with this. But it comes with a Gemma-specific license, which is more restrictive than Apache 2.0.&lt;/li&gt; &lt;li&gt;Mistral small 3.1 24b didn't impress me much; perhaps it needs more rigorous testing. &lt;/li&gt; &lt;li&gt;Both Gemma and Mistral Small have image support, so consider that as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the complete analysis, check out this blog post: &lt;a href="https://composio.dev/blog/qwq-32b-vs-gemma-3-mistral-small-vs-deepseek-r1/"&gt;Gemma 3 27b vs QwQ 32b vs Mistral 24b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to know which other model you're currently using and for what specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jguyty</id>
    <title>I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces</title>
    <updated>2025-03-21T23:39:38+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt; &lt;img alt="I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces" src="https://b.thumbs.redditmedia.com/ahdGQ2lOgkPoEe4ifpzcMZTGeQzhCkDStw1-ajCYsdo.jpg" title="I analyzed the word statistics in the reasoning traces of different llms - it seems many models are trained on R1 traces" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I extracted thinking traces from different LLMs for the prompt below and analyzed the frequency of the first word in each line. The heatmap below shows the frequency of the most used words in each LLM.&lt;/p&gt; &lt;p&gt;The aim is to identify relationships between different thinking models. For example, it is know that certain words/tokens like &amp;quot;wait&amp;quot; indicate backtracking in the thinking process. These patterns emerge during the reinforcement learning process and can also be trained by finetuning the model on thinking traces.&lt;/p&gt; &lt;p&gt;We can see that a lot of models show a word statistic similar to R1. This may be random, but could also mean that the model has seen R1 thinking traces at some point in the process.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xp3v4grjp4qe1.png?width=2789&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ecac49e05f3d6e483b341869d4c14d9f1f89bb"&gt;https://preview.redd.it/xp3v4grjp4qe1.png?width=2789&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ecac49e05f3d6e483b341869d4c14d9f1f89bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The prompt I used:&lt;br /&gt; &lt;em&gt;You have two ropes, each of which takes exactly 60 minutes to burn completely. However, the ropes burn unevenly, meaning some parts may burn faster or slower than others. You have no other timing device. How can you measure exactly 20 minutes using these two ropes and matches to light them?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jguyty/i_analyzed_the_word_statistics_in_the_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T23:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgmavv</id>
    <title>Llama 3.3 Nemotron 49B Super appears on LMSYS Arena</title>
    <updated>2025-03-21T17:25:23+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt; &lt;img alt="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" src="https://preview.redd.it/v330lkoru2qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0578ce6d6bf5ebdcb8c012e8e2f02052435408" title="Llama 3.3 Nemotron 49B Super appears on LMSYS Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v330lkoru2qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgmavv/llama_33_nemotron_49b_super_appears_on_lmsys_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T17:25:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgleax</id>
    <title>Hunyuan releases T1 reasoning model</title>
    <updated>2025-03-21T16:48:08+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"&gt; &lt;img alt="Hunyuan releases T1 reasoning model" src="https://b.thumbs.redditmedia.com/HdMGpIqacQMghH2w635c1CNcfB8i3TAb_aRI9P856rE.jpg" title="Hunyuan releases T1 reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan announces T1 reasoning model&lt;/p&gt; &lt;p&gt;Meet Hunyuan-T1, the latest breakthrough in AI reasoning! Powered by Hunyuan TurboS, it's built for speed, accuracy, and efficiency. ðŸ”¥&lt;/p&gt; &lt;p&gt;âœ… Hybrid-Mamba-Transformer MoE Architecture â€“ The first of its kind for ultra-large-scale reasoning âœ… Strong Logic &amp;amp; Concise Writing â€“ Precise following of complex instructions âœ… Low Hallucination in Summaries â€“Trustworthy and reliable outputs âœ… Blazing Fast â€“First character in 1 sec, 60-80 tokens/sec generation speed âœ… Excellent Long-Text Processing â€“Handle complex contexts with ease&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en"&gt;https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/spaces/tencent/Hunyuan-T1"&gt;https://huggingface.co/spaces/tencent/Hunyuan-T1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;** Model weights have not been released yet, but based on Hunyuanâ€™s promise to open source their models, I expect the weights to be released soon **&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgleax"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgleax/hunyuan_releases_t1_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqx85</id>
    <title>AITER: AI Tensor Engine For ROCm</title>
    <updated>2025-03-21T20:38:55+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/software-tools-optimization/aiter%3A-ai-tensor-engine-for-rocm%E2%84%A2/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqx85/aiter_ai_tensor_engine_for_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqx85/aiter_ai_tensor_engine_for_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgkqio</id>
    <title>New BitNet Model from Deepgrove</title>
    <updated>2025-03-21T16:20:41+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt; &lt;img alt="New BitNet Model from Deepgrove" src="https://external-preview.redd.it/75Zv26Tb9ec8ndEGYBOYu42vtVHBipVRmB1cGkts4ZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd92282c47c9015d2d9452a76f2cbf3d52257025" title="New BitNet Model from Deepgrove" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepgrove-ai/Bonsai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgft94</id>
    <title>ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity</title>
    <updated>2025-03-21T12:37:24+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt; &lt;img alt="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" src="https://preview.redd.it/efejft8gf1qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80345d3319d8c121635235946e7b1d2c0eb17a6" title="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flexible Photo Recrafting While Preserving Your Identity&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://bytedance.github.io/InfiniteYou/"&gt;https://bytedance.github.io/InfiniteYou/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/bytedance/InfiniteYou"&gt;https://github.com/bytedance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ByteDance/InfiniteYou"&gt;https://huggingface.co/ByteDance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efejft8gf1qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:37:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgnye9</id>
    <title>RTX Pro Blackwell Pricing Listed</title>
    <updated>2025-03-21T18:33:48+00:00</updated>
    <author>
      <name>/u/AlohaGrassDragon</name>
      <uri>https://old.reddit.com/user/AlohaGrassDragon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro Blackwell pricing is up on &lt;a href="http://connection.com"&gt;connection.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6000 (24064 cores, 96GB, 1.8 TB/s, 600W, 2-slot flow through) - $8565&lt;/p&gt; &lt;p&gt;6000 Max-Q (24064 cores, 96GB, 1.8 TB/s, 300W, 2-slot blower) - $8565&lt;/p&gt; &lt;p&gt;5000 (14080 cores, 48GB, 1.3 TB/s, 300W, 2-slot blower) - $4569&lt;/p&gt; &lt;p&gt;4500 (10496 cores, 32GB, 896 GB/s, 200W, 2-slot blower) - $2623&lt;/p&gt; &lt;p&gt;4000 (8960 cores, 24GB, 672 GB/s, 140W, 1-slot blower) - $1481&lt;/p&gt; &lt;p&gt;I'm not sure if this is real or final pricing, but I could see some of these models being compelling for local LLM. The 5000 is competitive with current A6000 used pricing, the 4500 is not too far away price-wise from a 5090 with better power/thermals, and the 4000 with 24 GB in a single slot for ~$1500 at 140W is very competitive with a used 3090. It costs more than a 3090, but comes with a warranty and you can fit many more in a system because of the size and power without having to implement an expensive watercooling or dual power supply setup.&lt;/p&gt; &lt;p&gt;All-in-all, if this is real pricing, it looks to me that they are marketing to us directly and they see their biggest competitor as used nVidia cards.&lt;/p&gt; &lt;p&gt;*Edited to add per-card specs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlohaGrassDragon"&gt; /u/AlohaGrassDragon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgnye9/rtx_pro_blackwell_pricing_listed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T18:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgsw1e</id>
    <title>We built an open source mock interviews platform empowered by ollama</title>
    <updated>2025-03-21T22:04:49+00:00</updated>
    <author>
      <name>/u/Boring_Rabbit2275</name>
      <uri>https://old.reddit.com/user/Boring_Rabbit2275</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt; &lt;img alt="We built an open source mock interviews platform empowered by ollama" src="https://preview.redd.it/4k3ec6po84qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24694323661f97c1dfe5bb63a0f6f9d073c2e8ef" title="We built an open source mock interviews platform empowered by ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come practice your interviews for free using our project on GitHub here: &lt;a href="https://github.com/Azzedde/aiva_mock_interviews"&gt;https://github.com/Azzedde/aiva_mock_interviews&lt;/a&gt; We are two junior AI engineers, and we would really appreciate feedback on our work. Please star it if you like it.&lt;/p&gt; &lt;p&gt;We find that the junior era is full of uncertainty, and we want to know if we are doing good work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boring_Rabbit2275"&gt; /u/Boring_Rabbit2275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k3ec6po84qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgsw1e/we_built_an_open_source_mock_interviews_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T22:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgfmn8</id>
    <title>Docker's response to Ollama</title>
    <updated>2025-03-21T12:27:37+00:00</updated>
    <author>
      <name>/u/Barry_Jumps</name>
      <uri>https://old.reddit.com/user/Barry_Jumps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one excited about this?&lt;/p&gt; &lt;p&gt;Soon we can &lt;code&gt;docker run model mistral/mistral-small&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.docker.com/llm/"&gt;https://www.docker.com/llm/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s"&gt;https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most exciting for me is that docker desktop will finally allow container to access my Mac's GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Barry_Jumps"&gt; /u/Barry_Jumps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:27:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgopeg</id>
    <title>Orpheus-FastAPI: Local TTS with 8 Voices &amp; Emotion Tags (OpenAI Endpoint Compatible)</title>
    <updated>2025-03-21T19:04:54+00:00</updated>
    <author>
      <name>/u/townofsalemfangay</name>
      <uri>https://old.reddit.com/user/townofsalemfangay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;I just released Orpheus-FastAPI, a high-performance Text-to-Speech server that connects to your local LLM inference server using Orpheus's latest release. You can hook it up to OpenWebui, SillyTavern, or just use the web interface to generate audio natively.&lt;/p&gt; &lt;p&gt;I'd very much recommend if you want to get the most out of it in terms of suprasegmental features (the modalities of human voice, ums, arrs, pauses, like Sesame has) you use a System prompt to make the model respond as such (including the Syntax baked into the model). I included examples on my git so you can see how close this is to Sesame's CSM.&lt;/p&gt; &lt;p&gt;It uses a quantised version of the Orpheus 3B model (I've also included a direct link to my Q8 GGUF) that can run on consumer hardware, and works with GPUStack (my favourite), LM Studio, or llama.cpp.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;https://github.com/Lex-au/Orpheus-FastAPI&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf"&gt;https://huggingface.co/lex-au/Orpheus-3b-FT-Q8_0.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/townofsalemfangay"&gt; /u/townofsalemfangay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgopeg/orpheusfastapi_local_tts_with_8_voices_emotion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgap0q</id>
    <title>SpatialLM: A large language model designed for spatial understanding</title>
    <updated>2025-03-21T06:43:28+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt; &lt;img alt="SpatialLM: A large language model designed for spatial understanding" src="https://external-preview.redd.it/Z2F4NmRpYWFvenBlMV9xklPr-alq2N0OOZexCtU6lC7spKP7fvQP_oR6XFl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d9dd664854a15924490e41428c31c299e3851e" title="SpatialLM: A large language model designed for spatial understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9hvol38aozpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgl41s</id>
    <title>Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!</title>
    <updated>2025-03-21T16:36:11+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt; &lt;img alt="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" src="https://preview.redd.it/vcb57bt1m2qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374682829fe92002bc36926e45cf71896aada6ea" title="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to their blog post here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vcb57bt1m2qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgio2g</id>
    <title>Qwen 3 is coming soon!</title>
    <updated>2025-03-21T14:53:25+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgp6sw</id>
    <title>China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd</title>
    <updated>2025-03-21T19:25:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt; &lt;img alt="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" src="https://b.thumbs.redditmedia.com/Uv9b5l37Z3HDbLOdsI_RvWZEDLPnFUNe8L0-bY4NmCE.jpg" title="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgp6sw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
</feed>
