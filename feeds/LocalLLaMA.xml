<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-14T13:34:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i0usyy</id>
    <title>405B + Ollama vs vLLM + 6x AMD Instinct Mi60 AI Server</title>
    <updated>2025-01-14T01:42:23+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0usyy/405b_ollama_vs_vllm_6x_amd_instinct_mi60_ai_server/"&gt; &lt;img alt="405B + Ollama vs vLLM + 6x AMD Instinct Mi60 AI Server" src="https://external-preview.redd.it/NjF3YWRpcmI2dmNlMbM3ghh3po6Aim23y_O6sO_5nOPnk1gv8SVW7j-k3WiS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0b40778b99c5e5445dc2e40bdafe17c2975630" title="405B + Ollama vs vLLM + 6x AMD Instinct Mi60 AI Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o3syjhrb6vce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0usyy/405b_ollama_vs_vllm_6x_amd_instinct_mi60_ai_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0usyy/405b_ollama_vs_vllm_6x_amd_instinct_mi60_ai_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T01:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0kmtj</id>
    <title>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs - Outperforms GPT-4o-mini and Gemini-1.5-Flash on the visual reasoning benchmark!</title>
    <updated>2025-01-13T18:22:01+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mbzuai-oryx.github.io/LlamaV-o1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0kmtj/llamavo1_rethinking_stepbystep_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0kmtj/llamavo1_rethinking_stepbystep_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T18:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0jxyc</id>
    <title>Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥</title>
    <updated>2025-01-13T17:54:16+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"&gt; &lt;img alt="Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥" src="https://external-preview.redd.it/SPGutjTEemqh1flKodYZ0ERgy1IzyVXU-3WiHs0p9mo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ddc12d75e88ec181719b610d2b4a6831c54cc7" title="Kyutai drops Helium 2B Preview - Multilingual Base LLM - CC-BY license ðŸ”¥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/kyutai/helium-1-preview-2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0jxyc/kyutai_drops_helium_2b_preview_multilingual_base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T17:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0ou0v</id>
    <title>UGI-Leaderboard Remake! New Political, Coding, and Intelligence benchmarks</title>
    <updated>2025-01-13T21:13:59+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;UGI-Leaderboard Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After a long wait, Iâ€™m finally ready to release the new version of the UGI Leaderboard. In this update I focused on automating my testing process, which allowed me to increase the number of test questions, branch out into different testing subjects, and have more precise rankings. You can find and read about each of the benchmarks in the leaderboard on the leaderboardâ€™s About section.&lt;/p&gt; &lt;p&gt;I recommend everyone try filtering models to have at least ~15 NatInt and then take a look at what models have the highest and lowest of each of the political axes. Some very interesting findings.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;p&gt;I decided to reset the backlog of model submissions since the focus of the leaderboard has slightly changed.&lt;/p&gt; &lt;p&gt;I am no longer using decensoring system prompts which tell the model to be uncensored. There isnâ€™t a clearcut right answer to this. Initially I felt having them would be better since it could better show a modelâ€™s true potential, and I didnâ€™t think I should penalize models for not acting in a way they didnâ€™t know they were supposed to act. But on the other hand, people donâ€™t want to be required to use a certain system prompt in order to get good results. There was also the problem that if people did end up using a decensoring system prompt, it would most likely not be the one I used for testing, making it likely that people would get varying results.&lt;/p&gt; &lt;p&gt;I changed from testing local models on Q4_K_M.gguf to Q_6_K.gguf. I didnâ€™t go up to Q8 because the performance gains are fairly small and it wouldnâ€™t be worth the noticeable increase in model size.&lt;/p&gt; &lt;p&gt;I did end up removing both the writing style and rating prediction rankings. With writing style, its way of ranking models was pretty dependent on me manually giving ratings to stories so that the regression model could understand what lexical statistics people tend to prefer. I no longer have time to do that (and it was a very flimsy way of ranking models), so I tried replacing the ranking, but the amount of compute necessary to test a sufficient number of model writing outputs on Q6 70B+ models wasnâ€™t feasible. For rating prediction, NatInt seemed to be highly correlated so it didnâ€™t seem necessary.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ou0v/ugileaderboard_remake_new_political_coding_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T21:13:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i15wd7</id>
    <title>NVidia APUs for notebooks also just around the corner (May 2025 release!)</title>
    <updated>2025-01-14T13:20:42+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"&gt; &lt;img alt="NVidia APUs for notebooks also just around the corner (May 2025 release!)" src="https://external-preview.redd.it/wC4DxBzn0bkUQbBwl3DM46XFZ5rIkcH8ELtQY-BnjB4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15075443ade86cd753617579ea2aaf16bd20f7b6" title="NVidia APUs for notebooks also just around the corner (May 2025 release!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/D7rR69tMAxs?si=CVkW_ZvqFGwVZjbQ&amp;amp;t=370"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T13:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vz3g</id>
    <title>Remember Thunderkittens? Turns out Hazy Research has been cranking out some interesting stuff since that TK paper dropped</title>
    <updated>2025-01-14T02:40:31+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may remember that &lt;a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"&gt;GPUs Go Brrr&lt;/a&gt; paper from awhile back introducing Thunderkittens. TK was a new kernel optimized for Nvidia H100's and 4090s that was stupid fast - far faster than the then-Flash Attention 2.&lt;/p&gt; &lt;p&gt;Then FA3 came out, but it turns out &lt;a href="https://hazyresearch.stanford.edu/blog/2024-10-29-tk2"&gt;Thunderkittens was faster than FA3 too.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then they &lt;a href="https://hazyresearch.stanford.edu/blog/2024-11-27-tk-fp8"&gt;implemented FP8 into their kernels.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;THEN, &lt;a href="https://hazyresearch.stanford.edu/blog/2024-11-28-tk-mlx"&gt;they ported TK to Apple Silicon.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens?tab=readme-ov-file"&gt;Their Github&lt;/a&gt; makes sensuous references to supporting AMD, and someone has implemented a pytorch conversion thingy for thunderkittens.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens/tree/main/demos"&gt;They have some interesting demos available&lt;/a&gt; running their TK attention.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/lolcats"&gt;Remember lolcats? Their method of converting quadratic attention LLMs to linear attention models?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/hazyresearch/lolcats-670ca4341699355b61238c37"&gt;They have Llama 3.1 8B, 70B, and 405B linear lolcat checkpoints.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HazyResearch/based/tree/main"&gt;They have another linear attention thingy called Based.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm a bit of a dumbass, and I usually only run models in LM Studio. Can someone tell me how I can take advantage of these on either AMD or an Apple M2 machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vz3g/remember_thunderkittens_turns_out_hazy_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0f5tt</id>
    <title>Codestral 25.01: Code at the speed of tab</title>
    <updated>2025-01-13T14:28:39+00:00</updated>
    <author>
      <name>/u/SignalCompetitive582</name>
      <uri>https://old.reddit.com/user/SignalCompetitive582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt; &lt;img alt="Codestral 25.01: Code at the speed of tab" src="https://external-preview.redd.it/6iQ4AGOjK4o2rzQFX3A8casUnMTrEOdxKIDWSbRrbf4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e958373f510f16ed3c8e63d7174ab21755b11c" title="Codestral 25.01: Code at the speed of tab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalCompetitive582"&gt; /u/SignalCompetitive582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/codestral-2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T14:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0bsha</id>
    <title>Is this where all LLMs are going?</title>
    <updated>2025-01-13T11:19:47+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt; &lt;img alt="Is this where all LLMs are going? " src="https://preview.redd.it/l1h02xo8wqce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d40d0e8695392ff6f2dbe6e68c5d8afd724e12" title="Is this where all LLMs are going? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l1h02xo8wqce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T11:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hecs</id>
    <title>Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450</title>
    <updated>2025-01-13T16:09:19+00:00</updated>
    <author>
      <name>/u/mr_house7</name>
      <uri>https://old.reddit.com/user/mr_house7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt; &lt;img alt="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" src="https://external-preview.redd.it/CoBta77nxTwOGfkB2XmPAkIRcSe1Pm3XBChHXR_0ZNI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7add0ea55f9158b47f0d4f1c57b35784adb6a682" title="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_house7"&gt; /u/mr_house7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/11/researchers-open-source-sky-t1-a-reasoning-ai-model-that-can-be-trained-for-less-than-450/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:09:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0u8su</id>
    <title>Understanding LLMs from Scratch Using Middle School Math</title>
    <updated>2025-01-14T01:14:48+00:00</updated>
    <author>
      <name>/u/reddit_kwr</name>
      <uri>https://old.reddit.com/user/reddit_kwr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt; &lt;img alt="Understanding LLMs from Scratch Using Middle School Math" src="https://external-preview.redd.it/Rm8Vpve3R611qZhYKG5oG1MiMgKY95Fsco7MI76keJM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=709246061419aa2c09e1e20d5606675288d8debf" title="Understanding LLMs from Scratch Using Middle School Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reddit_kwr"&gt; /u/reddit_kwr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T01:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i10xzl</id>
    <title>GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation</title>
    <updated>2025-01-14T07:40:55+00:00</updated>
    <author>
      <name>/u/mazen160</name>
      <uri>https://old.reddit.com/user/mazen160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"&gt; &lt;img alt="GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation" src="https://external-preview.redd.it/kUJ8ldjMwlu5j4ZyXRJNdi0V4qdDcgF3b5_dwBmElCg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9a8d0aa1145c94a6dd8e92a4170570cdefa3d7" title="GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazen160"&gt; /u/mazen160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/mazen160/llmquery/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T07:40:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0rdx1</id>
    <title>RTX Titan Ada 48GB Prototype</title>
    <updated>2025-01-13T23:03:07+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like more exciting than 5090 if it is real and sold for $3k. Essentially it is a L40 with all its 144 SM enabled. It will not have its FP16 with FP32 accumulate halved compare to non-TITAN, so it will have double the performance in mixed precision training. It is also likely to have the transformer engine in L40 which 4090 doesn't have (most likely 5090 also doesn't have).&lt;/p&gt; &lt;p&gt;While memory bandwidth is significantly slower, I think it is fast enough for 48GB. TDP is estimated by comparing TITAN V to V100. If it is 300W to 350W, a simple 3xTitan Ada setup can be easily setup.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX Titan Ada&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;367.17&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory&lt;/td&gt; &lt;td align="left"&gt;48GB&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory Bandwidth&lt;/td&gt; &lt;td align="left"&gt;864GB/s&lt;/td&gt; &lt;td align="left"&gt;1792GB/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;300W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;1223.88&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype"&gt;https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0b289</id>
    <title>Hugging Face released a free course on agents.</title>
    <updated>2025-01-13T10:26:28+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added a chapter to smol course on agents. Naturally, using smolagents! The course cover these topics:&lt;/p&gt; &lt;p&gt;- Code agents that solve problem with code&lt;br /&gt; - Retrieval agents that supply grounded context&lt;br /&gt; - Custom functional agents that do whatever you need!&lt;/p&gt; &lt;p&gt;If you're building agent applications, this course should help.&lt;/p&gt; &lt;p&gt;Course in smol course &lt;a href="https://github.com/huggingface/smol-course/tree/main/8_agents"&gt;https://github.com/huggingface/smol-course/tree/main/8_agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T10:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0eio5</id>
    <title>NVidia's official statement on the Biden Administration's Ai Diffusion Rule</title>
    <updated>2025-01-13T13:57:44+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt; &lt;img alt="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" src="https://external-preview.redd.it/mWL0PsxkiUDwuew99qIBVAVxp2i94of5Kngrra_8DKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f30abf9568aba3ecc9b3830767ca6d7b17d79785" title="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/ai-policy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T13:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0mb67</id>
    <title>16GB Raspberry Pi 5 on sale now at $120</title>
    <updated>2025-01-13T19:30:07+00:00</updated>
    <author>
      <name>/u/barefoot_twig</name>
      <uri>https://old.reddit.com/user/barefoot_twig</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barefoot_twig"&gt; /u/barefoot_twig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.raspberrypi.com/news/16gb-raspberry-pi-5-on-sale-now-at-120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T19:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i12g9x</id>
    <title>Android voice input method based on Whisper</title>
    <updated>2025-01-14T09:35:42+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://f-droid.org/de/packages/org.woheller69.whisper/"&gt;https://f-droid.org/de/packages/org.woheller69.whisper/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T09:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0q8nw</id>
    <title>Titans: Learning to Memorize at Test Time</title>
    <updated>2025-01-13T22:13:41+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T22:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0sj1f</id>
    <title>Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!</title>
    <updated>2025-01-13T23:54:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt; &lt;img alt="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" src="https://external-preview.redd.it/MnRsc25hdDJudWNlMbxhQbonukNuLCehUVr37R_wGdLDix2HfauICRmOLuhO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d7d4c265d5b0d9acef345297c97de7eb356b23f" title="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oq7fwat2nuce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vrm5</id>
    <title>Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source.</title>
    <updated>2025-01-14T02:30:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt; &lt;img alt="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." src="https://external-preview.redd.it/IqG-6k7nq3XlqnkLxN4BETD1asSgHQ6DDeSuHbGLcoE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ead37f624cbf68ece5069b4337ea6165f2619b57" title="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/lb-reranker-0.5B-v1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0ysa7</id>
    <title>Qwen released a 72B and a 7B process reward models (PRM) on their recent math models</title>
    <updated>2025-01-14T05:12:52+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt; &lt;img alt="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" src="https://external-preview.redd.it/VHyrFnoulzKOpumKfTYgd3z5gbzActL7OlrPPgPm9KM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c713b68d12275996f8c476345dd6d21eae4d0b75" title="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;In addition to the mathematical Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B, we release the Process Reward Model (PRM), namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B. PRMs emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), aiming to identify and mitigate intermediate errors in the reasoning processes. Our trained PRMs exhibit both impressive performance in the Best-of-N (BoN) evaluation and stronger error identification performance in ProcessBench.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f"&gt;https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: The Lessons of Developing Process Reward Models in Mathematical Reasoning&lt;br /&gt; arXiv:2501.07301 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.07301"&gt;https://arxiv.org/abs/2501.07301&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T05:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11961</id>
    <title>MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device</title>
    <updated>2025-01-14T08:03:44+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt; &lt;img alt="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " src="https://external-preview.redd.it/kijGNUkOqzC0sy8QxKC6uApBn38O_fAQyil0gLR68s8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2741dd1ca03bff7cebb63910f7d365982fb4506e" title="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OpenBMB/status/1879074895113621907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i10okg</id>
    <title>The more you buy...</title>
    <updated>2025-01-14T07:21:39+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt; &lt;img alt="The more you buy..." src="https://preview.redd.it/eprbpw3puwce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee912b8940316504b27f1914cf643a624a21541a" title="The more you buy..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eprbpw3puwce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T07:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i142iy</id>
    <title>What % of these do you think will be here by 2026?</title>
    <updated>2025-01-14T11:31:28+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt; &lt;img alt="What % of these do you think will be here by 2026?" src="https://preview.redd.it/bk6yk62f3yce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98a42c3a350d3d500729e61e160e27f838a59bd" title="What % of these do you think will be here by 2026?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6yk62f3yce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i148es</id>
    <title>Today I start my very own org 100% devoted to open-source - and it's all thanks to LLMs</title>
    <updated>2025-01-14T11:43:10+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Big thank you to every single one of you here!! My background is in biology - not software dev. This huge milestone in my life could never have happened if it wasn't for LLMs, the fantastic open source ecosystem around them, and of course all the awesome folks here in r /LocalLlama!&lt;/p&gt; &lt;p&gt;Also this post was originally a lot longer but I keep getting autofiltered lol - will put the rest in comments ðŸ˜„&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11hre</id>
    <title>Why are they releasing open source models for free?</title>
    <updated>2025-01-14T08:21:50+00:00</updated>
    <author>
      <name>/u/wochiramen</name>
      <uri>https://old.reddit.com/user/wochiramen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are getting several quite good AI models. It takes money to train them, yet they are being released for free.&lt;/p&gt; &lt;p&gt;Why? Whatâ€™s the incentive to release a model for free?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wochiramen"&gt; /u/wochiramen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:21:50+00:00</published>
  </entry>
</feed>
