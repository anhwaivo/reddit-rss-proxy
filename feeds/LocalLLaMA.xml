<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-05T13:09:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ihm8pl</id>
    <title>Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!</title>
    <updated>2025-02-04T16:52:46+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt; &lt;img alt="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" src="https://external-preview.redd.it/Isnr897tQLSa3f7knpDj7eFO6fjkOWORPvRfD442vlo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24f28b0bdfd72abaef01b7d591bc2878895b848b" title="Drummer's Anubis Pro 105B v1 - An upscaled L3.3 70B with continued training!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Anubis-Pro-105B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihm8pl/drummers_anubis_pro_105b_v1_an_upscaled_l33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T16:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihf0gb</id>
    <title>DeepSeek-R1's correct answers are generally shorter</title>
    <updated>2025-02-04T10:49:48+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt; &lt;img alt="DeepSeek-R1's correct answers are generally shorter" src="https://preview.redd.it/duiwqfpzq3he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29a0abef7ab5410cdc5f56f319bd47c9d15c366b" title="DeepSeek-R1's correct answers are generally shorter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/duiwqfpzq3he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihf0gb/deepseekr1s_correct_answers_are_generally_shorter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T10:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihrmf3</id>
    <title>Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction</title>
    <updated>2025-02-04T20:30:27+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"&gt; &lt;img alt="Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction" src="https://external-preview.redd.it/rYnB4HiKhgUDfmglDGVR2RBw6zihcpf49RKEaD8Ygz0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df76ccdc3d22de19b2ce8c76b6e1ee50cc8b2d3e" title="Beyond Reality: New LLaMA 3.1 Fine-Tune for Multi-Choice Interactive Fiction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lolzinventor/Llama-3.1-8B-BeyondReality"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihrmf3/beyond_reality_new_llama_31_finetune_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7oue</id>
    <title>[HotTake] QwQ is also in preview.</title>
    <updated>2025-02-05T11:12:43+00:00</updated>
    <author>
      <name>/u/MlNSOO</name>
      <uri>https://old.reddit.com/user/MlNSOO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The impact will dwarf that of the DeepSeek R1's. &lt;/p&gt; &lt;p&gt;I haven't really checked the info, but hear me out as they shouldn'tbe too far away from actual. I will correct it after work. &lt;/p&gt; &lt;p&gt;DeepSeek R1&lt;br /&gt; Preview release : late '24.Nov&lt;br /&gt; Size : 671B&lt;br /&gt; Preview score : o1 x 101% (?)&lt;br /&gt; Release : late '25.Jan &lt;/p&gt; &lt;p&gt;QwQ&lt;br /&gt; Preview release : early '24.Dec&lt;br /&gt; Size 32B&lt;br /&gt; Preview score : o1 x 85% (?)&lt;br /&gt; Release ????? &lt;/p&gt; &lt;p&gt;Not only that, look at the size difference. Imo,&lt;br /&gt; if there are 1 in 1,000,000 people who can run 671B on their PC,&lt;br /&gt; There are 1 in 1,000 people who can run 32B on their PC. &lt;/p&gt; &lt;p&gt;I am baffled with the imagination of everyone being able to run an OpenAI flagship-worthy AI in their house. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MlNSOO"&gt; /u/MlNSOO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7oue/hottake_qwq_is_also_in_preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7oue/hottake_qwq_is_also_in_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7oue/hottake_qwq_is_also_in_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:12:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5li1</id>
    <title>Is Phi finally any good?</title>
    <updated>2025-02-05T08:32:12+00:00</updated>
    <author>
      <name>/u/umataro</name>
      <uri>https://old.reddit.com/user/umataro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently (out of boredom) installed phi4:14b. Gave it a a few programming/devops tasks and the answers actually made sense. This had never happened with previous phi iterations. Phi2, 3, 3.5 were only good in benchmarks and spat out absolute bollocks for answers. So the question now is, is the model actually good or did they just train it on a dataset that makes sense for my use case but for everything else it's still dogsh*te? In my test tasks (Ansible with AWS, terraform) it wiped the floor with 14b-32b parameter sized competition and was roughly on par (sometimes better, sometimes worse) with qwen2.5-coder:14b.&lt;/p&gt; &lt;p&gt;If you feel it has improved significantly, which areas have you observed it in?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umataro"&gt; /u/umataro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5li1/is_phi_finally_any_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5li1/is_phi_finally_any_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5li1/is_phi_finally_any_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T08:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpzn2</id>
    <title>Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers</title>
    <updated>2025-02-04T19:24:03+00:00</updated>
    <author>
      <name>/u/thedudear</name>
      <uri>https://old.reddit.com/user/thedudear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt; &lt;img alt="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" src="https://b.thumbs.redditmedia.com/MEP1WTwX54qNwJ5gIxf_f4NzTdNmrqOfmjo_k4cVUrg.jpg" title="Epyc Turin (9355P) + 256 GB / 5600 mhz - Some CPU Inference Numbers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I decided that three RTX 3090s janked together with brackets and risers just wasn’t enough; I wanted a cleaner setup and a fourth 3090. To make that happen, I needed a new platform.&lt;/p&gt; &lt;p&gt;My requirements were: at least four double-spaced PCIe x16 slots, ample high-speed storage interfaces, and ideally, high memory bandwidth to enable some level of CPU offloading without tanking inference speed. Intel’s new Xeon lineup didn’t appeal to me, the P/E core setup seems more geared towards datacenters, and the pricing was brutal. Initially, I considered Epyc Genoa, but with the launch of Turin and its Zen 5 cores plus higher DDR5 speeds, I decided to go straight for it.&lt;/p&gt; &lt;p&gt;Due to the size of the SP5 socket and its 12 memory channels, boards with full 12-channel support sacrifice PCIe slots. The only board that meets my PCIe requirements, the ASRock GENOAD8X-2T/TCM, has just 8 DIMM slots, meaning we have to say goodbye to four whole memory channels.&lt;/p&gt; &lt;p&gt;Getting it up and running was an adventure. At the time, ASRock hadn’t released any Turin-compatible BIOS ROMs, despite claiming that an update to 10.03 was required (which wasn’t even available for download). The beta ROM they supplied refused to flash, failing with no discernible reason. Eventually, I had to resort to a ROM programmer (CH341a) and got it running on version 10.05.&lt;/p&gt; &lt;p&gt;If anyone has questions about the board, BIOS, or setup, feel free to ask, I’ve gotten way more familiar with this board than I ever intended to.&lt;/p&gt; &lt;p&gt;CPU: Epyc Turin 9355P - 32 Cores (8 CCD), 256 MB cache, 3.55 GHz Boosting 4.4 GHz - $3000 USD from cafe.electronics on Ebay (now ~$3300 USD).&lt;/p&gt; &lt;p&gt;RAM: 256 GB Corsair WS (CMA256GX5M8B5600C40) @ 5600 MHz - $1499 CAD (now ~$2400 - WTF!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.asrockrack.com/general/productdetail.asp?Model=GENOAD8X-2T/BCM#Specifications"&gt;Asrock GENOAD8X-2T/TCM Motherboard&lt;/a&gt; - ~$1500 CAD but going up in price&lt;/p&gt; &lt;p&gt;First off, a couple of benchmarks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fag5favty5he1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5a6b92917f908dedbe73201fc6fc48e820aa3a5"&gt;Passmark Memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p8e60vy946he1.png?width=879&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b08b8cc914a890e567b0e7aeb5f9e42251e855b9"&gt;Passmark CPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/slq3s3ub46he1.png?width=396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2f6711ae24b230edef6eeea872c229a293518be"&gt;CPU-Z Info Page - The chip seems to always be boosting to 4.4 GHz, which I don't mind. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekz7wf2d46he1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5112a56f91feb7ae1ea8bc946b5603e52a3ecb59"&gt;CPU-Z Bench - My i9 9820x would score ~7k @ 4.6 GHz. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;And finally some LMStudio (0 layers offloaded) tests:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/on0n624n66he1.png?width=340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d96479be841451a073caff569adb52d2e9387a00"&gt;Prompt: \&amp;quot;Write a 1000 word story about france's capital\&amp;quot; Llama-3.3-70B-Q8, 24 Threads. Model used 72 GB in RAM. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/je5ljie976he1.png?width=353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=809d046e8b19f1cdd903e09135bba50b734fae0f"&gt;Deepseek-R1-Distill-Llama-8B (Q8), 24 threads, 8.55 GB in memory. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm happy to run additional tests and benchmarks—just wanted to put this out there so people have the info and can weigh in on what they'd like to see. CPU inference is very usable for smaller models (&amp;lt;20B), while larger ones are still best left to GPUs/cloud (not that we didn’t already know this).&lt;/p&gt; &lt;p&gt;That said, we’re on a promising trajectory. With a 12-DIMM board (e.g., Supermicro H13-SSL) or a dual-socket setup (pending improvements in multi-socket inference), we could, within a year or two, see CPU inference becoming cost-competitive with GPUs on a per-GB-of-memory basis. Genoa chips have dropped significantly in price over the past six months—9654 (96-core) now sells for $2,500–$3,000—making this even more feasible.&lt;/p&gt; &lt;p&gt;I'm optimistic about continued development in CPU inference frameworks, as they could help alleviate the current bottleneck: VRAM and Nvidia’s AI hardware monopoly. My main issue is that for pure inference, GPU compute power is vastly underutilized—memory capacity and bandwidth are the real constraints. Yet consumers are forced to pay thousands for increasingly powerful GPUs when, for inference alone, that power is often unnecessary. Here’s hoping CPU inference keeps progressing!&lt;/p&gt; &lt;p&gt;Anyways, let me know your thoughts, and i'll do what I can to provide additional info.&lt;/p&gt; &lt;p&gt;Added:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zwvjz8nps6he1.png?width=946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1fb93ebb3d182906b528370fd2c17de20796b41"&gt;Likwid-Bench: 334 GB/s (likwid-bench -t load -i 128 -w M0:8GB)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek-R1-GGUF-IQ1_S:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;With Hyper V / SVM Disabled: }, &amp;quot;stats&amp;quot;: { &amp;quot;stopReason&amp;quot;: &amp;quot;eosFound&amp;quot;, &amp;quot;tokensPerSecond&amp;quot;: 6.620692403810844, &amp;quot;numGpuLayers&amp;quot;: -1, &amp;quot;timeToFirstTokenSec&amp;quot;: 1.084, &amp;quot;promptTokensCount&amp;quot;: 12, &amp;quot;predictedTokensCount&amp;quot;: 303, &amp;quot;totalTokensCount&amp;quot;: 315 } { &amp;quot;indexedModelIdentifier&amp;quot;: &amp;quot;unsloth/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf&amp;quot;, &amp;quot;identifier&amp;quot;: &amp;quot;deepseek-r1&amp;quot;, &amp;quot;loadModelConfig&amp;quot;: { &amp;quot;fields&amp;quot;: [ { &amp;quot;key&amp;quot;: &amp;quot;llm.load.llama.cpuThreadPoolSize&amp;quot;, &amp;quot;value&amp;quot;: 60 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.contextLength&amp;quot;, &amp;quot;value&amp;quot;: 4096 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.numExperts&amp;quot;, &amp;quot;value&amp;quot;: 24 }, { &amp;quot;key&amp;quot;: &amp;quot;llm.load.llama.acceleration.offloadRatio&amp;quot;, &amp;quot;value&amp;quot;: 0 } ] .... }, &amp;quot;useTools&amp;quot;: false } }, &amp;quot;stopStrings&amp;quot;: [] } }, { &amp;quot;key&amp;quot;: &amp;quot;llm.prediction.llama.cpuThreads&amp;quot;, &amp;quot;value&amp;quot;: 30 } ] }, &amp;quot;stats&amp;quot;: { &amp;quot;stopReason&amp;quot;: &amp;quot;eosFound&amp;quot;, &amp;quot;tokensPerSecond&amp;quot;: 5.173145579251154, &amp;quot;numGpuLayers&amp;quot;: -1, &amp;quot;timeToFirstTokenSec&amp;quot;: 1.149, &amp;quot;promptTokensCount&amp;quot;: 12, &amp;quot;predictedTokensCount&amp;quot;: 326, &amp;quot;totalTokensCount&amp;quot;: 338 } } --- Disabled Hyper V, got much better numbers, see above --- &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thedudear"&gt; /u/thedudear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpzn2/epyc_turin_9355p_256_gb_5600_mhz_some_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihd0rr</id>
    <title>Deepseek researcher says it only took 2-3 weeks to train R1&amp;R1-Zero</title>
    <updated>2025-02-04T08:18:16+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt; &lt;img alt="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" src="https://b.thumbs.redditmedia.com/btJal_QKjmZuASeULU0hjpBHdbEWu1y_eBgzmjTEd1U.jpg" title="Deepseek researcher says it only took 2-3 weeks to train R1&amp;amp;R1-Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ihd0rr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihd0rr/deepseek_researcher_says_it_only_took_23_weeks_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T08:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii0i9b</id>
    <title>Llama-3.3 and Qwen2.5 speed comparisons on a 4-GPU / 120GB VRAM system</title>
    <updated>2025-02-05T03:11:22+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a couple of speed tests with Llama-3.3 70B Instruct, Qwen-2.5 72B Instruct, and Qwen2.5 Coder 32B Instruct where I asked each of them to &amp;quot;write a flappy bird game in Python that will run on a MacBook&amp;quot;. For this test I didn't care about code quality or results, I just wanted the model to output approximately 1k tokens of code, for which the task of writing flappy bird is perfect. &lt;/p&gt; &lt;p&gt;The only data I was really interested in comparing was raw prompt processing speed and inference/generation speed. I figured some of the folks round here might be curious about the numbers, so here they are.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro M12SWA-TF motherboard&lt;/li&gt; &lt;li&gt;AMD Ryzen Threadripper Pro 3945WX&lt;/li&gt; &lt;li&gt;128GB DDR4 RAM&lt;/li&gt; &lt;li&gt;NVMe SSDs&lt;/li&gt; &lt;li&gt;1x EVGA RTX 3090 Ti 24GB&lt;/li&gt; &lt;li&gt;1x Pny RTX A6000 48GB&lt;/li&gt; &lt;li&gt;2x EVGA RTX 3090 FTW3 24GB&lt;/li&gt; &lt;li&gt;EVGA 2000W PSU running on dedicated 240V/20A 60Hz (USA)&lt;/li&gt; &lt;li&gt;All GPUs throttled at 250W&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Software setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu Linux&lt;/li&gt; &lt;li&gt;tabbyAPI / exllamav2 (8bpw exl2 quants unless otherwise noted)&lt;/li&gt; &lt;li&gt;tensor parallel enabled&lt;/li&gt; &lt;li&gt;speculative decoding (draft mode) enabled&lt;/li&gt; &lt;li&gt;context lengths for Llama and Qwen are empirically the ceiling of what I can fit in available VRAM (120GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLama-3.3 70B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Llama-3.2-3B-Instruct-exl2_8.obpw&lt;/li&gt; &lt;li&gt;Main Model: Llama-3.3-70B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 108,544 bytes&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 44.12 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 30.89 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 72B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-3B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-72B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 128,512&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 97.83 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 37.93 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 1.5B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-1.5B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 246.16 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 65.24 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-3B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 201.84 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 57.08 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I find it interesting that Qwen 72B was faster than Llama 70B by a whopping 7 tokens/sec despite each model using a 3B draft model, Qwen being 2B parameters larger, and Qwen having an extra 20k bytes of context. My guess is that the output of the smaller Qwen model is more closely matched to its larger counterpart than the Llama models, which therefore boosts the speed of speculative decoding... but I'm just pulling guesses out of my butt. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T03:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3yz2</id>
    <title>Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing.</title>
    <updated>2025-02-05T06:33:23+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"&gt; &lt;img alt="Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing." src="https://preview.redd.it/bxtskf7em9he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=565ebb1694849b7e6d120b14c4949f34ca56a7e2" title="Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The base model can be found here: &lt;a href="https://huggingface.co/katanemo/Arch-Function-3B"&gt;https://huggingface.co/katanemo/Arch-Function-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we have yet to publish the LoRA because we beta-testing it and want more practical real world scenarios validated. But in our internal benchmarks we were able to show it’s comparable to SOTA while be negligee in cost and leaps and bounds faster. So we integrated in Arch - the intelligent edge and LLM proxy for agentic apps - and hosted it. Would for folks to try it so that we release it in a few weeks &lt;/p&gt; &lt;p&gt;More on prompt targets: &lt;a href="https://docs.archgw.com/concepts/prompt_target.html"&gt;https://docs.archgw.com/concepts/prompt_target.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More on the project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bxtskf7em9he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihvrb8</id>
    <title>New (Evil) Thinking Model: Skynet-3B</title>
    <updated>2025-02-04T23:23:42+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Today, we are releasing a new experimental model: &lt;strong&gt;Art-Skynet-3B&lt;/strong&gt;, fine-tuned on &lt;strong&gt;LLaMa 3.2 3B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This experiment explores developing models capable of reasoning like &lt;strong&gt;DeepSeek-r1&lt;/strong&gt; and &lt;strong&gt;OpenAI-o3&lt;/strong&gt;, with a long-term goal of &lt;em&gt;world domination&lt;/em&gt; (as a test, of course 😉).&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Model card:&lt;/strong&gt; &lt;a href="https://huggingface.co/AGI-0/Art-Skynet-3B"&gt;https://huggingface.co/AGI-0/Art-Skynet-3B&lt;/a&gt; &lt;em&gt;(Leave a like on the repo if you enjoy this model!)&lt;/em&gt;&lt;br /&gt; 🔹 &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;Note: If the model doesn’t seem to work, try regenerating the answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T23:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihxypo</id>
    <title>Cheap, Effectual, Robot Arms Open Weight model, meet Pi0</title>
    <updated>2025-02-05T01:06:13+00:00</updated>
    <author>
      <name>/u/bennmann</name>
      <uri>https://old.reddit.com/user/bennmann</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.physicalintelligence.company/download/pi0.pdf"&gt;https://www.physicalintelligence.company/download/pi0.pdf&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/pi0"&gt;https://huggingface.co/blog/pi0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;open weights robotics VLM (SigLIP 400M + Gemma 2.6B at 50hz) &lt;/p&gt; &lt;p&gt;it can fold laundry, pair of robot arm can be made with BOM of less than $300&lt;/p&gt; &lt;p&gt;Build your robot arms:&lt;br /&gt; &lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;https://github.com/TheRobotStudio/SO-ARM100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bennmann"&gt; /u/bennmann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxypo/cheap_effectual_robot_arms_open_weight_model_meet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T01:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpdjh</id>
    <title>What to expect from Mistral's upcoming reasoning models?</title>
    <updated>2025-02-04T18:59:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt; &lt;img alt="What to expect from Mistral's upcoming reasoning models?" src="https://preview.redd.it/sa87uqtg66he1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4e05d3454d29e6d5755deb360afc707a7f84aa" title="What to expect from Mistral's upcoming reasoning models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sa87uqtg66he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T18:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihv2lz</id>
    <title>AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch</title>
    <updated>2025-02-04T22:53:21+00:00</updated>
    <author>
      <name>/u/Darkstar4125</name>
      <uri>https://old.reddit.com/user/Darkstar4125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt; &lt;img alt="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" src="https://external-preview.redd.it/3Nw1ySi8I9t9qNGxMrnWh6X9nJQqmR4w8i1-IlVJwSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa55aebd954efb86ff5114a964670c3f21850a30" title="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkstar4125"&gt; /u/Darkstar4125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/02/02/ai-systems-with-unacceptable-risk-are-now-banned-in-the-eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T22:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihh15n</id>
    <title>Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination</title>
    <updated>2025-02-04T12:57:18+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt; &lt;img alt="Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination" src="https://b.thumbs.redditmedia.com/EnNfi2ijJYZ4t9ufiyUC2FCEhoi78PEnlcIcOHPzAtE.jpg" title="Mistral boss says tech CEOs’ obsession with AI outsmarting humans is a ‘very religious’ fascination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0"&gt;https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/"&gt;https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T12:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5vi6</id>
    <title>Interview with Deepseek CEO Liang</title>
    <updated>2025-02-05T08:54:02+00:00</updated>
    <author>
      <name>/u/mesmerlord</name>
      <uri>https://old.reddit.com/user/mesmerlord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"&gt; &lt;img alt="Interview with Deepseek CEO Liang" src="https://external-preview.redd.it/65TzyKuMhkMpc_f2_BttXLzKse8GVSrynY3OE_u8jTY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5133312a80c92d85bc650060674afac9dd7c50f" title="Interview with Deepseek CEO Liang" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mesmerlord"&gt; /u/mesmerlord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HRsVZuEMlvI?si=2kgt1IFtaib8vstb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T08:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7dfq</id>
    <title>Upgrading 3090's from 24GB to 48GB</title>
    <updated>2025-02-05T10:49:34+00:00</updated>
    <author>
      <name>/u/CertainlyBright</name>
      <uri>https://old.reddit.com/user/CertainlyBright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm able to do this conversion but the raw parts cost is 550$ for all the new vram. &lt;/p&gt; &lt;p&gt;Plus labor, is that even worth it anymore for others to pay for the upgrade if I offer it as a service?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainlyBright"&gt; /u/CertainlyBright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T10:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihyutf</id>
    <title>Open Euro LLM launches</title>
    <updated>2025-02-05T01:49:49+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openeurollm.eu/launch-press-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T01:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihxbpe</id>
    <title>DeepSeek banned from Australian Government Devices</title>
    <updated>2025-02-05T00:36:23+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt; &lt;img alt="DeepSeek banned from Australian Government Devices" src="https://external-preview.redd.it/UT58snSyDCf1en1PY0Usy8X0-HIt9R0OcuYrukJSr_0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f710938759a6b3631d87cc438df355ecc2726248" title="DeepSeek banned from Australian Government Devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.abc.net.au/news/2025-02-04/deepseek-banned-from-federal-government-devices/104896770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T00:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihtpa2</id>
    <title>I just want to thank all organisations that did not stop open sourcing their results</title>
    <updated>2025-02-04T21:55:28+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a moment, I feared that entities like ClosedAI and Anthropic might alter the open-source paradigm in the realm of Machine Learning. Fortunately, it appears they have not succeeded, and the open-source community has emerged victorious. While the battle is far from over, and we may need to fight even harder, this initial triumph belongs to open source, to all of us.&lt;/p&gt; &lt;p&gt;Let's extend our gratitude to every organization, large and small, that has shared their models, papers, and code with the community. This collaborative spirit is essential for democratizing AI and achieving Artificial General Intelligence (AGI) collectively. By ensuring that the benefits of AI are accessible to all, rather than being monopolized by a few egomaniacs, we foster a more equitable future.&lt;/p&gt; &lt;p&gt;Let us continue to promote open-source initiatives and leave behind those who resist the democratization of AI. By embracing transparency and collaboration, we can build a future where AI serves the interests of all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T21:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii1rti</id>
    <title>If you are waiting for that cheap cable or even a P40 for your LLM build, don't hold your breath. The USPS has stopped all shipments from China and Hong Kong.</title>
    <updated>2025-02-05T04:18:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In accordance with Trump orders, the USPS has stopped shipments coming from China and HK. The China Post to USPS gateway is how much of this stuff is shipped.&lt;/p&gt; &lt;p&gt;Even after it starts back up. Expect to pay more since these small shipments are no longer exempt from tariffs. If it wasn't paid by the shipper, it'll be held at customs until you can pay the duty.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cbsnews.com/news/usps-suspends-packages-china-hong-kong/"&gt;https://www.cbsnews.com/news/usps-suspends-packages-china-hong-kong/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii1rti/if_you_are_waiting_for_that_cheap_cable_or_even_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii1rti/if_you_are_waiting_for_that_cheap_cable_or_even_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii1rti/if_you_are_waiting_for_that_cheap_cable_or_even_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T04:18:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqwnd</id>
    <title>OpenAI deep research but it's open source</title>
    <updated>2025-02-04T20:01:31+00:00</updated>
    <author>
      <name>/u/Thomjazz</name>
      <uri>https://old.reddit.com/user/Thomjazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt; &lt;img alt="OpenAI deep research but it's open source" src="https://external-preview.redd.it/VhiwZJj7J5TUIfA6ujpiUeYD8CI4AKINeo7sLJZlD5Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e05cfb506bcac565f349480a4b0d9ba18f4768b1" title="OpenAI deep research but it's open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46"&gt;https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thomjazz"&gt; /u/Thomjazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihph9f</id>
    <title>In case you thought your feedback was not being heard</title>
    <updated>2025-02-04T19:03:04+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt; &lt;img alt="In case you thought your feedback was not being heard" src="https://preview.redd.it/nvf2f1j876he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8871a5964dc8f2db918ba181e1157fc626b71" title="In case you thought your feedback was not being heard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nvf2f1j876he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7qfy</id>
    <title>I created a website that tracks AI regulations around the world</title>
    <updated>2025-02-05T11:15:59+00:00</updated>
    <author>
      <name>/u/techie_ray</name>
      <uri>https://old.reddit.com/user/techie_ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To help you stay on top of what governments are doing on AI, I created an interactive world map that tracks AI regulatory and policy developments around the world. Click on a region (or use the search bar) to view its profile. This website is updated regularly (including new regions to be added).&lt;/p&gt; &lt;p&gt;Free to access. No login required. This is for the community :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techieray.com/GlobalAIRegulationTracker"&gt;https://www.techieray.com/GlobalAIRegulationTracker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techie_ray"&gt; /u/techie_ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii82yg</id>
    <title>DeepSeek just released an official demo for DeepSeek VL2 Small - It's really powerful at OCR, text extraction and chat use-cases (Hugging Face Space)</title>
    <updated>2025-02-05T11:40:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small"&gt;https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Vaibhav (VB) Srivastav on X: &lt;a href="https://x.com/reach_vb/status/1887094223469515121"&gt;https://x.com/reach_vb/status/1887094223469515121&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Zizheng Pan on X: Our official huggingface space demo for DeepSeek-VL2 Small is out! A 16B MoE model for various vision-language tasks: &lt;a href="https://x.com/zizhpan/status/1887110842711162900"&gt;https://x.com/zizhpan/status/1887110842711162900&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3qvv</id>
    <title>Google Lifts a Ban on Using Its AI for Weapons and Surveillance</title>
    <updated>2025-02-05T06:18:38+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt; &lt;img alt="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" src="https://external-preview.redd.it/NrA5s-vSHIcN9SaUL0ETUoJ_dGVpcnD0UsdffV5wGi8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb92e2820ff849c88d693e6467404d7df633be8" title="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/google-responsible-ai-principles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:18:38+00:00</published>
  </entry>
</feed>
