<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-10T22:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j7s1ef</id>
    <title>Why Isn't There a Real-Time AI Translation App for Smartphones Yet?</title>
    <updated>2025-03-10T06:03:28+00:00</updated>
    <author>
      <name>/u/spbxspb</name>
      <uri>https://old.reddit.com/user/spbxspb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the advancements in AI, especially in language models and real-time processing, why don’t we have a truly seamless AI-powered translation app for smartphones? Something that works offline, translates speech in real-time with minimal delay, and supports multiple languages fluently.&lt;/p&gt; &lt;p&gt;Most current apps either require an internet connection, have significant lag, or struggle with natural-sounding translations. Given how powerful AI has become, it feels like we should already have a Star Trek-style universal translator by now.&lt;/p&gt; &lt;p&gt;Is it a technical limitation, a business decision, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spbxspb"&gt; /u/spbxspb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T06:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vwcr</id>
    <title>Deepseek coder v2</title>
    <updated>2025-03-10T10:53:20+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this model last night, for a 7B it is soooo good at web coding!!!&lt;/p&gt; &lt;p&gt;I have made a working calculator, pong, and flappy bird.&lt;/p&gt; &lt;p&gt;I'm using the lite model by lmstudio. best of all I'm getting 16 tps on my ryzen!!!&lt;/p&gt; &lt;p&gt;using this model in particular &lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vwcr/deepseek_coder_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8780s</id>
    <title>Kokoro: Improving LLM's Emotional Intelligence [Research]</title>
    <updated>2025-03-10T19:33:42+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yo community! Kokoro Research just dropped! It’s a prequel paper to upcoming research called, LOLI Trigger: Ludic Operant Learning Integration in Transcendent Emergence Triggering of LLMs’ about making AI more humane! Coming this week!&lt;/p&gt; &lt;p&gt;This one talks more about new classification approach which later can be directly merge into an LLM model!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence"&gt;https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can check other researches, especially TaMeR (novel training approach), and ELiTA (better datasets). Hope you like them! Note: this is mostly theoretical paper, do not expect too much math!&lt;/p&gt; &lt;p&gt;[THIS IS NOT AN AD, JUST SHARING STUFF WITH THE COMMUNITY]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j6cg</id>
    <title>&lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast</title>
    <updated>2025-03-09T22:11:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt; &lt;img alt="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" src="https://external-preview.redd.it/ZDFiNmN0NHptcW5lMdBuqabr-hQLmYC8Qi5X9EdtbTx_2YZ-hAZhcsR_hrB1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2014a6f010867f98da226a97f756cd7d035b3cb" title="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2wo0b8lqmqne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j82o15</id>
    <title>Fixed Ollama template for Mistral Small 3</title>
    <updated>2025-03-10T16:26:52+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was finding that Mistral Small 3 on Ollama (mistral-small:24b) had some trouble calling tools -- mainly, adding or dropping tokens that rendered the tool call as message content rather than an actual tool call.&lt;br /&gt; The chat template on the model's Huggingface page was actually not very helpful because it doesn't even include tool calling. I dug around a bit to find the Tekken V7 tokenizer, and sure enough the chat template for providing and calling tools didn't match up with Ollama's.&lt;/p&gt; &lt;p&gt;Here's a fixed version, and it's MUCH more consistent with tool calling:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- range $index, $_ := .Messages }} {{- if eq .Role &amp;quot;system&amp;quot; }}[SYSTEM_PROMPT]{{ .Content }}[/SYSTEM_PROMPT] {{- else if eq .Role &amp;quot;user&amp;quot; }} {{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS]{{ $.Tools }}[/AVAILABLE_TOOLS] {{- end }}[INST]{{ .Content }}[/INST] {{- else if eq .Role &amp;quot;assistant&amp;quot; }} {{- if .Content }}{{ .Content }} {{- if not (eq (len (slice $.Messages $index)) 1) }}&amp;lt;/s&amp;gt; {{- end }} {{- else if .ToolCalls }}[TOOL_CALLS] [ {{- range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{- end }}]&amp;lt;/s&amp;gt; {{- end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}[TOOL_RESULTS] [TOOL_CONTENT] {{ .Content }}[/TOOL_RESULTS] {{- end }} {{- end }} &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j82o15/fixed_ollama_template_for_mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T16:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87q8g</id>
    <title>Insights about the frontier math benchmark.</title>
    <updated>2025-03-10T19:55:45+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"&gt; &lt;img alt="Insights about the frontier math benchmark." src="https://preview.redd.it/9i47enyl3xne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9154ad9f0a41f80760fd091fd8b569a65f600129" title="Insights about the frontier math benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9i47enyl3xne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87q8g/insights_about_the_frontier_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j892ii</id>
    <title>RTX 3090 supply drying up on marketplaces in Europe</title>
    <updated>2025-03-10T20:51:17+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the flopped launches are leaving their traces in the GPU second hand markets. Even more so since the 4090 stopped production last fall already.&lt;/p&gt; &lt;p&gt;As popularity to self host models is on the rise and supply of new 24Gb+ cards stays dry, the all star for local AI models, the RTX 3090 is getting rare on marketplaces. In Switzerland they used to go for around CHF 650 - CHF 750. The lowest you find them now is 800.- if you're lucky, more likely CHF 900.-&lt;/p&gt; &lt;p&gt;Germany looks a little better at €650 the lowest but these are usually gone within three days and most supply is around €750 upwards. It's only a matter of time when sellers at the €650 mark will dry up.&lt;/p&gt; &lt;p&gt;On international Ebay the cards go for $800 upwards, used to be lower if I remember correctly.&lt;/p&gt; &lt;p&gt;What is your experience, are you looking for 3090s? What's your choice for your home servers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T20:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7uqv4</id>
    <title>v0.6.0 Update: Dive - An Open Source MCP Agent Desktop</title>
    <updated>2025-03-10T09:29:10+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt; &lt;img alt="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" src="https://external-preview.redd.it/emVhYWcxajZ6dG5lMatJSspljsEqCKIUqwl7TvTa14fhRRDdCNE6VsWU_1B_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d328715d2f1b9c124872513a6e543c4697240cf6" title="v0.6.0 Update: Dive - An Open Source MCP Agent Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e11wp2j6ztne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7uqv4/v060_update_dive_an_open_source_mcp_agent_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83md3</id>
    <title>Could GEMMA-3 Be Unveiled at GDC 2025 (March 18)?</title>
    <updated>2025-03-10T17:05:48+00:00</updated>
    <author>
      <name>/u/hCKstp4BtL</name>
      <uri>https://old.reddit.com/user/hCKstp4BtL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129"&gt;https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;in this session description, we can read that they will talk about &amp;quot;Gemma models&amp;quot; (among other things). I think everyone knows about &amp;quot;Gemma 2&amp;quot; and there is no need to mention it because everyone knows how it works, right? Bigger chance is that they will show &amp;quot;Gemma 3&amp;quot; and they will release it shorly? because it seems to me that the deadline of May 20-21 (Google I/O) is a bit too late.&lt;/p&gt; &lt;p&gt;It looks like Google wants to focus the eyes of game developers on Gemma, so that they can combine the models with their games to create: “new AI-based game features and mechanics.”&lt;/p&gt; &lt;p&gt;... and to make it work, I think such a &amp;quot;Gemma 3&amp;quot; model should be prioritize with &amp;quot;perfect JSON generation&amp;quot; for the interface model&amp;lt;-&amp;gt;game and also improved instruction following.&lt;/p&gt; &lt;p&gt;I waiting for a small model (7b-9b) to be good enough to make a game with llm controlling npc (not only talk).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hCKstp4BtL"&gt; /u/hCKstp4BtL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j88yr7</id>
    <title>llava seems to perform better the easier the answer is.. as do other models</title>
    <updated>2025-03-10T20:47:05+00:00</updated>
    <author>
      <name>/u/Blender-Fan</name>
      <uri>https://old.reddit.com/user/Blender-Fan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use llava:13b, which is not very big, so i had to squeeze as much performance as possible&lt;/p&gt; &lt;p&gt;And what i realized to get better outputs was:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Crop your images&lt;/li&gt; &lt;li&gt;Send your images smaller&lt;/li&gt; &lt;li&gt;Cleaner images work better&lt;/li&gt; &lt;li&gt;Demand less accuracy&lt;/li&gt; &lt;li&gt;Solve as much as possible of the task beforehand&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I sent a picture of three columns of handwritten words, and noticed that if i cropped the sides of the pages, the outputs improved. In fact, &lt;em&gt;cropping each list separately&lt;/em&gt; and sending each chunk in a different prompt also improved the output&lt;/p&gt; &lt;p&gt;Also, the supported resolution was 672x672, sending an image with a greater pixel count was kinda like sending a prompt greater than the context length&lt;/p&gt; &lt;p&gt;Typed text was easier to read than handwritten text. Says something about my handwriting, but also means &lt;/p&gt; &lt;p&gt;The more you tell about the picture, the better the output. If you send a living room's picture, say &amp;quot;this is a picture of a living room, describe it&amp;quot; than just saying &amp;quot;what's in this picture?&amp;quot;&lt;/p&gt; &lt;p&gt;Then, the less precision you demand, the less errors the model makes. Asking for a description of the living room will be fine, but you'll see errors if you ask for a list of the objects in the picture&lt;/p&gt; &lt;p&gt;Lessons: i don't think it was that much different than prompting a model like R1 (even tho R1 thinks, and llava doesn't). The less &lt;em&gt;thinkin&lt;/em&gt; the machine has to do, the better the result. The more space for error, the happier you'll be. Hence why image generators like DALL-E perform better when you give a detailed description, rather than just saying &amp;quot;a cat&amp;quot; (in fact they often change your prompt under the hood before actually processing it). It's better to ask &amp;quot;what do i need to start a lemonade stand&amp;quot; than to ask &amp;quot;give me ideas to make money in middle school&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blender-Fan"&gt; /u/Blender-Fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j88yr7/llava_seems_to_perform_better_the_easier_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j88yr7/llava_seems_to_perform_better_the_easier_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j88yr7/llava_seems_to_perform_better_the_easier_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T20:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7vx6z</id>
    <title>Open manus</title>
    <updated>2025-03-10T10:55:02+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;https://github.com/mannaandpoem/OpenManus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone got any views on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7vx6z/open_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T10:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7n2s5</id>
    <title>Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl</title>
    <updated>2025-03-10T01:18:27+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt; &lt;img alt="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" src="https://b.thumbs.redditmedia.com/aZROr3LtwGC89EWOHjMHoIbCvPQTB8Fs1jwIfYjEb8U.jpg" title="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1"&gt;https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Dorialexander/status/1898719861284454718"&gt;https://x.com/Dorialexander/status/1898719861284454718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837"&gt;https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jianxliao/status/1898861051183349870"&gt;https://x.com/jianxliao/status/1898861051183349870&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7zzdt</id>
    <title>All about LLMs</title>
    <updated>2025-03-10T14:32:28+00:00</updated>
    <author>
      <name>/u/meme_watcher69420</name>
      <uri>https://old.reddit.com/user/meme_watcher69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was given an offer to join this startup. They were impressed with my &amp;quot;knowledge&amp;quot; about AI and LLMs. But in reality, all my projects are made by pasting stuff from Claude, stackoverflow and improved with reading a few documents.&lt;/p&gt; &lt;p&gt;How do I get to know everything about setting up LLMs, integrating them into an application and deploying them? Is there a guide or a roadmap to it? I'll join this startup in a month so I got a bit of time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meme_watcher69420"&gt; /u/meme_watcher69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7zzdt/all_about_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7usrm</id>
    <title>EuroBERT: A High-Performance Multilingual Encoder Model</title>
    <updated>2025-03-10T09:33:09+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt; &lt;img alt="EuroBERT: A High-Performance Multilingual Encoder Model" src="https://external-preview.redd.it/CnsI7xgYaguHri1_uGabuT9boB9PVspWCoeHvZsE1IM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f747438c0575ca5fca91283bb815527cfb8627a" title="EuroBERT: A High-Performance Multilingual Encoder Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/EuroBERT/release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7usrm/eurobert_a_highperformance_multilingual_encoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T09:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85q5m</id>
    <title>every LLM metric you need to know</title>
    <updated>2025-03-10T18:31:58+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best way to improve LLM performance is to consistently benchmark your model using a well-defined set of metrics throughout development, rather than relying on “vibe check” coding—this approach helps ensure that any modifications don’t inadvertently cause regressions.&lt;/p&gt; &lt;p&gt;I’ve listed below some essential LLM metrics to know before you begin benchmarking your LLM. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Traditional NLP evaluation methods like BERT and ROUGE are fast, affordable, and reliable. However, their reliance on reference texts and inability to capture the nuanced semantics of open-ended, often complexly formatted LLM outputs make them less suitable for production-level evaluations. &lt;/p&gt; &lt;p&gt;LLM judges are much more effective if you care about evaluation accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG metrics&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates how effectively an LLM agent accomplishes a task as outlined in the input, based on tools called and the actual output of the agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-relevancy"&gt;Conversational Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Robustness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-prompt-alignment"&gt;Prompt Alignment:&lt;/a&gt; measures whether your LLM application is able to generate outputs that aligns with any instructions specified in your prompt template.&lt;/li&gt; &lt;li&gt;Output Consistency: measures the consistency of your LLM output given the same input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Custom metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Custom metrics are particularly effective when you have a specialized use case, such as in medicine or healthcare, where it is necessary to define your own criteria.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-llm-evals"&gt;GEval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; the most versatile custom metric for you to easily build deterministic decision trees for evaluation with the help of using LLM-as-a-judge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Red-teaming metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are hundreds of red-teaming metrics available, but bias, toxicity, and hallucination are among the most common. These metrics are particularly valuable for detecting harmful outputs and ensuring that the model maintains high standards of safety and reliability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Although this is quite lengthy, and a good starting place, it is by no means comprehensive. Besides this there are other categories of metrics like multimodal metrics, which can range from image quality metrics like image coherence to multimodal RAG metrics like multimodal contextual precision or recall. &lt;/p&gt; &lt;p&gt;For a more comprehensive list + calculations, you might want to visit &lt;a href="https://docs.confident-ai.com/"&gt;deepeval docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83zkt</id>
    <title>Don't underestimate the power of RAG</title>
    <updated>2025-03-10T17:21:09+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt; &lt;img alt="Don't underestimate the power of RAG" src="https://preview.redd.it/moz1h1pzbwne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=adc13823a2909ba4af349f77c5405bf1ce990c2e" title="Don't underestimate the power of RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moz1h1pzbwne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85snw</id>
    <title>[Experimental] Control the 'Thinking Effort' of QwQ &amp; R1 Models with a Custom Logits Processor</title>
    <updated>2025-03-10T18:34:42+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed several posts lately discussing how the QwQ model tends to produce an excessive amount of tokens, often leading it to &amp;quot;overthink&amp;quot; unnecessarily. I've also seen some creative attempts to control this behavior using carefully crafted system prompts.&lt;/p&gt; &lt;p&gt;To help address this issue more systematically, I've put together a small and simple solution using a custom &lt;strong&gt;logits processor&lt;/strong&gt;. This approach dynamically adjusts the likelihood of the end-of-thinking token (&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;) appearing during generation.&lt;/p&gt; &lt;p&gt;The basic idea:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can set a &amp;quot;thinking effort&amp;quot; parameter (&lt;code&gt;0.0&lt;/code&gt; = minimal thinking, token &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; quickly appears; &lt;code&gt;1.0&lt;/code&gt; = normal behavior, &amp;gt;&lt;code&gt;1.0&lt;/code&gt; = it takes longer to output the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token).&lt;/li&gt; &lt;li&gt;The logic is straightforward: once the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token has been generated, the processor stops adjusting logits for that sequence.&lt;/li&gt; &lt;li&gt;This allows controlling how much the model thinks (or “overthinks”) without complicated prompt engineering.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen good results in reducing unnecessary thinking tokens in simple tasks, but I haven't yet extensively tested how this might influence longer chain-of-thought (CoT) reasoning.&lt;/p&gt; &lt;p&gt;I'd love for others to try it out and share your experiences or thoughts!&lt;/p&gt; &lt;p&gt;Here’s the repo with code and examples for both llama-cpp-python (gguf models) and Hugging Face Transformers (&lt;strong&gt;Note:&lt;/strong&gt; The code is still very raw, not optimized, and not organized lol... —this is just to share the basic idea quickly with the community!):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/and270/thinking_effort_processor"&gt;https://github.com/and270/thinking_effort_processor&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Example (Qwen-1.5B R1-Distill)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; What is the capital of France?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular Inference:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Okay, so I need to figure out what the capital of France is. I've heard a few things before, but I'm not entirely sure. Let me start by recalling what I know about France. France is a country in Europe, known for its diverse landscapes and vibrant culture. The name &amp;quot;France&amp;quot; itself comes from the French word &amp;quot;français,&amp;quot; which means &amp;quot;french&amp;quot; or &amp;quot;colorful.&amp;quot; I think the capital is a significant city, maybe something like Paris or maybe another city...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(The model generates a lengthy reasoning sequence before concluding)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;...To summarize, I believe the capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thinking Effort Inference (0.1):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Any feedback or tests are very welcome!&lt;/p&gt; &lt;p&gt;Let me know your thoughts or experiences—I'm especially curious how this affects your use-cases with the QwQ or similar models. 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j80hbo</id>
    <title>Hunyuan-TurboS.</title>
    <updated>2025-03-10T14:54:37+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/TXhunyuan/status/1899105803073958010"&gt;https://twitter.com/TXhunyuan/status/1899105803073958010&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8554a</id>
    <title>Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark</title>
    <updated>2025-03-10T18:08:27+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt; &lt;img alt="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" src="https://b.thumbs.redditmedia.com/Yaa0ATPdfMfRcdIPRl3zAR-18YhojdxTeqLYJXaDdUk.jpg" title="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8554a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83imv</id>
    <title>We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models.</title>
    <updated>2025-03-10T17:01:38+00:00</updated>
    <author>
      <name>/u/ProKil_Chu</name>
      <uri>https://old.reddit.com/user/ProKil_Chu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt; &lt;img alt="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." src="https://external-preview.redd.it/j6U6CbNbZYwJeWKzZ2YpYx2JnlAn2s-v4-SyaPJ7zcM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b71aa459f95dab63628bf7dd6ae70b7382ecb25" title="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j83imv/video/t190t6fsewne1/player"&gt;https://reddit.com/link/1j83imv/video/t190t6fsewne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One thing that surprised us during benchmarking with EgoNormia is that Qwen 2.5 VL is indeed a very strong model for vision which rivals Gemini 1.5/2.0, better than GPT-4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/_Hao_Zhu/status/1899151181534134648"&gt;https://x.com/_Hao_Zhu/status/1899151181534134648&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://egonormia.org"&gt;https://egonormia.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eval code: &lt;a href="https://github.com/Open-Social-World/EgoNormia"&gt;https://github.com/Open-Social-World/EgoNormia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProKil_Chu"&gt; /u/ProKil_Chu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j84c79</id>
    <title>Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance</title>
    <updated>2025-03-10T17:35:19+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt; &lt;img alt="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" src="https://b.thumbs.redditmedia.com/GITV-BcVRUV84azdQvU9AjF2LmByzEd0hc-J34tPTRc.jpg" title="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j84c79"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87eum</id>
    <title>QwQ 32B can do it if you coach it 2 times</title>
    <updated>2025-03-10T19:41:49+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt; &lt;img alt="QwQ 32B can do it if you coach it 2 times" src="https://external-preview.redd.it/bGFmOXk2NDIxeG5lMalrzKbbY1wxsyua5vTpp1g3RTatq_ecPpvEXRJ-_J8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc2c544369e356552a9d78fa1f23bdc00fdf6c3" title="QwQ 32B can do it if you coach it 2 times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wn0l7421xne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8766b</id>
    <title>New rig who dis</title>
    <updated>2025-03-10T19:31:29+00:00</updated>
    <author>
      <name>/u/MotorcyclesAndBizniz</name>
      <uri>https://old.reddit.com/user/MotorcyclesAndBizniz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt; &lt;img alt="New rig who dis" src="https://b.thumbs.redditmedia.com/0XSP2n-GAI5n3Op8qnPsulZZgY7u_Dk_E6IZd3L-Ixg.jpg" title="New rig who dis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6x 3090 FE via 6x PCIe 4.0 x4 Oculink&lt;br /&gt; CPU: AMD 7950x3D&lt;br /&gt; MoBo: B650M WiFi&lt;br /&gt; RAM: 192GB DDR5 @ 4800MHz&lt;br /&gt; NIC: 10Gbe&lt;br /&gt; NVMe: Samsung 980 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MotorcyclesAndBizniz"&gt; /u/MotorcyclesAndBizniz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8766b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:31:29+00:00</published>
  </entry>
</feed>
