<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-21T17:05:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jfw3s9</id>
    <title>New Hugging Face and Unsloth guide on GRPO with Gemma 3</title>
    <updated>2025-03-20T18:40:05+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"&gt; &lt;img alt="New Hugging Face and Unsloth guide on GRPO with Gemma 3" src="https://preview.redd.it/ewr7fr183wpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f65f73fb39bd2b2c8a1075327ec39c9300440920" title="New Hugging Face and Unsloth guide on GRPO with Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewr7fr183wpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg5sbj</id>
    <title>Mistral-small 3.1 Vision for PDF RAG tested</title>
    <updated>2025-03-21T01:48:26+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. As promised from my previous post, Mistral 3.1 small vision tested.&lt;/p&gt; &lt;p&gt;TLDR - particularly noteworthy is that mistral-small 3.1 didn't just beat GPT-4o mini - it also outperformed both Pixtral 12B and Pixtral Large models. Also, this is a particularly hard test. only 2 models to score 100% are Sonnet 3.7 reasoning and O1 reasoning. We ask trick questions like things that are not in the image, ask it to respond in different languages and many other things that push the boundaries. Mistral-small 3.1 is the only open source model to score above 80% on this test.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ppGGEh1zEuU"&gt;https://www.youtube.com/watch?v=ppGGEh1zEuU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T01:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnw9x</id>
    <title>Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD</title>
    <updated>2025-03-20T12:36:16+00:00</updated>
    <author>
      <name>/u/Hyungsun</name>
      <uri>https://old.reddit.com/user/Hyungsun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt; &lt;img alt="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" src="https://b.thumbs.redditmedia.com/weO7zUFK46BYVtLrB9NR6mPsIETCehibf3c566iKGHI.jpg" title="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hyungsun"&gt; /u/Hyungsun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jfnw9x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg2xi1</id>
    <title>Switching back to llamacpp (from vllm)</title>
    <updated>2025-03-20T23:27:59+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was initially using llamacpp but switched to vllm as I need the &amp;quot;high-throughput&amp;quot; especially with parallel requests (metadata enrichment for my rag and only text models), but some points are pushing me to switch back to lcp:&lt;/p&gt; &lt;p&gt;- for new models (gemma 3 or mistral 3.1), getting the awq/gptq quants may take some time whereas llamacpp team is so reactive to support new models&lt;/p&gt; &lt;p&gt;- llamacpp throughput is now quite impressive and not so far from vllm for my usecase and GPUs (3090)!&lt;/p&gt; &lt;p&gt;- gguf take less VRAM than awq or gptq models&lt;/p&gt; &lt;p&gt;- once the models have been loaded, the time to reload in memory is very short&lt;/p&gt; &lt;p&gt;What are your experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T23:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg85ef</id>
    <title>QWQ can correct itself outside of &lt;think&gt; block</title>
    <updated>2025-03-21T03:55:28+00:00</updated>
    <author>
      <name>/u/Emergency-Map9861</name>
      <uri>https://old.reddit.com/user/Emergency-Map9861</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg85ef/qwq_can_correct_itself_outside_of_think_block/"&gt; &lt;img alt="QWQ can correct itself outside of &amp;lt;think&amp;gt; block" src="https://b.thumbs.redditmedia.com/zc8KKWF_9sXZ4JdtEesuv9IeVZCLftevtsSt9hgpguw.jpg" title="QWQ can correct itself outside of &amp;lt;think&amp;gt; block" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought this was pretty cool&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t21x5w6dtype1.jpg?width=1077&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=657809f0ecd3dba4c950a24f9397ffa63b564614"&gt;https://preview.redd.it/t21x5w6dtype1.jpg?width=1077&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=657809f0ecd3dba4c950a24f9397ffa63b564614&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emergency-Map9861"&gt; /u/Emergency-Map9861 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg85ef/qwq_can_correct_itself_outside_of_think_block/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg85ef/qwq_can_correct_itself_outside_of_think_block/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg85ef/qwq_can_correct_itself_outside_of_think_block/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T03:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgdm0t</id>
    <title>DeepSeek Distilled Qwen 7B and 14B on NPU for Windows on Snapdragon</title>
    <updated>2025-03-21T10:23:25+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot off the press, Microsoft just added Qwen 7B and 14B DeepSeek Distill models that run on NPUs. I think for the moment, only the Snapdragon X Hexagon NPU is supported using the QNN framework. I'm downloading them now and I'll report on their performance soon.&lt;/p&gt; &lt;p&gt;These are ONNX models that require Microsoft's AI Toolkit to run. You will need to install the AI Toolkit extension under Visual Studio Code.&lt;/p&gt; &lt;p&gt;My previous link on running the 1.5B model: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdm0t/deepseek_distilled_qwen_7b_and_14b_on_npu_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdm0t/deepseek_distilled_qwen_7b_and_14b_on_npu_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdm0t/deepseek_distilled_qwen_7b_and_14b_on_npu_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jghuer</id>
    <title>Aider setup for QwQ as architect and Qwen as editor with 24GB VRAM?</title>
    <updated>2025-03-21T14:16:30+00:00</updated>
    <author>
      <name>/u/RedditAddict6942O</name>
      <uri>https://old.reddit.com/user/RedditAddict6942O</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our lab has a 4090 and I would like to use these models together with Aider. We have a policy of &amp;quot;local models only&amp;quot; and use Qwen coder. QwQ is so much better at reasoning though. I would like to use it for Aiders architect stage and keep Qwen as editor, swapping the model loaded as needed. &lt;/p&gt; &lt;p&gt;Is there a pre-baked setup out there that does model switching with speculative decoding on both? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditAddict6942O"&gt; /u/RedditAddict6942O &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jghuer/aider_setup_for_qwq_as_architect_and_qwen_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jghuer/aider_setup_for_qwq_as_architect_and_qwen_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jghuer/aider_setup_for_qwq_as_architect_and_qwen_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:16:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg4ij5</id>
    <title>NEW MODEL: Reasoning Reka-Flash 3 21B (uncensored) - AUGMENTED.</title>
    <updated>2025-03-21T00:43:28+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;From DavidAU;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model has been augmented, and uses the NEO Imatrix dataset. Testing has shown a decrease in reasoning tokens up to 50%.&lt;/p&gt; &lt;p&gt;This model is also uncensored. (YES! - from the &amp;quot;factory&amp;quot;).&lt;/p&gt; &lt;p&gt;In &amp;quot;head to head&amp;quot; testing this model reasoning more smoothly, rarely gets &amp;quot;lost in the woods&amp;quot; and has stronger output.&lt;/p&gt; &lt;p&gt;And even the LOWEST quants it performs very strongly... with IQ2_S being usable for reasoning.&lt;/p&gt; &lt;p&gt;Lastly:&lt;/p&gt; &lt;p&gt;This model is reasoning/temp stable. Meaning you can crank the temp, and the reasoning is sound too.&lt;/p&gt; &lt;p&gt;7 Examples generation at repo, detailed instructions, additional system prompts to augment generation further and full quant repo here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech NOTE:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was a test case to see what augment(s) used during quantization would improve a reasoning model along with a number of different Imatrix datasets and augment options.&lt;/p&gt; &lt;p&gt;I am still investigate/testing different options at this time to apply not only to this model, but other reasoning models too in terms of Imatrix dataset construction, content, and generation and augment options.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For 37 more &amp;quot;reasoning/thinking models&amp;quot; go here: (all types,sizes, archs)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-thinking-reasoning-models-reg-and-moes-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-thinking-reasoning-models-reg-and-moes-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Service Note - Mistral Small 3.1 - 24B, &amp;quot;Creative&amp;quot; issues:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For those that found/find the new Mistral model somewhat flat (creatively) I have posted a System prompt here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Mistral-Small-3.1-24B-Instruct-2503-MAX-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Mistral-Small-3.1-24B-Instruct-2503-MAX-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(option #3) to improve it - it can be used with normal / augmented - it performs the same function.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T00:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jge0d3</id>
    <title>Any predictions for GPU pricing 6-12 months from now?</title>
    <updated>2025-03-21T10:50:08+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are we basically screwed as demand for local LLMs will only keep growing while GPU manufacturing output won't change much?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jge0d3/any_predictions_for_gpu_pricing_612_months_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jge0d3/any_predictions_for_gpu_pricing_612_months_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jge0d3/any_predictions_for_gpu_pricing_612_months_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfyqye</id>
    <title>Sesame CSM Gradio UI – Free, Local, High-Quality Text-to-Speech with Voice Cloning! (CUDA, Apple MLX and CPU)</title>
    <updated>2025-03-20T20:28:48+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just released &lt;strong&gt;Sesame CSM&lt;/strong&gt;, a &lt;strong&gt;100% local, free&lt;/strong&gt; text-to-speech tool with &lt;strong&gt;superior voice cloning&lt;/strong&gt;! No cloud processing, no API keys – just &lt;strong&gt;pure, high-quality AI-generated speech on your own machine&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🔥 Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Runs 100% locally&lt;/strong&gt; – No internet required!&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Free &amp;amp; Open Source&lt;/strong&gt; – No paywalls, no subscriptions.&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Superior Voice Cloning&lt;/strong&gt; – Built right into the UI!&lt;/p&gt; &lt;p&gt;✅ G&lt;strong&gt;radio UI&lt;/strong&gt; – A sleek interface for easy playback &amp;amp; control.&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Supports CUDA, MLX, and CPU&lt;/strong&gt; – Works on NVIDIA, Apple Silicon, and regular CPUs.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Check it out on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/akashjss/sesame-csm"&gt;Sesame CSM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts! Let me know if you try it out. Feedback &amp;amp; contributions are always welcome! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T20:28:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg9943</id>
    <title>Created a app as an alternative to Openwebui</title>
    <updated>2025-03-21T05:02:52+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9943/created_a_app_as_an_alternative_to_openwebui/"&gt; &lt;img alt="Created a app as an alternative to Openwebui" src="https://external-preview.redd.it/wBMT-jmCl5XYkWR1AWvetODsl1A2V4fj59LTyPe1WpI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b8331475460b9074f076a36ad33a972de04b5fe" title="Created a app as an alternative to Openwebui" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open web ui but its overwhelming and its taking up quite a lot of resources,&lt;/p&gt; &lt;p&gt;So i thought why not create an UI that has both ollama and comfyui support&lt;/p&gt; &lt;p&gt;And can create flow with both of them to create app or agents&lt;/p&gt; &lt;p&gt;And then created apps for Mac, Windows and Linux and Docker &lt;/p&gt; &lt;p&gt;And everything is stored in IndexDB.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9943/created_a_app_as_an_alternative_to_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9943/created_a_app_as_an_alternative_to_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T05:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgghuj</id>
    <title>Learning project - car assistant . My goal here was to create an in-car assistant that would process natural speech and operate various vehicle functions (satnav, hvac, entertainment, calendar management…) . Everything is running locally on a 4090 .</title>
    <updated>2025-03-21T13:12:31+00:00</updated>
    <author>
      <name>/u/Little_french_kev</name>
      <uri>https://old.reddit.com/user/Little_french_kev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgghuj/learning_project_car_assistant_my_goal_here_was/"&gt; &lt;img alt="Learning project - car assistant . My goal here was to create an in-car assistant that would process natural speech and operate various vehicle functions (satnav, hvac, entertainment, calendar management…) . Everything is running locally on a 4090 ." src="https://external-preview.redd.it/b3VleHh6MjJsMXFlMW1rLDBehdqFIqb8coLcozZe27jUC35kVaumDfoIT-Ol.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f7772a35d8a31b3da05ba01dc1edcd5181037b6" title="Learning project - car assistant . My goal here was to create an in-car assistant that would process natural speech and operate various vehicle functions (satnav, hvac, entertainment, calendar management…) . Everything is running locally on a 4090 ." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little_french_kev"&gt; /u/Little_french_kev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy8pj032l1qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgghuj/learning_project_car_assistant_my_goal_here_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgghuj/learning_project_car_assistant_my_goal_here_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T13:12:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgdb4a</id>
    <title>Using local QwQ-32B / Qwen2.5-Coder-32B in aider (24GB vram)</title>
    <updated>2025-03-21T10:02:31+00:00</updated>
    <author>
      <name>/u/bjodah</name>
      <uri>https://old.reddit.com/user/bjodah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently started using &lt;a href="https://aider.chat"&gt;aider&lt;/a&gt; and I was curious to see how Qwen's reasoning model and coder tune would perform as architect &amp;amp; editor respectively. I have a single 3090, so I need to use ~Q5 quants for both models, and I need to load/unload the models on the fly. I settled on using &lt;a href="https://docs.litellm.ai/docs/providers/litellm_proxy"&gt;litellm proxy&lt;/a&gt; (which is the endpoint recommended by aider's docs), together with &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; to automatically spawn &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md"&gt;llama.cpp server&lt;/a&gt; instances as needed.&lt;/p&gt; &lt;p&gt;Getting all these parts to play nice together in a container (I use podman, but docker should work with minimial tweaks, if any) was quite challenging. So I made an effort to collect my notes, configs and scripts and publish it as git repo over at: - &lt;a href="https://github.com/bjodah/local-aider"&gt;https://github.com/bjodah/local-aider&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Useage looks like: &lt;code&gt;console $ # the command below spawns a docker-compose config (or rather podman-compose) $ ./bin/local-model-enablement-wrapper \ aider \ --architect --model litellm_proxy/local-qwq-32b \ --editor-model litellm_proxy/local-qwen25-coder-32b &lt;/code&gt;&lt;/p&gt; &lt;p&gt;There are still some work to be done to get this working optimally. But hopefully my findings can be helpful for anyone trying something similar. If you try this out and spot any issue, please let me know, and if there are any similar resources, I'd love to hear about them too.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bjodah"&gt; /u/bjodah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdb4a/using_local_qwq32b_qwen25coder32b_in_aider_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdb4a/using_local_qwq32b_qwen25coder32b_in_aider_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdb4a/using_local_qwq32b_qwen25coder32b_in_aider_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgkqio</id>
    <title>New BitNet Model from Deepgrove</title>
    <updated>2025-03-21T16:20:41+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt; &lt;img alt="New BitNet Model from Deepgrove" src="https://external-preview.redd.it/75Zv26Tb9ec8ndEGYBOYu42vtVHBipVRmB1cGkts4ZM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd92282c47c9015d2d9452a76f2cbf3d52257025" title="New BitNet Model from Deepgrove" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepgrove-ai/Bonsai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgkqio/new_bitnet_model_from_deepgrove/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg9e35</id>
    <title>Just saw this, 32B sized Coder model trained for C++ coding made by HF? Looks cool. Any Cpp nerds wanna tell us how it performs?</title>
    <updated>2025-03-21T05:12:05+00:00</updated>
    <author>
      <name>/u/Ornery_Local_6814</name>
      <uri>https://old.reddit.com/user/Ornery_Local_6814</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9e35/just_saw_this_32b_sized_coder_model_trained_for_c/"&gt; &lt;img alt="Just saw this, 32B sized Coder model trained for C++ coding made by HF? Looks cool. Any Cpp nerds wanna tell us how it performs?" src="https://external-preview.redd.it/6pgHPYtDmwfDxvjUg8iN9Hj_K0BW8LHk4EcYXwR5unw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb9ec42fc1a63bd507bae656d8bcac8521db0603" title="Just saw this, 32B sized Coder model trained for C++ coding made by HF? Looks cool. Any Cpp nerds wanna tell us how it performs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ornery_Local_6814"&gt; /u/Ornery_Local_6814 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/open-r1/OlympicCoder-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9e35/just_saw_this_32b_sized_coder_model_trained_for_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg9e35/just_saw_this_32b_sized_coder_model_trained_for_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T05:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgf07w</id>
    <title>Vulkan 1.4.311 Released With New Extension For BFloat16</title>
    <updated>2025-03-21T11:51:53+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"&gt; &lt;img alt="Vulkan 1.4.311 Released With New Extension For BFloat16" src="https://external-preview.redd.it/2t1oSj_vzvn5bCPIAvxtQL6anEzXfRChmhjaylPaVXw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=738740aa1d4f0c6631304412cf45ecce910e09ab" title="Vulkan 1.4.311 Released With New Extension For BFloat16" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Vulkan-1.4.311-Released"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgf07w/vulkan_14311_released_with_new_extension_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T11:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgd0e8</id>
    <title>The Hugging Face Agents Course now includes three major agent frameworks (smolagents, langchain, and llamaindex)</title>
    <updated>2025-03-21T09:41:02+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Hugging Face Agents Course now includes three major agent frameworks.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/agents-course"&gt;https://huggingface.co/agents-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in distinctive ways so that learners can reflect on when and where to use each. &lt;/p&gt; &lt;p&gt;This also means that you can follow the course if you're already familiar with one of these frameworks, and soak up some of the fundamental knowledge in earlier units. &lt;/p&gt; &lt;p&gt;Hopefully, this makes the agents course as open to as many people as possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgd0e8/the_hugging_face_agents_course_now_includes_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T09:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgdvr7</id>
    <title>GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzen™ AI</title>
    <updated>2025-03-21T10:41:39+00:00</updated>
    <author>
      <name>/u/blazerx</name>
      <uri>https://old.reddit.com/user/blazerx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt; &lt;img alt="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzen™ AI" src="https://external-preview.redd.it/FfsH6_lD8ZDjWq58F-nI0b3jtl3po6p4fWNK_dOGUbY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=067fd5517027950d0c15b75fbdf4301c41337833" title="GAIA: An Open-Source Project from AMD for Running Local LLMs on Ryzen™ AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blazerx"&gt; /u/blazerx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/gaia-an-open-source-project-from-amd-for-running-local-llms-on-ryzen-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgdvr7/gaia_an_opensource_project_from_amd_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T10:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg0exn</id>
    <title>Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check</title>
    <updated>2025-03-20T21:38:25+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt; &lt;img alt="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check" src="https://external-preview.redd.it/KOwl-jl-bbq-ggDpuf4_ihRqXydkagmZwZ9bDY3co3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0adc6319f752a232be3be30ea4365da6e1a21d20" title="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick Breakdown (for those who don't want to read the full thing):&lt;/p&gt; &lt;p&gt;Intel’s former CEO, Pat Gelsinger, openly criticized NVIDIA, saying their AI GPUs are massively overpriced (he specifically said they're &amp;quot;10,000 times&amp;quot; too expensive) for AI inferencing tasks.&lt;/p&gt; &lt;p&gt;Gelsinger praised NVIDIA CEO Jensen Huang's early foresight and perseverance but bluntly stated Jensen &amp;quot;got lucky&amp;quot; with AI blowing up when it did.&lt;/p&gt; &lt;p&gt;His main argument: NVIDIA GPUs are optimized for AI training, but they're totally overkill for inferencing workloads—which don't require the insanely expensive hardware NVIDIA pushes.&lt;/p&gt; &lt;p&gt;Intel itself, though, hasn't delivered on its promise to challenge NVIDIA. They've struggled to launch competitive GPUs (Falcon Shores got canned, Gaudi has underperformed, and Jaguar Shores is still just a future promise).&lt;/p&gt; &lt;p&gt;Gelsinger thinks the next big wave after AI could be quantum computing, potentially hitting the market late this decade.&lt;/p&gt; &lt;p&gt;TL;DR: Even Intel’s former CEO thinks NVIDIA is price-gouging AI inferencing hardware—but admits Intel hasn't stepped up enough yet. CUDA dominance and lack of competition are keeping NVIDIA comfortable, while many of us just want affordable VRAM-packed alternatives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-former-ceo-claims-nvidia-ai-gpus-are-10000-times-more-expensive-than-what-is-needed-for-ai-inferencing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T21:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgau52</id>
    <title>Gemma 3 27b vs. Mistral 24b vs. QwQ 32b: I tested on personal benchmark, here's what I found out</title>
    <updated>2025-03-21T06:53:42+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for LLMs to use locally; the requirements are good enough reasoning and understanding, coding, and some elementary-level mathematics. I was looking into QwQ 32b, which seemed very promising.&lt;br /&gt; Last week, Google and Mistral released Gemma 3 27b and Mistral small 3.1 24b; from the benchmarks, both seem capable models approximating Deepseek r1 in ELO rating, which is impressive.&lt;/p&gt; &lt;p&gt;But, tbh, I have stopped caring about benchmarks, especially Lmsys; idk. The rankings always seem off when you try the models IRL.&lt;/p&gt; &lt;p&gt;So, I ran a small test to vibe-check which models to pick. I also benchmarked answers with Deepseek r1, as I use it often to get a better picture. &lt;/p&gt; &lt;p&gt;Here's what I found out&lt;/p&gt; &lt;h1&gt;For Coding&lt;/h1&gt; &lt;p&gt;QwQ 32b is just miles ahead in coding among the three. It sometimes does better code than Deepseek r1. They weren't lying in the benchmarks. It feels good to talk to you as well. Gemma is 2nd and does the job for easy tasks. Mistral otoh was bad.&lt;/p&gt; &lt;h1&gt;For Reasoning&lt;/h1&gt; &lt;p&gt;Again, Qwen was better. Well, ofc it's a reasoning model, but Gemma was also excellent. They made a good base model. Mistral was there but not there.&lt;/p&gt; &lt;h1&gt;For Math&lt;/h1&gt; &lt;p&gt;Gemma and QwQ were good enough for simple math tasks. Gemma, being a base model, was faster. I might test more with these two. Mistral was decent but 3rd again.&lt;/p&gt; &lt;h1&gt;What to pick?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;QwQ 32b is no doubt the best available model in its class. Great at coding, reasoning, and math. It's been a long since I used a local model, the last one was Mixtral, a year ago, and I never expected them to be this good. QwQ is promising; I can't wait for their new max model.&lt;/li&gt; &lt;li&gt;Gemma 3 27b is a solid base model. Great vibes. And you wouldn't be missing a lot with this. But it comes with a Gemma-specific license, which is more restrictive than Apache 2.0.&lt;/li&gt; &lt;li&gt;Mistral small 3.1 24b didn't impress me much; perhaps it needs more rigorous testing. &lt;/li&gt; &lt;li&gt;Both Gemma and Mistral Small have image support, so consider that as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the complete analysis, check out this blog post: &lt;a href="https://composio.dev/blog/qwq-32b-vs-gemma-3-mistral-small-vs-deepseek-r1/"&gt;Gemma 3 27b vs QwQ 32b vs Mistral 24b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to know which other model you're currently using and for what specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgau52/gemma_3_27b_vs_mistral_24b_vs_qwq_32b_i_tested_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgft94</id>
    <title>ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity</title>
    <updated>2025-03-21T12:37:24+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt; &lt;img alt="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" src="https://preview.redd.it/efejft8gf1qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80345d3319d8c121635235946e7b1d2c0eb17a6" title="ByteDance released on HuggingFace an open image model that generates Photo While Preserving Your Identity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flexible Photo Recrafting While Preserving Your Identity&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://bytedance.github.io/InfiniteYou/"&gt;https://bytedance.github.io/InfiniteYou/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/bytedance/InfiniteYou"&gt;https://github.com/bytedance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ByteDance/InfiniteYou"&gt;https://huggingface.co/ByteDance/InfiniteYou&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efejft8gf1qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgft94/bytedance_released_on_huggingface_an_open_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:37:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgl41s</id>
    <title>Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!</title>
    <updated>2025-03-21T16:36:11+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt; &lt;img alt="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" src="https://preview.redd.it/vcb57bt1m2qe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374682829fe92002bc36926e45cf71896aada6ea" title="Tencent introduces Hunyuan-T1, their large reasoning model. Competing with DeepSeek-R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to their blog post here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vcb57bt1m2qe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgl41s/tencent_introduces_hunyuant1_their_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T16:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgfmn8</id>
    <title>Docker's response to Ollama</title>
    <updated>2025-03-21T12:27:37+00:00</updated>
    <author>
      <name>/u/Barry_Jumps</name>
      <uri>https://old.reddit.com/user/Barry_Jumps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one excited about this?&lt;/p&gt; &lt;p&gt;Soon we can &lt;code&gt;docker run model mistral/mistral-small&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.docker.com/llm/"&gt;https://www.docker.com/llm/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s"&gt;https://www.youtube.com/watch?v=mk_2MIWxLI0&amp;amp;t=1544s&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most exciting for me is that docker desktop will finally allow container to access my Mac's GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Barry_Jumps"&gt; /u/Barry_Jumps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgfmn8/dockers_response_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T12:27:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgap0q</id>
    <title>SpatialLM: A large language model designed for spatial understanding</title>
    <updated>2025-03-21T06:43:28+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt; &lt;img alt="SpatialLM: A large language model designed for spatial understanding" src="https://external-preview.redd.it/Z2F4NmRpYWFvenBlMV9xklPr-alq2N0OOZexCtU6lC7spKP7fvQP_oR6XFl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94d9dd664854a15924490e41428c31c299e3851e" title="SpatialLM: A large language model designed for spatial understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9hvol38aozpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgap0q/spatiallm_a_large_language_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T06:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgio2g</id>
    <title>Qwen 3 is coming soon!</title>
    <updated>2025-03-21T14:53:25+00:00</updated>
    <author>
      <name>/u/themrzmaster</name>
      <uri>https://old.reddit.com/user/themrzmaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themrzmaster"&gt; /u/themrzmaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgio2g/qwen_3_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T14:53:25+00:00</published>
  </entry>
</feed>
