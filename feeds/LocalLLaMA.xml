<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-14T23:36:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lzyk1k</id>
    <title>GitHub - SrijanSriv211/Palm: Palm is a tree, not a language model</title>
    <updated>2025-07-14T21:05:41+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzyk1k/github_srijansriv211palm_palm_is_a_tree_not_a/"&gt; &lt;img alt="GitHub - SrijanSriv211/Palm: Palm is a tree, not a language model" src="https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c77dfededd901b5351e7adc5dd2be75ee167f27c" title="GitHub - SrijanSriv211/Palm: Palm is a tree, not a language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a simple experimental language model architecture based on Andrej Karpathy's nanoGPT project.&lt;/p&gt; &lt;p&gt;It's an experiment to try different improvements of transformers architecture. Some improvement has been brought about by the following techniques: - Modernized architecture: Rotary embeddings, QK-Norm, and ReLUÂ² - Untie head from embedding - SwiGLU in feed forward network. - Parallel layers proposed by Google's PaLM - Using a novel attention mechanism which I call &lt;code&gt;Attention On Detail&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;As well as many minor optimizations.&lt;/p&gt; &lt;h2&gt;How does &lt;code&gt;Attention On Detail&lt;/code&gt; works?&lt;/h2&gt; &lt;p&gt;It works by combining 3 ideas. - Multi-Headed Causal Self-Attention (MHA) - Attention Free Transformer (AFT) - A simple fourier series based equation &lt;code&gt;a*sin(x) + b*sin(x) + c*sin(x)*cos(x)&lt;/code&gt; where &lt;code&gt;x&lt;/code&gt; is normalized between &lt;code&gt;[-pi, pi]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The idea is simple. - Replace &lt;code&gt;Linear layers&lt;/code&gt; with an &lt;code&gt;AFT&lt;/code&gt; for each &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; in the &lt;code&gt;MHA&lt;/code&gt;. - In &lt;code&gt;AFT&lt;/code&gt;, generate 3 values, &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; from 3 different fourier series equations. - Compute output the &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; &amp;amp; &lt;code&gt;c&lt;/code&gt; values in each &lt;code&gt;AFT&lt;/code&gt;. - Now use those &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; values to calculate the attention score in the &lt;code&gt;MHA&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SrijanSriv211/Palm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzyk1k/github_srijansriv211palm_palm_is_a_tree_not_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzyk1k/github_srijansriv211palm_palm_is_a_tree_not_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:05:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzz13f</id>
    <title>What are the best practices for vector search + filtering with LLM?</title>
    <updated>2025-07-14T21:24:02+00:00</updated>
    <author>
      <name>/u/andrewshvv</name>
      <uri>https://old.reddit.com/user/andrewshvv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.&lt;/p&gt; &lt;p&gt;I am thinking about super simple solution:&lt;br /&gt; - use firecrawl to get the markdown content;&lt;br /&gt; - get vector / save into databse;&lt;br /&gt; - when text input comes I fill it with additional context for better vector search performance;&lt;br /&gt; - load N results&lt;br /&gt; - filter with gpt&lt;/p&gt; &lt;p&gt;but the last time I was doing it, it wasn't working really great, so i was wondering maybe there is better solution for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andrewshvv"&gt; /u/andrewshvv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzhqz8</id>
    <title>Responses keep dissolving into word salad - how to stop it?</title>
    <updated>2025-07-14T09:18:05+00:00</updated>
    <author>
      <name>/u/Gilgameshcomputing</name>
      <uri>https://old.reddit.com/user/Gilgameshcomputing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"&gt; &lt;img alt="Responses keep dissolving into word salad - how to stop it?" src="https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4" title="Responses keep dissolving into word salad - how to stop it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt; &lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt; &lt;p&gt;I've tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt; &lt;p&gt;I've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt; &lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gilgameshcomputing"&gt; /u/Gilgameshcomputing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lr7kq1452tcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T09:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzx039</id>
    <title>NVMe for local LLM is too slow. Any ideas?</title>
    <updated>2025-07-14T20:06:37+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, here is the problem. I'm actually facing it as I'm writing this post.&lt;/p&gt; &lt;p&gt;I use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It's like asking multiple experts with various opinions before making a decision. &lt;/p&gt; &lt;p&gt;I have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.&lt;/p&gt; &lt;p&gt;The problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?&lt;/p&gt; &lt;p&gt;Any ideas anyone? Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T20:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m013ou</id>
    <title>Moonshot AIâs open source Kimi K2 outperforms GPT-4 in key benchmarks</title>
    <updated>2025-07-14T22:46:29+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T22:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyy39n</id>
    <title>IndexTTS2, the most realistic and expressive text-to-speech model so far, has leaked their demos ahead of the official launch! And... wow!</title>
    <updated>2025-07-13T17:11:10+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech&lt;/h1&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully local with open weights.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.&lt;/li&gt; &lt;li&gt;Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.&lt;/li&gt; &lt;li&gt;Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.&lt;/li&gt; &lt;li&gt;Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard &amp;quot;free length&amp;quot; mode where it automatically lets the audio become as long as necessary.&lt;/li&gt; &lt;li&gt;Supported text to speech languages that it can output: English and Chinese. Like most models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's a few real-world use cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that's the perfect length for dubbing.&lt;/li&gt; &lt;li&gt;Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.&lt;/li&gt; &lt;li&gt;Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;So how did it leak?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;They have been preparing a website at &lt;a href="https://index-tts2.github.io/"&gt;https://index-tts2.github.io/&lt;/a&gt; which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they've been preparing, along with demo files.&lt;/li&gt; &lt;li&gt;Here's an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: &lt;a href="https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4"&gt;https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The entire presentation page is here: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;To download all demos and watch the HTML presentation locally, you can also &amp;quot;git clone &lt;a href="https://github.com/index-tts/index-tts2.github.io.git"&gt;https://github.com/index-tts/index-tts2.github.io.git&lt;/a&gt;&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual &lt;em&gt;acting!&lt;/em&gt; Bravo, Bilibili, the company behind this research!&lt;/p&gt; &lt;p&gt;They are planning to release it &amp;quot;soon&amp;quot;, and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I'd say it's coming this month or the next. Update: The public release will not be this month (they are still busy fine-tuning), but maybe next month.&lt;/p&gt; &lt;p&gt;Their previous model was Apache 2 license, both for the source code and the weights. Let's hope the next model is the same awesome license.&lt;/p&gt; &lt;h2&gt;Update:&lt;/h2&gt; &lt;p&gt;They contacted me and were surprised that I had already found their &amp;quot;hidden&amp;quot; paper and presentation. They haven't gone public yet. I hope I didn't cause them trouble by announcing the discovery too soon.&lt;/p&gt; &lt;p&gt;They're very happy that people are so excited about their new model, though! :) But they're still busy fine-tuning the model, and improving the tools and code for public release. So it will not release this month, but late next month is more likely.&lt;/p&gt; &lt;p&gt;And if I understood correctly, it will be free and open for non-commercial use (same as their older models). They are considering whether to require a separate commercial license for commercial usage, which makes sense since this is state of the art and very useful for dubbing movies/anime. I fully respect that and think that anyone using software to make money should compensate the people who made the software. But nothing is decided yet.&lt;/p&gt; &lt;p&gt;I am very excited for this new model and can't wait! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m021nx</id>
    <title>Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)</title>
    <updated>2025-07-14T23:25:45+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt; &lt;img alt="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" src="https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=427166a43aad977ff4e628d5d89073bd9fd90280" title="Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl35mhaybxcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T23:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzsnna</id>
    <title>Ollama, Why No Reka Flash, SmolLM3, GLM-4?</title>
    <updated>2025-07-14T17:27:25+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.&lt;/p&gt; &lt;p&gt;Still, it seems pretty odd that they're missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T17:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzocuk</id>
    <title>I ditch all LLM framework and use only OpenAI SDK for everything, I start loving building AI application this way.</title>
    <updated>2025-07-14T14:47:24+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I've also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they're &amp;quot;exhausting.&amp;quot;&lt;/p&gt; &lt;p&gt;I feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people's examples just to implement some simple control flow.&lt;/p&gt; &lt;p&gt;With just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it's so lean and easy, and you get full visibility on how each API call went.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T14:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lze1r3</id>
    <title>Diffusion model support in llama.cpp.</title>
    <updated>2025-07-14T05:20:04+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"&gt; &lt;img alt="Diffusion model support in llama.cpp." src="https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c75c6786f093153f6a5dc5065d5f9e2b741b5086" title="Diffusion model support in llama.cpp." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It's very cool to watch it do it's thing. Make sure to use the --diffusion-visual flag. It's still a PR but has been approved so it should be merged soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T05:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzts1z</id>
    <title>Is real-time voice-to-voice still science fiction?</title>
    <updated>2025-07-14T18:07:52+00:00</updated>
    <author>
      <name>/u/junior600</name>
      <uri>https://old.reddit.com/user/junior600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?&lt;br /&gt; I'd like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.&lt;br /&gt; It would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it's not a very &amp;quot;popular&amp;quot; language to train on. lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/junior600"&gt; /u/junior600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T18:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzcuom</id>
    <title>Kimi-K2 is a DeepSeek V3 with more experts</title>
    <updated>2025-07-14T04:12:33+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;dense layer#&lt;/th&gt; &lt;th align="left"&gt;MoE layer#&lt;/th&gt; &lt;th align="left"&gt;shared&lt;/th&gt; &lt;th align="left"&gt;active/routed&lt;/th&gt; &lt;th align="left"&gt;Shared&lt;/th&gt; &lt;th align="left"&gt;Active&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Active%&lt;/th&gt; &lt;th align="left"&gt;fp16 kv@128k&lt;/th&gt; &lt;th align="left"&gt;kv%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-MoE-16B&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;1.42B&lt;/td&gt; &lt;td align="left"&gt;2.83B&lt;/td&gt; &lt;td align="left"&gt;16.38B&lt;/td&gt; &lt;td align="left"&gt;17.28%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;85.47%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2-Lite&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;1.31B&lt;/td&gt; &lt;td align="left"&gt;2.66B&lt;/td&gt; &lt;td align="left"&gt;15.71B&lt;/td&gt; &lt;td align="left"&gt;16.93%&lt;/td&gt; &lt;td align="left"&gt;3.8GB&lt;/td&gt; &lt;td align="left"&gt;12.09%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/160&lt;/td&gt; &lt;td align="left"&gt;12.98B&lt;/td&gt; &lt;td align="left"&gt;21.33B&lt;/td&gt; &lt;td align="left"&gt;235.74B&lt;/td&gt; &lt;td align="left"&gt;8.41%&lt;/td&gt; &lt;td align="left"&gt;8.44GB&lt;/td&gt; &lt;td align="left"&gt;1.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;58&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/256&lt;/td&gt; &lt;td align="left"&gt;17.01B&lt;/td&gt; &lt;td align="left"&gt;37.45B&lt;/td&gt; &lt;td align="left"&gt;671.03B&lt;/td&gt; &lt;td align="left"&gt;5.58%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-K2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/384&lt;/td&gt; &lt;td align="left"&gt;11.56B&lt;/td&gt; &lt;td align="left"&gt;32.70B&lt;/td&gt; &lt;td align="left"&gt;1026.41B&lt;/td&gt; &lt;td align="left"&gt;3.19%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;1.53B&lt;/td&gt; &lt;td align="left"&gt;3.34B&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;10.94%&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;19.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;94&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;7.95B&lt;/td&gt; &lt;td align="left"&gt;22.14B&lt;/td&gt; &lt;td align="left"&gt;235.09B&lt;/td&gt; &lt;td align="left"&gt;9.42%&lt;/td&gt; &lt;td align="left"&gt;23.5GB&lt;/td&gt; &lt;td align="left"&gt;4.998%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout-17B-16E&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/16&lt;/td&gt; &lt;td align="left"&gt;11.13B&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;107.77B&lt;/td&gt; &lt;td align="left"&gt;15.93%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;11.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/128&lt;/td&gt; &lt;td align="left"&gt;14.15B&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;400.71B&lt;/td&gt; &lt;td align="left"&gt;4.28%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;2.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x7B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;1.60B&lt;/td&gt; &lt;td align="left"&gt;12.88B&lt;/td&gt; &lt;td align="left"&gt;46.70B&lt;/td&gt; &lt;td align="left"&gt;27.58%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;25.696%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;5.33B&lt;/td&gt; &lt;td align="left"&gt;39.15B&lt;/td&gt; &lt;td align="left"&gt;140.62B&lt;/td&gt; &lt;td align="left"&gt;27.84%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;9.956%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt; &lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt; &lt;p&gt;Edited: Per &lt;a href="/u/Aaaaaaaaaeeeee"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; 's request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T04:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzsoqc</id>
    <title>Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation</title>
    <updated>2025-07-14T17:28:30+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/"&gt; &lt;img alt="Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation" src="https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2488f13a539fdbdb3a0f013cfa8607c5679c8da" title="Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bx3hl3q5kvcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T17:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzql0b</id>
    <title>A practical handbook on Context Engineering with the latest research from IBM Zurich, ICML, Princeton, and more.</title>
    <updated>2025-07-14T16:10:54+00:00</updated>
    <author>
      <name>/u/recursiveauto</name>
      <uri>https://old.reddit.com/user/recursiveauto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidkimai/Context-Engineering"&gt;https://github.com/davidkimai/Context-Engineering&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/recursiveauto"&gt; /u/recursiveauto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T16:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzzcje</id>
    <title>MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation</title>
    <updated>2025-07-14T21:36:24+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"&gt; &lt;img alt="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation" src="https://b.thumbs.redditmedia.com/irH_IxYHAQEjBGgXU_AOln_EsxdXVQKIs5z4JjXsVfc.jpg" title="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.&lt;/p&gt; &lt;p&gt;The benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.ââââââââââââââââ&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://mmluprox.github.io"&gt;https://mmluprox.github.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.10497"&gt;https://arxiv.org/abs/2503.10497&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/weihao1115/MMLU-ProX"&gt;https://github.com/weihao1115/MMLU-ProX&lt;/a&gt; (still empty)&lt;/li&gt; &lt;li&gt;Full dataset: &lt;a href="https://huggingface.co/datasets/li-lab/MMLU-ProX"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Lite dataset: &lt;a href="https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lzzcje"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzhns3</id>
    <title>Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)</title>
    <updated>2025-07-14T09:12:20+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"&gt; &lt;img alt="Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)" src="https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4" title="Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt; &lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt; &lt;li&gt;Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt; &lt;li&gt;Note that quantizations are not same. It's just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mark the solution green if it's accepted.&lt;/li&gt; &lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt; &lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt; &lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didnât treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt; &lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt; &lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt; &lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt; &lt;p&gt;All questions are recent, with no data leakage involved. So donât come back saying âLeetCode problems are easy for models, this test isnât meaningfulâ. It's just your test questions have been seen by the model before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nyu5vpzx2tcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T09:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0115d</id>
    <title>Meta on track to be first lab with a 1GW supercluster</title>
    <updated>2025-07-14T22:43:36+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt; &lt;img alt="Meta on track to be first lab with a 1GW supercluster" src="https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f" title="Meta on track to be first lab with a 1GW supercluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/584vdadc4xcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T22:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzampg</id>
    <title>Training an LLM only on books from the 1800's - no modern bias</title>
    <updated>2025-07-14T02:16:53+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"&gt; &lt;img alt="Training an LLM only on books from the 1800's - no modern bias" src="https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e60398c5e2e84881134a46e0acf601c56ba81942" title="Training an LLM only on books from the 1800's - no modern bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that's why I didn't go the fine tune route. It will have no modern bias and will only be able to reason within the time period it's trained on. It's super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T02:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzw6yu</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-07-14T19:36:39+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a &lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt; that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Hereâs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;ð &lt;strong&gt;Feature&lt;/strong&gt;s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt; &lt;li&gt;50+ File extensions supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ðï¸ &lt;strong&gt;Podcast&lt;/strong&gt;s&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;â¹ï¸ &lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ð &lt;strong&gt;Cross-Browser Extensio&lt;/strong&gt;n&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T19:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzfhhq</id>
    <title>Apple âwill seriously considerâ buying Mistral | Bloomberg - Mark Gurman</title>
    <updated>2025-07-14T06:48:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt; &lt;img alt="Apple âwill seriously considerâ buying Mistral | Bloomberg - Mark Gurman" src="https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8" title="Apple âwill seriously considerâ buying Mistral | Bloomberg - Mark Gurman" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt; &lt;p&gt;I don't know how the French and European authorities could accept this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syyfccpldscf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T06:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzv16g</id>
    <title>Metaâs New Superintelligence Lab Is Discussing Major A.I. Strategy Changes</title>
    <updated>2025-07-14T18:54:00+00:00</updated>
    <author>
      <name>/u/showmeufos</name>
      <uri>https://old.reddit.com/user/showmeufos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"&gt; &lt;img alt="Metaâs New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" src="https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731" title="Metaâs New Superintelligence Lab Is Discussing Major A.I. Strategy Changes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/showmeufos"&gt; /u/showmeufos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T18:54:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzm645</id>
    <title>After Kimi K2 Is Released: No Longer Just a ChatBot</title>
    <updated>2025-07-14T13:18:06+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the authorâs insights genuinely thought-provoking. The original Chinese version is &lt;a href="https://bigeagle.me/2025/07/kimi-k2/"&gt;here&lt;/a&gt;âfeel free to read it in full (and of course you can use Kimi K2 as your translator). Hereâs my own distilled summary of the main points:&lt;/p&gt; &lt;p&gt;â¢ Beyond chatbots: Kimi K2 experiments with an âartifact-firstâ interaction model that has the AI immediately build interactive front-end deliverablesâPPT-like pages, diagrams, even mini-gamesârather than simply returning markdown text.&lt;/p&gt; &lt;p&gt;â¢ Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt; &lt;p&gt;â¢ What makes an agentic model: A minimal loopâthink, choose tools, observe results, iterateâcan be learned from synthetic trajectories. Todayâs agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt; &lt;p&gt;â¢ Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit âhackyâ hidden pipelines, forcing genuinely strong, general modelsâexactly what an AGI-oriented startup needs.&lt;/p&gt; &lt;p&gt;â¢ Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1âs viral rise proved that raw model quality markets itself and validates the âfoundation-model-firstâ path.&lt;/p&gt; &lt;p&gt;â¢ Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt; &lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A while ago, âAgentâ products were all the rage. I kept hearing people say that Kimi shouldnât compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we donât keep pushing that ceiling higher, I wonât stay here a single extra day.&lt;/p&gt; &lt;p&gt;Chasing AGI is an extremely narrow, perilous bridgeâthereâs no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, âAs an investor, I care about the ROI of AI applications.â In that moment I knew the company he founded wouldnât last long.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T13:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzywie</id>
    <title>Kimi K2 tops creative writing benchmark</title>
    <updated>2025-07-14T21:19:11+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt; &lt;img alt="Kimi K2 tops creative writing benchmark" src="https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855" title="Kimi K2 tops creative writing benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q48f55vcpwcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T21:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzps3b</id>
    <title>Kimi K2 1.8bit Unsloth Dynamic GGUFs</title>
    <updated>2025-07-14T15:41:16+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt; &lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt; &lt;p&gt;You need to use either &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14654"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href="https://github.com/unslothai/llama.cpp"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt; &lt;p&gt;The suggested parameters are:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.6 min_p = 0.01 (set it to a small number) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Docs has more details: &lt;a href="https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T15:41:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzl5zk</id>
    <title>UTCP: A safer, scalable tool-calling alternative to MCP</title>
    <updated>2025-07-14T12:33:01+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt; &lt;img alt="UTCP: A safer, scalable tool-calling alternative to MCP" src="https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95" title="UTCP: A safer, scalable tool-calling alternative to MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wv84vx7h3ucf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T12:33:01+00:00</published>
  </entry>
</feed>
