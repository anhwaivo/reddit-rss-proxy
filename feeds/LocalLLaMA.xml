<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-18T15:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1is8tug</id>
    <title>Deep research but using RAG?</title>
    <updated>2025-02-18T09:25:18+00:00</updated>
    <author>
      <name>/u/grumpyarcpal</name>
      <uri>https://old.reddit.com/user/grumpyarcpal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see a number of deep research projects that search online and create a report, these are great but are there any that give the option to only use RAG? I have a pile of industry specific documents and reports (mainly PDF) and something that could generate a report or research paper based on these would be a huge time-saver. I have to supply 'research papers' or reports for internal use when proposing public outreach, new projects etc for work, they are all based off a pile of documents which are basically reports from many years of previous projects.&lt;/p&gt; &lt;p&gt;Something that could provide in-line citations and a bibliography would be ideal, along the lines of notebook LM but producing a research paper style report. It's asking a lot I know, I'm happy to pay to a point but open-source is always exciting!&lt;/p&gt; &lt;p&gt;TL;DR I'm looking for an Incestuous love-child of Notebook LM and Gemini with deep research. The report style output but with in-line citations and using RAG rather than online search&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grumpyarcpal"&gt; /u/grumpyarcpal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is8tug/deep_research_but_using_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is8tug/deep_research_but_using_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is8tug/deep_research_but_using_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T09:25:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1is66p8</id>
    <title>$10k budget to run Deepseek locally for reasoning - what TPS can I expect?</title>
    <updated>2025-02-18T06:18:50+00:00</updated>
    <author>
      <name>/u/helpimalive24</name>
      <uri>https://old.reddit.com/user/helpimalive24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to the idea of running LLMs locally. Currently I have a web app that relies on LLMs for parsing descriptions into JSON objects. Ive found Deepseek (R1 and to a lesser but still usable extender V3) performs best but the deepseek API is unreliable, so I'm considering running it locally. &lt;/p&gt; &lt;p&gt;Would a 10K budget be reasonable to run these models locally? And if so what kind of TPS could I get? &lt;/p&gt; &lt;p&gt;Also side noob question - does TPS include reasoning time? I assume no since reasoning tasks vary widely, but if it doesn't include reasoning time then should TPS generally be really high? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/helpimalive24"&gt; /u/helpimalive24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is66p8/10k_budget_to_run_deepseek_locally_for_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is66p8/10k_budget_to_run_deepseek_locally_for_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is66p8/10k_budget_to_run_deepseek_locally_for_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T06:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpx0b</id>
    <title>Don’t sleep on The Allen Institute for AI (AI2)</title>
    <updated>2025-02-17T17:51:51+00:00</updated>
    <author>
      <name>/u/dontbanana</name>
      <uri>https://old.reddit.com/user/dontbanana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Allen Institute says its open-source model can beat DeepSeek&lt;/p&gt; &lt;p&gt;“The same tricks: AI2’s models use a novel reinforcement learning technique—training by way of “rewards” and “punishments” for right and wrong outputs—in which the model is taught to solve math or other problems with verifiable answers. DeepSeek used similar reinforcement learning techniques to train its models on reasoning tasks.&lt;/p&gt; &lt;p&gt;“It is pretty much, I would even argue, identical,” Hajishirzi said. “It is very simple… we had it in this paper in late November and DeepSeek came after us. Someone was asking me, ‘Did they actually copy what you did?’ I said, ‘I don’t know. It was so close that each team could come up with this independently.’ So, I don’t know, but it’s open research. A lot of these ideas could be shared.””&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dontbanana"&gt; /u/dontbanana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.emergingtechbrew.com/stories/2025/02/07/allen-institute-open-source-model-deepseek?mbcid=38624075.320719&amp;amp;mblid=76a9d29d5c33&amp;amp;mid=4bf97fa50758e4f9907627b7deaa5807&amp;amp;utm_campaign=etb&amp;amp;utm_medium=newsletter&amp;amp;utm_source=morning_brew"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpx0b/dont_sleep_on_the_allen_institute_for_ai_ai2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1irpozr</id>
    <title>Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!</title>
    <updated>2025-02-17T17:43:11+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt; &lt;img alt="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" src="https://external-preview.redd.it/R6NtBwOFUehmfI110Qr1QSx4QJoALZS5zC4GvG9AcZo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ceb3ec7bb36d8267e1328a5ec597a5541141024" title="Drummer's Skyfall 36B v2 - An upscale of Mistral's 24B 2501 with continued training; resulting in a stronger, 70B-like model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-36B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1irpozr/drummers_skyfall_36b_v2_an_upscale_of_mistrals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T17:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1is2s3q</id>
    <title>DeepSeek 1.5B on Android</title>
    <updated>2025-02-18T03:05:21+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"&gt; &lt;img alt="DeepSeek 1.5B on Android" src="https://external-preview.redd.it/M29td3JyazRkdGplMREgtlSh22tIg8ofupSivEXbZ27zomojPoV29y7odKjy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=878850029aea6767826fe847b74d4b0b586e60f0" title="DeepSeek 1.5B on Android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently release v0.8.5 of ChatterUI with some minor improvements to the app, including fixed support for DeepSeek-R1 distills and an entirely reworked styling system:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Vali-98/ChatterUI/releases/tag/v0.8.5"&gt;https://github.com/Vali-98/ChatterUI/releases/tag/v0.8.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall, I'd say the responses of the 1.5b and 8b distills are slightly better than the base models, but its still very limited output wise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3zz0ipp4dtje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is2s3q/deepseek_15b_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T03:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iscgfq</id>
    <title>Open source Grok-2 when?</title>
    <updated>2025-02-18T13:17:49+00:00</updated>
    <author>
      <name>/u/AfternoonOk5482</name>
      <uri>https://old.reddit.com/user/AfternoonOk5482</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When are Grok-2 weights going to be available for download?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AfternoonOk5482"&gt; /u/AfternoonOk5482 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iscgfq/open_source_grok2_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iscgfq/open_source_grok2_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iscgfq/open_source_grok2_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T13:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1is6g2r</id>
    <title>We're doing pretty well right now...</title>
    <updated>2025-02-18T06:35:50+00:00</updated>
    <author>
      <name>/u/mahiatlinux</name>
      <uri>https://old.reddit.com/user/mahiatlinux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6g2r/were_doing_pretty_well_right_now/"&gt; &lt;img alt="We're doing pretty well right now..." src="https://b.thumbs.redditmedia.com/3PlFxzMPX9gnpHEwnPWjESrABt15KbwNIcTFM0b7_sA.jpg" title="We're doing pretty well right now..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gp6hcqcwduje1.png?width=608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0923a479e800db9216a99fdf553f23a91199cf0f"&gt;https://preview.redd.it/gp6hcqcwduje1.png?width=608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0923a479e800db9216a99fdf553f23a91199cf0f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link for the people that want to see it: &lt;a href="https://nitter.net/sama/status/1891667332105109653#m"&gt;https://nitter.net/sama/status/1891667332105109653#m&lt;/a&gt; (non-X link).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mahiatlinux"&gt; /u/mahiatlinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6g2r/were_doing_pretty_well_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6g2r/were_doing_pretty_well_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is6g2r/were_doing_pretty_well_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T06:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1isazyj</id>
    <title>LM Studio vs Ollama vs Jan vs Llama.cpp vs GPT4All</title>
    <updated>2025-02-18T11:55:12+00:00</updated>
    <author>
      <name>/u/HornyGooner4401</name>
      <uri>https://old.reddit.com/user/HornyGooner4401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you use and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HornyGooner4401"&gt; /u/HornyGooner4401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isazyj/lm_studio_vs_ollama_vs_jan_vs_llamacpp_vs_gpt4all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isazyj/lm_studio_vs_ollama_vs_jan_vs_llamacpp_vs_gpt4all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isazyj/lm_studio_vs_ollama_vs_jan_vs_llamacpp_vs_gpt4all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T11:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1is5cxd</id>
    <title>I designed Prompt Targets - a higher level abstraction than function calling. Clarify, route and trigger actions.</title>
    <updated>2025-02-18T05:27:31+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5cxd/i_designed_prompt_targets_a_higher_level/"&gt; &lt;img alt="I designed Prompt Targets - a higher level abstraction than function calling. Clarify, route and trigger actions." src="https://preview.redd.it/bzvxfoxh2uje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf5990ce9a3afd7cfb7fc969b2aa06ab758c4812" title="I designed Prompt Targets - a higher level abstraction than function calling. Clarify, route and trigger actions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Function calling is now a core primitive now in building agentic applications - but there is still alot of engineering muck and duck tape required to build an accurate conversational experience&lt;/p&gt; &lt;p&gt;Meaning - sometimes you need to forward a prompt to the right down stream agent to handle a query, or ask for clarifying questions before you can trigger/ complete an agentic task. &lt;/p&gt; &lt;p&gt;I’ve designed a higher level abstraction inspired and modeled after traditional load balancers. In this instance, we process prompts, route prompts and extract critical information for a downstream task&lt;/p&gt; &lt;p&gt;To get the experience right I built &lt;a href="https://huggingface.co/katanemo/Arch-Function-3B"&gt;https://huggingface.co/katanemo/Arch-Function-3B&lt;/a&gt; and we have yet to release Arch-Intent a 2M LoRA for parameter gathering but that will be released in a week.&lt;/p&gt; &lt;p&gt;So how do you use prompt targets? We made them available here:&lt;br /&gt; &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; - the intelligent proxy for prompts &lt;/p&gt; &lt;p&gt;Hope you all like it. Would be curious to get your thoughts as well. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bzvxfoxh2uje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5cxd/i_designed_prompt_targets_a_higher_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is5cxd/i_designed_prompt_targets_a_higher_level/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T05:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4joi</id>
    <title>alibaba mnn released its full multimodal ios app, models fully run local</title>
    <updated>2025-02-18T04:39:29+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt; &lt;img alt="alibaba mnn released its full multimodal ios app, models fully run local" src="https://external-preview.redd.it/nFgqKmZCG_UKkOoGTTwMgu4Lps-UbpzBRzAqIHJJI9s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b425d8842d03ac390b8862521ef50395c10fca5b" title="alibaba mnn released its full multimodal ios app, models fully run local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6yfovbv0ttje1.png?width=2010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=310222dc12625a3f0e0382d0bb06a4d44c00d6c8"&gt;https://preview.redd.it/6yfovbv0ttje1.png?width=2010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=310222dc12625a3f0e0382d0bb06a4d44c00d6c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/o5napto8ttje1.gif"&gt;https://i.redd.it/o5napto8ttje1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;support text to text， image to text，audio to text&lt;br /&gt; github: &lt;a href="https://github.com/alibaba/MNN/blob/master/apps/iOS/MNNLLMChat/README.md"&gt;https://github.com/alibaba/MNN/blob/master/apps/iOS/MNNLLMChat/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previously released android app: &lt;a href="https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md"&gt;https://github.com/alibaba/MNN/blob/master/project/android/apps/MnnLlmApp/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4joi/alibaba_mnn_released_its_full_multimodal_ios_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1isduqr</id>
    <title>AMD 395: Asus Flow Z13 review</title>
    <updated>2025-02-18T14:26:19+00:00</updated>
    <author>
      <name>/u/MappyMcMapHead</name>
      <uri>https://old.reddit.com/user/MappyMcMapHead</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IVbm2a6lVBo"&gt;https://www.youtube.com/watch?v=IVbm2a6lVBo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Price starts at: $2.2k for 32GB RAM&lt;/p&gt; &lt;p&gt;Funny: At some point in the video he says it's 256 bit memory and calls it FAST VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MappyMcMapHead"&gt; /u/MappyMcMapHead &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isduqr/amd_395_asus_flow_z13_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isduqr/amd_395_asus_flow_z13_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isduqr/amd_395_asus_flow_z13_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1isefit</id>
    <title>218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review</title>
    <updated>2025-02-18T14:53:08+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"&gt; &lt;img alt="218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review" src="https://external-preview.redd.it/wN11nOKnxhb4X_jcsbcRwIcNxNTSj7M0aYJ70EXagdw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45e1cf5c89b225d98150e66cd20e132248282c66" title="218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=yiHr8CQRZi4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1eht</id>
    <title>ClosedAI Next Open Source</title>
    <updated>2025-02-18T01:55:55+00:00</updated>
    <author>
      <name>/u/cabsterman</name>
      <uri>https://old.reddit.com/user/cabsterman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"&gt; &lt;img alt="ClosedAI Next Open Source" src="https://preview.redd.it/grv77lpq0tje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fd9c4adfa555517144d41c5b320ff875075d9bf" title="ClosedAI Next Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/sama/status/1891667332105109653"&gt;https://x.com/sama/status/1891667332105109653&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cabsterman"&gt; /u/cabsterman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/grv77lpq0tje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1eht/closedai_next_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4fm6</id>
    <title>My new local inference rig</title>
    <updated>2025-02-18T04:33:05+00:00</updated>
    <author>
      <name>/u/Jackalzaq</name>
      <uri>https://old.reddit.com/user/Jackalzaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"&gt; &lt;img alt="My new local inference rig" src="https://a.thumbs.redditmedia.com/QFvwlttKs0TaTL0gcHjxfR2osnKgT81xH5Ciajzzy78.jpg" title="My new local inference rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Supermicro sys 2048gr trt2 with 8x instinct mi60s with a sysrack enclosure so i dont lose my mind.&lt;/p&gt; &lt;p&gt;R1 1.58bit dynamic quant (671b) runs at around 4-6 tok per second Llama 405b q4km at about 1.5 tok per second&lt;/p&gt; &lt;p&gt;With no cpu offloading my context is around 12k and 8k respectively. Havent tested it with partial cpu offloading yet.&lt;/p&gt; &lt;p&gt;Sound can get up to over 70db when the case is open and stays around 50db when running inference with case closed.&lt;/p&gt; &lt;p&gt;Also using two separate circuits for this build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jackalzaq"&gt; /u/Jackalzaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1is4fm6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4fm6/my_new_local_inference_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1is7q1k</id>
    <title>FUSEAI's DeepSeek R1 Distill (Merge) Really Seems Better</title>
    <updated>2025-02-18T08:03:31+00:00</updated>
    <author>
      <name>/u/MiaBchDave</name>
      <uri>https://old.reddit.com/user/MiaBchDave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been playing with marketing/coding capabilities of some small models on my Macbook M4 Max. The popular DeepSeek-R1-Distill-Qwen-32B was my first try at getting something actually done locally. It was OK, but then I ran across this version that shows it's scoring higher - tests are on the model page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn't see an 8-Bit Quant MLX version, so I rolled my own - and low and behold, this thing does work better. It's not even code focused, but codes better... at least as far as I can tell. It certainly communicates in a more congenial manner. Anyway, I have no idea what I'm doing really, but I suggest using 8-Bit Quant. &lt;/p&gt; &lt;p&gt;If using a Mac, there's a 6-Bit Quant MLX in the repository on HF, but that one definitely performed worse. Not sure how to get my MLX_8bit uploaded... but maybe someone who actually knows this stuff can get that handled better than I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiaBchDave"&gt; /u/MiaBchDave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7q1k/fuseais_deepseek_r1_distill_merge_really_seems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7q1k/fuseais_deepseek_r1_distill_merge_really_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is7q1k/fuseais_deepseek_r1_distill_merge_really_seems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T08:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iry4lu</id>
    <title>How can I optimize my 1.000.000B MoE Reasoning LLM?</title>
    <updated>2025-02-17T23:21:26+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, my mum built this LLM for me called Brain, it has a weird architecture that resembles MoE but its called MoL (Mixture of Lobes), it has around 1 000 000B parameters (synapses) but it's not performing that well on MMLU pro, it gives me a lot of errors with complicated tasks, and I'm struggling to activate the frontal &lt;del&gt;Expert&lt;/del&gt; lobe, it also hallucinates 1/3 of the time, especially at night. It might be some hardware issue since I had no money for an RTX 5090 and I'm instead running it on frozen food and coke. At least it is truly multimodal since it works well with audio and images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iry4lu/how_can_i_optimize_my_1000000b_moe_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-17T23:21:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1isaf0z</id>
    <title>Jan v0.5.15: More control over llama.cpp settings, advanced hardware control, and more (Details in the first comment)</title>
    <updated>2025-02-18T11:18:08+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isaf0z/jan_v0515_more_control_over_llamacpp_settings/"&gt; &lt;img alt="Jan v0.5.15: More control over llama.cpp settings, advanced hardware control, and more (Details in the first comment)" src="https://external-preview.redd.it/aTZxNXczdW9zdmplMacwxygveukUqbjuyDuxKAxl7yaHaaPFpN99EbSsu1wC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aebd929d9fd6138d7f600b5107f7caa66e8c4e4" title="Jan v0.5.15: More control over llama.cpp settings, advanced hardware control, and more (Details in the first comment)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8mc4t2uosvje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isaf0z/jan_v0515_more_control_over_llamacpp_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isaf0z/jan_v0515_more_control_over_llamacpp_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T11:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1is72j2</id>
    <title>DeepSeek Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</title>
    <updated>2025-02-18T07:17:30+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is72j2/deepseek_native_sparse_attention_hardwarealigned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is72j2/deepseek_native_sparse_attention_hardwarealigned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T07:17:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iserf9</id>
    <title>Deepseek R1 Distilled Models MMLU Pro Benchmarks</title>
    <updated>2025-02-18T15:07:30+00:00</updated>
    <author>
      <name>/u/RedditsBestest</name>
      <uri>https://old.reddit.com/user/RedditsBestest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt; &lt;img alt="Deepseek R1 Distilled Models MMLU Pro Benchmarks" src="https://preview.redd.it/s006z4fbnwje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68c5d9bdcbc0635cebefe1ea4e81c89993146c77" title="Deepseek R1 Distilled Models MMLU Pro Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditsBestest"&gt; /u/RedditsBestest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s006z4fbnwje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T15:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1is4geo</id>
    <title>GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1</title>
    <updated>2025-02-18T04:34:22+00:00</updated>
    <author>
      <name>/u/AIGuy3000</name>
      <uri>https://old.reddit.com/user/AIGuy3000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"&gt; &lt;img alt="GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1" src="https://preview.redd.it/8dwhr7o0ttje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1647f77aba24121cd40230ddff75d1a85e640b28" title="GROK-3 (SOTA) and GROK-3 mini both top O3-mini high and Deepseek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIGuy3000"&gt; /u/AIGuy3000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dwhr7o0ttje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is4geo/grok3_sota_and_grok3_mini_both_top_o3mini_high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T04:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ise5ly</id>
    <title>Speed up downloading Hugging Face models by 100x</title>
    <updated>2025-02-18T14:40:25+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure this is common knowledge, so sharing it here.&lt;/p&gt; &lt;p&gt;You may have noticed HF downloads caps at around 10.4MB/s (at least for me), apparently this is because of a Python limitation.&lt;/p&gt; &lt;p&gt;But if you install hf_transfer, which is written in Rust, you get uncapped speeds! I'm getting speeds of over &amp;gt; 1GB/s, and this saves me so much time!&lt;/p&gt; &lt;p&gt;Here is the step by step process to do it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # Install the HuggingFace CLI pip install -U &amp;quot;huggingface_hub[cli]&amp;quot; # Install hf_transfer for blazingly fast speeds pip install hf_transfer # Login to your HF account huggingface-cli login # Now you can download any model with uncapped speeds HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download &amp;lt;model-id&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1is6a0m</id>
    <title>Sama discussing the release of Phone-sized-model</title>
    <updated>2025-02-18T06:24:58+00:00</updated>
    <author>
      <name>/u/0ssamaak0</name>
      <uri>https://old.reddit.com/user/0ssamaak0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"&gt; &lt;img alt="Sama discussing the release of Phone-sized-model" src="https://preview.redd.it/lm2o9y9pcuje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9139d0f30628bce26eec5f98414cd3aa3aaad4ea" title="Sama discussing the release of Phone-sized-model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0ssamaak0"&gt; /u/0ssamaak0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lm2o9y9pcuje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T06:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1is5036</id>
    <title>We're winning by just a hair...</title>
    <updated>2025-02-18T05:05:51+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"&gt; &lt;img alt="We're winning by just a hair..." src="https://preview.redd.it/v8ygdpdlytje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc10efbbcaa94e78223dfb7fe2eba928afb34b7" title="We're winning by just a hair..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v8ygdpdlytje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T05:05:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1p2o</id>
    <title>The normies have failed us</title>
    <updated>2025-02-18T02:10:22+00:00</updated>
    <author>
      <name>/u/RenoHadreas</name>
      <uri>https://old.reddit.com/user/RenoHadreas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt; &lt;img alt="The normies have failed us" src="https://preview.redd.it/fosxvznb3tje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38f047fe1dccff4127d9e6709f4812f4bce14d3d" title="The normies have failed us" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenoHadreas"&gt; /u/RenoHadreas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fosxvznb3tje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T02:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1is7yei</id>
    <title>DeepSeek is still cooking</title>
    <updated>2025-02-18T08:20:59+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt; &lt;img alt="DeepSeek is still cooking" src="https://preview.redd.it/ikhcif5gxuje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659543f3bf6bd7985e0d2e63418ae9c0ba570196" title="DeepSeek is still cooking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Babe wake up, a new Attention just dropped&lt;/p&gt; &lt;p&gt;Sources: &lt;a href="https://x.com/deepseek_ai/status/1891745487071609327"&gt;Tweet&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.11089"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ikhcif5gxuje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T08:20:59+00:00</published>
  </entry>
</feed>
