<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-08T06:36:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j5hb4p</id>
    <title>Ensure you use the appropriate temperature of 0.6 with QwQ-32B</title>
    <updated>2025-03-07T06:35:39+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt; &lt;img alt="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j5hb4p/video/jjveqqjjo7ne1/player"&gt;ball bouncing inside of a hexagon as it rotates&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;can you give me a pygame script that allows me to have a ball bouncing inside of a hexagon as it rotates, makes sure to handle collisions and gravity&amp;quot;&lt;/p&gt; &lt;p&gt;Yesterday, I tried this prompt it failed. I was very disappointed since it spend 15 mins on it. &lt;/p&gt; &lt;p&gt;Today, I notice the Ollama setting have been updated to with the correct temperature.&lt;br /&gt; You can find the recommend settings here.&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json"&gt;https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j69zl7</id>
    <title>Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp; news app.</title>
    <updated>2025-03-08T05:15:45+00:00</updated>
    <author>
      <name>/u/clockentyne</name>
      <uri>https://old.reddit.com/user/clockentyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt; &lt;img alt="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." src="https://external-preview.redd.it/bzZmbWc3ZW9nZW5lMXX6-fvAK-sQelTG_mLXF8zDOxdsMWF5oRG9uoSpFfgL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3b18632d71a4317c761154b009ac0dbb3f2da7" title="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clockentyne"&gt; /u/clockentyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0wclz6eogene1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ac7d</id>
    <title>Do I really need local AI, and is Apple RAM pricing worth it for that platform?</title>
    <updated>2025-03-08T05:37:17+00:00</updated>
    <author>
      <name>/u/After-Cell</name>
      <uri>https://old.reddit.com/user/After-Cell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an old Macbook. I need to upgrade if I want to do local AI. Is it worth getting a powerful laptop and is it worth paying the Apple tax on RAM? And if so, how much? &lt;/p&gt; &lt;p&gt;In my country I cannot access ChatGPT or Claude API without bouncing off a proxy, ssh or a VPN which is annoying. Further, I am interested in DESKTOP tasks such as desktop publishing PDFs, so some projects don't support API keys well, especially with the service blocks here in China.&lt;/p&gt; &lt;p&gt;It would also help me do stuff more privately too. &lt;/p&gt; &lt;p&gt;The simplest thing is to get a new Macbook, but the price for 128GB of RAM is eye-watering. The alternative would be to run a homelab style server like a Minisforum X1 Pro and then remotely connect into that. This is better than having to spin up servers all the time, but not as good as having everything right there on the laptop. It would also let me run jobs in the background. &lt;/p&gt; &lt;p&gt;How often do you find a project that really needs local AI power that is a pain to outsource to something with a GPU? How have you set up your hardware? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/After-Cell"&gt; /u/After-Cell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ac7d/do_i_really_need_local_ai_and_is_apple_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ac7d/do_i_really_need_local_ai_and_is_apple_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ac7d/do_i_really_need_local_ai_and_is_apple_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l0sv</id>
    <title>Mistral's New OCR Model (SaaS) - Best in Class</title>
    <updated>2025-03-07T11:04:24+00:00</updated>
    <author>
      <name>/u/Auslieferator</name>
      <uri>https://old.reddit.com/user/Auslieferator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral just announced a new model specialized on OCR, beating Azure OCR and Google Document AI. Sadly it is only offered as SaaS and not open weights as a lot of their models were. &lt;a href="https://mistral.ai/fr/news/mistral-ocr"&gt;https://mistral.ai/fr/news/mistral-ocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you know about any other OCR specialized LLMs or stacks of LLMs and other computer vision software, achieving similar results? I am currently playing with Qwen2.5 VL 7B Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Auslieferator"&gt; /u/Auslieferator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:04:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ievy</id>
    <title>FT: Llama 4 w/ voice expected in coming weeks</title>
    <updated>2025-03-07T07:55:53+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like good self-hosted voice chat that isn't tacked on will soon be available from both Sesame and Meta! Can't wait!&lt;/p&gt; &lt;p&gt;P.S. Is anyone working on an iOS app with CarPlay that lets me talk to my private AI server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/a1014427-c2ce-4204-b41a-001277309cea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5lym7</id>
    <title>Lightweight Hallucination Detector for Local RAG Setups - No Extra LLM Calls Required</title>
    <updated>2025-03-07T12:05:37+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've been working on solving a common problem many of us face when running RAG systems with our local models - hallucinations. While our locally-hosted LLMs are impressive, they still tend to make things up when using RAG, especially when running smaller models with limited context windows.&lt;/p&gt; &lt;p&gt;I've released an &lt;strong&gt;open-source hallucination detector&lt;/strong&gt; that's specifically designed to be efficient enough to run on consumer hardware alongside your local LLMs. Unlike other solutions that require additional LLM API calls (which add latency and often external dependencies), this is a lightweight transformer-based classifier.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on modernBERT architecture&lt;/li&gt; &lt;li&gt;Inference speed: ~1 example/second on CPU, ~10-20 examples/second on modest GPU&lt;/li&gt; &lt;li&gt;Zero external API dependencies - runs completely local&lt;/li&gt; &lt;li&gt;Works with any LLM output, including Llama-2, Llama-3, Mistral, Phi-3, etc.&lt;/li&gt; &lt;li&gt;Integrates easily with LlamaIndex, LangChain, or your custom RAG pipeline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The detector evaluates your LLM's response against the retrieved context to identify when the model generates information not present in the source material. It achieves 80.7% recall on the RAGTruth benchmark, with particularly strong performance on data-to-text tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example integration with your local setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from adaptive_classifier import AdaptiveClassifier # Load the hallucination detector (downloads once, runs locally after) detector = AdaptiveClassifier.from_pretrained(&amp;quot;adaptive-classifier/llm-hallucination-detector&amp;quot;) # Your existing RAG pipeline context = retriever.get_relevant_documents(query) response = your_local_llm.generate(context, query) # Format for the detector input_text = f&amp;quot;Context: {context}\nQuestion: {query}\nAnswer: {response}&amp;quot; # Check for hallucinations prediction = detector.predict(input_text) if prediction[0][0] == 'HALLUCINATED' and prediction[0][1] &amp;gt; 0.6: print(&amp;quot;⚠️ Warning: Response appears to contain information not in the context&amp;quot;) # Maybe re-generate or add a disclaimer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The detector is part of the adaptive-classifier library which also has tools for routing between different local models based on query complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How have you been addressing hallucinations in your local RAG setups?&lt;/li&gt; &lt;li&gt;Would a token-level detector (highlighting exactly which parts are hallucinated) be useful?&lt;/li&gt; &lt;li&gt;What's your typical resource budget for this kind of auxiliary model in your stack?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://github.com/codelion/adaptive-classifier#hallucination-detector"&gt;https://github.com/codelion/adaptive-classifier#hallucination-detector&lt;/a&gt;&lt;br /&gt; Installation: &lt;code&gt;pip install adaptive-classifier&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T12:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66oe7</id>
    <title>RLAMA: A Simple RAG Interface to Chat with Your Documents via Ollama</title>
    <updated>2025-03-08T02:08:16+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.&lt;/p&gt; &lt;h1&gt;What it actually is&lt;/h1&gt; &lt;p&gt;RLAMA is a command-line tool that bridges your local documents and Ollama models. It implements RAG (Retrieval-Augmented Generation) in a minimalist way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Index a folder of documents rlama rag llama3 project-docs ./documentation # Start an interactive session rlama run project-docs &amp;gt; How does the authentication module work? &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You point the tool to a folder containing your files (.txt, .md, .pdf, source code, etc.)&lt;/li&gt; &lt;li&gt;RLAMA extracts text from the documents and generates embeddings via Ollama&lt;/li&gt; &lt;li&gt;When you ask a question, it retrieves relevant passages and sends them to the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The tool handles many formats automatically. For PDFs, it first tries pdftotext, then tesseract if necessary. For binary files, it has several fallback methods to extract what it can.&lt;/p&gt; &lt;h1&gt;Problems it solves&lt;/h1&gt; &lt;p&gt;I use it daily for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Finding information in old technical documents without having to reread everything&lt;/li&gt; &lt;li&gt;Exploring code I'm not familiar with (e.g., &amp;quot;explain how part X works&amp;quot;)&lt;/li&gt; &lt;li&gt;Creating summaries of long documents&lt;/li&gt; &lt;li&gt;Querying my research or meeting notes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real time-saver comes from being able to ask questions instead of searching for keywords. For example, I can ask &amp;quot;What are the possible errors in the authentication API?&amp;quot; and get consolidated answers from multiple files.&lt;/p&gt; &lt;h1&gt;Why use it?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It's simple&lt;/strong&gt;: four commands are enough (rag, run, list, delete)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's local&lt;/strong&gt;: no data is sent over the internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's lightweight&lt;/strong&gt;: no need for Docker or a complete stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's flexible&lt;/strong&gt;: compatible with all Ollama models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I created it because other solutions were either too complex to configure or required sending my documents to external services.&lt;/p&gt; &lt;p&gt;If you already have Ollama installed and are looking for a simple way to query your documents, this might be useful for you.&lt;/p&gt; &lt;h1&gt;In conclusion&lt;/h1&gt; &lt;p&gt;I've found that in discussions on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; point to several pressing needs for local RAG without cloud dependencies: we need to simplify the ingestion of data (PDFs, web pages, videos...) via tools that can automatically transform them into usable text, reduce hardware requirements or better leverage common hardware (model quantization, multi-GPU support) to improve performance, and integrate advanced retrieval methods (hybrid search, rerankers, etc.) to increase answer reliability.&lt;/p&gt; &lt;p&gt;The emergence of integrated solutions (OpenWebUI, LangChain/Langroid, RAGStack, etc.) moves in this direction: the ultimate goal is a tool where users only need to provide their local files to benefit from an AI assistant trained on their own knowledge, while remaining 100% private and local so I wanted to develop something easy to use!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dontizi/rlama"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j61cbx</id>
    <title>I just tried out a model and it blew me away: llama3.2 1b</title>
    <updated>2025-03-07T21:51:53+00:00</updated>
    <author>
      <name>/u/Firm_Newspaper3370</name>
      <uri>https://old.reddit.com/user/Firm_Newspaper3370</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thoroughly enjoying both llama3.3 70b and qwq 32b recently. Both are very impressive for their size, and these are the first models that a poor boy like me can run locally that I feel can give Chat got and Claude a run for their money.&lt;/p&gt; &lt;p&gt;But about an hour ago I had a thought.&lt;/p&gt; &lt;p&gt;If really good models are getting down to medium model sizes, how good are super tiny models getting?&lt;/p&gt; &lt;p&gt;So I downloaded llama3.3 1b.&lt;/p&gt; &lt;p&gt;Having run it through my normal phthon writing prompts, I am truly blown away. It's obviously not a 70b replacement, nor an 8b replacement. But I can hardly fathom that you can fit so much intelligence into 1.3gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm_Newspaper3370"&gt; /u/Firm_Newspaper3370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j62lfa</id>
    <title>Question about AI memory databases using new breakthrough technologies.</title>
    <updated>2025-03-07T22:47:37+00:00</updated>
    <author>
      <name>/u/Furai69</name>
      <uri>https://old.reddit.com/user/Furai69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SpacetimeDB 1.0 was released recently, and the technology used in it made me think its use case for AI could be extreamly beneficial if built to utilize this tech? I'm more curious if people who are advanced in AI databases, do you think this database is beneficial for AI use cases?&lt;/p&gt; &lt;p&gt;Maybe not just a Supabase alternative, but something more like a combination of a front end/backend/database that scales all-in-one type system?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ClockworkLabs/SpacetimeDB"&gt;https://github.com/ClockworkLabs/SpacetimeDB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/kzDnA_EVhTU?si=heESzxcW7EeufXZ5"&gt;https://youtu.be/kzDnA_EVhTU?si=heESzxcW7EeufXZ5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Furai69"&gt; /u/Furai69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T22:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68ijn</id>
    <title>Local WebUI like Google AIStudio used to be like.</title>
    <updated>2025-03-08T03:48:47+00:00</updated>
    <author>
      <name>/u/NihilisticAssHat</name>
      <uri>https://old.reddit.com/user/NihilisticAssHat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a good suggestion for a WebUI or other GUI that resembles how aistudio.google.com used to be? They had a toggle to switch roles for each turn, and they still let you move/delete/edit any turn in history. This would be best if &lt;em&gt;continue responding&lt;/em&gt; were an option.&lt;/p&gt; &lt;p&gt;I tried Open WebUI and was rather let down. Half the time I'm inferencing through Jupyter Notebook anyway, but would like something that offers more user control over generation/history.&lt;/p&gt; &lt;p&gt;I like projects that get the fundamentals right over ones that boast too many features to maintain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NihilisticAssHat"&gt; /u/NihilisticAssHat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5vjtr</id>
    <title>Cydonia 24B v2.1 - Bolder, better, brighter</title>
    <updated>2025-03-07T18:10:20+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt; &lt;img alt="Cydonia 24B v2.1 - Bolder, better, brighter" src="https://external-preview.redd.it/Y53sGNZqFT7O2LPxR0ifOBO74G3g4p3oD2d89H5ZiiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f99bb247d7af48f7df7184bd4ce33a8e1090f74f" title="Cydonia 24B v2.1 - Bolder, better, brighter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v2.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T18:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5s629</id>
    <title>The Genius of DeepSeek’s 57X Efficiency Boost [MLA]</title>
    <updated>2025-03-07T16:12:48+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt; &lt;img alt="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" src="https://external-preview.redd.it/cNA68CfuuchA-pKjC19aWS3fLxcXM9n1EmEaCjksBPI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6d61fc0d91e37ef5748cd5797b5f122d3ab207" title="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0VLAoVGf_74?si=OSwMMKsz9EpLOISJ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:12:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60bc4</id>
    <title>AMD &amp; tinygrad cooperation happening</title>
    <updated>2025-03-07T21:08:39+00:00</updated>
    <author>
      <name>/u/Danmoreng</name>
      <uri>https://old.reddit.com/user/Danmoreng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt; &lt;img alt="AMD &amp;amp; tinygrad cooperation happening" src="https://external-preview.redd.it/BbTfUmQNA0TFAIFilHM3Y4In9mUXZYXfhSxTgqgwTN8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b210445be3d20e6a3900593f94809ea783979b0" title="AMD &amp;amp; tinygrad cooperation happening" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danmoreng"&gt; /u/Danmoreng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AnushElangovan/status/1898101178728431637"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j681n3</id>
    <title>Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark</title>
    <updated>2025-03-08T03:22:26+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt; &lt;img alt="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" src="https://preview.redd.it/izw2ej1cwdne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73dec38ebaddd2720f9cf241a4ae7cf9de6d8481" title="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izw2ej1cwdne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66mpo</id>
    <title>Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark</title>
    <updated>2025-03-08T02:05:50+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt; &lt;img alt="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" src="https://preview.redd.it/ig84dy8oidne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1ee2b26596ddc8e881d50f0eb5e76449003b478" title="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ig84dy8oidne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wzea</id>
    <title>New AMD Driver Yields Up To 11% Performance Increase In koboldcpp</title>
    <updated>2025-03-07T19:02:32+00:00</updated>
    <author>
      <name>/u/WokeCapitalist</name>
      <uri>https://old.reddit.com/user/WokeCapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7900-xt.html"&gt;AMD's Adrenalin 25.3.1 driver&lt;/a&gt; release mentioned &lt;strong&gt;&lt;em&gt;&amp;quot;AI Performance Improvements on AMD Radeon™ RX 7000 Series&amp;quot;&lt;/em&gt;&lt;/strong&gt; in the release notes along with some large percentage increases for applications like Adobe Lightroom Denoise or DaVinci Resolve. As I had their previous WHQL recommended driver already installed, I decided to test it out in koboldcpp. It turns out there was a nice performance bump there, too. Worth a download if you haven't done so already!&lt;/p&gt; &lt;h1&gt;Hardware Test Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Microsoft Windows 11 Professional (x64) Build 26100.3194 (24H2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core Ultra 7 265K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 7900 XT (20GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Z890I Nova WiFi (BIOS 2.22)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk:&lt;/strong&gt; Lexar SSD NM800PRO 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 64GB (2×32GB DDR5 6400 CL32)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Boot, BitLocker &amp;amp; HVCI Enabled&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software Test Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; koboldcpp 1.83.1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; phi-4-q4,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 16384&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BLAS Batch Size&lt;/strong&gt;: 512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Layers&lt;/strong&gt;: 43/43&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Backend: Vulkan&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;30.312 s&lt;/td&gt; &lt;td align="left"&gt;27.607 s&lt;/td&gt; &lt;td align="left"&gt;8.95% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;537.21 T/s&lt;/td&gt; &lt;td align="left"&gt;589.85 T/s&lt;/td&gt; &lt;td align="left"&gt;9.84% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.203 s&lt;/td&gt; &lt;td align="left"&gt;5.301 s&lt;/td&gt; &lt;td align="left"&gt;1.87% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;19.22 T/s&lt;/td&gt; &lt;td align="left"&gt;18.86 T/s&lt;/td&gt; &lt;td align="left"&gt;1.88% lower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;35.515 s&lt;/td&gt; &lt;td align="left"&gt;32.908 s&lt;/td&gt; &lt;td align="left"&gt;7.35% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Backend: ROCm&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;24.861 s&lt;/td&gt; &lt;td align="left"&gt;22.370 s&lt;/td&gt; &lt;td align="left"&gt;10.06% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;655.00 T/s&lt;/td&gt; &lt;td align="left"&gt;727.94 T/s&lt;/td&gt; &lt;td align="left"&gt;11.15% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.831 s&lt;/td&gt; &lt;td align="left"&gt;5.586 s&lt;/td&gt; &lt;td align="left"&gt;4.20% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;17.15 T/s&lt;/td&gt; &lt;td align="left"&gt;17.90 T/s&lt;/td&gt; &lt;td align="left"&gt;4.32% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;30.692 s&lt;/td&gt; &lt;td align="left"&gt;27.956 s&lt;/td&gt; &lt;td align="left"&gt;8.97% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WokeCapitalist"&gt; /u/WokeCapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60wt1</id>
    <title>NVIDIA RTX "PRO" 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W</title>
    <updated>2025-03-07T21:34:40+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt; &lt;img alt="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" src="https://external-preview.redd.it/8nLxMIJQrz_2tTIhvuryMxrtnbjPwWSOP7OO-C_HgM0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8c481144f93be83af79ca767c97e43a6750c4ac" title="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-x-blackwell-leak-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
</feed>
