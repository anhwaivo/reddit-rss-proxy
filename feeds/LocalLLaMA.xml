<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-21T15:06:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iu7c24</id>
    <title>arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune</title>
    <updated>2025-02-20T19:41:51+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt; &lt;img alt="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" src="https://external-preview.redd.it/nAJEyVNIP8SWOhtVuMgvqEBinfu5P4u1oNYU06SZdto.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e08074fa2ad405c1ee359322e0fc01d2fb5148f8" title="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-Blitz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4gvf</id>
    <title>I changed my mind about DeepSeek-R1-Distill-Llama-70B</title>
    <updated>2025-02-20T17:46:52+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt; &lt;img alt="I changed my mind about DeepSeek-R1-Distill-Llama-70B" src="https://preview.redd.it/zknh3vk6xbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7defd62a3749692bb67bb3c597046e8dd3da633" title="I changed my mind about DeepSeek-R1-Distill-Llama-70B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zknh3vk6xbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iustfq</id>
    <title>When it comes to roleplaying chatbots, wouldn't it be better to have two AI instances instead of one?</title>
    <updated>2025-02-21T14:58:34+00:00</updated>
    <author>
      <name>/u/Pasta-hobo</name>
      <uri>https://old.reddit.com/user/Pasta-hobo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One acting as the character, and the other acting as the environment or DM, basically?&lt;/p&gt; &lt;p&gt;That way, one AI just has to act in-character, and the other just has to be consistent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pasta-hobo"&gt; /u/Pasta-hobo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iustfq/when_it_comes_to_roleplaying_chatbots_wouldnt_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iustfq/when_it_comes_to_roleplaying_chatbots_wouldnt_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iustfq/when_it_comes_to_roleplaying_chatbots_wouldnt_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T14:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuqw03</id>
    <title>Clarification on Transformer Scaling: Is My Understanding Correct?</title>
    <updated>2025-02-21T13:28:34+00:00</updated>
    <author>
      <name>/u/Standing_Appa8</name>
      <uri>https://old.reddit.com/user/Standing_Appa8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been researching how transformer models scale in terms of memory (VRAM) and compute, and I've come across some information from both ChatGPT and Perplexity that left me a bit confused. Here’s the summary I gathered:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM (Memory) Requirements:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;KV-Cache:&lt;/strong&gt; For every token processed, a key-value pair is stored in each attention layer. This causes a linear increase in memory usage as the token count grows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Weights &amp;amp; Intermediate Results:&lt;/strong&gt; These remain constant regardless of the sequence length when processing a single inference request.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compute Requirements:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Self-Attention:&lt;/strong&gt; The transformer calculates interactions between every pair of tokens. This results in a quadratic scaling of compute cost as the sequence length increases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Overheads:&lt;/strong&gt; During training, additional costs such as activations, gradients, and optimizer states further boost the compute requirements.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM vs. Compute Trade-off:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;The total VRAM needed is a sum of the model weights, the KV-cache (which grows linearly with tokens), and other temporary buffers. If this sum exceeds the available VRAM, it leads to an Out-of-Memory (OOM) error.&lt;/li&gt; &lt;li&gt;In contrast, while the VRAM requirement grows linearly, the compute cost (especially for self-attention) grows quadratically with the number of tokens.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Other Considerations:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Number of Parameters:&lt;/strong&gt; A higher number of parameters increases the baseline memory and compute requirements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision (e.g., FP16, 8-bit, 4-bit):&lt;/strong&gt; Using lower precision can reduce memory usage but may affect compute performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Measuring Inference Speed:&lt;/strong&gt; Inference speed can be measured in terms of FPS (frames per second) or FLOPS (floating point operations per second).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Short Summary:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory (VRAM):&lt;/strong&gt; Grows linearly with token count (due to the KV-cache).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compute:&lt;/strong&gt; Grows quadratically with token count (due to self-attention computations).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m a bit confused about whether this summary is completely accurate. Has anyone delved into the specifics of transformer scaling and can confirm or correct this understanding? Are there any nuances or important details I might be missing regarding inference vs. training costs?&lt;/p&gt; &lt;p&gt;Thanks in advance for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standing_Appa8"&gt; /u/Standing_Appa8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuqw03/clarification_on_transformer_scaling_is_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuqw03/clarification_on_transformer_scaling_is_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuqw03/clarification_on_transformer_scaling_is_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T13:28:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iungbr</id>
    <title>Efficient LLM inferencing (PhD), looking to answer your questions!</title>
    <updated>2025-02-21T10:00:49+00:00</updated>
    <author>
      <name>/u/trippleguy</name>
      <uri>https://old.reddit.com/user/trippleguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm finishing my PhD in conversational NLP this spring. While I am not planning on writing another paper, I was interested in doing a survey regardless, focusing on &lt;em&gt;model-level optimizations for faster inferencing&lt;/em&gt;. That is, from the second you load a model into memory, whether this is in a quantized setting or not.&lt;/p&gt; &lt;p&gt;I was hoping to get some input on things that may be unclear, or something you just would like to know more about, mostly regarding the following:&lt;/p&gt; &lt;p&gt;- quantization (post-training)&lt;/p&gt; &lt;p&gt;- pruning (structured/unstructured)&lt;/p&gt; &lt;p&gt;- knowledge distillation and distillation techniques (white/black-box)&lt;/p&gt; &lt;p&gt;There is already an abundance of research out there on the topic of efficient LLMs. Still, these studies often cover far too broad topics such as system applications, evaluation, pre-training ++.&lt;/p&gt; &lt;p&gt;If you have any requests or inputs, I'll do my best to cover them in a review that I plan on finishing within the next few weeks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trippleguy"&gt; /u/trippleguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iungbr/efficient_llm_inferencing_phd_looking_to_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iungbr/efficient_llm_inferencing_phd_looking_to_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iungbr/efficient_llm_inferencing_phd_looking_to_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T10:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuq86e</id>
    <title>SOCAMM analysis</title>
    <updated>2025-02-21T12:55:02+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is SOCAMM?&lt;/p&gt; &lt;p&gt;SOCAMM: Next-Generation Memory for On-Device AI&lt;/p&gt; &lt;p&gt;• Interest in SOCAMM (System On Chip with Advanced Memory Module) has been growing within the AI industry.&lt;/p&gt; &lt;p&gt;• In particular, with the unveiling of NVIDIA’s personal supercomputer “Digits” at CES this past January, there has been active discussion about the potential use of new types of memory modules in next-generation AI devices.&lt;/p&gt; &lt;p&gt;• While SOCAMM currently draws the most attention among next-generation memory modules, other varieties such as LLW (Low Latency Wide I/O) and LPCAMM (Low Power Compression Attached Memory Module) are also being considered for AI device memory.&lt;/p&gt; &lt;p&gt;• SOCAMM is a module that integrates an SoC (System on Chip), commonly used in smartphones and laptops, together with memory in a single package. It has garnered attention because AI devices require high bandwidth, low power consumption, and smaller form factors.&lt;/p&gt; &lt;p&gt;• AI computing demands high-bandwidth memory. However, in the conventional DIMM approach, the SoC and memory are relatively far apart, resulting in lower bandwidth and higher latency.&lt;/p&gt; &lt;p&gt;• Because SOCAMM places the SoC and memory in close physical proximity, it improves communication efficiency between the logic and the memory, enabling high bandwidth and low latency.&lt;/p&gt; &lt;p&gt;• For AI devices running on batteries, AI computation can consume significant power, so low-power operation is crucial. Under the conventional method (DIMM, MCP, etc.), the SoC and memory are connected through a PCB (Printed Circuit Board), requiring a complex communication path—SoC → memory controller → PCB traces → memory module.&lt;/p&gt; &lt;p&gt;• Such a long communication path demands higher voltage, which negatively impacts battery life.&lt;/p&gt; &lt;p&gt;• In contrast, SOCAMM allows the SoC and memory to communicate directly via the memory controller inside the SoC, enabling lower-voltage operation and reducing battery consumption.&lt;/p&gt; &lt;p&gt;• Under the conventional method, additional wiring space is needed on the PCB to connect the memory and SoC, causing unnecessary increases in form factor.&lt;/p&gt; &lt;p&gt;• By integrating the SoC and memory in a single package, PCB design is simplified, making a smaller form factor possible.&lt;/p&gt; &lt;p&gt;• SOCAMM is not yet in full-scale adoption, but preliminary steps toward its future implementation appear to be underway.&lt;/p&gt; &lt;p&gt;• As the AI industry continues to develop rapidly and AI devices become more widespread, SOCAMM, LPCAMM, and LLW are expected to serve as next-generation memory solutions.&lt;/p&gt; &lt;p&gt;Source: Hyundai Motor Securities via Jukanlosreve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuq86e/socamm_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuq86e/socamm_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuq86e/socamm_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T12:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iunuyg</id>
    <title>VimLM: Bringing LLM Assistance to Vim, Locally</title>
    <updated>2025-02-21T10:29:20+00:00</updated>
    <author>
      <name>/u/JosefAlbers05</name>
      <uri>https://old.reddit.com/user/JosefAlbers05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wanted seamless LLM integration inside Vim, without leaving your editor? &lt;strong&gt;VimLM&lt;/strong&gt; is a lightweight, keyboard-driven AI assistant designed specifically for Vim users. It runs locally, and keeps you in the flow. &lt;/p&gt; &lt;p&gt;![VimLM Demo](&lt;a href="https://raw.githubusercontent.com/JosefAlbers/VimLM/main/assets/captioned_vimlm.gif"&gt;https://raw.githubusercontent.com/JosefAlbers/VimLM/main/assets/captioned_vimlm.gif&lt;/a&gt;)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt AI inside Vim&lt;/strong&gt; (&lt;code&gt;Ctrl-l&lt;/code&gt; to ask, &lt;code&gt;Ctrl-j&lt;/code&gt; for follow-ups)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Locally run models&lt;/strong&gt; – works with &lt;code&gt;Llama&lt;/code&gt;, &lt;code&gt;DeepSeek&lt;/code&gt;, and others&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient workflow&lt;/strong&gt; – apply suggestions instantly (&lt;code&gt;Ctrl-p&lt;/code&gt;)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible context&lt;/strong&gt; – add files, diffs, or logs to prompts&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/JosefAlbers/VimLM"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you use LLMs inside Vim or are looking for a &lt;strong&gt;local AI workflow&lt;/strong&gt;, check it out! Feedback and contributions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JosefAlbers05"&gt; /u/JosefAlbers05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunuyg/vimlm_bringing_llm_assistance_to_vim_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunuyg/vimlm_bringing_llm_assistance_to_vim_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iunuyg/vimlm_bringing_llm_assistance_to_vim_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T10:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuicqb</id>
    <title>S*: Test Time Scaling for Code Generation</title>
    <updated>2025-02-21T04:23:14+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.14382"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuicqb/s_test_time_scaling_for_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuicqb/s_test_time_scaling_for_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iufbmy</id>
    <title>Deepseek R1 671b minimum hardware to get 20TPS running only in RAM</title>
    <updated>2025-02-21T01:47:06+00:00</updated>
    <author>
      <name>/u/therebrith</name>
      <uri>https://old.reddit.com/user/therebrith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking into full chatgpt replacement and shopping for hardware. I've seen the digital spaceport's $2k &lt;a href="https://digitalspaceport.com/how-to-run-deepseek-r1-671b-fully-locally-on-2000-epyc-rig/"&gt;build&lt;/a&gt; that gives 5ish TPS using an 7002/7003 EPYC and 512GB of DDR4 2400. It's a good experiment, but 5 token/s is not gonna replace chatgpt from day to day use. So I wonder what would be the minimum hardwares like to get minimum 20 token/s with 3~4s or less first token wait time, running only on RAM? &lt;/p&gt; &lt;p&gt;I'm sure not a lot of folks have tried this, but just throwing out there, that a setup with 1TB DDR5 at 4800 with dual EPYC 9005(192c/384t), would that be enough for the 20TPS ask?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therebrith"&gt; /u/therebrith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T01:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iun173</id>
    <title>SigLIP 2: A better multilingual vision language encoder</title>
    <updated>2025-02-21T09:31:07+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SigLIP 2 is out on Hugging Face!&lt;/p&gt; &lt;p&gt;A new family of multilingual vision-language encoders that crush it in zero-shot classification, image-text retrieval, and VLM feature extraction.&lt;/p&gt; &lt;p&gt;What’s new in SigLIP 2?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Builds on SigLIP’s sigmoid loss with decoder + self-distillation objectives&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Better semantic understanding, localization, and dense features&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Outperforms original SigLIP across all scales.&lt;/p&gt; &lt;p&gt;Killer feature: NaFlex variants! Dynamic resolution for tasks like OCR or document understanding. Plus, sizes from Base (86M) to Giant (1B) with patch/resolution options.&lt;/p&gt; &lt;p&gt;Why care?Not only a better vision encoder, but also a tool for better VLMs.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://huggingface.co/blog/siglip2"&gt;https://huggingface.co/blog/siglip2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iun173/siglip_2_a_better_multilingual_vision_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iun173/siglip_2_a_better_multilingual_vision_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iun173/siglip_2_a_better_multilingual_vision_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T09:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu2sdk</id>
    <title>SmolVLM2: New open-source video models running on your toaster</title>
    <updated>2025-02-20T16:39:27+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on zero-shot vision/multimodality 👋🏻&lt;/p&gt; &lt;p&gt;Today we released SmolVLM2, new vision LMs in three sizes: 256M, 500M, 2.2B. This release comes with zero-day support for transformers and MLX, and we built applications based on these, along with video captioning fine-tuning tutorial. &lt;/p&gt; &lt;p&gt;We release the following:&lt;br /&gt; &amp;gt; an iPhone app (runs on 500M model in MLX)&lt;br /&gt; &amp;gt; integration with VLC for segmentation of descriptions (based on 2.2B)&lt;br /&gt; &amp;gt; a video highlights extractor (based on 2.2B)&lt;/p&gt; &lt;p&gt;Here's a video from the iPhone app ⤵️ you can read and learn more from our blog and check everything in our collection 🤗&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player"&gt;https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T16:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4bc0</id>
    <title>New QwQ Confirmed to be in the works “no hurries”</title>
    <updated>2025-02-20T17:40:30+00:00</updated>
    <author>
      <name>/u/YTLupo</name>
      <uri>https://old.reddit.com/user/YTLupo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt; &lt;img alt="New QwQ Confirmed to be in the works “no hurries”" src="https://preview.redd.it/7e0x8lh3zbke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88979b41d573ee37d2cb08acc56da58b982176c2" title="New QwQ Confirmed to be in the works “no hurries”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of interesting replies &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A"&gt;https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As someone who uses QWEN2.5 and the existing QwQ model I’m pretty hype to see what happens. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YTLupo"&gt; /u/YTLupo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e0x8lh3zbke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:40:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu56o1</id>
    <title>10x longer contexts for reasoning training - 90% less memory GRPO in Unsloth</title>
    <updated>2025-02-20T18:15:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Thanks so much for the support on our GRPO release 2 weeks ago! Today, we're excited to announce that you can now train your own reasoning model with just &lt;strong&gt;5GB VRAM&lt;/strong&gt; for Qwen2.5 (1.5B) - down from 7GB in the previous &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; release!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is thanks to our newly derived Efficient GRPO algorithm which enables &lt;strong&gt;&lt;em&gt;10x longer context&lt;/em&gt;&lt;/strong&gt; lengths while using &lt;strong&gt;&lt;em&gt;90% less VRAM&lt;/em&gt;&lt;/strong&gt; vs. all other GRPO LoRA/QLoRA implementations, even those utilizing Flash Attention 2 (FA2).&lt;/li&gt; &lt;li&gt;With a GRPO setup using TRL + FA2, Llama 3.1 (8B) training at 20K context length demands &lt;strong&gt;510.8G&lt;/strong&gt; of VRAM. However, Unsloth’s 90% VRAM reduction brings the requirement down to &lt;strong&gt;just 54.3GB&lt;/strong&gt; in the same setup.&lt;/li&gt; &lt;li&gt;We leverage our &lt;a href="https://unsloth.ai/blog/long-context"&gt;gradient checkpointing&lt;/a&gt; algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. &lt;strong&gt;&lt;em&gt;This shaves a whopping 372GB VRAM&lt;/em&gt;&lt;/strong&gt; since we need num_generations = 8. We can reduce this memory usage even further through intermediate gradient accumulation.&lt;/li&gt; &lt;li&gt;We also implemented a highly memory efficient GRPO loss, which saves memory usage by 8x. Before 78GB was needed for 20K context length - now only 10GB!&lt;/li&gt; &lt;li&gt;Try our free GRPO notebook with 10x longer context: Llama 3.1 (8B) on Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Blog for more details on the algorithm, the Maths behind GRPO, issues we found and more: &lt;a href="https://unsloth.ai/blog/grpo"&gt;https://unsloth.ai/blog/grpo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GRPO VRAM Breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Unsloth&lt;/th&gt; &lt;th align="left"&gt;TRL + FA2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;42GB&lt;/td&gt; &lt;td align="left"&gt;414GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GRPO Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;9.8GB&lt;/td&gt; &lt;td align="left"&gt;78.3GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;0GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference KV Cache for 20K context (GB)&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Memory Usage&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.3GB (90% less)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;510.8GB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;We also now provide full logging details for all reward functions now! Previously we only showed the total aggregated reward function itself.&lt;/li&gt; &lt;li&gt;You can now run and do inference with our &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;4-bit dynamic quants&lt;/a&gt; directly in vLLM.&lt;/li&gt; &lt;li&gt;Also we spent a lot of time on our Guide for everything on GRPO + reward functions/verifiers so would highly recommend you guys to read it: &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;docs.unsloth.ai/basics/reasoning&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you guys once again for all the support it truly means so much to us! We also have a major release coming within the next few weeks which I know you guys have been waiting for - and we're also excited for it!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T18:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iue6n1</id>
    <title>OpenThinker is a decensored 32B reasoning deepseek distilled model</title>
    <updated>2025-02-21T00:53:25+00:00</updated>
    <author>
      <name>/u/NousJaccuzi</name>
      <uri>https://old.reddit.com/user/NousJaccuzi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://bespokelabs.ai/blog/openthinker-is-a-decensored-reasoning-model"&gt;https://bespokelabs.ai/blog/openthinker-is-a-decensored-reasoning-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/openthinker"&gt;https://ollama.com/library/openthinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NousJaccuzi"&gt; /u/NousJaccuzi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T00:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu6egg</id>
    <title>Even AI has some personality :)</title>
    <updated>2025-02-20T19:04:20+00:00</updated>
    <author>
      <name>/u/_idkwhattowritehere_</name>
      <uri>https://old.reddit.com/user/_idkwhattowritehere_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt; &lt;img alt="Even AI has some personality :)" src="https://preview.redd.it/3dlpqfoydcke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfb9019fc88f0d5f525fe174f20b501b99bf0c66" title="Even AI has some personality :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_idkwhattowritehere_"&gt; /u/_idkwhattowritehere_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dlpqfoydcke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujafd</id>
    <title>Best LLMs!? (Focus: Best &amp; 7B-32B) 02/21/2025</title>
    <updated>2025-02-21T05:16:02+00:00</updated>
    <author>
      <name>/u/DeadlyHydra8630</name>
      <uri>https://old.reddit.com/user/DeadlyHydra8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I am fairly new to this space and this is my first post here so go easy on me 😅&lt;/p&gt; &lt;pre&gt;&lt;code&gt;For those who are also new! What does this 7B, 14B, 32B parameters even mean? - It represents the number of trainable weights in the model, which determine how much data it can learn and process. - Larger models can capture more complex patterns but require more compute, memory, and data, while smaller models can be faster and more efficient. What do I need to run Local Models? - Ideally you'd want the most VRAM GPU possible allowing you to run bigger models - Though if you have a laptop with a NPU that's also great! - If you do not have a GPU focus on trying to use smaller models 7B and lower! - (Reference the Chart below) How do I run a Local Model? - Theres various guides online - I personally like using LMStudio it has a nice interface - I also use Ollama &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Quick Guide!&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: This chart could have issues, please correct me!&lt;/p&gt; &lt;p&gt;Note: For Android, Smolchat and Pocketpal are great apps to download models from Huggingface&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device Type&lt;/th&gt; &lt;th align="left"&gt;VRAM/RAM&lt;/th&gt; &lt;th align="left"&gt;Recommended Bit Precision&lt;/th&gt; &lt;th align="left"&gt;Max LLM Parameters (Approx.)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Smartphones&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Low-end phones&lt;/td&gt; &lt;td align="left"&gt;4 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;~1-2 billion&lt;/td&gt; &lt;td align="left"&gt;For basic tasks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range phones&lt;/td&gt; &lt;td align="left"&gt;6-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Good balance of performance and model size.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end phones&lt;/td&gt; &lt;td align="left"&gt;12 GB RAM&lt;/td&gt; &lt;td align="left"&gt;8-bit&lt;/td&gt; &lt;td align="left"&gt;~6 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;x86 Laptops&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Integrated GPU (e.g., Intel Iris)&lt;/td&gt; &lt;td align="left"&gt;8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;8-bit&lt;/td&gt; &lt;td align="left"&gt;~4 billion&lt;/td&gt; &lt;td align="left"&gt;Suitable for smaller to medium-sized models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gaming Laptops (e.g., RTX 3050)&lt;/td&gt; &lt;td align="left"&gt;4-6 GB VRAM + RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~2-6 billion&lt;/td&gt; &lt;td align="left"&gt;Seems crazy ik but we aim for model size that runs smoothly and responsively&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end Laptops (e.g., RTX 3060)&lt;/td&gt; &lt;td align="left"&gt;8-12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;8-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~4-6 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models, especially with 16-bit for higher quality.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ARM Devices&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Raspberry Pi 4&lt;/td&gt; &lt;td align="left"&gt;4-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Best for experimentation and smaller models due to memory constraints.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apple M1/M2 (Unified Memory)&lt;/td&gt; &lt;td align="left"&gt;8-24 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~4-12 billion&lt;/td&gt; &lt;td align="left"&gt;Unified memory allows for larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU Computers&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range GPU (e.g., RTX 4070)&lt;/td&gt; &lt;td align="left"&gt;12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~6-14 billion&lt;/td&gt; &lt;td align="left"&gt;Good for general LLM tasks and development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end GPU (e.g., RTX 3090)&lt;/td&gt; &lt;td align="left"&gt;24 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;16-bit&lt;/td&gt; &lt;td align="left"&gt;~12 billion&lt;/td&gt; &lt;td align="left"&gt;Big boi territory!&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Server GPU (e.g., A100)&lt;/td&gt; &lt;td align="left"&gt;40-80 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;16-bit to 32-bit&lt;/td&gt; &lt;td align="left"&gt;~20-40 billion&lt;/td&gt; &lt;td align="left"&gt;For the largest models and research.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The point of this post is to essentially find and keep updating this post with the best new models most people can actually use.&lt;/p&gt; &lt;p&gt;While sure the 70B, 405B, 671B and Closed sources models are incredible, some of us don't have the facilities for those huge models and don't want to give away our data 🙃&lt;/p&gt; &lt;p&gt;I will put up what &lt;strong&gt;I believe&lt;/strong&gt; are the best models for each of these categories &lt;strong&gt;CURRENTLY&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Please, please, please, those who are much much more knowledgeable, let me know what models I should put if I am missing any great models or categories I should include!)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I cannot find RRD2.5 for the life of me on HuggingFace.&lt;/p&gt; &lt;p&gt;I will have benchmarks, so those are more definitive. some other stuff will be subjective I will also have links to the repo (I'm also including links; I am no evil man but don't trust strangers on the world wide web)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; {Parameter}: {Model} - {Score}&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;MMLU-Pro (language comprehension and reasoning across diverse domains):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best:&lt;/em&gt; &lt;em&gt;DeepSeek-R1 - 0.84&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/bartowski/QwQ-32B-Preview-GGUF"&gt;QwQ-32B-Preview&lt;/a&gt; - 0.7097&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Phi-4&lt;/a&gt; - 0.704&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF"&gt;Qwen2.5-7B-Instruct&lt;/a&gt; - 0.4724&lt;br /&gt; ------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Math:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Gemini-2.0-Flash-exp - 0.8638&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.8053&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.6788&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5803&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Coding (conceptual, debugging, implementation, optimization):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: OpenAI O1 - 0.981 (148/148)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-32B Coder&lt;/a&gt; - 0.817&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral Small 3&lt;/a&gt; - 0.692&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF"&gt;Qwen2.5-Coder-14B-Instruct&lt;/a&gt; - 0.6707&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"&gt;Llama3.1-8B Instruct&lt;/a&gt; - 0.385&lt;/p&gt; &lt;p&gt;HM:&lt;br /&gt; 32B: &lt;a href="https://huggingface.co/waldie/DeepSeek-R1-Distill-Qwen-32B-4bpw-h6-exl2"&gt;DeepSeek-R1-Distill&lt;/a&gt; - (148/148)&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/THUDM/codegeex4-all-9b"&gt;CodeGeeX4-All&lt;/a&gt; - (146/148)&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Creative Writing:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Arena Creative Writing:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1422, OpenAI 4o - 1420&lt;/em&gt;&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; &lt;strong&gt;-&lt;/strong&gt; 1244&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1199&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1178&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EQ Bench (Emotional Intelligence Benchmarks for LLMs):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: DeepSeek-R1 - 87.11&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;9B:&lt;/em&gt; &lt;a href="https://huggingface.co/ifable/gemma-2-Ifable-9B"&gt;gemma-2-Ifable-9B&lt;/a&gt; &lt;em&gt;- 84.59&lt;/em&gt;&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Longer Query (&amp;gt;= 500 tokens)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1425, Gemini-2.0-Pro/Flash-Thinking-Exp - 1399/1395&lt;/em&gt;&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1264&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1261&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; - 1239&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt; - 1233&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Heathcare/Medical (USMLE, AIIMS &amp;amp; NEET PG, College/Profession level quesions)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(8B) Best Avg.&lt;/em&gt;: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 90.01&lt;/p&gt; &lt;p&gt;(8B) Best USMLE, AIIMS &amp;amp; NEET PG: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 81.07&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Business&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.8137&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7567&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7085&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.5539&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5412&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Economics&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.859&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7725&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7310&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.6552&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Sincerely, I do not trust myself yet to be benchmarking, so I used the web:&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard"&gt;https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard"&gt;https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/?leaderboard"&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/math-word-problem-solving-on-math"&gt;https://paperswithcode.com/sota/math-word-problem-solving-on-math&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/code-generation-on-humaneval"&gt;https://paperswithcode.com/sota/code-generation-on-humaneval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeadlyHydra8630"&gt; /u/DeadlyHydra8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu8f7s</id>
    <title>Speculative decoding can identify broken quants?</title>
    <updated>2025-02-20T20:26:14+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt; &lt;img alt="Speculative decoding can identify broken quants?" src="https://a.thumbs.redditmedia.com/XqCzop9kq738DVsg8GJQXPOa46DC9kly1ZUdEC5f5l4.jpg" title="Speculative decoding can identify broken quants?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iu8f7s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T20:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iudao8</id>
    <title>langchain is still a rabbit hole in 2025</title>
    <updated>2025-02-20T23:56:22+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;langchain is still a rabbit hole in 2025 And the langgraph framework as well&lt;/p&gt; &lt;p&gt;Is it just me or other people think this is the case as well?&lt;/p&gt; &lt;p&gt;Instead of spending hours going through the rabbit holes in these frameworks , I found out an ugly hard coded way is faster to implement. Yeah I know hard coed things are hard to maintain. But consider the break changes in langchain through 0.1, 0.2, 0.3. Things are hard to maintain in either way.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Sorry my language might not be very friendly when I posted this, but I had a bad day. So here is what happened: I tried to build a automatic workflow to do something for me. Like everyone said, agent x LLM is the future blah blah blah... &lt;/p&gt; &lt;p&gt;Anyway, I start looking, for a workflow framework. There are dify, langflow, flowise, pyspur, Laminar, comfyui_LLM_party... But I picked langgraph since they are more or less codebased, doesn't require to setup things like clickhouse for a simple demo, and I could write custom nodes. &lt;/p&gt; &lt;p&gt;So I run in, into the rabbit holes. Like everyone in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , I don't like OpenAI or other LLM provider, I like to host my own instance and make sure my data is mine. So I go with llama.cpp (which I've played with for a while) Then my bad day came:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp: The OpenAI compatible API doesn't work well with the tool calling &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11988"&gt;https://github.com/ggml-org/llama.cpp/issues/11988&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11847"&gt;https://github.com/ggml-org/llama.cpp/issues/11847&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: the jinja template is still buggy &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11938"&gt;https://github.com/ggml-org/llama.cpp/issues/11938&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: the tool calls doesn't return tool call id &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11992"&gt;https://github.com/ggml-org/llama.cpp/issues/11992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just want to build a custom workflow that has tool calling with my llama.cpp, with custom node / function that could intergate with my current projects, why is it so hard...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T23:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iunu3j</id>
    <title>What's with the too-good-to-be-true cheap GPUs from China on ebay lately? Obviously scammy, but strangely they stay up.</title>
    <updated>2025-02-21T10:27:32+00:00</updated>
    <author>
      <name>/u/Massive_Robot_Cactus</name>
      <uri>https://old.reddit.com/user/Massive_Robot_Cactus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've seen a lot of cheap A100, H100, etc being posted lately on ebay, like $856 for a 40GB pci-e A100. All coming from China, with cloned photos and fresh seller accounts...classic scam material. But they're not coming down so quickly.&lt;/p&gt; &lt;p&gt;Has anyone actually tried to purchase one of these to see what happens? Very much these seem too good to be true, but I'm wondering how the scam works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive_Robot_Cactus"&gt; /u/Massive_Robot_Cactus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iunu3j/whats_with_the_toogoodtobetrue_cheap_gpus_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T10:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu19zy</id>
    <title>2025 is an AI madhouse</title>
    <updated>2025-02-20T15:36:23+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt; &lt;img alt="2025 is an AI madhouse" src="https://preview.redd.it/ferhsryxcbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1632f508c8f4f33f22d5753531a2d6bc7a1ca3" title="2025 is an AI madhouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025 is straight-up wild for AI development. Just last year, it was mostly ChatGPT, Claude, and Gemini running the show. &lt;/p&gt; &lt;p&gt;Now? We’ve got an AI battle royale with everyone jumping in Deepseek, Kimi, Meta, Perplexity, Elon’s Grok&lt;/p&gt; &lt;p&gt;With all these options, the real question is: which one are you actually using daily?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ferhsryxcbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T15:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iupq6h</id>
    <title>Have we hit a scaling wall in base models? (non reasoning)</title>
    <updated>2025-02-21T12:27:39+00:00</updated>
    <author>
      <name>/u/CH1997H</name>
      <uri>https://old.reddit.com/user/CH1997H</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet&lt;/p&gt; &lt;p&gt;Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the &amp;quot;scaling laws&amp;quot; where the chart just says &amp;quot;line goes up&amp;quot;)&lt;/p&gt; &lt;p&gt;Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling&lt;/p&gt; &lt;p&gt;It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CH1997H"&gt; /u/CH1997H &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iupq6h/have_we_hit_a_scaling_wall_in_base_models_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T12:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iur927</id>
    <title>I tested Grok 3 against Deepseek r1 on my personal benchmark. Here's what I found out</title>
    <updated>2025-02-21T13:46:16+00:00</updated>
    <author>
      <name>/u/goddamnit_1</name>
      <uri>https://old.reddit.com/user/goddamnit_1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, the Grok 3 is here. And as a Whale user, I wanted to know if it's as big a deal as they are making out to be.&lt;/p&gt; &lt;p&gt;Though I know it's unfair for Deepseek r1 to compare with Grok 3 which was trained on 100k h100 behemoth cluster.&lt;/p&gt; &lt;p&gt;But I was curious about how much better Grok 3 is compared to Deepseek r1. So, I tested them on my personal set of questions on reasoning, mathematics, coding, and writing.&lt;/p&gt; &lt;p&gt;Here are my observations.&lt;/p&gt; &lt;h1&gt;Reasoning and Mathematics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 and Deepseek r1 are practically neck-and-neck in these categories.&lt;/li&gt; &lt;li&gt;Both models handle complex reasoning problems and mathematics with ease. Choosing one over the other here doesn't seem to make much of a difference.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 leads in this category. Its code quality, accuracy, and overall answers are simply better than Deepseek r1's.&lt;/li&gt; &lt;li&gt;Deepseek r1 isn't bad, but it doesn't come close to Grok 3. If coding is your primary use case, Grok 3 is the clear winner.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Writing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both models are equally better for creative writing, but I personally prefer Grok 3’s responses.&lt;/li&gt; &lt;li&gt;For my use case, which involves technical stuff, I liked the Grok 3 better. Deepseek has its own uniqueness; I can't get enough of its autistic nature.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Who Should Use Which Model?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grok 3 is the better option if you're focused on coding.&lt;/li&gt; &lt;li&gt;For reasoning and math, you can't go wrong with either model. They're equally capable.&lt;/li&gt; &lt;li&gt;If technical writing is your priority, Grok 3 seems slightly better than Deepseek r1 for my personal use cases, for schizo talks, no one can beat Deepseek r1.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a detailed analysis,&lt;a href="https://composio.dev/blog/grok-3-vs-deepseek-r1/"&gt; Grok 3 vs Deepseek r1&lt;/a&gt;, for a more detailed breakdown, including specific examples and test cases.&lt;/p&gt; &lt;p&gt;What are your experiences with the new Grok 3? Did you find the model useful for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goddamnit_1"&gt; /u/goddamnit_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iur927/i_tested_grok_3_against_deepseek_r1_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T13:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iulq4o</id>
    <title>We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE</title>
    <updated>2025-02-21T07:56:29+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt; &lt;img alt="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" src="https://external-preview.redd.it/dzQ4N25sbTM1Z2tlMVF8vLuY1I7D-30miO4pvAdRk1TFvpSr9DfFmva9zHJp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b308a1599c976871d72ed516a3f26b46303ad50a" title="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vkth2pm35gke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T07:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujig7</id>
    <title>Deepseek will publish 5 open source repos next week.</title>
    <updated>2025-02-21T05:29:06+00:00</updated>
    <author>
      <name>/u/WashWarm8360</name>
      <uri>https://old.reddit.com/user/WashWarm8360</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt; &lt;img alt="Deepseek will publish 5 open source repos next week." src="https://preview.redd.it/rdzshzfihfke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e7b8ca74fce95adc4de966cdb0f09467f784c54" title="Deepseek will publish 5 open source repos next week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WashWarm8360"&gt; /u/WashWarm8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rdzshzfihfke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iui6nk</id>
    <title>Starting next week, DeepSeek will open-source 5 repos</title>
    <updated>2025-02-21T04:13:54+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt; &lt;img alt="Starting next week, DeepSeek will open-source 5 repos" src="https://preview.redd.it/syeh0rmm3fke1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3667b8c2ba5c4d6506f21080ba3334e6724119" title="Starting next week, DeepSeek will open-source 5 repos" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syeh0rmm3fke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:13:54+00:00</published>
  </entry>
</feed>
