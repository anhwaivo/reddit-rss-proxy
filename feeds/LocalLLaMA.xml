<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-26T06:55:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n0bvvy</id>
    <title>Easy XTTS V2 voice cloning on CPU (link)</title>
    <updated>2025-08-26T03:58:06+00:00</updated>
    <author>
      <name>/u/Sidran</name>
      <uri>https://old.reddit.com/user/Sidran</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to make this post in case it might help someone.&lt;/p&gt; &lt;p&gt;If you are interested in good voice cloning without all the python/command prompt/docker/etc infinitely dependent and tangled crap, you have a link here for already built and portable version: &lt;a href="https://huggingface.co/spaces/Olivier-Truong/XTTS_V2_CPU_working/discussions/1"&gt;https://huggingface.co/spaces/Olivier-Truong/XTTS_V2_CPU_working/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tested it and it works great using a provided web UI. Extremely easy to use. Works only on CPU.&lt;/p&gt; &lt;p&gt;Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sidran"&gt; /u/Sidran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bvvy/easy_xtts_v2_voice_cloning_on_cpu_link/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bvvy/easy_xtts_v2_voice_cloning_on_cpu_link/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bvvy/easy_xtts_v2_voice_cloning_on_cpu_link/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04qry</id>
    <title>OpenWebTTS: Open-Source Speechify/ElevenLabs Alternative looking for contributors</title>
    <updated>2025-08-25T22:27:25+00:00</updated>
    <author>
      <name>/u/Material_Abies2307</name>
      <uri>https://old.reddit.com/user/Material_Abies2307</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm working on a new open-source project called &lt;strong&gt;OpenWebTTS&lt;/strong&gt;, and I'm looking for contributors who might be interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is OpenWebTTS?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea is to make an open-source alternative to popular text-to-speech platforms like Speechify and ElevenLabs. The goal is to create a free and customizable TTS tool that facilitates reading articles, texts and books using local models or API-friendly TTS, while making sure the UX is up to standard to modern TTS platforms. Right now, the codebase is relatively simple but already &lt;strong&gt;100% usable&lt;/strong&gt; with support for Piper and Kokoro as well as PDF and Epub parsing. We are using &lt;strong&gt;Python&lt;/strong&gt; for the backend and &lt;strong&gt;JavaScript&lt;/strong&gt; for the frontend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How can you contribute?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Any help is welcome:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt; (Python, JavaScript).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI/UX&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ideas and feedback&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're interested in contributing, please &lt;a href="https://github.com/Gyyyn/OpenWebTTS"&gt;check out the project&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Abies2307"&gt; /u/Material_Abies2307 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04qry/openwebtts_opensource_speechifyelevenlabs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06z1t</id>
    <title>Running LLMs &amp; Multimodal models on Qualcomm Snapdragon NPU</title>
    <updated>2025-08-26T00:03:21+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been working on &lt;strong&gt;nexaSDK&lt;/strong&gt; — a lightweight runtime that runs &lt;strong&gt;latest LLMs and multimodal models&lt;/strong&gt; directly on &lt;strong&gt;Qualcomm Snapdragon NPUs&lt;/strong&gt;. It supports pure NPU inference: faster, leaner, battery-friendly. The developer experience feels like &lt;strong&gt;Ollama on NPUs&lt;/strong&gt;, but with full multimodal support (text, image, audio) and extra performance optimizations.&lt;/p&gt; &lt;h1&gt;Key results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; &amp;gt;95% NPU usage, ~25% faster than Qualcomm GENIE (23 t/s on OmniNeural-4B vs. 18 t/s on Llama-3.2-3B by GENIE).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal &amp;amp; multiround:&lt;/strong&gt; conversational multi-image + multi-audio supported natively.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; NexaQuant cuts perplexity by ~10% vs. baseline.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Support latest SOTA models:&lt;/strong&gt; OmniNeural, Qwen3, YOLOv12, PaddleOCR v4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developer UX:&lt;/strong&gt; Ollama-style install, 1 line to run → &lt;code&gt;nexa infer omni-neural&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 2× longer context windows, JSON structured decoding for agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This makes &lt;strong&gt;local copilots, private mobile assistants, in-car copilots, and edge OCR/speech agents&lt;/strong&gt; actually practical on CPU/GPU limited or battery limited devices.&lt;/p&gt; &lt;h1&gt;Demos:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Multimodal (OmniNeural-4B) → &lt;a href="https://huggingface.co/NexaAI/OmniNeural-4B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;We also helped Qwen team to bring their latest models to NPU:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Qwen3-2507 on NPU → &lt;a href="https://x.com/nexa_ai/status/1959302777353736593"&gt;demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3 on cars + IoT → &lt;a href="https://x.com/nexa_ai/status/1958797993676783792"&gt;demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub → &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;nexaSDK&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Supported models → &lt;a href="https://sdk.nexa.ai/model"&gt;Model Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’d love feedback, critiques, and ideas. Curious to hear from this community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which other models should we prioritize for NPU support?&lt;/li&gt; &lt;li&gt;Are there workloads that still make more sense on GPU/CPU?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06z1t/running_llms_multimodal_models_on_qualcomm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06iyh</id>
    <title>llama.cpp Lazy Swap</title>
    <updated>2025-08-25T23:43:53+00:00</updated>
    <author>
      <name>/u/unrulywind</name>
      <uri>https://old.reddit.com/user/unrulywind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because I'm totally lazy and I hate typing. I usually us a wrapper to run local models. But, recently I had to set up llama.cpp directly and, of course, being the lazy person I am, I created a bunch of command strings that I saved in a text file that I could copy into the terminal for each model.&lt;/p&gt; &lt;p&gt;Then I thought.... why am I doing this when I could make an old fashioned script menu. At that moment I realized, I never saw anyone post one. Maybe it's just too simple so everyone just made one eventually. Well, I thought, if I'm gonna write it, I might as well post it. So, here it is. All written up a a script creation script. part mine, but prettied up compliments of some help from gpt-oss-120b. The models used as examples are my setups for a 5090.&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;📦 Full checklist – copy‑paste this to get a working launcher&lt;/h2&gt; &lt;p&gt;This is a one time set up and creates a command: l-server 1. Copy entire script to clipboard 2. Open terminal inside WSL2 3. Right click to paste, or ctrl-v 4. Hit enter 5. Choose server 6. done 7. ctrl-c to stop server 8. It recycles to the menu, hit return to pull up the list again 9. To edit models edit the file in a Linux file editor or vscode ```&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;1️⃣ Make sure a place for personal scripts exists and is in $PATH&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;mkdir -p ~/bin&lt;/p&gt; &lt;h1&gt;If ~/bin is not yet in PATH, add it:&lt;/h1&gt; &lt;p&gt;if [[ &amp;quot;:$PATH:&amp;quot; != &lt;em&gt;&amp;quot;:$HOME/bin:&amp;quot;&lt;/em&gt; ]]; then echo 'export PATH=&amp;quot;$HOME/bin:$PATH&amp;quot;' &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc fi&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;2️⃣ Write the script (the &amp;lt;&amp;lt;'EOF' … EOF trick writes the exact text)&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;cat &amp;gt; ~/bin/l-server &amp;lt;&amp;lt;'EOF'&lt;/p&gt; &lt;h1&gt;!/usr/bin/env bash&lt;/h1&gt; &lt;h1&gt;------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;l-server – launcher for llama-server configurations&lt;/h1&gt; &lt;h1&gt;------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;cd ~/llama.cpp || { echo &amp;quot;❌ Could not cd to ~/llama.cpp&amp;quot;; exit 1; }&lt;/p&gt; &lt;p&gt;options=( &amp;quot;GPT‑OSS‑MXFP4‑20b server&amp;quot; &amp;quot;GPT‑OSS‑MXFPp4‑120b with moe offload&amp;quot; &amp;quot;GLM‑4.5‑Air_IQ4_XS&amp;quot; &amp;quot;Gemma‑3‑27b&amp;quot; &amp;quot;Quit&amp;quot; )&lt;/p&gt; &lt;p&gt;commands=( &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gpt-oss-20b-MXFP4.gguf \ -c 131072 \ -ub 2048 -b 4096 \ -ngl 99 -fa \ --jinja&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gpt-oss-120b-MXFP4-00001-of-00002.gguf \ -c 65536 \ -ub 2048 -b 2048 \ -ngl 99 -fa \ --jinja \ --n-cpu-moe 24&amp;quot; &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf \ -c 65536 \ -ub 2048 -b 2048 \ -ctk q8_0 -ctv q8_0 \ -ngl 99 -fa \ --jinja \ --n-cpu-moe 33&amp;quot; &amp;quot;./build-cuda/bin/llama-server \ -m ~/models/gemma-3-27B-it-QAT-Q4_0.gguf \ -c 65536 \ -ub 2048 -b 4096 \ -ctk q8_0 -ctv q8_0 \ -ngl 99 -fa \ --mmproj ~/models/mmproj-model-f16.gguf \ --no-mmproj-offload&amp;quot; &amp;quot;&amp;quot; # placeholder for Quit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;)&lt;/p&gt; &lt;p&gt;PS3=$'\nSelect a server (1‑'${#options[@]}'): ' select choice in &amp;quot;${options[@]}&amp;quot;; do [[ -z $choice ]] &amp;amp;&amp;amp; { echo &amp;quot;❌ Invalid selection – try again.&amp;quot;; continue; } idx=$(( REPLY - 1 )) [[ &amp;quot;$choice&amp;quot; == &amp;quot;Quit&amp;quot; || $REPLY -eq 0 ]] &amp;amp;&amp;amp; { echo &amp;quot;👋 Bye.&amp;quot;; break; }&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmd=&amp;quot;${commands[$idx]}&amp;quot; echo -e &amp;quot;\n🚀 Starting \&amp;quot;$choice\&amp;quot; …&amp;quot; echo &amp;quot; $cmd&amp;quot; echo &amp;quot;-----------------------------------------------------&amp;quot; eval &amp;quot;$cmd&amp;quot; echo -e &amp;quot;\n--- finished ---\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;done EOF&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;3️⃣ Make it executable&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;chmod +x ~/bin/l-server&lt;/p&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;h1&gt;4️⃣ Test it&lt;/h1&gt; &lt;h1&gt;-----------------------------------------------------------------&lt;/h1&gt; &lt;p&gt;l-server # should bring up the menu ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unrulywind"&gt; /u/unrulywind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06iyh/llamacpp_lazy_swap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dwjy</id>
    <title>Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025</title>
    <updated>2025-08-26T05:54:55+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"&gt; &lt;img alt="Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025" src="https://external-preview.redd.it/-JwExha6KVfoLydMnfzEw3abME2jLPqYjYEHLkxDPAk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9b2da8cd3912ed21dd7375c45a8e7c5c8636a0" title="Intel Xeon Clearwater Forest with 288 Cores on Intel 18A at Hot Chips 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The highlights for running LLMs are * 1300GB/s measured (real world) memory bandwidth in a dual socket configuration! * 576GB/s measured bandwidth between two sockets. * 96 PCIe Gen 5 lanes per socket, 32 of which support CXL, for an additional 128GB/s memory bandwidth per socket over CXL.&lt;/p&gt; &lt;p&gt;In a few years, when the industry moves to DDR6, these will hopefully become cheap enough to run big LLMs without or with only one GPU for prompt processing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.servethehome.com/intel-xeon-clearwater-forest-with-288-cores-on-intel-18a-at-hot-chips-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dwjy/intel_xeon_clearwater_forest_with_288_cores_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvk44</id>
    <title>Codebase to Knowledge Graph generator</title>
    <updated>2025-08-25T16:39:59+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/aXlnMWRvdXExN2xmMW6IHesd2IpIEgbCcYmw7k3fEr5nk2vPdZm2_jU5G_lC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf22eb683182f2e28b2652a8c7fac245c1add93" title="Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-based chatbot. It runs entirely client-side in the browser, making it privacy-focused. I’m using &lt;strong&gt;tree-sitter.wasm&lt;/strong&gt; to parse code inside the browser and logic to use the generated AST to map out all relations. Now trying to optimize it through parallel processing with Web Workers, worker pool. For the in-memory graph database, I’m using &lt;strong&gt;KuzuDB&lt;/strong&gt;, which also runs through WebAssembly (&lt;strong&gt;kuzu.wasm&lt;/strong&gt;). Graph RAG chatbot uses langchains ReAct agent, generating cypher queries to get information.&lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Need advice from anyone who has experience in graph rag agents, will this be better than rag based grep features which is popular in all AI IDEs.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gix425uq17lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvk44/codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dnp1</id>
    <title>A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed</title>
    <updated>2025-08-26T05:39:21+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"&gt; &lt;img alt="A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed" src="https://preview.redd.it/ut7b266wwalf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98224b002731e25972ece7ed84bf2f67e5ff05e1" title="A practical RAG Problem Map for LocalLLaMA. short checklists, real fixes, MIT licensed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i’m PSBigBig. i’ve been publishing a plain-text, MIT repo that hit 600 stars in 60 days. the whole thing started as a rescue kit for teams who got stuck in “it should work, but it doesn’t” RAG projects. today i’m sharing the part people asked for most often — a Problem Map that turns fuzzy symptoms into numbered, auditable fixes.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;what it is&lt;/h2&gt; &lt;p&gt;a compact set of failure modes, each with a short checklist and a guardrail you can copy into your pipeline. no infra changes, no sdk lock-in. it behaves like a semantic firewall sitting beside your LLM flow. goal is simple. stop silent collapses before they poison your fine-tunes or your vector store.&lt;/p&gt; &lt;p&gt;example entries people keep hitting&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;No.1 bootstrap ordering / empty ingestion looks fine in logs, then queries return air. root cause is ingestion windows happen out of order or pre-deploy triggers fire too early.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.5 semantic + embedding leakage adding special tokens seems to help then drift returns. mismatch across semantic layers, cosine won’t save you.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.8 vectorstore contamination mixed namespaces, re-index on write, faiss rebuild timing, stale shards. accuracy swings for no “obvious” reason.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.12 chunking illusions you think it’s about chunk size. it’s usually mixed layout signals, lost anchors, or table regions pretending to be prose.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No.16 agent loop collapse tool calls work, yet the agent never reaches goal state. usually caused by unguarded retries or an eval mirage.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;“you think the problem is …” vs “the real problem is …”&lt;/h2&gt; &lt;p&gt;you think&lt;/p&gt; &lt;ul&gt; &lt;li&gt;our model is weak, so add more tokens, switch vector DBs, or “upgrade embeddings”.&lt;/li&gt; &lt;li&gt;chunk sizes are wrong.&lt;/li&gt; &lt;li&gt;prompt is not strong enough.&lt;/li&gt; &lt;li&gt;we need LoRA or RAG-as-a-service.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the real thing&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you have a bootstrap timing fault. ingestion completed in the wrong order so the model “never saw” the data.&lt;/li&gt; &lt;li&gt;semantic drift happened across two layers, so cosine looks healthy while meaning has moved.&lt;/li&gt; &lt;li&gt;layout anchors were lost when converting PDF to markdown, tables became fake paragraphs.&lt;/li&gt; &lt;li&gt;a silent vector contamination mixed namespaces after a reindex, so you are retrieving ghosts from last week.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;each entry in the map is designed to be checked in minutes, not days. if you pass the checklist, you move on. if you fail, you apply the guardrail and re-run the same test.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;a small story, very real&lt;/h2&gt; &lt;p&gt;a team asked me why their “finance memos” bot kept citing the wrong quarter. they had already&lt;/p&gt; &lt;ul&gt; &lt;li&gt;switched from chroma to qdrant&lt;/li&gt; &lt;li&gt;tried three embedding families&lt;/li&gt; &lt;li&gt;doubled context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;looked like a vector db choice problem. it wasn’t. timeline showed uploads completing while a background re-index still held stale shards. the retriever was sober, the store was not. this matches No.8 in the map. fix was two lines of guardrail around ingestion gates and a post-commit verify. accuracy jumped, they never changed model or db again.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;why this seems to work&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;short checklists, not recipes. you can audit decisions later.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;all text-only, so you can paste it right into your LocalLLaMA notes or your agent’s system prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;model-agnostic. works with llama, claude, gpt, mistral, grok.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;free and MIT. keep it, fork it, print it, throw it at your teammates.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;coming next&lt;/h2&gt; &lt;p&gt;🛠 coming next: the Semantic Surgery Room and the Global Fix Map&lt;/p&gt; &lt;p&gt;this expands beyond RAG into n8n, GHL, Make.com and related automation stacks. think of it as a global AI clinic. we will publish real guardrails for orchestration loops, webhooks, field drift, file watchers, queue poisoning. target date by Sep 1. if your team runs ops on these tools, now is a good time to follow along.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;i’m posting here because LocalLLaMA folks often push close to the metal and need fast, real fixes. hope the map makes your next debug day shorter.&lt;/p&gt; &lt;p&gt;link: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md"&gt;https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ut7b266wwalf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dnp1/a_practical_rag_problem_map_for_localllama_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzn0zm</id>
    <title>InternVL3_5 series is out!!</title>
    <updated>2025-08-25T10:40:58+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt; &lt;img alt="InternVL3_5 series is out!!" src="https://external-preview.redd.it/oVE1-EnaLKFKvov2KcAAd41NTqlkCry1b2bYAP90Upw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e47ab110109abf15025f25857e6f9890fe89966c" title="InternVL3_5 series is out!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/organizations/internlm/activity/all"&gt;internlm (InternLM)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f"&gt;https://preview.redd.it/resy0a6n95lf1.png?width=1294&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9db47fa8a145b99f6bd74750e0d3a0791d85137f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzn0zm/internvl3_5_series_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T10:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n06ahz</id>
    <title>InternVL3_5 GGUF here</title>
    <updated>2025-08-25T23:33:37+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt; &lt;img alt="InternVL3_5 GGUF here" src="https://b.thumbs.redditmedia.com/EdxxIyVz6m8Y6DmR8UexDPcp-CYBhmw8ygu2dfJ8LCY.jpg" title="InternVL3_5 GGUF here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tested the &lt;a href="https://huggingface.co/collections/QuantStack/internvl3-5-ggufs-68acef206837c4f661a9b0a5"&gt;InternVL3_5&lt;/a&gt; 1b fp16 GGUF, it works&lt;br /&gt; (that's means the model architect is supported now in llama.cpp, I tested on LM studio) &lt;/p&gt; &lt;p&gt;every models now, just fp16,&lt;br /&gt; I think the QuantStack team is quantizing to different quants,&lt;br /&gt; if you want a quick try, just like and watch this repo, you may get surprised in few hours&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n06ahz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n06ahz/internvl3_5_gguf_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzykeu</id>
    <title>DeepSeek 3.1 Update is Awesome!</title>
    <updated>2025-08-25T18:29:57+00:00</updated>
    <author>
      <name>/u/lovetootiesteele</name>
      <uri>https://old.reddit.com/user/lovetootiesteele</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who countless conversations interrupted due to length limits, the ability to re-visit those chats and pick-up where we left off has been a dream come true. Even though we would try to continue our projects in new chats, the foundation had been set in another. This update is awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lovetootiesteele"&gt; /u/lovetootiesteele &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzykeu/deepseek_31_update_is_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T18:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzquqi</id>
    <title>GRPO please stop punishing your correct token</title>
    <updated>2025-08-25T13:42:56+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt; &lt;img alt="GRPO please stop punishing your correct token" src="https://preview.redd.it/mdaobm9t56lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9172793aa7a56b0f2e4540faa0f91d3bddb43291" title="GRPO please stop punishing your correct token" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a training approach I’m calling &lt;strong&gt;GTPO (Group-relative Trajectory-based Policy Optimization)&lt;/strong&gt;.&lt;br /&gt; It started as a way to fix some quirks I ran into with GRPO, like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conflicting gradients&lt;/strong&gt;: tokens showing up in both “good” and “bad” completions getting pulled in opposite directions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy collapse&lt;/strong&gt;: models flattening out when some completions had strong negative updates.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I tried&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I added a small mechanism to &lt;em&gt;skip negative updates&lt;/em&gt; on “conflict tokens.”&lt;/li&gt; &lt;li&gt;Instead of using KL with a reference model, I tried filtering out high-entropy completions (trajectories that are basically too noisy).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I noticed&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Training was more stable and didn’t wreck formatting.&lt;/li&gt; &lt;li&gt;I didn’t need a reference model, which made runs lighter.&lt;/li&gt; &lt;li&gt;Even on Colab (using Unsloth) I could fine-tune without things blowing up.&lt;/li&gt; &lt;li&gt;On reasoning datasets like &lt;strong&gt;GSM8K, MATH, AIME 2024 (see Figure)&lt;/strong&gt; with LLaMA 8B and Qwen 3B, results were consistently better than my GRPO baselines.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links if you want to poke around&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab example: &lt;a href="https://colab.research.google.com/github/winstonsmith1897/GTPO/blob/main/colab/GTPO_training_example.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious what others think, especially folks who’ve been fine-tuning with GRPO or similar. Do you have any benchmarks or setups you’d like me to test it on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mdaobm9t56lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzquqi/grpo_please_stop_punishing_your_correct_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0c58h</id>
    <title>Update llama.cpp for a big speed boost with gpt-oss and cuda.</title>
    <updated>2025-08-26T04:11:45+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few cuda commits landed today that have made a big difference in performance. Testing with gpt-oss-120B, I saw a 14.5% increase in tokens per second with 2x3090 and 1xP40. It went from 51.6 tok/sec to 59.1 tok/sec. &lt;/p&gt; &lt;p&gt;With gptoss-20B I stayed at 130 tok/sec on a single 3090 power limited to 300W. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T04:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bbnx</id>
    <title>Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it.</title>
    <updated>2025-08-26T03:28:31+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt; &lt;img alt="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." src="https://external-preview.redd.it/IQUYk0UKH8LL_chH4rY6LMG-79G-lG-wrUjPogrDpXU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec62af39d7821e4b67b4c3bcde089ba116620d78" title="Anyone starting creative writing fine-tuning on Seed-OSS-36B-Base-woSyn? I've thought about it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to create a creative writing fine tune, but I've never done it and I don't know if to 3090's are up for the task for this model. So, I thought I'd start with inspiring those who have to take a look at this model if they missed it. While Seed-OSS was developed by ByteDance's Seed Team for reasoning and agent type stuff, it also has general capabilities and powerful long-context features. Also, this base doesn't have synthetic data in it. All sounds promising. What do you think? Does it have a shot at being a good base for creative models? Anyone attempting anything? Anyone up for helping me head down the fine tuning road? Is it even possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base-woSyn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bbnx/anyone_starting_creative_writing_finetuning_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzrb4l</id>
    <title>llama.ui - minimal privacy focused chat interface</title>
    <updated>2025-08-25T14:01:17+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt; &lt;img alt="llama.ui - minimal privacy focused chat interface" src="https://preview.redd.it/6g2icqwi96lf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93145b5e6ac2c5f127d14e540cb4261819454a6b" title="llama.ui - minimal privacy focused chat interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6g2icqwi96lf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzrb4l/llamaui_minimal_privacy_focused_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T14:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n02e7s</id>
    <title>How does huggingface make money?</title>
    <updated>2025-08-25T20:55:17+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I sure download from it a lot. What’s their way to bring profitably safe from shenanigans? Will it be stuff like GitHub? What’s the backup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n02e7s/how_does_huggingface_make_money/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T20:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzu2e6</id>
    <title>GLM-4.5 appreciation post</title>
    <updated>2025-08-25T15:45:21+00:00</updated>
    <author>
      <name>/u/wolttam</name>
      <uri>https://old.reddit.com/user/wolttam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.5 is my favorite model at the moment, full stop.&lt;/p&gt; &lt;p&gt;I don't work on insanely complex problems; I develop pretty basic web applications and back-end services. I don't vibe code. LLMs come in when I have a well-defined task, and I have generally always been able to get frontier models to one or two-shot the code I'm looking for with the context I manually craft for it.&lt;/p&gt; &lt;p&gt;I've kept (near religious) watch on open models, and it's only been since the recent Qwen updates, Kimi, and GLM-4.5 that I've really started to take them seriously. All of these models are fantastic, but GLM-4.5 especially has completely removed any desire I've had to reach for a proprietary frontier model for the tasks I work on.&lt;/p&gt; &lt;p&gt;Chinese models have effectively captured me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wolttam"&gt; /u/wolttam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzu2e6/glm45_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T15:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzvns5</id>
    <title>You can run GGUFs with Lemonade straight from Hugging Face now</title>
    <updated>2025-08-25T16:43:50+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt; &lt;img alt="You can run GGUFs with Lemonade straight from Hugging Face now" src="https://b.thumbs.redditmedia.com/dwJPSl-GCLGC8P_zJkDjJc59pTe5_mdagvacnnAFmhc.jpg" title="You can run GGUFs with Lemonade straight from Hugging Face now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huge shoutout to the Hugging Face team for this, along with all the other amazing libraries and services they provide for free to the community.&lt;/p&gt; &lt;p&gt;Quick way to run any GGUF model on your PC with Lemonade:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to any model page, like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Unsloth's Qwen3-Coder-30B-A3B&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &amp;quot;Use this model&amp;quot; in the top-right.&lt;/li&gt; &lt;li&gt;Clicking Lemonade will give you instructions like &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF?local-app=lemonade"&gt;this&lt;/a&gt; (second picture in the post).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in comments if anyone wants to tinker with us.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzvns5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzvns5/you_can_run_ggufs_with_lemonade_straight_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T16:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0am1b</id>
    <title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title>
    <updated>2025-08-26T02:52:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;https://arxiv.org/abs/2508.18265v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05× inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks—narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Models:&lt;/p&gt; &lt;p&gt;1B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-2B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-4B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-14B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;38B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-38B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-38B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;20BA4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30BA3B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;241BA28B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0aijh</id>
    <title>GPT OSS 120B</title>
    <updated>2025-08-26T02:47:05+00:00</updated>
    <author>
      <name>/u/vinigrae</name>
      <uri>https://old.reddit.com/user/vinigrae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the best function calling model I’ve used, don’t think twice, just use it. &lt;/p&gt; &lt;p&gt;We gave it a multi scenario difficulty 300 tool call test, where even 4o and GPT 5 mini performed poorly. &lt;/p&gt; &lt;p&gt;Ensure you format the system properly for it, you will find the model won’t even execute things that are actually done in a faulty manner and are detrimental to the pipeline.&lt;/p&gt; &lt;p&gt;I’m &lt;strong&gt;extremely&lt;/strong&gt; impressed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinigrae"&gt; /u/vinigrae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dl84</id>
    <title>Been working on something... A teaser</title>
    <updated>2025-08-26T05:35:04+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt; &lt;img alt="Been working on something... A teaser" src="https://b.thumbs.redditmedia.com/gJsXHozTMr8Q_MnsobeoZsA017tBJOzE3DqbO7Z1inw.jpg" title="Been working on something... A teaser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty excited about this project i have been working on lately, be back soon with more info, but in the meantime thought a teaser wouldn't hurt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n0dl84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers&amp;quot;&lt;/li&gt; &lt;li&gt;Based on Qwen2.5-1.5B&lt;/li&gt; &lt;li&gt;7B variant &amp;quot;coming soon&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bhd7</id>
    <title>Microsoft VibeVoice TTS : Open-Sourced, Supports 90 minutes speech, 4 distinct speakers at a time</title>
    <updated>2025-08-26T03:36:48+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. &lt;/p&gt; &lt;p&gt;Demo Video : &lt;a href="https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ"&gt;https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
