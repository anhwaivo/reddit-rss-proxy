<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-14T01:34:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1klwbto</id>
    <title>Debug Agent2Agent (A2A) without code - Open Source</title>
    <updated>2025-05-13T20:22:00+00:00</updated>
    <author>
      <name>/u/Educational_Bus5043</name>
      <uri>https://old.reddit.com/user/Educational_Bus5043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klwbto/debug_agent2agent_a2a_without_code_open_source/"&gt; &lt;img alt="Debug Agent2Agent (A2A) without code - Open Source" src="https://external-preview.redd.it/dzRlc2xsamZ5bDBmMdXCkBeBUlCvm-J8tHzw2-ke_pqFaQ1vH3DWk0h32Jjh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ee1096b3089d7178dfafa865bf6cd00a3d3d1dc" title="Debug Agent2Agent (A2A) without code - Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üî• Streamline your A2A development workflow in one minute!&lt;/p&gt; &lt;p&gt;Elkar is an open-source tool providing a dedicated UI for debugging agent2agent communications.&lt;/p&gt; &lt;p&gt;It helps developers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Simulate &amp;amp; test tasks:&lt;/strong&gt; Easily send and configure A2A tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inspect payloads:&lt;/strong&gt; View messages and artifacts exchanged between agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accelerate troubleshooting:&lt;/strong&gt; Get clear visibility to quickly identify and fix issues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Simplify building robust multi-agent systems. Check out Elkar!&lt;/p&gt; &lt;p&gt;Would love your feedback or feature suggestions if you‚Äôre working on A2A!&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/elkar-ai/elkar"&gt;https://github.com/elkar-ai/elkar&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sign up to &lt;a href="https://app.elkar.co/"&gt;https://app.elkar.co/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;#opensource #agent2agent #A2A #MCP #developer #multiagentsystems #agenticAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Bus5043"&gt; /u/Educational_Bus5043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6kt1esjfyl0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klwbto/debug_agent2agent_a2a_without_code_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klwbto/debug_agent2agent_a2a_without_code_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T20:22:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1km2jyz</id>
    <title>Gemini 2.5 exp death.</title>
    <updated>2025-05-14T00:57:56+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that 2.5 exp free it's dead, what alternatives are you guys using for coding ?üòû (Free alternatives) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km2jyz/gemini_25_exp_death/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T00:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1klur6q</id>
    <title>localAI ‚Äì Run LLMs Completely Locally on Your iPhone, iPad &amp; Mac</title>
    <updated>2025-05-13T19:19:06+00:00</updated>
    <author>
      <name>/u/CrazySymphonie</name>
      <uri>https://old.reddit.com/user/CrazySymphonie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm excited to share about my first app &lt;strong&gt;localAI&lt;/strong&gt;, a SwiftUI-based open-source app that lets you run large language models entirely on your device‚Äîno internet required.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Offline&lt;/strong&gt;: All inference happens locally‚Äîyour data never leaves your device.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Platform&lt;/strong&gt;: Universal app for iOS (16.0+) and macOS (13.0+).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-Bundled Models&lt;/strong&gt;: Llama 3.2 3B Instruct, Qwen3 4B, plus support for any GGUF model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Model Loading&lt;/strong&gt;: Import your own GGUF models with ease.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Tuning&lt;/strong&gt;: Adjust temperature, top-K, top-P, context window, and more in real time.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Monitoring&lt;/strong&gt;: Watch token generation speed, memory usage, and context utilization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debug Mode&lt;/strong&gt;: Detailed logs to help you troubleshoot and optimize.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Get Started&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the repo: &lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/sse-97/localAI.git"&gt;&lt;code&gt;https://github.com/sse-97/localAI.git&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Build in Xcode (iOS/macOS target)&lt;/li&gt; &lt;li&gt;Launch and start chatting‚Äîyour data stays 100% local!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Call for Feedback &amp;amp; Contributions&lt;br /&gt; I‚Äôd love to hear your thoughts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What features would you like to see?&lt;/li&gt; &lt;li&gt;Any performance tweaks or UI improvements?&lt;/li&gt; &lt;li&gt;Got a favorite GGUF model to test?&lt;/li&gt; &lt;li&gt;Can you contribute to make this app even better?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out on GitHub and drop a ‚≠ê if you find it useful! Let‚Äôs make on-device AI even better together. üöÄ&lt;/p&gt; &lt;p&gt;GitHub: [&lt;a href="https://github.com/sse-97/localAI%5D(vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)"&gt;https://github.com/sse-97/localAI](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;br /&gt; Happy hacking!&lt;br /&gt; (sse-97)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrazySymphonie"&gt; /u/CrazySymphonie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klur6q/localai_run_llms_completely_locally_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klur6q/localai_run_llms_completely_locally_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klur6q/localai_run_llms_completely_locally_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkh4w</id>
    <title>The Hidden Algorithms Powering Your Coding Assistant - How Cursor and Windsurf Work Under the Hood</title>
    <updated>2025-05-13T12:19:30+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just published a deep dive into the algorithms powering AI coding assistants like Cursor and Windsurf. If you've ever wondered how these tools seem to magically understand your code, this one's for you.&lt;/p&gt; &lt;p&gt;In this (free) post, you'll discover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The hidden context system&lt;/strong&gt; that lets AI understand your entire codebase, not just the file you're working on&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The ReAct loop&lt;/strong&gt; that powers decision-making (hint: it's a lot like how humans approach problem-solving)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why multiple specialized models&lt;/strong&gt; work better than one giant model and how they're orchestrated behind the scenes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How real-time adaptation&lt;/strong&gt; happens when you edit code, run tests, or hit errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/diamantai/p/the-hidden-algorithms-powering-your?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;Read the full post here ‚Üí&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkh4w/the_hidden_algorithms_powering_your_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kldquv</id>
    <title>Architecture Review of the new MoE models</title>
    <updated>2025-05-13T05:03:33+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the release of DeepSeek V3, there is a rush of new MoE models. I read their papers and looked at config.json and modeling_*.py files and summarized their data in the following table. Here are some observations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek becomes highly KV cache efficient after introduction of MLA in DeepSeek V2&lt;/li&gt; &lt;li&gt;Qwen's MoE architecture is basically the same as Mixtral but with more experts and more layers.&lt;/li&gt; &lt;li&gt;Llama-4 and DeepSeek are both MoE with shared experts. While Scout has no non-MoE (ie dense) layers, all other models have some dense layers. Maverick even has interleaved&lt;/li&gt; &lt;li&gt;Performance-wise, it seems like Qwen3-235B-A22B &amp;gt; DeepSeek-V3 &amp;gt;&amp;gt; Llama-4-Maverick accordin g to lmarena and livebench. Qwen3 seems to excel in all areas except coding compare to DSV3.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;dense layer#&lt;/th&gt; &lt;th align="left"&gt;MoE layer#&lt;/th&gt; &lt;th align="left"&gt;shared&lt;/th&gt; &lt;th align="left"&gt;active/routed&lt;/th&gt; &lt;th align="left"&gt;Active&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Active%&lt;/th&gt; &lt;th align="left"&gt;fp16 kv@128k&lt;/th&gt; &lt;th align="left"&gt;kv%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-MoE-16B&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.83B&lt;/td&gt; &lt;td align="left"&gt;16.38B&lt;/td&gt; &lt;td align="left"&gt;17.28%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;85.47%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2-Lite&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;2.66B&lt;/td&gt; &lt;td align="left"&gt;15.71B&lt;/td&gt; &lt;td align="left"&gt;16.93%&lt;/td&gt; &lt;td align="left"&gt;3.8GB&lt;/td&gt; &lt;td align="left"&gt;12.09%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/160&lt;/td&gt; &lt;td align="left"&gt;21.33B&lt;/td&gt; &lt;td align="left"&gt;235.74B&lt;/td&gt; &lt;td align="left"&gt;8.41%&lt;/td&gt; &lt;td align="left"&gt;8.44GB&lt;/td&gt; &lt;td align="left"&gt;1.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/256&lt;/td&gt; &lt;td align="left"&gt;37.45B&lt;/td&gt; &lt;td align="left"&gt;671.03B&lt;/td&gt; &lt;td align="left"&gt;5.58%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;3.34B&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;10.94%&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;19.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;94&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;22.14B&lt;/td&gt; &lt;td align="left"&gt;235.09B&lt;/td&gt; &lt;td align="left"&gt;9.42%&lt;/td&gt; &lt;td align="left"&gt;23.5GB&lt;/td&gt; &lt;td align="left"&gt;4.998%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout-17B-16E&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/16&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;107.77B&lt;/td&gt; &lt;td align="left"&gt;15.93%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;11.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/128&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;400.71B&lt;/td&gt; &lt;td align="left"&gt;4.28%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;2.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x7B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;12.88B&lt;/td&gt; &lt;td align="left"&gt;46.70B&lt;/td&gt; &lt;td align="left"&gt;27.58%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;25.696%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;39.15B&lt;/td&gt; &lt;td align="left"&gt;140.62B&lt;/td&gt; &lt;td align="left"&gt;27.84%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;9.956%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kldquv/architecture_review_of_the_new_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T05:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1km2ss6</id>
    <title>Has anyone created a fine tune or LORA for AutoHotkey V1 code?</title>
    <updated>2025-05-14T01:10:09+00:00</updated>
    <author>
      <name>/u/UsingThis4Questions</name>
      <uri>https://old.reddit.com/user/UsingThis4Questions</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models I've tried so far suck bad at generating valid AutoHotkey code.&lt;/p&gt; &lt;p&gt;Has anyone found/made a model or lora that actually works?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UsingThis4Questions"&gt; /u/UsingThis4Questions &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2ss6/has_anyone_created_a_fine_tune_or_lora_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1km2ss6/has_anyone_created_a_fine_tune_or_lora_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1km2ss6/has_anyone_created_a_fine_tune_or_lora_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T01:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1klhvvx</id>
    <title>On the Hugging Face Hub, you can now add Collections within Collections</title>
    <updated>2025-05-13T09:47:28+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"&gt; &lt;img alt="On the Hugging Face Hub, you can now add Collections within Collections" src="https://preview.redd.it/psitfubvri0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=927e4c70666a8537d067a9396b391eba03765991" title="On the Hugging Face Hub, you can now add Collections within Collections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Bertrand Chevrier on X: &lt;a href="https://x.com/kramp/status/1922221760193187939"&gt;https://x.com/kramp/status/1922221760193187939&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/psitfubvri0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klhvvx/on_the_hugging_face_hub_you_can_now_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T09:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkrw0</id>
    <title>final version of Skywork-OR1 (Open Reasoner 1) series of models</title>
    <updated>2025-05-13T12:34:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-OR1-32B"&gt;https://huggingface.co/Skywork/Skywork-OR1-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-OR1-7B"&gt;https://huggingface.co/Skywork/Skywork-OR1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Skywork-OR1-32B-GGUF"&gt;https://huggingface.co/mradermacher/Skywork-OR1-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Skywork-OR1-7B-GGUF"&gt;https://huggingface.co/mradermacher/Skywork-OR1-7B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkrw0/final_version_of_skyworkor1_open_reasoner_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1klxl20</id>
    <title>Two RTX 6000 Pro Blackwell..what's it get you?</title>
    <updated>2025-05-13T21:11:56+00:00</updated>
    <author>
      <name>/u/SteveRD1</name>
      <uri>https://old.reddit.com/user/SteveRD1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What would you all do if you had 192Gb VRAM available to you on Blackwell hardware.&lt;/p&gt; &lt;p&gt;Is there anything it would open up that the 3090 stackers can't currently do?&lt;/p&gt; &lt;p&gt;What could it still not do?&lt;/p&gt; &lt;p&gt;Not thinking just LLM, but image/video stuff, anything else at all AI adjacent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SteveRD1"&gt; /u/SteveRD1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxl20/two_rtx_6000_pro_blackwellwhats_it_get_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxl20/two_rtx_6000_pro_blackwellwhats_it_get_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klxl20/two_rtx_6000_pro_blackwellwhats_it_get_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T21:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kli1hf</id>
    <title>AMD Ryzen AI Max+ PRO 395 Linux Benchmarks</title>
    <updated>2025-05-13T09:58:25+00:00</updated>
    <author>
      <name>/u/Kirys79</name>
      <uri>https://old.reddit.com/user/Kirys79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be wrong but it seems to be slower than a 4060ti from an LLM point of view... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kirys79"&gt; /u/Kirys79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-ryzen-ai-max-pro-395/7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kli1hf/amd_ryzen_ai_max_pro_395_linux_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kli1hf/amd_ryzen_ai_max_pro_395_linux_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T09:58:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrdgt</id>
    <title>The Titan 18U AI Homelab Build Log and Lessons Learned</title>
    <updated>2025-05-13T17:07:18+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrdgt/the_titan_18u_ai_homelab_build_log_and_lessons/"&gt; &lt;img alt="The Titan 18U AI Homelab Build Log and Lessons Learned" src="https://external-preview.redd.it/JvIS1Gku4UQsepo0kSIVq7g-1DsMy9F0IzSaUqaTW44.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=574f070d0e1d67db677cfd5141028c8d26bd6f75" title="The Titan 18U AI Homelab Build Log and Lessons Learned" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good afternoon friends!&lt;/p&gt; &lt;p&gt;Adam Savage once famously said &amp;quot;The only difference between screwing around and Science is writing it down&amp;quot; and I've been rather busy screwing in the lab so figure its about time to write some things down.&lt;/p&gt; &lt;p&gt;Meet &lt;strong&gt;The Titan,&lt;/strong&gt; my 18U AI Homelab.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c5mfkrqipk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a1e3381de4e5148cb2a6242a6d7e1995ba7848ac"&gt;The Titan: 18U AI Homelab (with llama for scale)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my 4th multi-GPU build and I've come a long way from IKEA tables and mining frames. There's a couple of unique features that are worth discussing here, but lets start at the beginning and go through the build log.&lt;/p&gt; &lt;h1&gt;The Rack&lt;/h1&gt; &lt;p&gt;I've wanted to do a rackmount build for some time, they have all the benefits of open frames but also support building vertically much easier and offer a common form factor to mount supporting equipment.&lt;/p&gt; &lt;p&gt;I came upon the &lt;a href="https://www.amazon.ca/Frame-Server-Network-Casters-Sysracks/dp/B079M19BXD"&gt;SysRacks 18U&lt;/a&gt; and it was love at first sight: perfect height, four post, adjustable depths and cheap!&lt;/p&gt; &lt;p&gt;I added two sets of &lt;a href="https://www.amazon.ca/JINGCHENGMEI-Universal-4-Post-Server-Compaq/dp/B0BRN254KP"&gt;Universal Rack Rails&lt;/a&gt; and a &lt;a href="https://www.amazon.ca/Server-Rack-Shelf-Universal-PLRSTN42U/dp/B01HTG508E"&gt;2U Shelf&lt;/a&gt; and that's basically it, the overall frame assembly was easy and fun.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rjy4ao3mqk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b4e3434302a0b6bee102798ef4bfec6529d5fe52"&gt;Bare-bones frame with racks installed and some test pieces mounted.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Motherboard, CPU and Memory&lt;/h1&gt; &lt;p&gt;Being an AI inference machine the goals were to balance high RAM bandwidth with enough compute to be able to take advantage of that bandwidth and to offer as much GPU connectivity as possible.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;ASRock Rack ROMED8-2T&lt;/strong&gt; is a popular choice around here for good reason - this motherboard checks all the boxes, and offers out of the box first party ReBAR support. The big selling feature here 7 full x16 PCIe slots with all the bifurcation options and a high quality BIOS: 13 GPUs work with stock, and with a beta BIOS you can push it to 16 GPUs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g64b84x5rk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ac42440e1efc00132c3b4502edec45d1eb80d625"&gt;ROMED8-2T mounted on a 2020 frame waiting to be populated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It was here I ran into the first hitch: this motherboard is HUGE. And by that I specifically mean that's really, really deep. The kit I originally bought did not have long enough rails to mount this beast so I had to replace them with longer parts.&lt;/p&gt; &lt;p&gt;Install the RAM carefully, starting from the insides and seating each module firmly until you hear the click. 8x 32GB PC3200 modules have a theoretical maximum bandwidth of 208GB/sec, I measure 143 GB/sec in practice.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/frc59pqzrk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2f887d98b3d977f47e464e104b606595b06dc1d0"&gt;SP3 socket, maw of the beast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I selected the EPYC 7532 for CPU, it was really cheap and offers incredible value as far as compute and memory bandwidth go. There is a plastic cover on these CPUs that STAYS IN PLACE, you slide the entire thing into the black frame on top of the socket. So many pins. So, so many. Tightening the CPU is made much easier if you have a specialized tool, you can see the weird torx wrench with an orange handle in the first pic above. Follow the instructions on the socket and you'll be fine. The 2U cooler I selected also had some torque requirements but the screws basically stop spinning at the right torque so you don't need to worry about a torque driver (a fact I wish I knew before I bought a torque driver, but sharing experiences is why we're here right?).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mn459nw9sk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=680a81f222ae2dcb17b102a307ea237f189d00de"&gt;Finished Host Frame with PSU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0b7nvdd1uk0f1.jpg?width=4079&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa22346994a75718702217b630993d7290a9f7ee"&gt;Host installed into rack.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I used 4.66U for this level to both give a little extra space for the PSU and to properly align with the 15cm PCIe risers we're going to use to physically connect the bottom layer of GPUs.&lt;/p&gt; &lt;h1&gt;GPUs: Mounting and Power&lt;/h1&gt; &lt;p&gt;I have a total of 10 GPUs acquired over the past 2 years:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5 x Tesla P40&lt;/li&gt; &lt;li&gt;1 x Tesla P102-100&lt;/li&gt; &lt;li&gt;2 x RTX 3090 FE&lt;/li&gt; &lt;li&gt;2 x RTX 3060&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The P102-100 is a backup card that goes into the storage host at the bottom of the rack, so we will focus our discussion here on how to mount the rest of the GPUs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/omrhh0dqsk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bbc76e87fa79713339fc4c7f140bc3800c1eb7d5"&gt;Original V1 prototype of the GPU frame&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Back when I built my very first rig, I cobbled together this mostly-wood GPU frame. For this rack build I wanted to 1) simplify, 2) incorporate power and 3) upgrade to all-metal. I am happy to have achieved all of these goals with my V2 frame design:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6rc6krn0tk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=711a694d30f29362bd42c4d1b61108e7af9bd038"&gt;V2 GPU frame, rear view with 4 GPUs and PSU populated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/koa7j3t4tk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=600eb843e68d4d4d008e36cc5acccddca3e6b062"&gt;All the parts to make 2 GPU frames&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The GPU frames are assembled out of the same 2020 aluminum rails as the host frame, but this one is fully custom designed. V1 had two steel support bars running under the GPUs, I've downgraded to just the one to support the rear of the cards while the L-bar at the front takes care of the rest.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f7stwd8etk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d431d9caac70497fa6dd02c6e0306cbc8266d73f"&gt;V2 Frame with just PSU installed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The frames feature handles to make it easier to get in and out of the rack, and a mounting mechanism for the CSPS power supplies I'm using.&lt;/p&gt; &lt;p&gt;These frames simply slide into the two rail-racks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b923axf9uk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8263ad7ae4df4f9c3dfe9a253187c60a8ffd62ce"&gt;Final rack ~8U assembly - the two GPU levels&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Height wise, I built one of these 3U (bottom) and the other 4U (top) but things are pretty flexible here.&lt;/p&gt; &lt;p&gt;For GPU power, I rely on Dell 1100W CRPS supplies. These supplies can actually deliver the full power rating without anything bad happening and feature all the protections required to not burn your house down if anything goes wrong.&lt;/p&gt; &lt;p&gt;The bottom shelf is 4x250 = &lt;strong&gt;1000W&lt;/strong&gt; and the top 2x350+2x170 = &lt;strong&gt;1040W&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The straggler 5th P40 is connected directly to the host machine on the bottom level.&lt;/p&gt; &lt;h1&gt;GPU: Connectivity&lt;/h1&gt; &lt;p&gt;The bottom Pascal rack is using a pair of x8x8 Bifurcators + 15cm PCIE4.0 90 degree extensions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n0agcqtkvk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=11da6f1755773932c211dbb64c190ca98958fad7"&gt;Rear view close-up from an older build showing the Pascal extension setup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The top Ampere rack is using a pair of SFF-8654 x8x8 bifurcators and 4x SFF-8654 x8 Host interfaces.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pd09ntcyuk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a440a9d8ef3ac3d6a7c3ba6cb64d462855b9da34"&gt;Rear view of the rack showing the bifurcators and extensions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The passive x8x8 boards have SATA connectors but you don't actually need to power them. The SFF-8654 boards you do have to power. I did not find I need to use use retimers, I have 0 pcie errors going on and things are pretty solid. The one thing to watch out for is that the &lt;strong&gt;RTX cards need to be downgraded to PCIE3.0, at PCIE4.0 the 2nd port on the SFF-8654 extensions throws PCIE errors at 4.0 speeds.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Cooling and Lights&lt;/h1&gt; &lt;p&gt;There are a total of &lt;strong&gt;5x 40mm Magnetic Levitation fans&lt;/strong&gt; on the Pascals and &lt;strong&gt;4x 120mm intake fans&lt;/strong&gt; on the Amperes and I wanted something attractive to be able to control them so I made it myself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pfiavo5wvk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=63048e8ff9c3c1c9f91f33f9de2452bba667f97b"&gt;Dual PWM controller 3D model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ue73uyvzvk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fbf630931050825eff3a0b3787f2546c28c7a195"&gt;Completed Dual PWM RackModSlide module&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use the wonderful &lt;a href="https://makerworld.com/en/models/1040867-rackmod-1u-slide-a-modular-server-rack-system"&gt;RackMod Slide&lt;/a&gt; as a base frame and form factor and use it to build a cheap and attractive current monitored dual-PWM controller that sits just above the host mothoboard on the right.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p6idtjaowk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d926d5c55ee8d3cf009e18184bf3334b1c79ef59"&gt;Dual PWM controller in action, green knob is the P40 red knob is the intakes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The ampere intake fans are located on top and are directly feeding the 'intake' fan on the bottom/left side of the 3090FE. I originally had them on the front but they ended up fighting the exhaust fans on the top/right side.&lt;/p&gt; &lt;p&gt;Lighting is provided by an 8-way wireless lighting controller:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6sjcclm0xk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c21bc1c46b6ae7487616cc7d3981b910f3e0b8c5"&gt;Close-up view of the lighting controller&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's 2 strips on the sides of the rack and the 4 intake fans on top are all RGB and daisy-chained into a single connector.&lt;/p&gt; &lt;h1&gt;It's Never Done&lt;/h1&gt; &lt;p&gt;In case its not obvious, I really enjoy doing builds like this and as a result they are never 'quite' finished - always something I want to improve...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6qctvvc9xk0f1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ecae6b7abf8d2d0b42755d3b2c23fb94663809b2"&gt;A CSPS quad XT60 breakout board and some XT60 to GPU cables&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why do we use those silly little molex connectors for power delivery? Do we really need hundreds of little 18AWG wires? I've found some vendors in china that make gear with quad XT60 connectors and fat wires, but the CRPS supplies I have are incompatible so I am waiting for some CSPS supplies to arrive before I can test this out.&lt;/p&gt; &lt;h1&gt;Closing Thoughts&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqy5m7m2zk0f1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38011a75ccd5feb689d12362357fee355907815d"&gt;The Titan front angled view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am incredibly happy with this system but it was honestly more work then I anticipated: this build took me 4 months from planning to completion, working evenings and weekends. It would probably have taken longer if I didn't have prior builds to start from and had to start totally from scratch.&lt;/p&gt; &lt;p&gt;I sit on the shoulders of giants, without information I learned on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; I would never have made it this far.&lt;/p&gt; &lt;p&gt;I could say a lot more about software stack I run on this machine but I'm afraid I've run out of characters so that will have to be a post for another day. Let me know if there's any questions or if you guys are interested in STL files and I'll upload them. I could also probably throw together some more details parts/instructions for the V2 GPU shelf.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrdgt/the_titan_18u_ai_homelab_build_log_and_lessons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrdgt/the_titan_18u_ai_homelab_build_log_and_lessons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrdgt/the_titan_18u_ai_homelab_build_log_and_lessons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kljute</id>
    <title>Geotracking in Gpus‚Ä¶</title>
    <updated>2025-05-13T11:47:51+00:00</updated>
    <author>
      <name>/u/Ashefromapex</name>
      <uri>https://old.reddit.com/user/Ashefromapex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.pcgamer.com/hardware/graphics-cards/us-senator-announces-a-bill-requiring-geotracking-in-high-end-gpus-to-prevent-the-chinese-government-from-wielding-the-ruinous-power-of-your-nvidia-rtx-4090/"&gt;https://www.pcgamer.com/hardware/graphics-cards/us-senator-announces-a-bill-requiring-geotracking-in-high-end-gpus-to-prevent-the-chinese-government-from-wielding-the-ruinous-power-of-your-nvidia-rtx-4090/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashefromapex"&gt; /u/Ashefromapex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kljute/geotracking_in_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T11:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kluh52</id>
    <title>A new promising chip ?</title>
    <updated>2025-05-13T19:07:49+00:00</updated>
    <author>
      <name>/u/ReadyCocconut</name>
      <uri>https://old.reddit.com/user/ReadyCocconut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://vsora.com/"&gt;https://vsora.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A french start up who make a risk v chip designed for inference that could be interesting. They recevied for their third rounds of investissement money from the European Comission, so maybe it's a bit serious. Some articles say they will use it for the software part.&lt;/p&gt; &lt;p&gt;Informations in french are not very sourced and a bit sparse, I saw 8T/s for bandwith and a scalable memory ? The maximum numbers of memory seems absurds so if someone more intelligent that me can confirm.&lt;/p&gt; &lt;p&gt;This kind of chip is just good for inference or it's can be use for training too ? With their huge ram (or nram?) available ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReadyCocconut"&gt; /u/ReadyCocconut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kluh52/a_new_promising_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1klqw5a</id>
    <title>More free VRAM for your LLMs on Windows</title>
    <updated>2025-05-13T16:48:48+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When you have a dedicated GPU, a recent CPU with an iGPU, and look at the performance tab of your task manager just to see that 2 GB of your precious dGPU VRAM is already in use, instead of just 0.6 GB, then this is for you.&lt;/p&gt; &lt;p&gt;Of course there's an easy solution: just plug your monitor into the iGPU. But that's not really good for gaming, and your 4k60fps YouTube videos might also start to stutter. The way out of this is to selectively move applications and parts of Windows to the iGPU, and leave everything that demands more performance, but doesn't run all the time, on the dGPU. The screen stays connected to the dGPU and just the iGPU output is mirrored to your screen via dGPU - which is rather cheap in terms of VRAM and processing time.&lt;/p&gt; &lt;p&gt;First, identify which applications and part of Windows occupy your dGPU memory:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open the task manager, switch to &amp;quot;details&amp;quot; tab.&lt;/li&gt; &lt;li&gt;Right-click the column headers, &amp;quot;select columns&amp;quot;.&lt;/li&gt; &lt;li&gt;Select &amp;quot;Dedicated GPU memory&amp;quot; and add it.&lt;/li&gt; &lt;li&gt;Click the new column to sort by that.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now you can move every application (including dwm - the Windows manager) that doesn't require a dGPU to the iGPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type &amp;quot;Graphics settings&amp;quot; in your start menu and open it.&lt;/li&gt; &lt;li&gt;Select &amp;quot;Desktop App&amp;quot; for normal programs and click &amp;quot;Browse&amp;quot;.&lt;/li&gt; &lt;li&gt;Navigate and select the executable. &lt;ul&gt; &lt;li&gt;This can be easier when right-clicking the process in the task manager details and selecting &amp;quot;open location&amp;quot;, then you can just copy and paste it to the &amp;quot;Browse&amp;quot; dialogue.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;It gets added to the list below the Browse button.&lt;/li&gt; &lt;li&gt;Select it and click &amp;quot;Options&amp;quot;.&lt;/li&gt; &lt;li&gt;Select your iGPU - usually labeled as &amp;quot;Energy saving mode&amp;quot;&lt;/li&gt; &lt;li&gt;For some applications like &amp;quot;WhatsApp&amp;quot; you'll need to select &amp;quot;Microsoft Store App&amp;quot; instead of &amp;quot;Desktop App&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's it. You'll need to restart Windows to get the new setting to apply to DWM and others. Don't forget to check the dedicated and shared iGPU memory in the task manager afterwards, it should now be rather full, while your dGPU has more free VRAM for your LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqw5a/more_free_vram_for_your_llms_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqw5a/more_free_vram_for_your_llms_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klqw5a/more_free_vram_for_your_llms_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T16:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1klsg3r</id>
    <title>How to make your MCP clients share context with each other</title>
    <updated>2025-05-13T17:49:12+00:00</updated>
    <author>
      <name>/u/anmolbaranwal</name>
      <uri>https://old.reddit.com/user/anmolbaranwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all this recent hype around MCP, I still feel like missing out when working with different MCP clients (especially in terms of context). &lt;/p&gt; &lt;p&gt;What if there could be a way to have a personal, portable LLM ‚Äúmemory layer‚Äù that lives locally on your system, with complete control over your data?&lt;/p&gt; &lt;p&gt;Mem0 (memory layer for AI agents) launched &lt;a href="https://github.com/mem0ai/mem0/tree/main/openmemory"&gt;OpenMemory&lt;/a&gt; (open source) solution to this problem, which plugs into any MCP client (like Cursor, Windsurf, Claude) over SSE and adds a private, vector-backed memory layer. &lt;/p&gt; &lt;p&gt;It acts as a middle layer between your LLM-powered client and a vector database:&lt;/p&gt; &lt;p&gt;- Stores and recalls arbitrary chunks of text (&lt;code&gt;memories&lt;/code&gt;) across sessions&lt;br /&gt; - Uses a vector store (&lt;code&gt;Qdrant&lt;/code&gt;) under the hood to perform relevance-based retrieval&lt;br /&gt; - Runs fully on your infrastructure (&lt;code&gt;Docker + Postgres + Qdrant&lt;/code&gt;) with no data sent outside&lt;br /&gt; - Includes a dashboard (&lt;code&gt;next.js &amp;amp; redux&lt;/code&gt;) showing who‚Äôs reading/writing memories and a history of state changes&lt;/p&gt; &lt;p&gt;Here‚Äôs a &lt;a href="https://mem0.ai/blog/how-to-make-your-clients-more-context-aware-with-openmemory-mcp/"&gt;complete tutorial&lt;/a&gt; that shows how to set it up locally, the underlying components involved, complete overview of architecture and some real-world use cases with examples. &lt;/p&gt; &lt;p&gt;It also explains the basic flow, why the project even matters, security, access control and what's actually happening behind the UI.&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anmolbaranwal"&gt; /u/anmolbaranwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klsg3r/how_to_make_your_mcp_clients_share_context_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klsg3r/how_to_make_your_mcp_clients_share_context_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klsg3r/how_to_make_your_mcp_clients_share_context_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1klro7w</id>
    <title>Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!</title>
    <updated>2025-05-13T17:19:06+00:00</updated>
    <author>
      <name>/u/xnick77x</name>
      <uri>https://old.reddit.com/user/xnick77x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"&gt; &lt;img alt="Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!" src="https://external-preview.redd.it/28dYK69dRtCoerxir1Uy4KNdFaXjdEzgRZMm02CEZ2Q.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4a15bfa2378088eba19c5ccfb541505d24054c9" title="Introducing BaldEagle: 3x Faster Inference; Easily Train Speculative Decoding Models Locally!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent quite some time hunting for small (&amp;lt;1B params) language models I could comfortably train at home on my RTX 3090 setup. Then I found speculative decoding through EAGLE models, which achieve a 3x inference speedup!&lt;/p&gt; &lt;p&gt;But the official EAGLE codebase was tough to navigate, so I created BaldEagle, an unofficial implementation that simplifies everything from data generation to training to benchmarking. It's now open-source, and I'm excited to see community-driven improvements and experiments. Feel free to ask any questions here or submit issues in the repo!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/NickL77/BaldEagle/"&gt;https://github.com/NickL77/BaldEagle/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xnick77x"&gt; /u/xnick77x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://frugalgpu.substack.com/p/introducing-baldeagle"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klro7w/introducing_baldeagle_3x_faster_inference_easily/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:19:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1klh6h4</id>
    <title>Intel Partner Prepares Dual Arc "Battlemage" B580 GPU with 48 GB of VRAM</title>
    <updated>2025-05-13T08:57:55+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt; &lt;img alt="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" src="https://external-preview.redd.it/jpmGpdPWJLe0CTi-snjYV4vMSX3vWCL1VBf0G3Bgwcg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=706cf667fea0a8034c97f841fc9b7a5c0d2e2f28" title="Intel Partner Prepares Dual Arc &amp;quot;Battlemage&amp;quot; B580 GPU with 48 GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/336687/intel-partner-prepares-dual-arc-battlemage-b580-gpu-with-48-gb-of-vram"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klh6h4/intel_partner_prepares_dual_arc_battlemage_b580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T08:57:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1klltt4</id>
    <title>The Qwen3 chat template is *still bugged*</title>
    <updated>2025-05-13T13:23:17+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I hope everyone remembers all the twists and turns with the Qwen3 template. First, it was not working at all, then, the Unsloth team fixed the little bug with iterating over the messages. But, alas, it's not over yet!&lt;/p&gt; &lt;p&gt;I had a hint something was wrong when the biggest Qwen3 model available on OpenRouter wouldn't execute a web search twice. But it was only once I started testing my own agent framework that I realized what was wrong.&lt;/p&gt; &lt;p&gt;Qwen3 uses an XML tool calling syntax that the Jinja template transforms into the known OpenAI-compatible structure. But there's a catch. Once you call a tool once, you save that tool call in the chat history. And that tool call entry has:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;tool_calls&amp;quot;: [...] } &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The problem is, the current template code expects every history item to have a &amp;quot;content&amp;quot; block:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- for message in messages %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + message.content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set content = message.content %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Therefore, whenever you use any OpenAI-compatible client that saves the chat history and you use &lt;em&gt;more than one tool call&lt;/em&gt;, the conversation will become broken and the server will start reporting an error:&lt;/p&gt; &lt;p&gt;&lt;code&gt; got exception: {&amp;quot;code&amp;quot;:500,&amp;quot;message&amp;quot;:&amp;quot;[json.exception.out_of_range.403] key 'content' not found&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;server_error&amp;quot;} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I think the fix is to patch the assistant branch similar to the &amp;quot;forward messages&amp;quot; branch:&lt;/p&gt; &lt;p&gt;&lt;code&gt; {%- set content = message.content if message.content is not none else '' %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;and then to refer to &lt;code&gt;content&lt;/code&gt; instead of &lt;code&gt;message.content&lt;/code&gt; later on. If someone could poke the Unsloth people to fix the template, that would be pretty neat (for now, I hacked my agent's code to always append an empty code block into tool call assistant history messages since I use my own API for whatever reason, but that's not something you can do if you're using standard libraries).&lt;/p&gt; &lt;p&gt;UPDATE: I believe this is the how the corrected template should look like: &lt;code&gt;jinja {%- if tools %} {{- '&amp;lt;|im_start|&amp;gt;system\n' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + '\n\n' }} {%- endif %} {{- &amp;quot;# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags:\n&amp;lt;tools&amp;gt;&amp;quot; }} {%- for tool in tools %} {{- &amp;quot;\n&amp;quot; }} {{- tool | tojson }} {%- endfor %} {{- &amp;quot;\n&amp;lt;/tools&amp;gt;\n\nFor each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags:\n&amp;lt;tool_call&amp;gt;\n{\&amp;quot;name\&amp;quot;: &amp;lt;function-name&amp;gt;, \&amp;quot;arguments\&amp;quot;: &amp;lt;args-json-object&amp;gt;}\n&amp;lt;/tool_call&amp;gt;&amp;lt;|im_end|&amp;gt;\n&amp;quot; }} {%- else %} {%- if messages[0].role == 'system' %} {{- '&amp;lt;|im_start|&amp;gt;system\n' + messages[0].content + '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %} {%- for forward_message in messages %} {%- set index = (messages|length - 1) - loop.index0 %} {%- set message = messages[index] %} {%- set current_content = message.content if message.content is defined and message.content is not none else '' %} {%- set tool_start = '&amp;lt;tool_response&amp;gt;' %} {%- set tool_start_length = tool_start|length %} {%- set start_of_message = current_content[:tool_start_length] %} {%- set tool_end = '&amp;lt;/tool_response&amp;gt;' %} {%- set tool_end_length = tool_end|length %} {%- set start_pos = (current_content|length) - tool_end_length %} {%- if start_pos &amp;lt; 0 %} {%- set start_pos = 0 %} {%- endif %} {%- set end_of_message = current_content[start_pos:] %} {%- if ns.multi_step_tool and message.role == &amp;quot;user&amp;quot; and not(start_of_message == tool_start and end_of_message == tool_end) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} {%- for message in messages %} {%- set m_content = message.content if message.content is defined and message.content is not none else '' %} {%- if (message.role == &amp;quot;user&amp;quot;) or (message.role == &amp;quot;system&amp;quot; and not loop.first) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content + '&amp;lt;|im_end|&amp;gt;' + '\n' }} {%- elif message.role == &amp;quot;assistant&amp;quot; %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is defined and message.reasoning_content is not none %} {%- set reasoning_content = message.reasoning_content %} {%- else %} {%- if '&amp;lt;/think&amp;gt;' in m_content %} {%- set m_content = (m_content.split('&amp;lt;/think&amp;gt;')|last).lstrip('\n') %} {%- set reasoning_content = (m_content.split('&amp;lt;/think&amp;gt;')|first).rstrip('\n') %} {%- set reasoning_content = (reasoning_content.split('&amp;lt;think&amp;gt;')|last).lstrip('\n') %} {%- endif %} {%- endif %} {%- if loop.index0 &amp;gt; ns.last_query_index %} {%- if loop.last or (not loop.last and (not reasoning_content.strip() == &amp;quot;&amp;quot;)) %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n&amp;lt;think&amp;gt;\n' + reasoning_content.strip('\n') + '\n&amp;lt;/think&amp;gt;\n\n' + m_content.lstrip('\n') }} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- else %} {{- '&amp;lt;|im_start|&amp;gt;' + message.role + '\n' + m_content }} {%- endif %} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {%- if (loop.first and m_content) or (not loop.first) %} {{- '\n' }} {%- endif %} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '&amp;lt;tool_call&amp;gt;\n{&amp;quot;name&amp;quot;: &amp;quot;' }} {{- tool_call.name }} {{- '&amp;quot;, &amp;quot;arguments&amp;quot;: ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments | tojson }} {%- endif %} {{- '}\n&amp;lt;/tool_call&amp;gt;' }} {%- endfor %} {%- endif %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- elif message.role == &amp;quot;tool&amp;quot; %} {%- if loop.first or (messages[loop.index0 - 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_start|&amp;gt;user' }} {%- endif %} {{- '\n&amp;lt;tool_response&amp;gt;\n' }} {{- message.content if message.content is defined and message.content is not none else '' }} {{- '\n&amp;lt;/tool_response&amp;gt;' }} {%- if loop.last or (messages[loop.index0 + 1].role != &amp;quot;tool&amp;quot;) %} {{- '&amp;lt;|im_end|&amp;gt;\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '&amp;lt;|im_start|&amp;gt;assistant\n' }} {%- if enable_thinking is defined and enable_thinking is false %} {{- '&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;\n\n' }} {%- endif %} {%- endif %} &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Seems to work correctly, I've made it work with Roo Code using this. UPDATE: more fixes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klltt4/the_qwen3_chat_template_is_still_bugged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T13:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1klvja8</id>
    <title>Local Benchmark on local models</title>
    <updated>2025-05-13T19:50:55+00:00</updated>
    <author>
      <name>/u/Expensive-Apricot-25</name>
      <uri>https://old.reddit.com/user/Expensive-Apricot-25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt; &lt;img alt="Local Benchmark on local models" src="https://preview.redd.it/rrkggcovrl0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd6c517847054aeb3ac1cd91751e04d6c36c8c67" title="Local Benchmark on local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the results of the local models I have been testing over the last year. The test is a modified version of the HumanEval dataset. I picked this data set because there is no answer key to train on, and smaller models didn't seem to overfit it, so it seemed like a good enough benchmark.&lt;/p&gt; &lt;p&gt;I have been running this benchmark over the last year, and qwen 3 made HUGE strides on this benchmark, both reasoning and non-reasoning, very impressive. Most notably, qwen3:4b scores in the top 3 within margin of error.&lt;/p&gt; &lt;p&gt;I ran the benchmarks using ollama, all models are Q4 with the exception of gemma3 4b 16fp, which scored extremely low, and the reason is due to gemma3 arcitecture bugs when gemma3 was first released, and I just never re-tested it. I tried testing qwen3:30b reasoning, but I just dont have the proper hardware, and it would have taken a week.&lt;/p&gt; &lt;p&gt;Anyways, thought it was interesting so I thought I'd share. Hope you guys find it interesting/helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Apricot-25"&gt; /u/Expensive-Apricot-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rrkggcovrl0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klvja8/local_benchmark_on_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T19:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrony</id>
    <title>The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names.</title>
    <updated>2025-05-13T17:19:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt; &lt;img alt="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." src="https://preview.redd.it/p5s9pcsd1l0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=938c01763e5e47522657359535bc0c0b28ee9579" title="The Scariest Thing In LLMs/AI Isn't the Models or the Math... It's the Names." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5s9pcsd1l0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrony/the_scariest_thing_in_llmsai_isnt_the_models_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1klxlbx</id>
    <title>BitNet Finetunes of R1 Distills</title>
    <updated>2025-05-13T21:12:14+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt; &lt;img alt="BitNet Finetunes of R1 Distills" src="https://external-preview.redd.it/DkDKsAS_zzAadrbN0EABgGOgbPBi8t0wwT1ePj0VWZI.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7e8dd284821c06c24c1fb34c18947d53d363c4e" title="BitNet Finetunes of R1 Distills" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My group recently discovered that you can finetune directly to ternary ({-1, 0, 1}) BitNet if you add an extra RMS Norm to the intput of linear layers. We are releasing the preview of two models - bitnet-r1-llama-8b and bitnet-r1-qwen-32b. These models are &amp;lt;3GB and &amp;lt;10GB respectively.&lt;/p&gt; &lt;p&gt;We also have a PR out in HF transformers so that anyone can load these models with an extra RMS norm by changing the quant_config, and finetune themselves&lt;/p&gt; &lt;p&gt;Try these out and see if they are good for a BitNet model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/0xCodyS/status/1922077684948996229"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klxlbx/bitnet_finetunes_of_r1_distills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T21:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1klqir8</id>
    <title>WizardLM Team has joined Tencent</title>
    <updated>2025-05-13T16:34:03+00:00</updated>
    <author>
      <name>/u/GTT444</name>
      <uri>https://old.reddit.com/user/GTT444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt; &lt;img alt="WizardLM Team has joined Tencent" src="https://external-preview.redd.it/ILHoDHQUFu7tKCNSAM9UVMgUHxifQhr_Q9wIcfRI8lA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b01c639188d58a880692f842b9d003ae1c11a2f7" title="WizardLM Team has joined Tencent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See attached post, looks like they are training Tencent's Hunyuan Turbo Model's now? But I guess these models aren't open source or even available via API outside of China?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTT444"&gt; /u/GTT444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/CanXu20/status/1922303283890397264"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klqir8/wizardlm_team_has_joined_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T16:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1klrio8</id>
    <title>LLM trained to gaslight people</title>
    <updated>2025-05-13T17:13:04+00:00</updated>
    <author>
      <name>/u/LividResearcher7818</name>
      <uri>https://old.reddit.com/user/LividResearcher7818</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finetuned gemma 3 12b using RL to be an expert at gaslighting and demeaning it‚Äôs users. I‚Äôve been training LLMs using RL with soft rewards for a while now, and seeing OpenAI‚Äôs experiments with sycophancy I wanted to see if we can apply it to make the model behave on the other end of the spectrum..&lt;/p&gt; &lt;p&gt;It is not perfect (i guess no eval exists for measuring this), but can be really good in some situations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gaslight-gpt.com/"&gt;https://www.gaslight-gpt.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(A lot of people using the website at once, way more than my single gpu machine can handle so i will share weights on hf)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LividResearcher7818"&gt; /u/LividResearcher7818 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klrio8/llm_trained_to_gaslight_people/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T17:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1klkmah</id>
    <title>Qwen3 Technical Report</title>
    <updated>2025-05-13T12:26:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt; &lt;img alt="Qwen3 Technical Report" src="https://preview.redd.it/kku7lzsulj0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d8d566f0f7c92d2b0575c613f30a76aafba7a29" title="Qwen3 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Technical Report released.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf"&gt;https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kku7lzsulj0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klkmah/qwen3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T12:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1klx9q2</id>
    <title>Real-time webcam demo with SmolVLM using llama.cpp</title>
    <updated>2025-05-13T20:59:50+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt; &lt;img alt="Real-time webcam demo with SmolVLM using llama.cpp" src="https://external-preview.redd.it/OHg0YjZidWQ0bTBmMduXqqISYSTmhZJt9j6zzJp3o5OEqUQPvF7tZjxvn6li.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbb3d3b1a7db42b1a83c7e14926531c1ab78b9f" title="Real-time webcam demo with SmolVLM using llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81evi7ud4m0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-13T20:59:50+00:00</published>
  </entry>
</feed>
