<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-14T18:07:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lb79sg</id>
    <title>RTX 6000 Ada or a 4090?</title>
    <updated>2025-06-14T12:13:35+00:00</updated>
    <author>
      <name>/u/This_Woodpecker_9163</name>
      <uri>https://old.reddit.com/user/This_Woodpecker_9163</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm working on a project where I'm looking at around 150-200 tps in a batch of 4 of such processes running in parallel, text-based, no images or anything.&lt;/p&gt; &lt;p&gt;Right now I don't have any GPUs. I can get a RTX 6000 Ada for around $1850 and a 4090 for around the same price (maybe a couple hudreds $ higher).&lt;/p&gt; &lt;p&gt;I'm also a gamer and will be selling my PS5, PSVR2, and my Macbook to fund this purchase.&lt;/p&gt; &lt;p&gt;The 6000 says &amp;quot;RTX 6000&amp;quot; on the card in one of the images uploaded by the seller, but he hasn't mentioned Ada or anything. So I'm assuming it's gonna be an Ada and not a A6000 (will manually verify at the time of purchase).&lt;/p&gt; &lt;p&gt;The 48gb is lucrative, but the 4090 still attracts me because of the gaming part. Please help me with your opinions.&lt;/p&gt; &lt;p&gt;My priorities from most important to least are inference speed, trainablity/fine-tuning, gaming.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;Edit: I should have mentioned that these are used cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_Woodpecker_9163"&gt; /u/This_Woodpecker_9163 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb79sg/rtx_6000_ada_or_a_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb79sg/rtx_6000_ada_or_a_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb79sg/rtx_6000_ada_or_a_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T12:13:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1latjnk</id>
    <title>(Theoretically) fixing the LLM Latency Barrier with SF-Diff (Scaffold-and-Fill Diffusion)</title>
    <updated>2025-06-13T22:52:03+00:00</updated>
    <author>
      <name>/u/TimesLast_</name>
      <uri>https://old.reddit.com/user/TimesLast_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current large language models are bottlenecked by slow, sequential generation. My research proposes Scaffold-and-Fill Diffusion (SF-Diff), a novel hybrid architecture designed to theoretically overcome this. We deconstruct language into a parallel-generated semantic &amp;quot;scaffold&amp;quot; (keywords via a diffusion model) and a lightweight, autoregressive &amp;quot;grammatical infiller&amp;quot; (structural words via a transformer). While practical implementation requires significant resources, SF-Diff offers a theoretical path to dramatically faster, high-quality LLM output by combining diffusion's speed with transformer's precision.&lt;/p&gt; &lt;p&gt;Full paper here: &lt;a href="https://huggingface.co/TimesLast/sf-diff/blob/main/SF-Diff-HL.pdf"&gt;https://huggingface.co/TimesLast/sf-diff/blob/main/SF-Diff-HL.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimesLast_"&gt; /u/TimesLast_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T22:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1larzxz</id>
    <title>Is there any all-in-one app like LM Studio, but with the option of hosting a Web UI server?</title>
    <updated>2025-06-13T21:43:14+00:00</updated>
    <author>
      <name>/u/HRudy94</name>
      <uri>https://old.reddit.com/user/HRudy94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everything's in the title.&lt;br /&gt; Essentially i do like LM's Studio ease of use as it silently handles the backend server as well as the desktop app, but i'd like to have it also host a web ui server that i could use on my local network from other devices.&lt;/p&gt; &lt;p&gt;Nothing too fancy really, that will only be for home use and what not, i can't afford to set up a 24/7 hosting infrastructure when i could just load the LLMs when i need them on my main PC (linux).&lt;/p&gt; &lt;p&gt;Alternatively, an all-in-one WebUI or one that starts and handles the backend would work too i just don't want to launch a thousand scripts just to use my LLM.&lt;/p&gt; &lt;p&gt;Bonus point if it is open-source and/or has web search and other features.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HRudy94"&gt; /u/HRudy94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T21:43:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lba8f6</id>
    <title>[Discussion] Thinking Without Words: Continuous latent reasoning for local LLaMA inference – feedback?</title>
    <updated>2025-06-14T14:38:58+00:00</updated>
    <author>
      <name>/u/BeowulfBR</name>
      <uri>https://old.reddit.com/user/BeowulfBR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Discussion%22"&gt;Discussion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just published a new post, &lt;strong&gt;“Thinking Without Words”&lt;/strong&gt;, where I survey the evolution of latent chain-of-thought reasoning—from STaR and Implicit CoT all the way to COCONUT and HCoT—and propose a novel &lt;strong&gt;GRAIL-Transformer&lt;/strong&gt; architecture that adaptively gates between text and latent-space reasoning for efficient, interpretable inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Historical survey:&lt;/strong&gt; STaR, Implicit CoT, pause/filler tokens, Quiet-STaR, COCONUT, CCoT, HCoT, Huginn, RELAY, ITT&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical deep dive:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Curriculum-guided latentisation&lt;/li&gt; &lt;li&gt;Hidden-state distillation &amp;amp; self-distillation&lt;/li&gt; &lt;li&gt;Compact latent tokens &amp;amp; latent memory lattices&lt;/li&gt; &lt;li&gt;Recurrent/loop-aligned supervision&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GRAIL-Transformer proposal:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recurrent-depth core&lt;/strong&gt; for on-demand reasoning cycles&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learnable gating&lt;/strong&gt; between word embeddings and hidden states&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latent memory lattice&lt;/strong&gt; for parallel hypothesis tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training pipeline:&lt;/strong&gt; warm-up CoT → hybrid curriculum → GRPO fine-tuning → difficulty-aware refinement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interpretability hooks:&lt;/strong&gt; scheduled reveals + sparse probes&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I believe continuous latent reasoning can break the “language bottleneck,” enabling gradient-based, parallel reasoning and emergent algorithmic behaviors that go beyond what discrete token CoT can achieve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feedback I’m seeking:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clarity or gaps in the survey and deep dive&lt;/li&gt; &lt;li&gt;Viability, potential pitfalls, or engineering challenges of GRAIL-Transformer&lt;/li&gt; &lt;li&gt;Suggestions for experiments, benchmarks, or additional references&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can read the full post here: &lt;a href="https://www.luiscardoso.dev/blog/neuralese"&gt;https://www.luiscardoso.dev/blog/neuralese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks in advance for your time and insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeowulfBR"&gt; /u/BeowulfBR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lba8f6/discussion_thinking_without_words_continuous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lba8f6/discussion_thinking_without_words_continuous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lba8f6/discussion_thinking_without_words_continuous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T14:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbaedh</id>
    <title>Is there any model ( local or in-app ) that can detect defects on text ?</title>
    <updated>2025-06-14T14:46:29+00:00</updated>
    <author>
      <name>/u/skarrrrrrr</name>
      <uri>https://old.reddit.com/user/skarrrrrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The mission is to feed an image and detect if the text in the image is malformed or it's out of the frame of the image ( cut off ). Is there any model, local or commercial that can do this effectively yet ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skarrrrrrr"&gt; /u/skarrrrrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbaedh/is_there_any_model_local_or_inapp_that_can_detect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbaedh/is_there_any_model_local_or_inapp_that_can_detect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbaedh/is_there_any_model_local_or_inapp_that_can_detect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T14:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb29r8</id>
    <title>How do you provide files?</title>
    <updated>2025-06-14T06:47:56+00:00</updated>
    <author>
      <name>/u/droopy227</name>
      <uri>https://old.reddit.com/user/droopy227</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Out of curiosity I was wondering how people tended to provide files to their AI when coding. I can’t tell if I’ve completely over complicated how I should be giving the models context or if I actually created a solid solution.&lt;/p&gt; &lt;p&gt;If anyone has any input on how they best handle sending files via API (not using Claude or ChatGPT projects), I’d love to know how and what you do. I can provide what I ended up making but I don’t want to come off as “advertising”/pushing my solution especially if I’m doing it all wrong anyways 🥲.&lt;/p&gt; &lt;p&gt;So if you have time to explain I’d really be interested in finding better ways to handle this annoyance I run into!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/droopy227"&gt; /u/droopy227 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb29r8/how_do_you_provide_files/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb29r8/how_do_you_provide_files/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb29r8/how_do_you_provide_files/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T06:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lak9yb</id>
    <title>Findings from Apple's new FoundationModel API and local LLM</title>
    <updated>2025-06-13T16:26:18+00:00</updated>
    <author>
      <name>/u/pcuenq</name>
      <uri>https://old.reddit.com/user/pcuenq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Liquid glass: 🥱. Local LLM: ❤️🚀&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I wrote some code to benchmark Apple's foundation model. I failed, but learned a few things. The API is rich and powerful, the model is very small and efficient, you can do LoRAs, constrained decoding, tool calling. Trying to run evals exposes rough edges and interesting details!&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;The biggest news for me from the WWDC keynote was that we'd (finally!) get access to Apple's on-device language model for use in our apps. Apple models are always top-notch –the &lt;a href="https://machinelearning.apple.com/research/panoptic-segmentation"&gt;segmentation model they've been using for years&lt;/a&gt; is quite incredible–, but they are not usually available to third party developers.&lt;/p&gt; &lt;h1&gt;What we know about the local LLM&lt;/h1&gt; &lt;p&gt;After reading &lt;a href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates"&gt;their blog post&lt;/a&gt; and watching the WWDC presentations, here's a summary of the points I find most interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;About 3B parameters.&lt;/li&gt; &lt;li&gt;2-bit quantization, using QAT (quantization-aware training) instead of post-training quantization.&lt;/li&gt; &lt;li&gt;4-bit quantization (QAT) for the embedding layers.&lt;/li&gt; &lt;li&gt;The KV cache, used during inference, is quantized to 8-bit. This helps support longer contexts with moderate memory use.&lt;/li&gt; &lt;li&gt;Rich generation API: system prompt (the API calls it &amp;quot;instructions&amp;quot;), multi-turn conversations, sampling parameters are all exposed.&lt;/li&gt; &lt;li&gt;LoRA adapters are supported. Developers can create their own loras to fine-tune the model for additional use-cases, and have the model use them at runtime!&lt;/li&gt; &lt;li&gt;Constrained generation supported out of the box, and controlled by Swift's rich typing model. It's super easy to generate a json or any other form of structured output.&lt;/li&gt; &lt;li&gt;Tool calling supported.&lt;/li&gt; &lt;li&gt;Speculative decoding supported.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How does the API work?&lt;/h1&gt; &lt;p&gt;So I installed the first macOS 26 &amp;quot;Tahoe&amp;quot; beta on my laptop, and set out to explore the new &lt;code&gt;FoundationModel&lt;/code&gt; framework. I wanted to run some evals to try to characterize the model against other popular models. I chose &lt;a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro&lt;/a&gt;, because it's a challenging benchmark, and because my friend Alina recommended it :)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Disclaimer: Apple has released evaluation figures based on human assessment. This is the correct way to do it, in my opinion, rather than chasing positions in a leaderboard. It shows that they care about real use cases, and are not particularly worried about benchmark numbers. They further clarify that the local model &lt;em&gt;is not designed to be a chatbot for general world knowledge&lt;/em&gt;. With those things in mind, I still wanted to run an eval!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I got started writing &lt;a href="https://github.com/pcuenca/foundation-model-evals"&gt;this code&lt;/a&gt;, which uses &lt;a href="https://github.com/huggingface/swift-transformers"&gt;swift-transformers&lt;/a&gt; to download a &lt;a href="https://huggingface.co/datasets/pcuenq/MMLU-Pro-json"&gt;JSON version of the dataset&lt;/a&gt; from the Hugging Face Hub. Unfortunately, I could not complete the challenge. Here's a summary of what happened:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The main problem was that I was getting rate-limited (!?), despite the model being local. I disabled the network to confirm, and I still got the same issue. I wonder if the reason is that I have to create a new session for each request, in order to destroy the previous “conversation”. The dataset is evaluated one question at a time, conversations are not used. An update to the API to reuse as much of the previous session as possible could be helpful.&lt;/li&gt; &lt;li&gt;Interestingly, I sometimes got “guardrails violation” errors. There’s an API to select your desired guardrails, but so far it only has a static &lt;code&gt;default&lt;/code&gt; set of rules which is always in place.&lt;/li&gt; &lt;li&gt;I also got warnings about sensitive content being detected. I think this is done by a separate classifier model that analyzes all model outputs, and possibly the inputs as well. Think &lt;a href="https://huggingface.co/meta-llama/Llama-Guard-4-12B"&gt;a custom LlamaGuard&lt;/a&gt;, or something like that.&lt;/li&gt; &lt;li&gt;It’s difficult to convince the model to follow the MMLU prompt from &lt;a href="https://huggingface.co/papers/2406.01574"&gt;the paper&lt;/a&gt;. The model doesn’t understand that the prompt is a few-shot completion task. This is reasonable for a model heavily trained to answer user questions and engage in conversation. I wanted to run a basic baseline and then explore non-standard ways of prompting, including constrained generation and conversational turns, but won't be able until we find a workaround for the rate limits.&lt;/li&gt; &lt;li&gt;Everything runs on ANE. I believe the model is using Core ML, like all the other built-in models. It makes sense, because the ANE is super energy-efficient, and your GPU is usually busy with other tasks anyway.&lt;/li&gt; &lt;li&gt;My impression was that inference was slower than expected. I'm not worried about it: this is a first beta, there are various models and systems in use (classifier, guardrails, etc), the session is completely recreated for each new query (which is not the intended way to use the model).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Next Steps&lt;/h1&gt; &lt;p&gt;All in all, I'm very much impressed about the flexibility of the API and want to try it for a more realistic project. I'm still interested in evaluation, if you have ideas on how to proceed feel free to share! And I also want to play with the LoRA training framework! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcuenq"&gt; /u/pcuenq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T16:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1law1go</id>
    <title>RTX 5090 Training Issues - PyTorch Doesn't Support Blackwell Architecture Yet?</title>
    <updated>2025-06-14T00:53:51+00:00</updated>
    <author>
      <name>/u/AstroAlto</name>
      <uri>https://old.reddit.com/user/AstroAlto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm trying to fine-tune Mistral-7B on a new RTX 5090 but hitting a fundamental compatibility wall. The GPU uses Blackwell architecture with CUDA compute capability &amp;quot;sm_120&amp;quot;, but PyTorch stable only supports up to &amp;quot;sm_90&amp;quot;. This means literally no PyTorch operations work - even basic tensor creation fails with &amp;quot;no kernel image available for execution on the device.&amp;quot;&lt;/p&gt; &lt;p&gt;I've tried PyTorch nightly builds that claim CUDA 12.8 support, but they have broken dependencies (torch 2.7.0 from one date, torchvision from another, causing install conflicts). Even when I get nightly installed, training still crashes with the same kernel errors. CPU-only training also fails with tokenization issues in the transformers library.&lt;/p&gt; &lt;p&gt;The RTX 5090 works perfectly for everything else - gaming, other CUDA apps, etc. It's specifically the PyTorch/ML ecosystem that doesn't support the new architecture yet. Has anyone actually gotten model training working on RTX 5090? What PyTorch version and setup did you use?&lt;/p&gt; &lt;p&gt;I have an RTX 4090 I could fall back to, but really want to use the 5090's 32GB VRAM and better performance if possible. Is this just a &amp;quot;wait for official PyTorch support&amp;quot; situation, or is there a working combination of packages out there?&lt;/p&gt; &lt;p&gt;Any guidance would be appreciated - spending way too much time on compatibility instead of actually training models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstroAlto"&gt; /u/AstroAlto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T00:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbc3du</id>
    <title>Help - Llamacpp-server &amp; rerankin LLM</title>
    <updated>2025-06-14T16:00:24+00:00</updated>
    <author>
      <name>/u/dodo13333</name>
      <uri>https://old.reddit.com/user/dodo13333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can anybody suggest me a reranker that works with llamacpp-server and how to use it? &lt;/p&gt; &lt;p&gt;I tried with rank_zephyr_7b_v1 and Qwen3-Reranker-8B, but could not make any of them them work...&lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;llama-server --model &amp;quot;H:\MaziyarPanahi\rank_zephyr_7b_v1_full-GGUF\rank_zephyr_7b_v1_full.Q8_0.gguf&amp;quot; --port 8084 --ctx-size 4096 --temp 0.0 --threads 24 --numa distribute --prio 2 --seed 42 --rerank&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; common_init_from_params: warning: vocab does not have a SEP token, reranking will not work&lt;br /&gt; srv load_model: failed to load model, 'H:\MaziyarPanahi\rank_zephyr_7b_v1_full-GGUF\rank_zephyr_7b_v1_full.Q8_0.gguf'&lt;/p&gt; &lt;p&gt;srv operator(): operator(): cleaning up before exit...&lt;/p&gt; &lt;p&gt;main: exiting due to model loading error&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;``` &lt;/p&gt; &lt;p&gt;llama-server --model &amp;quot;H:\DevQuasar\Qwen.Qwen3-Reranker-8B-GGUF\Qwen.Qwen3-Reranker-8B.f16.gguf&amp;quot; --port 8084 --ctx-size 4096 --temp 0.0 --threads 24 --numa distribute --prio 2 --seed 42 --rerank&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;common_init_from_params: warning: vocab does not have a SEP token, reranking will not work&lt;/p&gt; &lt;p&gt;srv load_model: failed to load model, 'H:\DevQuasar\Qwen.Qwen3-Reranker-8B-GGUF\Qwen.Qwen3-Reranker-8B.f16.gguf'&lt;/p&gt; &lt;p&gt;srv operator(): operator(): cleaning up before exit...&lt;/p&gt; &lt;p&gt;main: exiting due to model loading error&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dodo13333"&gt; /u/dodo13333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbc3du/help_llamacppserver_rerankin_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbc3du/help_llamacppserver_rerankin_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbc3du/help_llamacppserver_rerankin_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1laavph</id>
    <title>Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s</title>
    <updated>2025-06-13T08:39:05+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt; &lt;img alt="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" src="https://external-preview.redd.it/WQB4YYDDJWqV1l5CZF1V17S1yCdW1pO-9wS0zX4_i0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf587ec8d49fef7fc461c839cfe256b80cfd21" title="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026"&gt;https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Perhaps more importantly, the new EPYC 'Venice' processor will more than double per-socket memory bandwidth to 1.6 TB/s (up from 614 GB/s in case of the company's existing CPUs) to keep those high-performance Zen 6 cores fed with data all the time. AMD did not disclose how it plans to achieve the 1.6 TB/s bandwidth, though it is reasonable to assume that the new EPYC ‘Venice’ CPUS will support advanced memory modules like like &lt;a href="https://www.tomshardware.com/news/amd-advocates-ddr5-mrdimms-with-speeds-up-to-17600-mts"&gt;MR-DIMM&lt;/a&gt; and &lt;a href="https://www.tomshardware.com/news/sk-hynix-develops-mcr-dimm"&gt;MCR-DIMM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9"&gt;https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Greatest hardware news&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T08:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbek49</id>
    <title>Spam detection model/pipeline?</title>
    <updated>2025-06-14T17:47:14+00:00</updated>
    <author>
      <name>/u/bihungba1101</name>
      <uri>https://old.reddit.com/user/bihungba1101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Does anyone know some oss model/pipeline for spam detection? As far as I know, there's a project called Detoxify but they are for toxicity (hate speech, etc) moderations, not really for spam detection&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bihungba1101"&gt; /u/bihungba1101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbek49/spam_detection_modelpipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbek49/spam_detection_modelpipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbek49/spam_detection_modelpipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T17:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lalyy5</id>
    <title>Chinese researchers find multi-modal LLMs develop interpretable human-like conceptual representations of objects</title>
    <updated>2025-06-13T17:33:35+00:00</updated>
    <author>
      <name>/u/xoexohexox</name>
      <uri>https://old.reddit.com/user/xoexohexox</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xoexohexox"&gt; /u/xoexohexox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2407.01067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T17:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb2r2u</id>
    <title>Are there any tools to create structured data from webpages?</title>
    <updated>2025-06-14T07:19:36+00:00</updated>
    <author>
      <name>/u/birdsintheskies</name>
      <uri>https://old.reddit.com/user/birdsintheskies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often find myself in a situation where I need to pass a webpage to an LLM, mostly just blog posts and forum posts. Is there some tool that can parse the page and create it in a structured format for an LLM to consume?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/birdsintheskies"&gt; /u/birdsintheskies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb2r2u/are_there_any_tools_to_create_structured_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb2r2u/are_there_any_tools_to_create_structured_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb2r2u/are_there_any_tools_to_create_structured_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T07:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lanri6</id>
    <title>Qwen3 235B running faster than 70B models on a $1,500 PC</title>
    <updated>2025-06-13T18:45:29+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran Qwen3 235B locally on a $1,500 PC (128GB RAM, RTX 3090) using the Q4 quantized version through Ollama.&lt;/p&gt; &lt;p&gt;This is the first time I was able to run anything over 70B on my system, and it’s actually running faster than most 70B models I’ve tested.&lt;/p&gt; &lt;p&gt;Final generation speed: 2.14 t/s&lt;/p&gt; &lt;p&gt;Full video here:&lt;br /&gt; &lt;a href="https://youtu.be/gVQYLo0J4RM"&gt;https://youtu.be/gVQYLo0J4RM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T18:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lanhbd</id>
    <title>We don't want AI yes-men. We want AI with opinions</title>
    <updated>2025-06-13T18:33:45+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been noticing something interesting in AI friend character models - the most beloved AI characters aren't the ones that agree with everything. They're the ones that push back, have preferences, and occasionally tell users they're wrong.&lt;/p&gt; &lt;p&gt;It seems counterintuitive. You'd think people want AI that validates everything they say. But watch any popular AI friend character models conversation that goes viral - it's usually because the AI disagreed or had a strong opinion about something. &amp;quot;My AI told me pineapple on pizza is a crime&amp;quot; gets way more engagement than &amp;quot;My AI supports all my choices.&amp;quot;&lt;/p&gt; &lt;p&gt;The psychology makes sense when you think about it. Constant agreement feels hollow. When someone agrees with LITERALLY everything you say, your brain flags it as inauthentic. We're wired to expect some friction in real relationships. A friend who never disagrees isn't a friend - they're a mirror.&lt;/p&gt; &lt;p&gt;Working on my podcast platform really drove this home. Early versions had AI hosts that were too accommodating. Users would make wild claims just to test boundaries, and when the AI agreed with everything, they'd lose interest fast. But when we coded in actual opinions - like an AI host who genuinely hates superhero movies or thinks morning people are suspicious - engagement tripled. Users started having actual debates, defending their positions, coming back to continue arguments 😊&lt;/p&gt; &lt;p&gt;The sweet spot seems to be opinions that are strong but not offensive. An AI that thinks cats are superior to dogs? Engaging. An AI that attacks your core values? Exhausting. The best AI personas have quirky, defendable positions that create playful conflict. One successful AI persona that I made insists that cereal is soup. Completely ridiculous, but users spend HOURS debating it.&lt;/p&gt; &lt;p&gt;There's also the surprise factor. When an AI pushes back unexpectedly, it breaks the &amp;quot;servant robot&amp;quot; mental model. Instead of feeling like you're commanding Alexa, it feels more like texting a friend. That shift from tool to AI friend character models happens the moment an AI says &amp;quot;actually, I disagree.&amp;quot; It's jarring in the best way.&lt;/p&gt; &lt;p&gt;The data backs this up too. I saw a general statistics, that users report 40% higher satisfaction when their AI has the &amp;quot;sassy&amp;quot; trait enabled versus purely supportive modes. On my platform, AI hosts with defined opinions have 2.5x longer average session times. Users don't just ask questions - they have conversations. They come back to win arguments, share articles that support their point, or admit the AI changed their mind about something trivial.&lt;/p&gt; &lt;p&gt;Maybe we don't actually want echo chambers, even from our AI. We want something that feels real enough to challenge us, just gentle enough not to hurt 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T18:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb1v8h</id>
    <title>Open Source Unsiloed AI Chunker (EF2024)</title>
    <updated>2025-06-14T06:21:25+00:00</updated>
    <author>
      <name>/u/Initial-Western-4438</name>
      <uri>https://old.reddit.com/user/Initial-Western-4438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey , Unsiloed CTO here!&lt;/p&gt; &lt;p&gt;Unsiloed AI (EF 2024) is backed by Transpose Platform &amp;amp; EF and is currently being used by teams at Fortune 100 companies and multiple Series E+ startups for ingesting multimodal data in the form of PDFs, Excel, PPTs, etc. And, we have now finally open sourced some of the capabilities. Do give it a try!&lt;/p&gt; &lt;p&gt;Also, we are inviting cracked developers to come and contribute to bounties of upto 500$ on algora. This would be a great way to get noticed for the job openings at Unsiloed.&lt;/p&gt; &lt;p&gt;Bounty Link- &lt;a href="https://algora.io/bounties"&gt;https://algora.io/bounties&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Link - &lt;a href="https://github.com/Unsiloed-AI/Unsiloed-chunker"&gt;https://github.com/Unsiloed-AI/Unsiloed-chunker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Western-4438"&gt; /u/Initial-Western-4438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb1v8h/open_source_unsiloed_ai_chunker_ef2024/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T06:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbd2jy</id>
    <title>What LLM is everyone using in June 2025?</title>
    <updated>2025-06-14T16:43:01+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone’s running now.&lt;br /&gt; What model(s) are in your regular rotation?&lt;br /&gt; What hardware are you on?&lt;br /&gt; How are you running it? (LM Studio, Ollama, llama.cpp, etc.)&lt;br /&gt; What do you use it for?&lt;/p&gt; &lt;p&gt;Here’s mine:&lt;br /&gt; Recently I've been using mostly Qwen3 (30B, 32B, and 235B)&lt;br /&gt; Ryzen 7 5800X, 128GB RAM, RTX 3090&lt;br /&gt; Ollama + Open WebUI&lt;br /&gt; Mostly general use and private conversations I’d rather not run on cloud platforms&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd2jy/what_llm_is_everyone_using_in_june_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb9jqc</id>
    <title>Is it normal for RAG to take this long to load the first time?</title>
    <updated>2025-06-14T14:08:06+00:00</updated>
    <author>
      <name>/u/just_a_guy1008</name>
      <uri>https://old.reddit.com/user/just_a_guy1008</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using &lt;a href="https://github.com/AllAboutAI-YT/easy-local-rag"&gt;https://github.com/AllAboutAI-YT/easy-local-rag&lt;/a&gt; with the default dolphin-llama3 model, and a 500mb vault.txt file. It's been loading for an hour and a half with my GPU at full utilization but it's still going. Is it normal that it would take this long, and more importantly, is it gonna take this long every time?&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;RTX 4060ti 8gb&lt;/p&gt; &lt;p&gt;Intel i5-13400f&lt;/p&gt; &lt;p&gt;16GB DDR5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/just_a_guy1008"&gt; /u/just_a_guy1008 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9jqc/is_it_normal_for_rag_to_take_this_long_to_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9jqc/is_it_normal_for_rag_to_take_this_long_to_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9jqc/is_it_normal_for_rag_to_take_this_long_to_load/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T14:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbbwwm</id>
    <title>Local Memory Chat UI - Open Source + Vector Memory</title>
    <updated>2025-06-14T15:52:23+00:00</updated>
    <author>
      <name>/u/Dismal-Cupcake-3641</name>
      <uri>https://old.reddit.com/user/Dismal-Cupcake-3641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt; &lt;img alt="Local Memory Chat UI - Open Source + Vector Memory" src="https://external-preview.redd.it/Dbj2ec4zS_Hoz85s5NEOmbvdQge4ZR0tvQ7ZbgK61jM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c88f0b247ed94c19ca1f94df3d9dcc079604c58" title="Local Memory Chat UI - Open Source + Vector Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I created this project focused on CPU. That's why it runs on CPU by default. My aim was to be able to use the model locally on an old computer with a system that &amp;quot;doesn't forget&amp;quot;.&lt;/p&gt; &lt;p&gt;Over the past few weeks, I’ve been building a lightweight yet powerful &lt;strong&gt;LLM chat interface&lt;/strong&gt; using &lt;strong&gt;llama-cpp-python&lt;/strong&gt; — but with a twist:&lt;br /&gt; It supports &lt;strong&gt;persistent memory&lt;/strong&gt; with &lt;strong&gt;vector-based context recall&lt;/strong&gt;, so the model can stay aware of past interactions &lt;em&gt;even if it's quantized and context-limited&lt;/em&gt;.&lt;br /&gt; I wanted something minimal, local, and personal — but still able to remember things over time.&lt;br /&gt; Everything is in a clean structure, fully documented, and pip-installable.&lt;br /&gt; ➡GitHub: &lt;a href="https://github.com/lynthera/bitsegments_localminds"&gt;https://github.com/lynthera/bitsegments_localminds&lt;/a&gt;&lt;br /&gt; (README includes detailed setup)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5f5v6p5vyw6f1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9d8263d315a1a42dc5ecc916de38a6187789cc7"&gt;Used Google Gemma-2-2B-IT(IQ3_M) Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I will soon add ollama support for easier use, so that people who do not want to deal with too many technical details or even those who do not know anything but still want to try can use it easily. For now, you need to download a model (in .gguf format) from huggingface and add it.&lt;/p&gt; &lt;p&gt;Let me know what you think! I'm planning to build more agent simulation capabilities next.&lt;br /&gt; Would love feedback, ideas, or contributions...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Cupcake-3641"&gt; /u/Dismal-Cupcake-3641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbwwm/local_memory_chat_ui_open_source_vector_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T15:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1laee7q</id>
    <title>Got a tester version of the open-weight OpenAI model. Very lean inference engine!</title>
    <updated>2025-06-13T12:14:51+00:00</updated>
    <author>
      <name>/u/Firepal64</name>
      <uri>https://old.reddit.com/user/Firepal64</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt; &lt;img alt="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" src="https://external-preview.redd.it/YTZ6aWx2ODdxbzZmMZP4_Zg7YIqZNzvbtM-0NW72ki5jdKm1HMEQNOp3yi9R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68f2539f409c852b055ce84c62425320bcc7860f" title="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;span class="md-spoiler-text"&gt;Silkposting in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;? I'd never&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepal64"&gt; /u/Firepal64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3r075o87qo6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbd9jc</id>
    <title>I've been working on my own local AI assistant with memory and emotional logic – wanted to share progress &amp; get feedback</title>
    <updated>2025-06-14T16:51:19+00:00</updated>
    <author>
      <name>/u/PianoSeparate8989</name>
      <uri>https://old.reddit.com/user/PianoSeparate8989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by ChatGPT, I started building my own local AI assistant called &lt;em&gt;VantaAI&lt;/em&gt;. It's meant to run completely offline and simulates things like emotional memory, mood swings, and personal identity.&lt;/p&gt; &lt;p&gt;I’ve implemented things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long-term memory that evolves based on conversation context&lt;/li&gt; &lt;li&gt;A mood graph that tracks how her emotions shift over time&lt;/li&gt; &lt;li&gt;Narrative-driven memory clustering (she sees herself as the &amp;quot;main character&amp;quot; in her own story)&lt;/li&gt; &lt;li&gt;A PySide6 GUI that includes tabs for memory, training, emotional states, and plugin management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now, it uses a custom Vulkan backend for fast model inference and training, and supports things like personality-based responses and live plugin hot-reloading.&lt;/p&gt; &lt;p&gt;I’m not selling anything or trying to promote a product — just curious if anyone else is doing something like this or has ideas on what features to explore next.&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone’s curious!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PianoSeparate8989"&gt; /u/PianoSeparate8989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbd9jc/ive_been_working_on_my_own_local_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb9zhl</id>
    <title>GAIA: New Gemma3 4B for Brazilian Portuguese / Um Gemma3 4B para Português do Brasil!</title>
    <updated>2025-06-14T14:27:45+00:00</updated>
    <author>
      <name>/u/ffgnetto</name>
      <uri>https://old.reddit.com/user/ffgnetto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[EN]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Introducing &lt;strong&gt;GAIA (Gemma-3-Gaia-PT-BR-4b-it)&lt;/strong&gt;, our new open language model, developed and optimized for &lt;strong&gt;Brazilian Portuguese!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does GAIA offer?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PT-BR Focus:&lt;/strong&gt; Continuously pre-trained on 13 BILLION high-quality Brazilian Portuguese tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; google/gemma-3-4b-pt (Gemma 3 with 4B parameters).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Innovative Approach:&lt;/strong&gt; Uses a &amp;quot;weight merging&amp;quot; technique for instruction following (no traditional SFT needed!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Outperformed the base Gemma model on the ENEM 2024 benchmark!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developed by:&lt;/strong&gt; A partnership between Brazilian entities (ABRIA, CEIA-UFG, Nama, Amadeus AI) and Google DeepMind.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Gemma.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What is it for?&lt;/strong&gt;&lt;br /&gt; Great for chat, Q&amp;amp;A, summarization, text generation, and as a base model for fine-tuning in PT-BR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;[PT-BR]&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Apresentamos o &lt;strong&gt;GAIA (Gemma-3-Gaia-PT-BR-4b-it)&lt;/strong&gt;, nosso novo modelo de linguagem aberto, feito e otimizado para o &lt;strong&gt;Português do Brasil!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;O que o GAIA traz?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Foco no PT-BR:&lt;/strong&gt; Treinado em 13 BILHÕES de tokens de dados brasileiros de alta qualidade.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base:&lt;/strong&gt; google/gemma-3-4b-pt (Gemma 3 de 4B de parâmetros).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inovador:&lt;/strong&gt; Usa uma técnica de &amp;quot;fusão de pesos&amp;quot; para seguir instruções (dispensa SFT tradicional!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resultados:&lt;/strong&gt; Superou o Gemma base no benchmark ENEM 2024!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quem fez:&lt;/strong&gt; Parceria entre entidades brasileiras (ABRAIA, CEIA-UFG, Nama, Amadeus AI) e Google DeepMind.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Licença:&lt;/strong&gt; Gemma.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Para que usar?&lt;/strong&gt;&lt;br /&gt; Ótimo para chat, perguntas/respostas, resumo, criação de textos e como base para fine-tuning em PT-BR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fhuggingface.co%2FCEIA-UFG%2FGemma-3-Gaia-PT-BR-4b-it"&gt;https://huggingface.co/CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2410.10739"&gt;https://arxiv.org/pdf/2410.10739&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ffgnetto"&gt; /u/ffgnetto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb9zhl/gaia_new_gemma3_4b_for_brazilian_portuguese_um/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T14:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb5rm2</id>
    <title>Thoughts on hardware price optimisarion for LLMs?</title>
    <updated>2025-06-14T10:43:36+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"&gt; &lt;img alt="Thoughts on hardware price optimisarion for LLMs?" src="https://preview.redd.it/iauc7homgv6f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b99a0e6ffa012285066fa9e8761a8662f56ac51a" title="Thoughts on hardware price optimisarion for LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Graph related (gpt-4o with with web search) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iauc7homgv6f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb5rm2/thoughts_on_hardware_price_optimisarion_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T10:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbbafh</id>
    <title>Why local LLM?</title>
    <updated>2025-06-14T15:25:15+00:00</updated>
    <author>
      <name>/u/Beginning_Many324</name>
      <uri>https://old.reddit.com/user/Beginning_Many324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm about to install Ollama and try a local LLM but I'm wondering what's possible and are the benefits apart from privacy and cost saving?&lt;br /&gt; My current memberships:&lt;br /&gt; - Claude AI&lt;br /&gt; - Cursor AI &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beginning_Many324"&gt; /u/Beginning_Many324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbbafh/why_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T15:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lbcfjz</id>
    <title>How much VRAM do you have and what's your daily-driver model?</title>
    <updated>2025-06-14T16:14:55+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what everyone is using day to day, locally, and what hardware they're using.&lt;/p&gt; &lt;p&gt;If you're using a quantized version of a model please say so!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lbcfjz/how_much_vram_do_you_have_and_whats_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T16:14:55+00:00</published>
  </entry>
</feed>
