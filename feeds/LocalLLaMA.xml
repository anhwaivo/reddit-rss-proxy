<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-03T23:24:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mgo662</id>
    <title>Jin 3.5 - Does anyone know anything about this model?</title>
    <updated>2025-08-03T16:53:16+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I literally can't find anything on this model. I saw somewhere on discord that it's similar to claude (which I doubt). any info? and no i'm not promoting this website or any bs like that idk anything about it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://jin.elpa.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgo662/jin_35_does_anyone_know_anything_about_this_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgo662/jin_35_does_anyone_know_anything_about_this_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3i48</id>
    <title>HRM solved thinking more than current "thinking" models (this needs more hype)</title>
    <updated>2025-08-02T22:44:39+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article: &lt;a href="https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context:&lt;/p&gt; &lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href="https://x.com/VictorTaelin/status/1950512015899840768"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt; &lt;a href="https://github.com/sapientinc/HRM/issues/12"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgr13d</id>
    <title>What’s the Best Open-Source Small LLM (≤ 8B) for Agentic Web Page Interactions?</title>
    <updated>2025-08-03T18:45:33+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m looking for recommendations for &lt;strong&gt;open-source multimoal LLMs no larger than 8B parameters&lt;/strong&gt; that perform well as &lt;em&gt;agents&lt;/em&gt; for interacting with web pages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context / Constraints:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Max size:&lt;/strong&gt; 8B params (need to run locally on an 8 GB GPU without major slowdowns)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Complex browser automation — navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent setup:&lt;/strong&gt; Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; I can run FP16 or quantized (8-bit/4-bit) models if that helps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Which small &lt;strong&gt;open-source multimodal models&lt;/strong&gt; have you found most capable for this kind of task?&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;quantized&lt;/strong&gt; versions you recommend for best VRAM fit + speed on consumer GPUs?&lt;/li&gt; &lt;li&gt;Have you seen measurable differences between models in &lt;strong&gt;agentic benchmarks&lt;/strong&gt; like Mind2Web, WebArena, or WorkArena?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T18:45:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgvbw6</id>
    <title>Looking to build or buy a mini pc for LLM</title>
    <updated>2025-08-03T21:36:36+00:00</updated>
    <author>
      <name>/u/nemuro87</name>
      <uri>https://old.reddit.com/user/nemuro87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.&lt;/p&gt; &lt;p&gt;I now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB&lt;/p&gt; &lt;p&gt;I would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.&lt;/p&gt; &lt;p&gt;Based on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500&lt;/li&gt; &lt;li&gt;21 t/s - 120 GB/s Mac Mini M4 32GB 1000$&lt;/li&gt; &lt;li&gt;46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$&lt;/li&gt; &lt;li&gt;43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro&lt;/li&gt; &lt;li&gt;410 GB/s Mac Studio M4 Max 36-128 GB tons of $&lt;/li&gt; &lt;li&gt;819 GB/s Mac Studio M3 Ultra up to 512GB tons of $&lt;/li&gt; &lt;li&gt;132 t/s - 750 GB/s RTX 4090 24GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Because I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I need something between M4 and the GPU in terms of speed.&lt;/li&gt; &lt;li&gt;I need 32GB but could do with more if it’s not a ripoff.&lt;/li&gt; &lt;li&gt;Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale&lt;/li&gt; &lt;li&gt;For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.&lt;/li&gt; &lt;li&gt;M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nemuro87"&gt; /u/nemuro87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T21:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgc0v0</id>
    <title>I created an app to run local AI as if it were the App Store</title>
    <updated>2025-08-03T06:12:39+00:00</updated>
    <author>
      <name>/u/Deivih-4774</name>
      <uri>https://old.reddit.com/user/Deivih-4774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt; &lt;img alt="I created an app to run local AI as if it were the App Store" src="https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg" title="I created an app to run local AI as if it were the App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; — a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt; &lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt; &lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href="https://getdione.app"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly… &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt; &lt;p&gt;The project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://getdione.app/github"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deivih-4774"&gt; /u/Deivih-4774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mgc0v0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg5xlb</id>
    <title>I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system</title>
    <updated>2025-08-03T00:41:41+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt; &lt;p&gt;🧠 Features:&lt;/p&gt; &lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt; &lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt; &lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt; &lt;p&gt;- Cross-platform and fully tested&lt;/p&gt; &lt;p&gt;- Open source and modular&lt;/p&gt; &lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/savantskie/persistent-ai-memory"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T00:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgl1qz</id>
    <title>Is EXL3 doomed?</title>
    <updated>2025-08-03T14:45:24+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"&gt; &lt;img alt="Is EXL3 doomed?" src="https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=778afd8299a6ccb54136a78390cc8473e58bebed" title="Is EXL3 doomed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turboderp-org/exllamav3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgbs6r</id>
    <title>We enabled Multi-GPU training in Unsloth AI — a feature that’s usually paid — using just 2 Copilot prompts!</title>
    <updated>2025-08-03T05:58:01+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/oevortex/unsloth"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgt2om</id>
    <title>Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?</title>
    <updated>2025-08-03T20:07:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"&gt; &lt;img alt="Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?" src="https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=089c7afe3ec4ed1899bdb5dd07aa791fcb05f74a" title="Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e1348s852vgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T20:07:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgccyc</id>
    <title>ByteDance drops Seed-Prover</title>
    <updated>2025-08-03T06:34:03+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt; &lt;p&gt;The key innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt; &lt;p&gt;Check simple explanantion here : &lt;a href="https://www.youtube.com/watch?v=os1QcHEpgZQ"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2507.23726"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgm8d3</id>
    <title>Daydreaming of a new Gemma model</title>
    <updated>2025-08-03T15:33:53+00:00</updated>
    <author>
      <name>/u/Jazzlike_Source_5983</name>
      <uri>https://old.reddit.com/user/Jazzlike_Source_5983</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only person who can't stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I'm gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere's 112 Command A Vision if not for the license) - I just can't help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jazzlike_Source_5983"&gt; /u/Jazzlike_Source_5983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T15:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgv53t</id>
    <title>GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?</title>
    <updated>2025-08-03T21:28:42+00:00</updated>
    <author>
      <name>/u/jcmyang</name>
      <uri>https://old.reddit.com/user/jcmyang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on: I tried to reproduce the Flappy Bird game featured in the &lt;a href="http://z.ai/blog/glm-4.5"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.&lt;/p&gt; &lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant. But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt. This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt; &lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem? Here's the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt; &lt;p&gt;PS. I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio. Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcmyang"&gt; /u/jcmyang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T21:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgn94g</id>
    <title>If Horizon Models is not from OpenAI, who would be?</title>
    <updated>2025-08-03T16:15:20+00:00</updated>
    <author>
      <name>/u/AMOVCS</name>
      <uri>https://old.reddit.com/user/AMOVCS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It's big enough to be smart without being so huge it steps on OpenAI’s toes. In my experience with the model, here some notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It works &lt;em&gt;really&lt;/em&gt; well with agents and tools&lt;/li&gt; &lt;li&gt;It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.&lt;/li&gt; &lt;li&gt;It’s fantastic with languages other than English, a weakness often sees in Chinese models.&lt;/li&gt; &lt;li&gt;It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models&lt;/li&gt; &lt;li&gt;The way it writes is very similar to OpenAI’s style.&lt;/li&gt; &lt;li&gt;Plus, whoever made this has &lt;em&gt;serious&lt;/em&gt; computing power... they're giving away billions of tokens for &amp;quot;free&amp;quot; at a really fast speed&lt;/li&gt; &lt;li&gt;The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But ok, lets consider other players:&lt;br /&gt; - Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly&lt;br /&gt; - Anthropic: Naah&lt;br /&gt; - Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don't see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.&lt;br /&gt; - Amazon or Microsoft: Would be my second guess&lt;br /&gt; - Google: Naah, they have Aistudio, when wants feedback they launch the model there&lt;br /&gt; - IBM or Cohere: Hard to think, but they are very capable companies&lt;/p&gt; &lt;p&gt;Honestly, it’s hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren’t fans of CloseAI, but if they say they’re releasing an open-source model, let’s be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.&lt;/p&gt; &lt;p&gt;So, what you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMOVCS"&gt; /u/AMOVCS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgis6h</id>
    <title>NVIDIA's "Highly Optimistic" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues</title>
    <updated>2025-08-03T13:06:01+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"&gt; &lt;img alt="NVIDIA's &amp;quot;Highly Optimistic&amp;quot; DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues" src="https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09c3744f2431f90355d37d937f1352192cc87780" title="NVIDIA's &amp;quot;Highly Optimistic&amp;quot; DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mghy1u</id>
    <title>qihoo360/Light-IF-32B</title>
    <updated>2025-08-03T12:24:28+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"&gt; &lt;img alt="qihoo360/Light-IF-32B" src="https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73" title="qihoo360/Light-IF-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt; &lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt; &lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt; &lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt; &lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt; &lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/Light-IF-32B"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;technical report &lt;a href="https://huggingface.co/papers/2503.10460"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous popular models by this company:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/Light-R1-32B"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6vaf0crhrsgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T12:24:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggku0</id>
    <title>XBai-04 Is It Real?</title>
    <updated>2025-08-03T11:07:25+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt; &lt;img alt="XBai-04 Is It Real?" src="https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg" title="XBai-04 Is It Real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt; &lt;p&gt;Another open model outperforms closed ones!&lt;br /&gt; XBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic's Claude Opus.&lt;/p&gt; &lt;p&gt;•Parameters: 32.8 B •Training: Long-CoT RL + Process Reward Learning (SPRM) •Benchmarks (High-Modus): •AIME24: 86.5 •AIME25: 77.9 •LiveCodeBench v5: 67.2 •C-EVAL: 89.7&lt;/p&gt; &lt;p&gt;🔗Open source weights: &lt;a href="https://huggingface.co/MetaStoneTec/XBai-o4"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mggku0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T11:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgnwnx</id>
    <title>Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!</title>
    <updated>2025-08-03T16:42:24+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"&gt; &lt;img alt="Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!" src="https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189" title="Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgmx8w</id>
    <title>Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM</title>
    <updated>2025-08-03T16:01:53+00:00</updated>
    <author>
      <name>/u/dlp_randombk</name>
      <uri>https://old.reddit.com/user/dlp_randombk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"&gt; &lt;img alt="Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM" src="https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d" title="Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dlp_randombk"&gt; /u/dlp_randombk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/randombk/chatterbox-vllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgjlek</id>
    <title>Are Chinese LLM companies effectively price dumping?</title>
    <updated>2025-08-03T13:43:23+00:00</updated>
    <author>
      <name>/u/uutnt</name>
      <uri>https://old.reddit.com/user/uutnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt; &lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about &lt;a href="https://gwern.net/complement"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt; &lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt; &lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href="https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt; &lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt; &lt;p&gt;What prompted this question, was the &lt;a href="https://youtu.be/mYDSSRS-B5U?t=2203"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uutnt"&gt; /u/uutnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgny8p</id>
    <title>When DeepSeek r2?</title>
    <updated>2025-08-03T16:44:12+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"&gt; &lt;img alt="When DeepSeek r2?" src="https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94" title="When DeepSeek r2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dz0i0w1j2ugf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgpb8t</id>
    <title>Reimplemention of Qwen 2 from scratch</title>
    <updated>2025-08-03T17:38:43+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch A few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt; &lt;p&gt;What makes this build special: ✅ Implemented without access to source code 📖 Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers 🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!) ⚠️ Does not support Mixture of Experts (MoE) yet&lt;/p&gt; &lt;p&gt;This project pushed my understanding of transformer architectures even further, and I’m excited to keep going. If you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/introlix/Swiftlet"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt; Kaggle: &lt;a href="https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T17:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgky8g</id>
    <title>This might be the largest un-aligned open-source model</title>
    <updated>2025-08-03T14:41:20+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgiyg4</id>
    <title>Why doesn't "OpenAI" just release one of the models they already have? Like 3.5</title>
    <updated>2025-08-03T13:14:11+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are they really gonna train a model that's absolutely useless to give to us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgtboa</id>
    <title>Horizon Beta is OpenAI</title>
    <updated>2025-08-03T20:16:52+00:00</updated>
    <author>
      <name>/u/MiddleLobster9191</name>
      <uri>https://old.reddit.com/user/MiddleLobster9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt; &lt;img alt="Horizon Beta is OpenAI" src="https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg" title="Horizon Beta is OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiddleLobster9191"&gt; /u/MiddleLobster9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T20:16:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgkiti</id>
    <title>Use local LLM to neutralise the headers on the web</title>
    <updated>2025-08-03T14:23:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt; &lt;img alt="Use local LLM to neutralise the headers on the web" src="https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a68cd5e26b5f21da9a193d716904dfd8485c857" title="Use local LLM to neutralise the headers on the web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt; &lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt; &lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href="https://github.com/av/unhype"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/niaha18uctgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:23:05+00:00</published>
  </entry>
</feed>
