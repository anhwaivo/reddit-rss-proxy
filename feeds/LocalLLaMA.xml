<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-16T14:49:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1knh0dq</id>
    <title>Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes</title>
    <updated>2025-05-15T19:27:35+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"&gt; &lt;img alt="Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes" src="https://external-preview.redd.it/fOELlCefhPVcX_I27jAk8-oOBjhtXlke2ANY2PCUgkA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1294dd31165e64cd951bf768591cb3a24a57502f" title="Created a tool that converts podcasts into clean speech datasets - handles diarization, removes overlapping speech, and transcribes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ReisCook/Voice_Extractor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knh0dq/created_a_tool_that_converts_podcasts_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T19:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1knnyco</id>
    <title>Mistral Small/Medium vs Qwen 3 14/32B</title>
    <updated>2025-05-16T00:37:51+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since things have been a little slow over the past couple weeks, figured throw mistral's new releases against Qwen3. I chose 14/32B, because the scores seem in the same ballpark.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=IgyP5EWW6qk"&gt;https://www.youtube.com/watch?v=IgyP5EWW6qk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key Findings:&lt;/p&gt; &lt;p&gt;Mistral medium is definitely an improvement over mistral small, but not by a whole lot, mistral small in itself is a very strong model. Qwen is a clear winner in coding, even the 14b beats both mistral models. The NER (structured json) test Qwen struggles but this is because of its weakness in non English questions. RAG I feel mistral medium is better than the rest. Overall, I feel Qwen 32b &amp;gt; mistral medium &amp;gt; mistral small &amp;gt; Qwen 14b. But again, as with anything llm, YMMV.&lt;/p&gt; &lt;p&gt;Here is a summary table&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Timestamp&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Harmful Question Detection&lt;/td&gt; &lt;td align="left"&gt;Mistral Medium&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mistral Small&lt;/td&gt; &lt;td align="left"&gt;95%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 14B&lt;/td&gt; &lt;td align="left"&gt;75%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=236"&gt;03:56&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Named Entity Recognition&lt;/td&gt; &lt;td align="left"&gt;Both Mistral&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=412"&gt;06:52&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Both Qwen&lt;/td&gt; &lt;td align="left"&gt;80%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=412"&gt;06:52&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SQL Query Generation&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 models&lt;/td&gt; &lt;td align="left"&gt;Perfect&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=602"&gt;10:02&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Both Mistral&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=691"&gt;11:31&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Retrieval Augmented Generation&lt;/td&gt; &lt;td align="left"&gt;Mistral Medium&lt;/td&gt; &lt;td align="left"&gt;93%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;92.5%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mistral Small&lt;/td&gt; &lt;td align="left"&gt;90.75%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=786"&gt;13:06&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 14B&lt;/td&gt; &lt;td align="left"&gt;90%&lt;/td&gt; &lt;td align="left"&gt;[&lt;a href="http://www.youtube.com/watch?v=IgyP5EWW6qk&amp;amp;t=796"&gt;13:16&lt;/a&gt;]&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knnyco/mistral_smallmedium_vs_qwen_3_1432b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T00:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1knvgva</id>
    <title>Why do I need to share my contact information/get a HF token with Mistral to use their models in vLLM but not with Ollama?</title>
    <updated>2025-05-16T08:04:25+00:00</updated>
    <author>
      <name>/u/sebovzeoueb</name>
      <uri>https://old.reddit.com/user/sebovzeoueb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working with Ollama on a locally hosted AI project, and I was looking to try some alternatives to see what the performance is like. vLLM appears to be a performance focused alternative so I've got that downloaded in Docker, however there are models it can't use without accepting to share my contact information on the HuggingFace website and setting the HF token in the environment for vLLM. I would like to avoid this step as one of the selling points of the project I'm working on is that it's easy for the user to install, and having the user make an account somewhere and get an access token is contrary to that goal.&lt;/p&gt; &lt;p&gt;How come Ollama has direct access to the Mistral models without requiring this extra step? Furthermore, the Mistral website says 7B is released under the Apache 2.0 license and can be &amp;quot;used without restrictions&amp;quot;, so could someone please shed some light on why they need my contact information if I go through HF, and if there's an alternative route as a workaround? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebovzeoueb"&gt; /u/sebovzeoueb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knvgva/why_do_i_need_to_share_my_contact_informationget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knvgva/why_do_i_need_to_share_my_contact_informationget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knvgva/why_do_i_need_to_share_my_contact_informationget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T08:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1knl587</id>
    <title>Meta is delaying the rollout of its flagship AI model (WSJ)</title>
    <updated>2025-05-15T22:20:53+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"&gt; &lt;img alt="Meta is delaying the rollout of its flagship AI model (WSJ)" src="https://preview.redd.it/gdsyodsot01f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a311d3625dba19a91ac3b06067a91ee8aa3bc7a" title="Meta is delaying the rollout of its flagship AI model (WSJ)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the article: &lt;a href="https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7"&gt;https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gdsyodsot01f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knl587/meta_is_delaying_the_rollout_of_its_flagship_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T22:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1knji91</id>
    <title>Soon if a model architecture is supported by "transformers", you can expect it to be supported in the rest of the ecosystem.</title>
    <updated>2025-05-15T21:10:11+00:00</updated>
    <author>
      <name>/u/behradkhodayar</name>
      <uri>https://old.reddit.com/user/behradkhodayar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"&gt; &lt;img alt="Soon if a model architecture is supported by &amp;quot;transformers&amp;quot;, you can expect it to be supported in the rest of the ecosystem." src="https://external-preview.redd.it/vXXJM0Qn_BM8I_YhlKgXpMf8jgjpmMwwyNtmu7BK1pM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1dfc69125f94a6206257f867746e11c5b8f79e49" title="Soon if a model architecture is supported by &amp;quot;transformers&amp;quot;, you can expect it to be supported in the rest of the ecosystem." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More model interoperability through HF's joint efforts w lots of model builders.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/behradkhodayar"&gt; /u/behradkhodayar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/transformers-model-definition"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knji91/soon_if_a_model_architecture_is_supported_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T21:10:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1tg5</id>
    <title>If you are comparing models, please state the task you are using them for!</title>
    <updated>2025-05-16T14:10:07+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The amount of posts like &amp;quot;Why is deepseek so much better than qwen 235,&amp;quot; with no information about the task that the poster is comparing the models on, is maddening. ALL models' performance levels vary across domains, and many models are highly domain specific. Some people are creating waifus, some are coding, some are conducting medical research, etc. &lt;/p&gt; &lt;p&gt;The posts read like &amp;quot;The Miata is the absolute superior vehicle over the Cessna Skyhawk. It has been the best driving experience since I used my Rolls Royce as a submarine&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:10:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kntez5</id>
    <title>🚀 Embedding 10,000 text chunks per second on a CPU?!</title>
    <updated>2025-05-16T05:41:32+00:00</updated>
    <author>
      <name>/u/aagmon</name>
      <uri>https://old.reddit.com/user/aagmon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When working with large volumes of documents, embedding can quickly become both a performance bottleneck and a cost driver. I recently experimented with &lt;em&gt;static embedding&lt;/em&gt; — and was blown away by the speed. No self-attention, no feed-forward layers, just direct token key access. The result? Incredibly fast embedding with minimal overhead.&lt;br /&gt; I built a lightweight sample implementation in Rust using HF Candle and exposed it via Python so you can try it yourself.&lt;/p&gt; &lt;p&gt;Checkout the repo at: &lt;a href="https://github.com/a-agmon/static-embedding"&gt;https://github.com/a-agmon/static-embedding&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Read more about static embedding: &lt;a href="https://huggingface.co/blog/static-embeddings"&gt;https://huggingface.co/blog/static-embeddings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;or just give it a try:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install static_embed from static_embed import Embedder # 1. Use the default public model (no args) embedder = Embedder() # 2. OR specify your own base-URL that hosts the weights/tokeniser # (must contain the same two files: ``model.safetensors`` &amp;amp; ``tokenizer.json``) # custom_url = &amp;quot;https://my-cdn.example.com/static-retrieval-mrl-en-v1&amp;quot; # embedder = Embedder(custom_url) texts = [&amp;quot;Hello world!&amp;quot;, &amp;quot;Rust + Python via PyO3&amp;quot;] embeddings = embedder.embed(texts) print(len(embeddings), &amp;quot;embeddings&amp;quot;, &amp;quot;dimension&amp;quot;, len(embeddings[0])) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aagmon"&gt; /u/aagmon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntez5/embedding_10000_text_chunks_per_second_on_a_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntez5/embedding_10000_text_chunks_per_second_on_a_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kntez5/embedding_10000_text_chunks_per_second_on_a_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T05:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1knx47e</id>
    <title>Qwen3 local 14B Q4_K_M or 30B A3B Q2_K_L who has higher quality</title>
    <updated>2025-05-16T10:04:51+00:00</updated>
    <author>
      <name>/u/Consistent_Winner596</name>
      <uri>https://old.reddit.com/user/Consistent_Winner596</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 comes in the xxB AxB flavors and that can be run locally. If you choose said combination 14B Q4_K_M vs 30B A3B Q2_K_L the performance speed wise in generation matches given the same context size on my test bench. The question is (and what I don't understand) how does the agents affect the quality of the output? Could I read 14B as 14B A14B meaning 1Agent is active with the full 14B over all layers and 30B A3B means 10Agents are active parallel on different layers with each 3B or how does it work technically?&lt;/p&gt; &lt;p&gt;Normally my rule of thumb is higher B with lower Q above Q2 is always better than lower B with higher Q. In this special case I am unsure if that still applies.&lt;/p&gt; &lt;p&gt;Did someone of you own a benchmark that can test quality of outputs and perception and would be willing to test this rather small quants against each other? The normal benchmarks only test the full versions, but for reasonable local it must be a smaller approach here to fit memory and speed demands. What is the quality?&lt;/p&gt; &lt;p&gt;Thank you for technical inputs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Winner596"&gt; /u/Consistent_Winner596 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knx47e/qwen3_local_14b_q4_k_m_or_30b_a3b_q2_k_l_who_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knx47e/qwen3_local_14b_q4_k_m_or_30b_a3b_q2_k_l_who_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knx47e/qwen3_local_14b_q4_k_m_or_30b_a3b_q2_k_l_who_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T10:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2kzx</id>
    <title>Photoshop using Local Computer Use agents.</title>
    <updated>2025-05-16T14:42:18+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2kzx/photoshop_using_local_computer_use_agents/"&gt; &lt;img alt="Photoshop using Local Computer Use agents." src="https://external-preview.redd.it/Y3luNno5cXJvNTFmMRXG3T7XldLhbQ1-zZgDOchlvuRH1Qq3ebSvcq1i84vf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abfef187ef56996b22e9ef1e4357191d68c7cac6" title="Photoshop using Local Computer Use agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Photoshop using c/ua.&lt;/p&gt; &lt;p&gt;No code. Just a user prompt, picking models and a Docker, and the right agent loop.&lt;/p&gt; &lt;p&gt;A glimpse at the more managed experience c/ua is building to lower the barrier for casual vibe-coders.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jhyeu60so51f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2kzx/photoshop_using_local_computer_use_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2kzx/photoshop_using_local_computer_use_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1knqfw3</id>
    <title>Simple generation speed test with 2x Arc B580</title>
    <updated>2025-05-16T02:48:57+00:00</updated>
    <author>
      <name>/u/prompt_seeker</name>
      <uri>https://old.reddit.com/user/prompt_seeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt; &lt;img alt="Simple generation speed test with 2x Arc B580" src="https://b.thumbs.redditmedia.com/Z5P1XL9jS1O1ASJhe2Dv2-OiirlE7J5MmCa_bH9LdMQ.jpg" title="Simple generation speed test with 2x Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There have been recent rumors about the B580 24GB, so I ran some new tests using my B580s. I used llama.cpp with some backends to test text generation speed using google_gemma-3-27b-it-IQ4_XS.gguf.&lt;/p&gt; &lt;h1&gt;Tested backends&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;IPEX-LLM llama.cpp &lt;ul&gt; &lt;li&gt;build: 1 (3b94b45) with Intel(R) oneAPI DPC++/C++ Compiler 2025.0.4 (2025.0.4.20241205) for x86_64-unknown-linux-gnu&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;official llama.cpp SYCL &lt;ul&gt; &lt;li&gt;build: 5400 (c6a2c9e7) with Intel(R) oneAPI DPC++/C++ Compiler 2025.1.1 (2025.1.1.20250418) for x86_64-unknown-linux-gnu&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;official llama.cpp VULKAN &lt;ul&gt; &lt;li&gt;build: 5395 (9c404ed5) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu &lt;em&gt;(from release)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Base command&lt;/h1&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m AI-12/google_gemma-3-27b-it-Q4_K_S.gguf -ngl 99 -c 8192 -b 512 -p &amp;quot;Why is sky blue?&amp;quot; -no-cnv&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Build&lt;/th&gt; &lt;th align="left"&gt;&lt;code&gt;-fa&lt;/code&gt; Option&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Eval Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Total Tokens Generated&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;3b94b45 (IPEX-LLM)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;52.22&lt;/td&gt; &lt;td align="left"&gt;8.18&lt;/td&gt; &lt;td align="left"&gt;393&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3b94b45 (IPEX-LLM)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;(corrupted text)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;c6a2c9e7 (SYCL)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;13.72&lt;/td&gt; &lt;td align="left"&gt;5.66&lt;/td&gt; &lt;td align="left"&gt;545&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;c6a2c9e7 (SYCL)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;10.73&lt;/td&gt; &lt;td align="left"&gt;5.04&lt;/td&gt; &lt;td align="left"&gt;362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9c404ed5 (vulkan)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;35.38&lt;/td&gt; &lt;td align="left"&gt;4.85&lt;/td&gt; &lt;td align="left"&gt;487&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9c404ed5 (vulkan)&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;32.99&lt;/td&gt; &lt;td align="left"&gt;4.78&lt;/td&gt; &lt;td align="left"&gt;559&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Thoughts&lt;/h1&gt; &lt;p&gt;The results are disappointing. I previously tested google-gemma-2-27b-IQ4_XS.gguf with 2x 3060 GPUs, and achieved around 15 t/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xuijd9iz121f1.png?width=606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b280fe6c9e3ca8f752ae59208008fed818f1d8d1"&gt;https://preview.redd.it/xuijd9iz121f1.png?width=606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b280fe6c9e3ca8f752ae59208008fed818f1d8d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With image generation models, the B580 achieves generation speeds close to the RTX 4070, but its performance with LLMs seems to fall short of expectations.&lt;/p&gt; &lt;p&gt;I don’t know how much the PRO version (B580 with 24GB) will cost, but if you’re looking for a budget-friendly way to get more RAM, it might be better to consider the AI MAX+ 395 (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;I’ve heard it can reach 6.4 tokens per second with 32B Q8&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;I tested this on Linux, but since Arc GPUs are said to perform better on Windows, you might get faster results there. If anyone has managed to get better performance with the B580, please let me know in the comments.&lt;/p&gt; &lt;p&gt;* Interestingly, generation is fast up to around 100–200 tokens, but then it gradually slows down. so using&lt;code&gt;llama-bench&lt;/code&gt; with tg512/pp128 is not a good way to test this GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prompt_seeker"&gt; /u/prompt_seeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knqfw3/simple_generation_speed_test_with_2x_arc_b580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T02:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1knorbe</id>
    <title>Grok prompts are now open source on GitHub</title>
    <updated>2025-05-16T01:19:49+00:00</updated>
    <author>
      <name>/u/FreemanDave</name>
      <uri>https://old.reddit.com/user/FreemanDave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knorbe/grok_prompts_are_now_open_source_on_github/"&gt; &lt;img alt="Grok prompts are now open source on GitHub" src="https://external-preview.redd.it/5uwLTcCO_xgjsJvNop0QnSOEwQGDRqjKJtrO-U6w_F8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29eff9f426f9096b211ed527068b986189b4c955" title="Grok prompts are now open source on GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreemanDave"&gt; /u/FreemanDave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/xai-org/grok-prompts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knorbe/grok_prompts_are_now_open_source_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knorbe/grok_prompts_are_now_open_source_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T01:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1knjm0s</id>
    <title>Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24</title>
    <updated>2025-05-15T21:14:28+00:00</updated>
    <author>
      <name>/u/TokyoCapybara</name>
      <uri>https://old.reddit.com/user/TokyoCapybara</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"&gt; &lt;img alt="Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24" src="https://external-preview.redd.it/aTdnbWV3c25kMDFmMckumtgWbpWBlQZ_vRBN65fbuS7eF6LKJlM_WmjlxhmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81dfc6fb92a74b03033c4ffb473c5f4ea11f0bde" title="Qwen3 4B running at ~20 tok/s on Samsung Galaxy 24" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow-up on a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kckxgg/qwen3_06b_running_at_75_toks_on_iphone_15_pro/"&gt;previous post&lt;/a&gt;, but this time for Android and on a larger Qwen3 model for those who are interested. Here is 4-bit quantized Qwen3 4B with thinking mode running on a Samsung Galaxy 24 using ExecuTorch - runs at up to 20 tok/s.&lt;/p&gt; &lt;p&gt;Instructions on how to export and run the model on ExecuTorch &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokyoCapybara"&gt; /u/TokyoCapybara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/drks9osnd01f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knjm0s/qwen3_4b_running_at_20_toks_on_samsung_galaxy_24/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T21:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1knh1yd</id>
    <title>Meta delaying the release of Behemoth</title>
    <updated>2025-05-15T19:29:28+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7"&gt;https://www.wsj.com/tech/ai/meta-is-delaying-the-rollout-of-its-flagship-ai-model-f4b105f7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knh1yd/meta_delaying_the_release_of_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T19:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2gq1</id>
    <title>AM-Thinking-v1</title>
    <updated>2025-05-16T14:37:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt; &lt;img alt="AM-Thinking-v1" src="https://b.thumbs.redditmedia.com/A9KnNLWs_QCLkRUARjeLGj48uiRGS0tLMTp-saM_Uyk.jpg" title="AM-Thinking-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/a-m-team/AM-Thinking-v1"&gt;https://huggingface.co/a-m-team/AM-Thinking-v1&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We release &lt;strong&gt;AM-Thinking‑v1&lt;/strong&gt;, a 32B dense language model focused on enhancing reasoning capabilities. Built on Qwen 2.5‑32B‑Base, AM-Thinking‑v1 shows strong performance on reasoning benchmarks, comparable to much larger MoE models like &lt;strong&gt;DeepSeek‑R1&lt;/strong&gt;, &lt;strong&gt;Qwen3‑235B‑A22B&lt;/strong&gt;, &lt;strong&gt;Seed1.5-Thinking&lt;/strong&gt;, and larger dense model like &lt;strong&gt;Nemotron-Ultra-253B-v1&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.08311"&gt;https://arxiv.org/abs/2505.08311&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://a-m-team.github.io/am-thinking-v1/"&gt;https://a-m-team.github.io/am-thinking-v1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/79z2klmbn51f1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18a3b5a0d06b75e6712891b7c19853ec1de3e737"&gt;https://preview.redd.it/79z2klmbn51f1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18a3b5a0d06b75e6712891b7c19853ec1de3e737&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;I'm not affiliated with the model provider, just sharing the news.&lt;/strong&gt;*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2mq7</id>
    <title>what happened to Stanford</title>
    <updated>2025-05-16T14:44:17+00:00</updated>
    <author>
      <name>/u/BoringAd6806</name>
      <uri>https://old.reddit.com/user/BoringAd6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt; &lt;img alt="what happened to Stanford" src="https://preview.redd.it/l9ap08t4p51f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99406294d0642388d4c739930b9569d685129d1" title="what happened to Stanford" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoringAd6806"&gt; /u/BoringAd6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l9ap08t4p51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko0d4w</id>
    <title>ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps</title>
    <updated>2025-05-16T13:05:35+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"&gt; &lt;img alt="ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps" src="https://external-preview.redd.it/pEL8WVAo4mDFDpBgOq60y0m4cdA7556rb7t3GIF-8DM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b2baa5a8ddd53ef6c8acdfc70e71cf12f8444d6" title="ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ValiantLabs/Qwen3-14B-Esper3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kntnfn</id>
    <title>New Wayfarer</title>
    <updated>2025-05-16T05:57:44+00:00</updated>
    <author>
      <name>/u/ScavRU</name>
      <uri>https://old.reddit.com/user/ScavRU</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"&gt; &lt;img alt="New Wayfarer" src="https://external-preview.redd.it/LU2gTXNE2BU0Un_eM36qvVexiACBVgpQzKg0ygmj_bE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50b539fa8660c67ab3d2b2b6de1d79bf8ba7373b" title="New Wayfarer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScavRU"&gt; /u/ScavRU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Harbinger-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kntnfn/new_wayfarer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T05:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kndp9f</id>
    <title>TTS Fine-tuning now in Unsloth!</title>
    <updated>2025-05-15T17:14:19+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt; &lt;img alt="TTS Fine-tuning now in Unsloth!" src="https://external-preview.redd.it/bXI4dnBsa3phejBmMfDzohHQ2IN6C0pCi0KaT-g2AEXeep08I3DgQhQN5vF7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57583ee26a1b7da14346a3cc45ff72ecbf34831b" title="TTS Fine-tuning now in Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Not the usual LLMs talk but we’re excited to announce that you can now train Text-to-Speech (TTS) models in &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;! Training is ~1.5x faster with 50% less VRAM compared to all other setups with FA2. :D&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support includes &lt;code&gt;Sesame/csm-1b&lt;/code&gt;, &lt;code&gt;OpenAI/whisper-large-v3&lt;/code&gt;, &lt;code&gt;CanopyLabs/orpheus-3b-0.1-ft&lt;/code&gt;, and any Transformer-style model including LLasa, Outte, Spark, and more.&lt;/li&gt; &lt;li&gt;The goal of TTS fine-tuning to minic voices, adapt speaking styles and tones, support new languages, handle specific tasks etc.&lt;/li&gt; &lt;li&gt;We’ve made notebooks to train, run, and save these models for free on Google Colab. Some models aren’t supported by llama.cpp and will be saved only as safetensors, but others should work. See our TTS docs and notebooks: &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The training process is similar to SFT, but the dataset includes audio clips with transcripts. We use a dataset called ‘Elise’ that embeds emotion tags like &amp;lt;sigh&amp;gt; or &amp;lt;laughs&amp;gt; into transcripts, triggering expressive audio that matches the emotion.&lt;/li&gt; &lt;li&gt;Since TTS models are usually small, you can train them using 16-bit LoRA, or go with FFT. Loading a 16-bit LoRA model is simple.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've uploaded most of the TTS models (quantized and original) to &lt;a href="https://huggingface.co/collections/unsloth/text-to-speech-tts-models-68007ab12522e96be1e02155"&gt;Hugging Face here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And here are our TTS notebooks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B"&gt;Sesame-CSM (1B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B"&gt;Orpheus-TTS (3B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb"&gt;Whisper Large V3&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B"&gt;Spark-TTS (0.5B)&lt;/a&gt;.ipynb)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thank you for reading and please do ask any questions!!&lt;/p&gt; &lt;p&gt;P.S. We also now support Qwen3 GRPO. We use the base model + a new custom proximity-based reward function to favor near-correct answers and penalize outliers. Pre-finetuning mitigates formatting bias and boosts evaluation accuracy via regex matching: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/faqjz7kzaz0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T17:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kno67v</id>
    <title>Ollama now supports multimodal models</title>
    <updated>2025-05-16T00:49:35+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"&gt; &lt;img alt="Ollama now supports multimodal models" src="https://external-preview.redd.it/pRVigNZNHcUydRnImgoAZkA_b3OfVw4eace1TFmQGPk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f3c5bd5d3b3d4eebb1d060ea3a5f9818c6d5026" title="Ollama now supports multimodal models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.7.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kno67v/ollama_now_supports_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T00:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1v1k</id>
    <title>I built a tiny Linux OS to make your LLMs actually useful on your machine</title>
    <updated>2025-05-16T14:12:00+00:00</updated>
    <author>
      <name>/u/iluxu</name>
      <uri>https://old.reddit.com/user/iluxu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt; &lt;img alt="I built a tiny Linux OS to make your LLMs actually useful on your machine" src="https://external-preview.redd.it/Opn0lWenfUSxX1FlZaKUoyxIpn8_sSk-rxtkMoj2byo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62958b7668163a6b32bc9aa0eddc4ec07f59c982" title="I built a tiny Linux OS to make your LLMs actually useful on your machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I’ve been working on llmbasedos, a minimal Arch-based Linux distro that turns your local environment into a first-class citizen for any LLM frontend (like Claude Desktop, VS Code, ChatGPT+browser, etc).&lt;/p&gt; &lt;p&gt;The problem: every AI app has to reinvent the wheel — file pickers, OAuth flows, plugins, sandboxing… The idea: expose local capabilities (files, mail, sync, agents) via a clean, JSON-RPC protocol called MCP (Model Context Protocol).&lt;/p&gt; &lt;p&gt;What you get: • An MCP gateway (FastAPI) that routes requests • Small Python daemons that expose specific features (FS, mail, sync, agents) • Auto-discovery via .cap.json — your new feature shows up everywhere • Optional offline mode (llama.cpp included), or plug into GPT-4o, Claude, etc.&lt;/p&gt; &lt;p&gt;It’s meant to be dev-first. Add a new capability in under 50 lines. Zero plugins, zero hacks — just a clean system-wide interface for your AI.&lt;/p&gt; &lt;p&gt;Open-core, Apache-2.0 license.&lt;/p&gt; &lt;p&gt;Curious to hear what features you’d build with it — happy to collab if anyone’s down!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iluxu"&gt; /u/iluxu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iluxu/llmbasedos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko27bi</id>
    <title>Did Standford HuggingFace account got Hacked?</title>
    <updated>2025-05-16T14:26:25+00:00</updated>
    <author>
      <name>/u/ObscuraMirage</name>
      <uri>https://old.reddit.com/user/ObscuraMirage</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObscuraMirage"&gt; /u/ObscuraMirage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0j4j7z8yl51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1knqap9</id>
    <title>Are we finally hitting THE wall right now?</title>
    <updated>2025-05-16T02:41:06+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw in multiple articles today that Llama Behemoth is delayed: &lt;a href="https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html"&gt;https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html&lt;/a&gt; . I tried the open models from Llama 4 and felt not that great progress. I am also getting underwhelming vibes from the qwen 3, compared to qwen 2.5. Qwen team used 36 trillion tokens to train these models, which even had trillions of STEM tokens in mid-training and did all sorts of post training, the models are good, but not that great of a jump as we expected. &lt;/p&gt; &lt;p&gt;With RL we definitely got a new paradigm on making the models think before speaking and this has led to great models like Deepseek R1, OpenAI O1, O3 and possibly the next ones are even greater, but the jump from O1 to O3 seems to be not that much, me being only a plus user and have not even tried the Pro tier. Anthropic Claude Sonnet 3.7 is not better than Sonnet 3.5, where the latest version seems to be good but mainly for programming and web development. I feel the same for Google where Gemini 2.5 Pro 1 seemed to be a level above the rest of the models, I finally felt that I could rely on a model and company, then they also rug pulled the model totally with Gemini 2.5 Pro 2 where I do not know how to access the version 1 and they are field testing a lot in lmsys arena which makes me wonder that they are not seeing those crazy jumps as they were touting.&lt;/p&gt; &lt;p&gt;I think Deepseek R2 will show us the ultimate conclusion on this, whether scaling this RL paradigm even further will make models smarter.&lt;/p&gt; &lt;p&gt;Do we really need a new paradigm? Or do we need to go back to architectures like T5? Or totally novel like JEPA from Yann Lecunn, twitter has hated him for not agreeing that the autoregressors can actually lead to AGI, but sometimes I feel it too with even the latest and greatest models do make very apparent mistakes and makes me wonder what would it take to actually have really smart and reliable models.&lt;/p&gt; &lt;p&gt;I love training models using SFT and RL especially GRPO, my favorite, I have even published some work on it and making pipelines for clients, but seems like when used in production for longer, the customer sentiment seems to always go down and not even maintain as well.&lt;/p&gt; &lt;p&gt;What do you think? Is my thinking in this saturation of RL for Autoregressor LLMs somehow flawed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T02:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1knv4bq</id>
    <title>Falcon-E: A series of powerful, fine-tunable and universal BitNet models</title>
    <updated>2025-05-16T07:38:42+00:00</updated>
    <author>
      <name>/u/JingweiZUO</name>
      <uri>https://old.reddit.com/user/JingweiZUO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TII announced today the release of Falcon-Edge, a set of compact language models with 1B and 3B parameters, sized at 600MB and 900MB respectively. They can also be reverted back to bfloat16 with little performance degradation.&lt;br /&gt; Initial results show solid performance: better than other small models (SmolLMs, Microsoft bitnet, Qwen3-0.6B) and comparable to Qwen3-1.7B, with 1/4 memory footprint.&lt;br /&gt; They also released a fine-tuning library, &lt;code&gt;onebitllms&lt;/code&gt;: &lt;a href="https://github.com/tiiuae/onebitllms"&gt;https://github.com/tiiuae/onebitllms&lt;/a&gt;&lt;br /&gt; Blogposts: &lt;a href="https://huggingface.co/blog/tiiuae/falcon-edge"&gt;https://huggingface.co/blog/tiiuae/falcon-edge&lt;/a&gt; / &lt;a href="https://falcon-lm.github.io/blog/falcon-edge/"&gt;https://falcon-lm.github.io/blog/falcon-edge/&lt;/a&gt;&lt;br /&gt; HF collection: &lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JingweiZUO"&gt; /u/JingweiZUO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T07:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1iob</id>
    <title>Ollama violating llama.cpp license for over a year</title>
    <updated>2025-05-16T13:57:38+00:00</updated>
    <author>
      <name>/u/op_loves_boobs</name>
      <uri>https://old.reddit.com/user/op_loves_boobs</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/op_loves_boobs"&gt; /u/op_loves_boobs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44003741"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko0khr</id>
    <title>Stanford has dropped AGI</title>
    <updated>2025-05-16T13:15:17+00:00</updated>
    <author>
      <name>/u/Abject-Huckleberry13</name>
      <uri>https://old.reddit.com/user/Abject-Huckleberry13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt; &lt;img alt="Stanford has dropped AGI" src="https://external-preview.redd.it/RLiqoJrn4RdLs0J4_egpcYM7T2LlLp_klpSUS3M3qFg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa5717d8c431adaa645d19436f3ab2adbc6cfc8" title="Stanford has dropped AGI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Huckleberry13"&gt; /u/Abject-Huckleberry13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Stanford/Rivermind-AGI-12B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:15:17+00:00</published>
  </entry>
</feed>
