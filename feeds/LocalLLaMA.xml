<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-11T11:05:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j83md3</id>
    <title>Could GEMMA-3 Be Unveiled at GDC 2025 (March 18)?</title>
    <updated>2025-03-10T17:05:48+00:00</updated>
    <author>
      <name>/u/hCKstp4BtL</name>
      <uri>https://old.reddit.com/user/hCKstp4BtL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129"&gt;https://schedule.gdconf.com/session/beyond-the-hype-real-world-applications-of-google-ai-in-gaming-presented-by-google-play/911129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;in this session description, we can read that they will talk about &amp;quot;Gemma models&amp;quot; (among other things). I think everyone knows about &amp;quot;Gemma 2&amp;quot; and there is no need to mention it because everyone knows how it works, right? Bigger chance is that they will show &amp;quot;Gemma 3&amp;quot; and they will release it shorly? because it seems to me that the deadline of May 20-21 (Google I/O) is a bit too late.&lt;/p&gt; &lt;p&gt;It looks like Google wants to focus the eyes of game developers on Gemma, so that they can combine the models with their games to create: “new AI-based game features and mechanics.”&lt;/p&gt; &lt;p&gt;... and to make it work, I think such a &amp;quot;Gemma 3&amp;quot; model should be prioritize with &amp;quot;perfect JSON generation&amp;quot; for the interface model&amp;lt;-&amp;gt;game and also improved instruction following.&lt;/p&gt; &lt;p&gt;I waiting for a small model (7b-9b) to be good enough to make a game with llm controlling npc (not only talk).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hCKstp4BtL"&gt; /u/hCKstp4BtL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83md3/could_gemma3_be_unveiled_at_gdc_2025_march_18/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:05:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8780s</id>
    <title>Kokoro: Improving LLM's Emotional Intelligence [Research]</title>
    <updated>2025-03-10T19:33:42+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yo community! Kokoro Research just dropped! It’s a prequel paper to upcoming research called, LOLI Trigger: Ludic Operant Learning Integration in Transcendent Emergence Triggering of LLMs’ about making AI more humane! Coming this week!&lt;/p&gt; &lt;p&gt;This one talks more about new classification approach which later can be directly merge into an LLM model!&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence"&gt;https://www.academia.edu/128122586/Kokoro_Improving_LLMs_Emotional_Intelligence&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can check other researches, especially TaMeR (novel training approach), and ELiTA (better datasets). Hope you like them! Note: this is mostly theoretical paper, do not expect too much math!&lt;/p&gt; &lt;p&gt;[THIS IS NOT AN AD, JUST SHARING STUFF WITH THE COMMUNITY]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8780s/kokoro_improving_llms_emotional_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j892ii</id>
    <title>RTX 3090 supply drying up on marketplaces in Europe</title>
    <updated>2025-03-10T20:51:17+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the flopped launches are leaving their traces in the GPU second hand markets. Even more so since the 4090 stopped production last fall already.&lt;/p&gt; &lt;p&gt;As popularity to self host models is on the rise and supply of new 24Gb+ cards stays dry, the all star for local AI models, the RTX 3090 is getting rare on marketplaces. In Switzerland they used to go for around CHF 650 - CHF 750. The lowest you find them now is 800.- if you're lucky, more likely CHF 900.-&lt;/p&gt; &lt;p&gt;Germany looks a little better at €650 the lowest but these are usually gone within three days and most supply is around €750 upwards. It's only a matter of time when sellers at the €650 mark will dry up.&lt;/p&gt; &lt;p&gt;On international Ebay the cards go for $800 upwards, used to be lower if I remember correctly.&lt;/p&gt; &lt;p&gt;What is your experience, are you looking for 3090s? What's your choice for your home servers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j892ii/rtx_3090_supply_drying_up_on_marketplaces_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T20:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j80hbo</id>
    <title>Hunyuan-TurboS.</title>
    <updated>2025-03-10T14:54:37+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/TXhunyuan/status/1899105803073958010"&gt;https://twitter.com/TXhunyuan/status/1899105803073958010&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j80hbo/hunyuanturbos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T14:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8kd3x</id>
    <title>Created an open-source alternative to Manus AI!</title>
    <updated>2025-03-11T06:18:58+00:00</updated>
    <author>
      <name>/u/ComfortableArm121</name>
      <uri>https://old.reddit.com/user/ComfortableArm121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone’s talking about Manus AI (an agent that can research, browse, code, and automate tasks.)&lt;br /&gt; But it's only available with an invite code!&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://github.com/The-Pocket-World/PocketManus"&gt;opensource project, PocketManus, &lt;/a&gt;combines &lt;a href="https://github.com/The-Pocket-World/Pocket-Flow-Framework"&gt;Pocketflow Framework &lt;/a&gt;and &lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;OpenManus&lt;/a&gt; to execute actions.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI break down complex tasks into Pocketflow Nodes&lt;/li&gt; &lt;li&gt;AI creates detailed execution strategies and interact with tools&lt;/li&gt; &lt;li&gt;Tools / Tool agents interface with external services and APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real-World Capabilities&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Autonomous research, coding, and web browsing&lt;/li&gt; &lt;li&gt;Supports top LLMs (easily integrated with GPT-4O, Claude 3.7, Gemini, Mistral, DeepSeek , Qwen, Ollama, Groq more)&lt;/li&gt; &lt;li&gt;Simple Setup. No restrictions. No invites. No paywalls. Just powerful multi-agent collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's a video of PocketManus in action: &lt;a href="https://x.com/helenaeverley/status/1899221716464959855"&gt;https://x.com/helenaeverley/status/1899221716464959855&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfortableArm121"&gt; /u/ComfortableArm121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8kd3x/created_an_opensource_alternative_to_manus_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T06:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8o089</id>
    <title>Qwen Chat New UI</title>
    <updated>2025-03-11T10:53:57+00:00</updated>
    <author>
      <name>/u/palyer69</name>
      <uri>https://old.reddit.com/user/palyer69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8o089/qwen_chat_new_ui/"&gt; &lt;img alt="Qwen Chat New UI" src="https://preview.redd.it/kdwba65uj1oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c2fcda06b0923692fc09eb117d80efafd7a3cf7" title="Qwen Chat New UI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palyer69"&gt; /u/palyer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kdwba65uj1oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8o089/qwen_chat_new_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8o089/qwen_chat_new_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T10:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83zkt</id>
    <title>Don't underestimate the power of RAG</title>
    <updated>2025-03-10T17:21:09+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt; &lt;img alt="Don't underestimate the power of RAG" src="https://preview.redd.it/moz1h1pzbwne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=adc13823a2909ba4af349f77c5405bf1ce990c2e" title="Don't underestimate the power of RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moz1h1pzbwne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83zkt/dont_underestimate_the_power_of_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85snw</id>
    <title>[Experimental] Control the 'Thinking Effort' of QwQ &amp; R1 Models with a Custom Logits Processor</title>
    <updated>2025-03-10T18:34:42+00:00</updated>
    <author>
      <name>/u/ASL_Dev</name>
      <uri>https://old.reddit.com/user/ASL_Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed several posts lately discussing how the QwQ model tends to produce an excessive amount of tokens, often leading it to &amp;quot;overthink&amp;quot; unnecessarily. I've also seen some creative attempts to control this behavior using carefully crafted system prompts.&lt;/p&gt; &lt;p&gt;To help address this issue more systematically, I've put together a small and simple solution using a custom &lt;strong&gt;logits processor&lt;/strong&gt;. This approach dynamically adjusts the likelihood of the end-of-thinking token (&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;) appearing during generation.&lt;/p&gt; &lt;p&gt;The basic idea:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can set a &amp;quot;thinking effort&amp;quot; parameter (&lt;code&gt;0.0&lt;/code&gt; = minimal thinking, token &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; quickly appears; &lt;code&gt;1.0&lt;/code&gt; = normal behavior, &amp;gt;&lt;code&gt;1.0&lt;/code&gt; = it takes longer to output the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token).&lt;/li&gt; &lt;li&gt;The logic is straightforward: once the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token has been generated, the processor stops adjusting logits for that sequence.&lt;/li&gt; &lt;li&gt;This allows controlling how much the model thinks (or “overthinks”) without complicated prompt engineering.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've seen good results in reducing unnecessary thinking tokens in simple tasks, but I haven't yet extensively tested how this might influence longer chain-of-thought (CoT) reasoning.&lt;/p&gt; &lt;p&gt;I'd love for others to try it out and share your experiences or thoughts!&lt;/p&gt; &lt;p&gt;Here’s the repo with code and examples for both llama-cpp-python (gguf models) and Hugging Face Transformers (&lt;strong&gt;Note:&lt;/strong&gt; The code is still very raw, not optimized, and not organized lol... —this is just to share the basic idea quickly with the community!):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/and270/thinking_effort_processor"&gt;https://github.com/and270/thinking_effort_processor&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quick Example (Qwen-1.5B R1-Distill)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; What is the capital of France?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular Inference:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Okay, so I need to figure out what the capital of France is. I've heard a few things before, but I'm not entirely sure. Let me start by recalling what I know about France. France is a country in Europe, known for its diverse landscapes and vibrant culture. The name &amp;quot;France&amp;quot; itself comes from the French word &amp;quot;français,&amp;quot; which means &amp;quot;french&amp;quot; or &amp;quot;colorful.&amp;quot; I think the capital is a significant city, maybe something like Paris or maybe another city...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(The model generates a lengthy reasoning sequence before concluding)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;...To summarize, I believe the capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thinking Effort Inference (0.1):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;The capital of France is Paris.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Any feedback or tests are very welcome!&lt;/p&gt; &lt;p&gt;Let me know your thoughts or experiences—I'm especially curious how this affects your use-cases with the QwQ or similar models. 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASL_Dev"&gt; /u/ASL_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85snw/experimental_control_the_thinking_effort_of_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j83imv</id>
    <title>We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models.</title>
    <updated>2025-03-10T17:01:38+00:00</updated>
    <author>
      <name>/u/ProKil_Chu</name>
      <uri>https://old.reddit.com/user/ProKil_Chu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt; &lt;img alt="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." src="https://external-preview.redd.it/Y3fzRflurvaH7gQ6GXbY5iJSr6_6d8TJO5p2Fiagr1c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64d3affd381babd5da7c46900f14ec9e95d3813" title="We tested open and closed models for embodied decision alignment, and we found Qwen 2.5 VL is surprisingly stronger than most closed frontier models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j83imv/video/t190t6fsewne1/player"&gt;https://reddit.com/link/1j83imv/video/t190t6fsewne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One thing that surprised us during benchmarking with EgoNormia is that Qwen 2.5 VL is indeed a very strong model for vision which rivals Gemini 1.5/2.0, better than GPT-4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Please read the blog: &lt;a href="https://opensocial.world/articles/egonormia"&gt;https://opensocial.world/articles/egonormia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard: &lt;a href="https://egonormia.org"&gt;https://egonormia.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eval code: &lt;a href="https://github.com/Open-Social-World/EgoNormia"&gt;https://github.com/Open-Social-World/EgoNormia&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProKil_Chu"&gt; /u/ProKil_Chu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j83imv/we_tested_open_and_closed_models_for_embodied/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j85q5m</id>
    <title>every LLM metric you need to know</title>
    <updated>2025-03-10T18:31:58+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best way to improve LLM performance is to consistently benchmark your model using a well-defined set of metrics throughout development, rather than relying on “vibe check” coding—this approach helps ensure that any modifications don’t inadvertently cause regressions.&lt;/p&gt; &lt;p&gt;I’ve listed below some essential LLM metrics to know before you begin benchmarking your LLM. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Traditional NLP evaluation methods like BERT and ROUGE are fast, affordable, and reliable. However, their reliance on reference texts and inability to capture the nuanced semantics of open-ended, often complexly formatted LLM outputs make them less suitable for production-level evaluations. &lt;/p&gt; &lt;p&gt;LLM judges are much more effective if you care about evaluation accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG metrics&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates how effectively an LLM agent accomplishes a task as outlined in the input, based on tools called and the actual output of the agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-conversation-relevancy"&gt;Conversational Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Robustness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-prompt-alignment"&gt;Prompt Alignment:&lt;/a&gt; measures whether your LLM application is able to generate outputs that aligns with any instructions specified in your prompt template.&lt;/li&gt; &lt;li&gt;Output Consistency: measures the consistency of your LLM output given the same input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Custom metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Custom metrics are particularly effective when you have a specialized use case, such as in medicine or healthcare, where it is necessary to define your own criteria.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-llm-evals"&gt;GEval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; the most versatile custom metric for you to easily build deterministic decision trees for evaluation with the help of using LLM-as-a-judge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Red-teaming metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are hundreds of red-teaming metrics available, but bias, toxicity, and hallucination are among the most common. These metrics are particularly valuable for detecting harmful outputs and ensuring that the model maintains high standards of safety and reliability.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.confident-ai.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Although this is quite lengthy, and a good starting place, it is by no means comprehensive. Besides this there are other categories of metrics like multimodal metrics, which can range from image quality metrics like image coherence to multimodal RAG metrics like multimodal contextual precision or recall. &lt;/p&gt; &lt;p&gt;For a more comprehensive list + calculations, you might want to visit &lt;a href="https://docs.confident-ai.com/"&gt;deepeval docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j85q5m/every_llm_metric_you_need_to_know/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8f6nf</id>
    <title>Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti</title>
    <updated>2025-03-11T01:22:20+00:00</updated>
    <author>
      <name>/u/Status-Hearing-4084</name>
      <uri>https://old.reddit.com/user/Status-Hearing-4084</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt; &lt;img alt="Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti" src="https://b.thumbs.redditmedia.com/7cMOZxgSRdWpuSo77V1xrJreEKqUtJqAKTR_RaISKXw.jpg" title="Running QwQ-32B LLM locally: Model sharding between M1 MacBook Pro + RTX 4060 Ti" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Successfully running QwQ-32B (@Alibaba_Qwen) across M1 MacBook Pro and RTX 4060 Ti through model sharding.&lt;/p&gt; &lt;p&gt;Demo video exceeds Reddit's size limit. You can view it here: [ &lt;a href="https://x.com/tensorblock_aoi/status/1899266661888512004"&gt;https://x.com/tensorblock_aoi/status/1899266661888512004&lt;/a&gt; ]&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;p&gt;- MacBook Pro 2021 (M1 Pro, 16GB RAM)&lt;/p&gt; &lt;p&gt;- RTX 4060 Ti (16GB VRAM)&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;p&gt;- QwQ-32B (Q4_K_M quantization)&lt;/p&gt; &lt;p&gt;- Original size: 20GB&lt;/p&gt; &lt;p&gt;- Distributed across devices with 16GB limitation&lt;/p&gt; &lt;p&gt;Implementation:&lt;/p&gt; &lt;p&gt;- Cross-architecture model sharding&lt;/p&gt; &lt;p&gt;- Custom memory management&lt;/p&gt; &lt;p&gt;- Parallel inference pipeline&lt;/p&gt; &lt;p&gt;- TensorBlock orchestration&lt;/p&gt; &lt;p&gt;Current Progress:&lt;/p&gt; &lt;p&gt;- Model successfully loaded and running&lt;/p&gt; &lt;p&gt;- Stable inference achieved&lt;/p&gt; &lt;p&gt;- Optimization in progress&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ij3j83poryne1.jpg?width=3176&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4dfa6929805da97868dda9ee90bfcee19d08a011"&gt;https://preview.redd.it/ij3j83poryne1.jpg?width=3176&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4dfa6929805da97868dda9ee90bfcee19d08a011&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're excited to announce TensorBlock, our upcoming local inference solution. The software enables efficient cross-device LLM deployment, featuring:&lt;/p&gt; &lt;p&gt;- Distributed inference across multiple hardware platforms&lt;/p&gt; &lt;p&gt;- Comprehensive support for Intel, AMD, NVIDIA, and Apple Silicon&lt;/p&gt; &lt;p&gt;- Smart memory management for resource-constrained devices&lt;/p&gt; &lt;p&gt;- Real-time performance monitoring and optimization&lt;/p&gt; &lt;p&gt;- User-friendly interface for model deployment and management&lt;/p&gt; &lt;p&gt;- Advanced parallel computing capabilities&lt;/p&gt; &lt;p&gt;We'll be releasing detailed benchmarks, comprehensive documentation, and deployment guides along with the software launch. Stay tuned for more updates on performance metrics and cross-platform compatibility testing.&lt;/p&gt; &lt;p&gt;Technical questions and feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status-Hearing-4084"&gt; /u/Status-Hearing-4084 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8f6nf/running_qwq32b_llm_locally_model_sharding_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T01:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8554a</id>
    <title>Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark</title>
    <updated>2025-03-10T18:08:27+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt; &lt;img alt="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" src="https://b.thumbs.redditmedia.com/Yaa0ATPdfMfRcdIPRl3zAR-18YhojdxTeqLYJXaDdUk.jpg" title="Qwen QwQ-32B joins DeepSeek R1 and Claude Sonnets at the top of the Creative Story-Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8554a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8554a/qwen_qwq32b_joins_deepseek_r1_and_claude_sonnets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T18:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j84c79</id>
    <title>Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance</title>
    <updated>2025-03-10T17:35:19+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt; &lt;img alt="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" src="https://b.thumbs.redditmedia.com/GITV-BcVRUV84azdQvU9AjF2LmByzEd0hc-J34tPTRc.jpg" title="Qwen QwQ-32B is the LLM most frequently voted out first by its peers in the Elimination Game Benchmark, resulting in poor overall performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j84c79"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j84c79/qwen_qwq32b_is_the_llm_most_frequently_voted_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T17:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8lp6d</id>
    <title>RubyLLM 1.0</title>
    <updated>2025-03-11T08:00:24+00:00</updated>
    <author>
      <name>/u/crmne</name>
      <uri>https://old.reddit.com/user/crmne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I just released RubyLLM 1.0, a library that makes working with AI feel natural and Ruby-like.&lt;/p&gt; &lt;p&gt;While building a RAG application for business documents, I wanted an AI library that felt like Ruby: elegant, expressive, and focused on developer happiness.&lt;/p&gt; &lt;h2&gt;What makes it different?&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Beautiful interfaces&lt;/strong&gt; &lt;code&gt;ruby chat = RubyLLM.chat embedding = RubyLLM.embed(&amp;quot;Ruby is elegant&amp;quot;) image = RubyLLM.paint(&amp;quot;a sunset over mountains&amp;quot;) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Works with multiple providers through one API&lt;/strong&gt; ```ruby&lt;/p&gt; &lt;h1&gt;Start with GPT&lt;/h1&gt; &lt;p&gt;chat = RubyLLM.chat(model: 'gpt-4o-mini')&lt;/p&gt; &lt;h1&gt;Switch to Claude? No problem&lt;/h1&gt; &lt;p&gt;chat.with_model('claude-3-5-sonnet') ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming that makes sense&lt;/strong&gt; &lt;code&gt;ruby chat.ask &amp;quot;Write a story&amp;quot; do |chunk| print chunk.content # Same chunk format for all providers end &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rails integration that just works&lt;/strong&gt; &lt;code&gt;ruby class Chat &amp;lt; ApplicationRecord acts_as_chat end &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools without the JSON Schema pain&lt;/strong&gt; ```ruby class Search &amp;lt; RubyLLM::Tool description &amp;quot;Searches our database&amp;quot; param :query, desc: &amp;quot;The search query&amp;quot;&lt;/p&gt; &lt;p&gt;def execute(query:) Document.search(query).map(&amp;amp;:title) end end ```&lt;/p&gt; &lt;p&gt;It supports vision, PDFs, audio, and more - all with minimal dependencies.&lt;/p&gt; &lt;p&gt;Check it out at &lt;a href="https://github.com/crmne/ruby_llm"&gt;https://github.com/crmne/ruby_llm&lt;/a&gt; or &lt;code&gt;gem install ruby_llm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What do you think? I'd love your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crmne"&gt; /u/crmne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8lp6d/rubyllm_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T08:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i5s2</id>
    <title>Why doesn't Groq Sell its LPUs? By Extension, Why doesn't Google do that?</title>
    <updated>2025-03-11T03:57:03+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When Groq first announced and demoed its LPUs cluster, I was so excited. I believed that finally we get HW that's cost effective. But, it seems the company is not interested in selling its HW at all. &lt;/p&gt; &lt;p&gt;And I DON'T UNDERSTAND THE LOGIC BEHIND such a decision. Does is have something to do with Google since the founder of Groq are ex-Google engineers who worked and developed Googles TPUs?&lt;/p&gt; &lt;p&gt;Why doesn't Google sell its own TPUs? I think now is the right time to enter the HW market.&lt;/p&gt; &lt;p&gt;Can someone shed some light on this topic, please?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i5s2/why_doesnt_groq_sell_its_lpus_by_extension_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T03:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8iqns</id>
    <title>Hello world :)</title>
    <updated>2025-03-11T04:30:38+00:00</updated>
    <author>
      <name>/u/No-Abalone1029</name>
      <uri>https://old.reddit.com/user/No-Abalone1029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt; &lt;img alt="Hello world :)" src="https://preview.redd.it/hy3131ghnzne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57704da9936e283da78771a5685c4f870a144bfe" title="Hello world :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA rtx 3060 12gb vram, hyte revolt 3 asrock b760 w wifi intel i5 16gb t-force vulcan ram&lt;/p&gt; &lt;p&gt;$1k. what do we think, and what should I do for my first project?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abalone1029"&gt; /u/No-Abalone1029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hy3131ghnzne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8iqns/hello_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j87eum</id>
    <title>QwQ 32B can do it if you coach it 2 times</title>
    <updated>2025-03-10T19:41:49+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt; &lt;img alt="QwQ 32B can do it if you coach it 2 times" src="https://external-preview.redd.it/bGFmOXk2NDIxeG5lMalrzKbbY1wxsyua5vTpp1g3RTatq_ecPpvEXRJ-_J8E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc2c544369e356552a9d78fa1f23bdc00fdf6c3" title="QwQ 32B can do it if you coach it 2 times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wn0l7421xne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j87eum/qwq_32b_can_do_it_if_you_coach_it_2_times/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mtsc</id>
    <title>I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain</title>
    <updated>2025-03-11T09:29:29+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"&gt; &lt;img alt="I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain" src="https://external-preview.redd.it/ZWY0aWR0OGY0MW9lMZgswa6t9U1L_elgvz9f8oIRyRLGhWXXy5P6nPcZPdz9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02250a73a7824bb867a8348c19f1edaa5ad03c80" title="I created an Open Source Perplexity-Style Unified Search for Your Distributed Second Brain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q4apht8f41oe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mtsc/i_created_an_open_source_perplexitystyle_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8i9rc</id>
    <title>NVLINK improves dual RTX 3090 inference performance by nearly 50%</title>
    <updated>2025-03-11T04:02:55+00:00</updated>
    <author>
      <name>/u/hp1337</name>
      <uri>https://old.reddit.com/user/hp1337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt; &lt;img alt="NVLINK improves dual RTX 3090 inference performance by nearly 50%" src="https://external-preview.redd.it/vlUgVTNeRZS9-bbTFaZ7ayYcVwvIPXEw74izW1rJLuI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ba6a9bb89057dbbff64dd02958275d4ac3df306" title="NVLINK improves dual RTX 3090 inference performance by nearly 50%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hp1337"&gt; /u/hp1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://himeshp.blogspot.com/2025/03/vllm-performance-benchmarks-4x-rtx-3090.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8i9rc/nvlink_improves_dual_rtx_3090_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8766b</id>
    <title>New rig who dis</title>
    <updated>2025-03-10T19:31:29+00:00</updated>
    <author>
      <name>/u/MotorcyclesAndBizniz</name>
      <uri>https://old.reddit.com/user/MotorcyclesAndBizniz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt; &lt;img alt="New rig who dis" src="https://b.thumbs.redditmedia.com/0XSP2n-GAI5n3Op8qnPsulZZgY7u_Dk_E6IZd3L-Ixg.jpg" title="New rig who dis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: 6x 3090 FE via 6x PCIe 4.0 x4 Oculink&lt;br /&gt; CPU: AMD 7950x3D&lt;br /&gt; MoBo: B650M WiFi&lt;br /&gt; RAM: 192GB DDR5 @ 4800MHz&lt;br /&gt; NIC: 10Gbe&lt;br /&gt; NVMe: Samsung 980 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MotorcyclesAndBizniz"&gt; /u/MotorcyclesAndBizniz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j8766b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8766b/new_rig_who_dis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T19:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mrju</id>
    <title>Alibaba just dropped R1-Omni!</title>
    <updated>2025-03-11T09:24:29+00:00</updated>
    <author>
      <name>/u/Optifnolinalgebdirec</name>
      <uri>https://old.reddit.com/user/Optifnolinalgebdirec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba just dropped R1-Omni! Redefining emotional intelligence with Omni-Multimodal Emotion Recognition and Reinforcement Learning!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optifnolinalgebdirec"&gt; /u/Optifnolinalgebdirec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8ibs2</id>
    <title>Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)</title>
    <updated>2025-03-11T04:06:03+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt; &lt;img alt="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" src="https://external-preview.redd.it/aHB6YWN6MG1pem5lMRehscSTBN6MsWNS82nQXiny-IBLyecHf_sStrTrfL-k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe0eebd0390412c4dbf32e51fec56621c4f2ca18" title="Don't underestimate the power of local models executing recursive agent workflows. (mistral-small)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/51m4yx0mizne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8ibs2/dont_underestimate_the_power_of_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T04:06:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mrxj</id>
    <title>DeepSeek-R2 may be released ahead of schedule next Monday</title>
    <updated>2025-03-11T09:25:21+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Media Zhizhong Finance Cites &amp;quot;Sources&amp;quot; Claiming DeepSeek's Next-Generation AI Model, DeepSeek-R2, Will Be Released on March 17.&lt;/p&gt; &lt;p&gt;&amp;quot; New God Arrives&amp;quot;: Reports Say DeepSeek-R2 AI Model Will Be Released on March 17&lt;/p&gt; &lt;p&gt;According to reports, DeepSeek-R2 has achieved breakthroughs in several key areas, including superior programming capabilities, multi-language reasoning skills, and delivering higher accuracy at lower costs. Sources believe that if these features are realized, it could position DeepSeek as a significant contender in the global AI race.&lt;/p&gt; &lt;p&gt;DeepSeek has not yet officially announced the exact release date or technical details of R2. Previously, the market expected the DeepSeek-R2 model to be released in May.&lt;/p&gt; &lt;p&gt;&lt;a href="https://news.futunn.com/en/flash/18545711/deepseek-r2-will-be-released-on-march-17-and-there"&gt;https://news.futunn.com/en/flash/18545711/deepseek-r2-will-be-released-on-march-17-and-there&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrxj/deepseekr2_may_be_released_ahead_of_schedule_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrxj/deepseekr2_may_be_released_ahead_of_schedule_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrxj/deepseekr2_may_be_released_ahead_of_schedule_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:25:21+00:00</published>
  </entry>
</feed>
