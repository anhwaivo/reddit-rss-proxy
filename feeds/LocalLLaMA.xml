<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-28T19:24:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mb7tb7</id>
    <title>Watch Alibaba Cloud Founder on China’s AI Future</title>
    <updated>2025-07-28T05:25:13+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"&gt; &lt;img alt="Watch Alibaba Cloud Founder on China’s AI Future" src="https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe991643a370499bccf6b3299fa7518b3c1e355e" title="Watch Alibaba Cloud Founder on China’s AI Future" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/videos/2025-07-28/alibaba-cloud-founder-on-china-s-ai-future-video"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T05:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb2y1z</id>
    <title>The Untold Revolution in iOS 26: WebGPU Is Coming</title>
    <updated>2025-07-28T01:08:57+00:00</updated>
    <author>
      <name>/u/WooFL</name>
      <uri>https://old.reddit.com/user/WooFL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"&gt; &lt;img alt="The Untold Revolution in iOS 26: WebGPU Is Coming" src="https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bac2a180967dbb6dd0c4544eaf16660950fa7c43" title="The Untold Revolution in iOS 26: WebGPU Is Coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WooFL"&gt; /u/WooFL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T01:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb6jzz</id>
    <title>Why I'm Betting Against AI Agents in 2025 (Despite Building Them)</title>
    <updated>2025-07-28T04:12:40+00:00</updated>
    <author>
      <name>/u/Ilovekittens345</name>
      <uri>https://old.reddit.com/user/Ilovekittens345</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ilovekittens345"&gt; /u/Ilovekittens345 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://utkarshkanwat.com/writing/betting-against-agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6jzz/why_im_betting_against_ai_agents_in_2025_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6jzz/why_im_betting_against_ai_agents_in_2025_despite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T04:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbavi1</id>
    <title>My first finetune: Gemma 3 4B unslop via GRPO</title>
    <updated>2025-07-28T08:41:18+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Training code is included, so maybe someone with more hardware than me can do cooler stuff.&lt;/p&gt; &lt;p&gt;I also uploaded a Q4_K_M GGUF made with unsloth's imatrix.&lt;/p&gt; &lt;p&gt;It's released as a LoRA adapter because my internet sucks and I can't successfully upload the whole thing. If you want full quality you'll need to merge it with &lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The method is based on my own statistical analysis of lots of gemma 3 4b text, plus some patterns i don't like. i also reinforce the correct number of words asked for in the prompt, and i reward lexical diversity &amp;gt; 100.&lt;/p&gt; &lt;p&gt;dataset not included, but i did include an example of what my dataset looks like for anyone trying to recreate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO"&gt;https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T08:41:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbocxc</id>
    <title>What’s the most reliable STT engine you’ve used in noisy, multi-speaker environments?</title>
    <updated>2025-07-28T18:34:56+00:00</updated>
    <author>
      <name>/u/ASR_Architect_91</name>
      <uri>https://old.reddit.com/user/ASR_Architect_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing a bunch of speech-to-text APIs over the past few months for a voice agent pipeline that needs to work in less-than-ideal audio (background chatter, overlapping speakers, and heavy accents).&lt;/p&gt; &lt;p&gt;A few engines do well in clean, single-speaker setups. But once you throw in real-world messiness (especially for diarization or fast partials), things start to fall apart.&lt;/p&gt; &lt;p&gt;What are you using that actually holds up under pressure, can be open source or commercial. Real-time is a must. &lt;strong&gt;Bonus&lt;/strong&gt; if it works well in low-bandwidth or edge-device scenarios too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASR_Architect_91"&gt; /u/ASR_Architect_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T18:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbowe3</id>
    <title>GLM 4.5 Failing to use search tool in LM studio</title>
    <updated>2025-07-28T18:54:47+00:00</updated>
    <author>
      <name>/u/Loighic</name>
      <uri>https://old.reddit.com/user/Loighic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/"&gt; &lt;img alt="GLM 4.5 Failing to use search tool in LM studio" src="https://b.thumbs.redditmedia.com/1WhZfgGZqriSB9lKdYrpf4FisvjO9XfCpFQr5Socebk.jpg" title="GLM 4.5 Failing to use search tool in LM studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b"&gt;https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 correctly uses the search tool. But GLM 4.5 does not. Is there something on my end I can do to fix this? As tool use and multi step reasoning are supposed to be one of GLM 4.5 greatest strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loighic"&gt; /u/Loighic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb98cm</id>
    <title>Granite 4 small and medium might be 30B6A/120B30A?</title>
    <updated>2025-07-28T06:53:38+00:00</updated>
    <author>
      <name>/u/Kryesh</name>
      <uri>https://old.reddit.com/user/Kryesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"&gt; &lt;img alt="Granite 4 small and medium might be 30B6A/120B30A?" src="https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71914d4ace28f4302812fa4b60aacea3654d064d" title="Granite 4 small and medium might be 30B6A/120B30A?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kryesh"&gt; /u/Kryesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=UxUD88TRlBY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T06:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb6uhm</id>
    <title>Pi AI studio</title>
    <updated>2025-07-28T04:28:59+00:00</updated>
    <author>
      <name>/u/koumoua01</name>
      <uri>https://old.reddit.com/user/koumoua01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"&gt; &lt;img alt="Pi AI studio" src="https://b.thumbs.redditmedia.com/CP9KFtIHMzNxz_IXwevaJpIQ_DH-LieoKpFIOifsV_Q.jpg" title="Pi AI studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This 96GB device cost around $1000. Has anyone tried it before? Can it host small LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koumoua01"&gt; /u/koumoua01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mb6uhm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T04:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbp4nm</id>
    <title>The walled garden gets higher walls: Anthropic is adding weekly rate limits for paid Claude subscribers</title>
    <updated>2025-07-28T19:02:58+00:00</updated>
    <author>
      <name>/u/Resident_Egg5765</name>
      <uri>https://old.reddit.com/user/Resident_Egg5765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Got an interesting email from Anthropic today. Looks like they're adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.&lt;/p&gt; &lt;p&gt;The email mentions it's a way to handle policy violations and &amp;quot;advanced usage patterns,&amp;quot; like running Claude 24/7. They estimate the new weekly cap for their top &amp;quot;Max&amp;quot; tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.&lt;/p&gt; &lt;p&gt;This definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.&lt;/p&gt; &lt;p&gt;It really highlights some of the inherent strengths of the local approach we have here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Your workflow is insulated from sudden policy changes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Freedom:&lt;/strong&gt; You have the freedom to run intensive or long-running tasks without hitting a usage cap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predictability:&lt;/strong&gt; The only real limits are your own hardware and time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious to hear how the community sees this.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does this kind of change make you lean more heavily into your local setup?&lt;/li&gt; &lt;li&gt;For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?&lt;/li&gt; &lt;li&gt;And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to the discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Egg5765"&gt; /u/Resident_Egg5765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T19:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbf3dz</id>
    <title>GLM-4.5-Demo</title>
    <updated>2025-07-28T12:41:01+00:00</updated>
    <author>
      <name>/u/Dr_Me_123</name>
      <uri>https://old.reddit.com/user/Dr_Me_123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbf3dz/glm45demo/"&gt; &lt;img alt="GLM-4.5-Demo" src="https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40f00fd91f181f535bed35d54b6fc14b0ae5b15d" title="GLM-4.5-Demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Me_123"&gt; /u/Dr_Me_123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5-Space"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbf3dz/glm45demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbf3dz/glm45demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbei14</id>
    <title>support for SmallThinker model series has been merged into llama.cpp</title>
    <updated>2025-07-28T12:12:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbei14/support_for_smallthinker_model_series_has_been/"&gt; &lt;img alt="support for SmallThinker model series has been merged into llama.cpp" src="https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76a396512223ddde08b85788022a284e7843ac6a" title="support for SmallThinker model series has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14898"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbei14/support_for_smallthinker_model_series_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbei14/support_for_smallthinker_model_series_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbeecr</id>
    <title>Wan-AI/Wan2.2-TI2V-5B · Hugging Face</title>
    <updated>2025-07-28T12:07:29+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbeecr/wanaiwan22ti2v5b_hugging_face/"&gt; &lt;img alt="Wan-AI/Wan2.2-TI2V-5B · Hugging Face" src="https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27e634b9b7a9f310e89c9de904713a31626c729c" title="Wan-AI/Wan2.2-TI2V-5B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan-AI/Wan2.2-I2V-A14B &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wan-AI/Wan2.2-T2V-A14B &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbeecr/wanaiwan22ti2v5b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbeecr/wanaiwan22ti2v5b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb15g2</id>
    <title>UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.</title>
    <updated>2025-07-27T23:42:37+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"&gt; &lt;img alt="UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." src="https://b.thumbs.redditmedia.com/AX8rU0Ar21fTE1Um6Zy39yTbpXN7nfUgowthOtgI49Y.jpg" title="UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt; Releasing 4B in 24 hours and 32B now. &lt;/p&gt; &lt;p&gt;Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mb15g2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T23:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbhqs0</id>
    <title>mlx-community/GLM-4.5-Air-4bit · Hugging Face</title>
    <updated>2025-07-28T14:30:54+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbhqs0/mlxcommunityglm45air4bit_hugging_face/"&gt; &lt;img alt="mlx-community/GLM-4.5-Air-4bit · Hugging Face" src="https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c537929021522aee1b17419300504e1442fedb5" title="mlx-community/GLM-4.5-Air-4bit · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mlx-community/GLM-4.5-Air-4bit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbhqs0/mlxcommunityglm45air4bit_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbhqs0/mlxcommunityglm45air4bit_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T14:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbm4a0</id>
    <title>Tried Wan2.2 on RTX 4090, quite impressed</title>
    <updated>2025-07-28T17:12:24+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : &lt;a href="https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8"&gt;https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T17:12:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbny6o</id>
    <title>100x faster and 100x cheaper transcription with open models vs proprietary</title>
    <updated>2025-07-28T18:19:36+00:00</updated>
    <author>
      <name>/u/crookedstairs</name>
      <uri>https://old.reddit.com/user/crookedstairs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;HuggingFace's ASR leaderboard&lt;/a&gt; they're posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.&lt;/p&gt; &lt;p&gt;We at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: &lt;a href="https://modal.com/blog/fast-cheap-batch-transcription"&gt;https://modal.com/blog/fast-cheap-batch-transcription&lt;/a&gt;. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you're currently using either open source or proprietary ASR models would love to know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crookedstairs"&gt; /u/crookedstairs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T18:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbflkv</id>
    <title>GLM-4.5 - a zai-org Collection</title>
    <updated>2025-07-28T13:03:43+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflkv/glm45_a_zaiorg_collection/"&gt; &lt;img alt="GLM-4.5 - a zai-org Collection" src="https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b" title="GLM-4.5 - a zai-org Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflkv/glm45_a_zaiorg_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflkv/glm45_a_zaiorg_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T13:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbfhgp</id>
    <title>Early GLM 4.5 Benchmarks, Claiming to surpass Qwen 3 Coder</title>
    <updated>2025-07-28T12:59:10+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfhgp/early_glm_45_benchmarks_claiming_to_surpass_qwen/"&gt; &lt;img alt="Early GLM 4.5 Benchmarks, Claiming to surpass Qwen 3 Coder" src="https://b.thumbs.redditmedia.com/d8z0Eqv5ElbJ5Bba2iScsXyRYp3-oFkkQDHFInTFYDc.jpg" title="Early GLM 4.5 Benchmarks, Claiming to surpass Qwen 3 Coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#overall-performance"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mbfhgp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfhgp/early_glm_45_benchmarks_claiming_to_surpass_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfhgp/early_glm_45_benchmarks_claiming_to_surpass_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbdm6t</id>
    <title>GLM 4.5 possibly releasing today according to Bloomberg</title>
    <updated>2025-07-28T11:26:56+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"&gt; &lt;img alt="GLM 4.5 possibly releasing today according to Bloomberg" src="https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11470efb2209e35e2be0d434f089cd6d797726ba" title="GLM 4.5 possibly releasing today according to Bloomberg" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bloomberg writes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/organizations/zai-org/activity/collections"&gt;https://huggingface.co/organizations/zai-org/activity/collections&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T11:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbefh4</id>
    <title>Wan 2.2 T2V,I2V 14B MoE Models</title>
    <updated>2025-07-28T12:09:02+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbefh4/wan_22_t2vi2v_14b_moe_models/"&gt; &lt;img alt="Wan 2.2 T2V,I2V 14B MoE Models" src="https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383cb90569524b8ee389cbf51df12c411b89660a" title="Wan 2.2 T2V,I2V 14B MoE Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re proud to introduce &lt;strong&gt;Wan2.2&lt;/strong&gt;, a major leap in open video generation, featuring a novel &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; diffusion architecture, high-compression HD generation, and benchmark-leading performance.&lt;/p&gt; &lt;h1&gt;🔍 Key Innovations&lt;/h1&gt; &lt;h1&gt;🧠 Mixture-of-Experts (MoE) Diffusion Architecture&lt;/h1&gt; &lt;p&gt;Wan2.2 integrates &lt;strong&gt;two specialized 14B experts&lt;/strong&gt; in its 27B-parameter MoE design:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;High-noise expert&lt;/strong&gt; for early denoising stages — focusing on layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-noise expert&lt;/strong&gt; for later stages — refining fine details.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Only one expert is active per step (14B params), so &lt;strong&gt;inference remains efficient&lt;/strong&gt; despite the added capacity.&lt;/p&gt; &lt;p&gt;The expert transition is based on the &lt;strong&gt;Signal-to-Noise Ratio (SNR)&lt;/strong&gt; during diffusion. As SNR drops, the model smoothly switches from the high-noise to low-noise expert at a learned threshold (&lt;code&gt;t_moe&lt;/code&gt;), ensuring optimal handling of different generation phases.&lt;/p&gt; &lt;p&gt;📈 &lt;strong&gt;Visual Overview&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Left: Expert switching based on SNR&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Right: Validation loss comparison across model variants&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The final &lt;strong&gt;Wan2.2 (MoE)&lt;/strong&gt; model shows the &lt;strong&gt;lowest validation loss&lt;/strong&gt;, confirming better convergence and fidelity than Wan2.1 or hybrid expert configurations.&lt;/p&gt; &lt;h1&gt;⚡ TI2V-5B: Fast, Compressed, HD Video Generation&lt;/h1&gt; &lt;p&gt;Wan2.2 also introduces &lt;strong&gt;TI2V-5B&lt;/strong&gt;, a &lt;strong&gt;5B dense model&lt;/strong&gt; with impressive efficiency:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Utilizes &lt;strong&gt;Wan2.2-VAE&lt;/strong&gt; with $4\times16\times16$ spatial compression.&lt;/li&gt; &lt;li&gt;Achieves &lt;strong&gt;$4\times32\times32$ total compression&lt;/strong&gt; with patchification.&lt;/li&gt; &lt;li&gt;Can generate &lt;strong&gt;5s 720P@24fps videos in &amp;lt;9 minutes&lt;/strong&gt; on a consumer GPU.&lt;/li&gt; &lt;li&gt;Natively supports &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; in one unified architecture.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This makes Wan2.2 not only powerful but also highly practical for real-world applications.&lt;/p&gt; &lt;h1&gt;🧪 Benchmarking: Wan2.2 vs Commercial SOTAs&lt;/h1&gt; &lt;p&gt;We evaluated Wan2.2 against leading proprietary models on &lt;strong&gt;Wan-Bench 2.0&lt;/strong&gt;, scoring across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Aesthetics&lt;/li&gt; &lt;li&gt;Dynamic motion&lt;/li&gt; &lt;li&gt;Text rendering&lt;/li&gt; &lt;li&gt;Camera control&lt;/li&gt; &lt;li&gt;Fidelity&lt;/li&gt; &lt;li&gt;Object accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📊 &lt;strong&gt;Benchmark Results&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;🚀 &lt;strong&gt;Wan2.2-T2V-A14B leads in 5/6 categories&lt;/strong&gt;, outperforming commercial models like KLING 2.0, Sora, and Seedance in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dynamic Degree&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Rendering&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Object Accuracy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;And more…&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧵 Why Wan2.2 Matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Brings &lt;strong&gt;MoE advantages&lt;/strong&gt; to video generation with no added inference cost.&lt;/li&gt; &lt;li&gt;Achieves &lt;strong&gt;industry-leading HD generation speeds&lt;/strong&gt; on consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Openly benchmarked&lt;/strong&gt; with results that rival or beat closed-source giants.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbefh4/wan_22_t2vi2v_14b_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbefh4/wan_22_t2vi2v_14b_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbihcz</id>
    <title>GLM shattered the record for "worst benchmark JPEG ever published" - wow.</title>
    <updated>2025-07-28T14:59:02+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/"&gt; &lt;img alt="GLM shattered the record for &amp;quot;worst benchmark JPEG ever published&amp;quot; - wow." src="https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ba8857bb5cf2336d48720fb4df5c2b74feec965" title="GLM shattered the record for &amp;quot;worst benchmark JPEG ever published&amp;quot; - wow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5gs5tl2vpmff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T14:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb9uy8</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-28T07:33:42+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No model card as of yet&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T07:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbflsw</id>
    <title>GLM 4.5 Collection Now Live!</title>
    <updated>2025-07-28T13:03:59+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T13:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbfa3y</id>
    <title>Wan 2.2 is Live! Needs only 8GB of VRAM!</title>
    <updated>2025-07-28T12:49:51+00:00</updated>
    <author>
      <name>/u/Comed_Ai_n</name>
      <uri>https://old.reddit.com/user/Comed_Ai_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"&gt; &lt;img alt="Wan 2.2 is Live! Needs only 8GB of VRAM!" src="https://preview.redd.it/w2tqvij93mff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa487bb7dc2bff5b7326e25dfec4967cd6c8e51" title="Wan 2.2 is Live! Needs only 8GB of VRAM!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comed_Ai_n"&gt; /u/Comed_Ai_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w2tqvij93mff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T12:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbg1ck</id>
    <title>GLM4.5 released!</title>
    <updated>2025-07-28T13:22:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"&gt; &lt;img alt="GLM4.5 released!" src="https://b.thumbs.redditmedia.com/h1a9hbYRlufo6ZLB7b1IgSekwr0g4qcrXjR2rdPGMPU.jpg" title="GLM4.5 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air — our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.&lt;/p&gt; &lt;p&gt;Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://z.ai/blog/glm-4.5"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mbg1ck"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbg1ck/glm45_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T13:22:25+00:00</published>
  </entry>
</feed>
