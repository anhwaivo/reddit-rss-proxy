<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-23T12:26:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv069</id>
    <title>llama.cpp is installed and running but it is not using my gpu ?</title>
    <updated>2025-03-23T08:53:59+00:00</updated>
    <author>
      <name>/u/Professional_Helper_</name>
      <uri>https://old.reddit.com/user/Professional_Helper_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have installed both files for llama.cpp for cuda 12.4 (my gpu supports it). When I am running a model I noticed my cpu usage is high (97%) and gpu is near to 3-5%. (I have also checked the CUDA tab in task manager)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Helper_"&gt; /u/Professional_Helper_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv069/llamacpp_is_installed_and_running_but_it_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv069/llamacpp_is_installed_and_running_but_it_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv069/llamacpp_is_installed_and_running_but_it_is_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T08:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6j47</id>
    <title>ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!</title>
    <updated>2025-03-22T11:39:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt; &lt;img alt="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" src="https://b.thumbs.redditmedia.com/ScRkwidkM9zx6RUmuOk3MXdlYXgdVEQFsbGekaQlRYs.jpg" title="ðŸš€ Running vLLM with 2 GPUs on my home server - automated in minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve got vLLM running on a dual-GPU home server, complete with my Sbnb Linux distro tailored for AI, Grafana GPU utilization dashboards, and automated benchmarking - all set up in just a few minutes thanks to Ansible.&lt;/p&gt; &lt;p&gt;If youâ€™re into LLMs, home labs, or automation, I put together a detailed how-to here: ðŸ”— &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to help if anyone wants to get started!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh6j47"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhe3lq</id>
    <title>gemma3 vision</title>
    <updated>2025-03-22T17:44:22+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok im gonna write in all lower case because the post keeps getting auto modded. its almost like local llama encourage low effort post. super annoying. imagine there was a fully compliant gemma3 vision model, wouldn't that be nice?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha"&gt;https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4s2h</id>
    <title>LLama.cpp smillar speed but in pure Rust, local LLM inference alternatives.</title>
    <updated>2025-03-22T09:35:49+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, every time I want to run a LLM locally, the only choice is llama.cpp or other tools with magical optimization. However, llama.cpp is not always easy to set up especially when it comes to a new model and new architecture. Without help from the community, you can hardly convert a new model into GGUF. Even if you can, it is still very hard to make it work in llama.cpp.&lt;/p&gt; &lt;p&gt;Now, we can have an alternative way to infer LLM locally with maximum speed. And it's in pure Rust! No C++ needed. With pyo3 you can still call it with python, but Rust is easy enough, right?&lt;/p&gt; &lt;p&gt;I made a minimal example the same as llama.cpp chat cli. It runs 6 times faster than using pytorch, based on the Candle framework.Check it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lucasjinreal/Crane"&gt;https://github.com/lucasjinreal/Crane&lt;/a&gt;&lt;/p&gt; &lt;p&gt;next I would adding Spark-TTS and &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; support, if you interested in Rust and fast inference, please join to develop with rust!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhfz4n</id>
    <title>What's the status of using a local LLM for software development?</title>
    <updated>2025-03-22T19:05:30+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please help an old programmer navigate the maze that is the current LLM-enabled SW stacks.&lt;/p&gt; &lt;p&gt;I'm sure that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;I won't use Claude or any online LLM&lt;/strong&gt;. Just a local model that is small enough to leave enough room for context (eg Qwen2.5 Coder 14B).&lt;/li&gt; &lt;li&gt;I need a tool that can feed an entire project to an LLM as context.&lt;/li&gt; &lt;li&gt;I know how to code but want to use an LLM to do the boilerplate stuff, not to take full control of a project.&lt;/li&gt; &lt;li&gt;Preferably FOSS.&lt;/li&gt; &lt;li&gt;Preferably integrated into a solid IDE, rather then being standalone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhbxr9</id>
    <title>Token impact by long-Chain-of-Thought Reasoning Models</title>
    <updated>2025-03-22T16:10:20+00:00</updated>
    <author>
      <name>/u/dubesor86</name>
      <uri>https://old.reddit.com/user/dubesor86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt; &lt;img alt="Token impact by long-Chain-of-Thought Reasoning Models" src="https://preview.redd.it/hxrz73n2l9qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be6c35234bc628ab1e2c263ab5a9a084397d8793" title="Token impact by long-Chain-of-Thought Reasoning Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dubesor86"&gt; /u/dubesor86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hxrz73n2l9qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T16:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhxiei</id>
    <title>Ways the batch generate embeddings (python). is vLLM the only way?</title>
    <updated>2025-03-23T11:51:05+00:00</updated>
    <author>
      <name>/u/Moreh</name>
      <uri>https://old.reddit.com/user/Moreh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;as per title. I am trying to use vLLM but it doesnt play nice with those that are GPU poor!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreh"&gt; /u/Moreh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh7c6e</id>
    <title>My 4x3090 eGPU collection</title>
    <updated>2025-03-22T12:28:25+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt; &lt;img alt="My 4x3090 eGPU collection" src="https://b.thumbs.redditmedia.com/tuwbOdIfpLg-K_qo2ArzC4oMvVIIdECI4tmNxUjTuKA.jpg" title="My 4x3090 eGPU collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 more 3090s ready to hook up to the 2nd Thunderbolt port in the back when I get the UT4g docks in. &lt;/p&gt; &lt;p&gt;Will need to find an area with more room though ðŸ˜…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh7c6e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T12:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhx20u</id>
    <title>14B @ 8Bit or 27B @ 4Bit -- T/s, quality of response, max context size in VRAM limits</title>
    <updated>2025-03-23T11:21:17+00:00</updated>
    <author>
      <name>/u/Professional_Row_967</name>
      <uri>https://old.reddit.com/user/Professional_Row_967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL'DR: 14B Model @ 8bit or 27B Model @ 4bit is likely to be better&lt;/p&gt; &lt;p&gt;Short of running extensive benchmarks, just casual observation using limited test scenarios might not reveal the right picture, so wondering if there any well-established consensus already in the community around this, i.e. which of the 2 models is going to perform better, 14B model (say gemma3) with 8bit quantization or 27B model with 4bit quantization under following constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM limited to max 20GB (basically 20GB out of 24GB URAM of Mac M4 mini)&lt;/li&gt; &lt;li&gt;Need large context window (min 32K but in some cases perhaps 64K or even 128K, VRAM permitting, but also with acceptable output token/sec)&lt;/li&gt; &lt;li&gt;Quality of response (hallucination, relevance, repetition, bias, contextual understanding issues etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Can the answers be safely considered to be pretty much true for other models (say phi4, or llama-3.3) as well ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Row_967"&gt; /u/Professional_Row_967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhx20u/14b_8bit_or_27b_4bit_ts_quality_of_response_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhrg80</id>
    <title>I updated Deep Research at Home to collect user input and output way better reports. Here's a PDF of a search in action</title>
    <updated>2025-03-23T04:39:55+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sapphire-maryrose-59.tiiny.site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrg80/i_updated_deep_research_at_home_to_collect_user/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrg80/i_updated_deep_research_at_home_to_collect_user/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T04:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhwkx0</id>
    <title>Looking for a feedback on something I am working on, open to criticism</title>
    <updated>2025-03-23T10:48:40+00:00</updated>
    <author>
      <name>/u/lladhibhutall</name>
      <uri>https://old.reddit.com/user/lladhibhutall</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Question - What if AI systems could instantly adapt based on their errors and optimize tasks based on previous runs? &lt;/p&gt; &lt;p&gt;Problem - AI agents consistently struggle with complex, multi-step tasks. The most frustrating issue is their tendency to repeat the same errors! Even when agents successfully complete tasks, they rarely optimize their approach, resulting in poor performance and unnecessarily high inference costs for users.&lt;/p&gt; &lt;p&gt;Solution - Imagine when an agent is given a task it goes through a loop, while in the loop it generates internal monologue and thinking process. It takes steps while solving the task and storing those steps help the agent optimise. Imagine how a human solves a problem, humans think and take notes and while something goes wrong, reviews the notes and readjusts the plan. Doing the same for AI agents. An inherent capability of the human mind is to create connections between those notes and evolve those notes as new informations come, that is the core thesis.&lt;/p&gt; &lt;p&gt;Current status - Wrote a primary MVP, tested on browser-use, while browser-use with GPT-4o takes 20+ steps to do a task, with the help of this memory management tool, reduced it to 12 steps in first run(provided some seed memory) and then it optimised automatically to 9 steps for the same task for follow-on runs. &lt;/p&gt; &lt;p&gt;Will Open-source in a few days, if anyone is interested in working together, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lladhibhutall"&gt; /u/lladhibhutall &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwkx0/looking_for_a_feedback_on_something_i_am_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwkx0/looking_for_a_feedback_on_something_i_am_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwkx0/looking_for_a_feedback_on_something_i_am_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T10:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpgum</id>
    <title>Llama 3.3 70B vs Nemotron Super 49B (Based on Lllama 3.3)</title>
    <updated>2025-03-23T02:43:49+00:00</updated>
    <author>
      <name>/u/Prestigious-Use5483</name>
      <uri>https://old.reddit.com/user/Prestigious-Use5483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys like using better? I haven't tested Nemotron Super 49B much, but I absolute loved llama 3.3 70B. Please share the reason you prefer one over the other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Use5483"&gt; /u/Prestigious-Use5483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T02:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhf6x3</id>
    <title>Has anyone switched from remote models (claude, etc.) models to local? Meaning did your investment pay off?</title>
    <updated>2025-03-22T18:31:34+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously a 70b or 32b model won't be as good as Claude API, on the other hand, many are spending $10 to $30+ per day on the API, so it could be a lot cheaper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhdpjk</id>
    <title>Fallen Gemma3 4B 12B 27B - An unholy trinity with no positivity! For users, mergers and cooks!</title>
    <updated>2025-03-22T17:27:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a complete decensor tune, but it should be absent of positivity.&lt;/p&gt; &lt;p&gt;Vision works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6lsx</id>
    <title>OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision.</title>
    <updated>2025-03-22T11:44:18+00:00</updated>
    <author>
      <name>/u/lessis_amess</name>
      <uri>https://old.reddit.com/user/lessis_amess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt; &lt;img alt="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." src="https://preview.redd.it/x942twbra8qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79683f47809a02571ff90500acb5d28a046d6940" title="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;O1 Pro costs 33 times more than Claude 3.7 Sonnet, yet in many cases delivers less capability. GPT-4.5 costs 25 times more and itâ€™s an old model with a cut-off date from November.&lt;/p&gt; &lt;p&gt;Why release old, overpriced models to developers who care most about cost efficiency?&lt;/p&gt; &lt;p&gt;This isn't an accident.&lt;/p&gt; &lt;p&gt;It's anchoring.&lt;/p&gt; &lt;p&gt;Anchoring works by establishing an initial reference point. Once that reference exists, subsequent judgments revolve around it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Show something expensive.&lt;/li&gt; &lt;li&gt;Show something less expensive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The second thing seems like a bargain.&lt;/p&gt; &lt;p&gt;The expensive API models reset our expectations. For years, AI got cheaper while getting smarter. OpenAI wants to break that pattern. They're saying high intelligence costs money. Big models cost money. They're claiming they don't even profit from these prices.&lt;/p&gt; &lt;p&gt;When they release their next frontier model at a &amp;quot;lower&amp;quot; price, you'll think it's reasonable. But it will still cost more than what we paid before this reset. The new &amp;quot;cheap&amp;quot; will be expensive by last year's standards.&lt;/p&gt; &lt;p&gt;OpenAI claims these models lose money. Maybe. But they're conditioning the market to accept higher prices for whatever comes next. The API release is just the first move in a longer game.&lt;/p&gt; &lt;p&gt;This was not a confused move. Itâ€™s smart business. (i'm VERY happy we have open-source)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro"&gt;https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lessis_amess"&gt; /u/lessis_amess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x942twbra8qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhuow6</id>
    <title>Looks like RWKV v7 support is in llama now?</title>
    <updated>2025-03-23T08:30:43+00:00</updated>
    <author>
      <name>/u/TJSnider1984</name>
      <uri>https://old.reddit.com/user/TJSnider1984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12412"&gt;https://github.com/ggml-org/llama.cpp/pull/12412&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll have to build it and see..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TJSnider1984"&gt; /u/TJSnider1984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhuow6/looks_like_rwkv_v7_support_is_in_llama_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T08:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhhsgv</id>
    <title>Qwen2.5-Omni Incoming? Huggingface Transformers PR 36752</title>
    <updated>2025-03-22T20:25:00+00:00</updated>
    <author>
      <name>/u/Inevitable_Sea8804</name>
      <uri>https://old.reddit.com/user/Inevitable_Sea8804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Haven't seen anyone bring this up, so making a post here...&lt;/p&gt; &lt;p&gt;Using DeepSeek-R1 to summarize the features of this model based on PR commits:&lt;/p&gt; &lt;hr /&gt; &lt;h1&gt;&lt;strong&gt;Qwen2.5-Omni Technical Summary&lt;/strong&gt;&lt;/h1&gt; &lt;h2&gt;&lt;strong&gt;1. Basic Information&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Scale&lt;/strong&gt;: 7B parameter version (&amp;quot;Qwen/Qwen2.5-Omni-7B&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Fully open-sourced under Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;2. Input/Output Modalities&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language instructions&lt;/li&gt; &lt;li&gt;&lt;em&gt;Images&lt;/em&gt;: Common formats (JPEG/PNG)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Audio&lt;/em&gt;: WAV/MP3 (requires FFmpeg)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Video&lt;/em&gt;: MP4 with audio track extraction&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Capabilities&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language responses&lt;/li&gt; &lt;li&gt;&lt;em&gt;Speech&lt;/em&gt;: 24kHz natural speech (streaming supported)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;3. Architectural Design&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Encoder&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Block-wise Processing&lt;/em&gt;: Decouples long-sequence handling between encoder (perception) and LLM (sequence modeling)&lt;/li&gt; &lt;li&gt;&lt;em&gt;TMRoPE&lt;/em&gt;: Time-aligned Multimodal Rotary Positional Encoding for audio-video synchronization&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual-path Generation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Thinker&lt;/em&gt;: Text-generating LLM backbone&lt;/li&gt; &lt;li&gt;&lt;em&gt;Talker&lt;/em&gt;: Dual-track AR model for audio token generation using Thinker's hidden states&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming Optimization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Sliding-window Diffusion Transformer (DiT) reduces audio latency&lt;/li&gt; &lt;li&gt;Simultaneous text/speech streaming output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;4. Technical Highlights&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Processing&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;End-to-end joint training without intermediate representations&lt;/li&gt; &lt;li&gt;Supports arbitrary modality combinations (single/mixed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Attention&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Native FlashAttention 2 support&lt;/li&gt; &lt;li&gt;Compatible with PyTorch SDPA&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Customization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Prebuilt voices: &lt;code&gt;Cherry&lt;/code&gt; (female) &amp;amp; &lt;code&gt;Ethan&lt;/code&gt; (male)&lt;/li&gt; &lt;li&gt;Dynamic voice switching via &lt;code&gt;spk&lt;/code&gt; parameter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment Flexibility&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Disable speech output to save VRAM (~2GB)&lt;/li&gt; &lt;li&gt;Text-only mode (&lt;code&gt;return_audio=False&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;5. Performance&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Benchmarks&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;SOTA on Omni-Bench&lt;/li&gt; &lt;li&gt;Outperforms same-scale Qwen2-VL/Qwen2-Audio in vision/audio tasks&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Understanding&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;First open-source model with text-level E2E speech instruction following&lt;/li&gt; &lt;li&gt;Matches text-input performance on MMLU/GSM8K with speech inputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;6. Implementation Details&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Auto device mapping (&lt;code&gt;device_map=&amp;quot;auto&amp;quot;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Mixed precision (&lt;code&gt;bfloat16/float16&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processing Pipeline&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Unified &lt;code&gt;Qwen2_5OmniProcessor&lt;/code&gt; handles multimodal inputs&lt;/li&gt; &lt;li&gt;Batch processing of mixed media combinations&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;7. Requirements&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;System Prompt&lt;/strong&gt;: Mandatory for full functionality:&lt;br /&gt; &lt;code&gt; &amp;quot;You are Qwen... capable of generating text and speech.&amp;quot; &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;FlashAttention 2 (optional acceleration)&lt;/li&gt; &lt;li&gt;FFmpeg (video/non-WAV audio processing)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;This architecture achieves deep multimodal fusion through innovative designs while maintaining strong text capabilities, significantly advancing audiovisual understanding/generation for multimodal agent development.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Also from the PR:&lt;/p&gt; &lt;p&gt;&lt;em&gt;We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omniâ€™s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can the community help confirm whether this PR is legit?&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(Original PR: &lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Sea8804"&gt; /u/Inevitable_Sea8804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T20:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6y0</id>
    <title>Are any of the big API providers (OpenAI, Anthropic, etc) actually making money, or are all of them operating at a loss and burning through investment cash?</title>
    <updated>2025-03-22T23:03:34+00:00</updated>
    <author>
      <name>/u/AnticitizenPrime</name>
      <uri>https://old.reddit.com/user/AnticitizenPrime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a consensus right now that local LLMs are not cheaper to run than the myriad of APIs out there at this time, when you consider the initial investment in hardware, the cost of energy, etc. The reasons for going local are for privacy, independence, hobbyism, tinkering/training your own stuff, working offline, or just the wow factor of being able to hold a conversation with your GPU.&lt;/p&gt; &lt;p&gt;But is that necessarily the case? Is it possible that these low API costs are unsustainable in the long term?&lt;/p&gt; &lt;p&gt;Genuinely curious. As far as I know, no LLM provider has turned a profit thus far, but I'd welcome a correction if I'm wrong.&lt;/p&gt; &lt;p&gt;I'm just wondering if the conception that 'local isn't as cheap as APIs' might not hold true anymore after all the investment money dries up and these companies need to actually price their API usage in a way that keeps the lights on and the GPUs going brrr.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnticitizenPrime"&gt; /u/AnticitizenPrime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhsqlr</id>
    <title>How does Groq.com do it? (Groq not Elon's grok)</title>
    <updated>2025-03-23T06:05:49+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does groq run llms so fast? Is it just very high power or they use some technique?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T06:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhrqov</id>
    <title>Here's another AMD Strix Halo Mini PC announcement with video of it running a 70B Q8 model.</title>
    <updated>2025-03-23T04:58:43+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the Sixunited 395+ Mini PC. It's also supposed to come out in May. It's all in Chinese. I do see what appears to be 3 token scroll across the screen. Which I assume means it's 3tk/s. Considering it's a 70GB model, that makes sense considering the memory bandwidth of Strix Halo.&lt;/p&gt; &lt;p&gt;The LLM stuff starts at about the 4 min mark.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1xhKsenE4T"&gt;https://www.bilibili.com/video/BV1xhKsenE4T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T04:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhgpew</id>
    <title>nsfw orpheus tts?</title>
    <updated>2025-03-22T19:37:43+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im currently in the data curation / filtering / cleaning phase&lt;/p&gt; &lt;p&gt;but i would like to see how many local guys would be interested in a tts for there anime waifus that can make &amp;quot;interesting&amp;quot; emotional noises&lt;/p&gt; &lt;p&gt;Total audio events found: 181218&lt;/p&gt; &lt;p&gt;(sighs): 8594&lt;/p&gt; &lt;p&gt;(laughs): 68590&lt;/p&gt; &lt;p&gt;(gasps): 14113&lt;/p&gt; &lt;p&gt;(moans): 20576&lt;/p&gt; &lt;p&gt;(whimpers): 418&lt;/p&gt; &lt;p&gt;(breathing): 114&lt;/p&gt; &lt;p&gt;(pants): 776&lt;/p&gt; &lt;p&gt;and many more ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhv57y</id>
    <title>Finally some good news for older hardware pricing</title>
    <updated>2025-03-23T09:04:21+00:00</updated>
    <author>
      <name>/u/xlrz28xd</name>
      <uri>https://old.reddit.com/user/xlrz28xd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3"&gt;https://www.businessinsider.com/nvidia-ceo-jensen-huang-joke-blackwell-hopper-gpu-customers-2025-3&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I said before that when Blackwell starts shipping in volume, you couldn't give Hoppers away,&amp;quot; he said at Nvidia's big AI conference Tuesday.&lt;/p&gt; &lt;p&gt;&amp;quot;There are circumstances where Hopper is fine,&amp;quot; he added. &amp;quot;Not many.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And then:&lt;/p&gt; &lt;p&gt;CFO Brian Olsavsky said on Amazon's earnings call last month that the company &amp;quot;observed an increased pace of technology development, particularly in the area of artificial intelligence and machine learning.&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;As a result, we're decreasing the useful life for a subset of our servers and networking equipment from 6 years to 5 years, beginning in January 2025,&amp;quot; Olsavsky said, adding that this will cut operating income this year by about $700 million.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Then, more bad news: Amazon &amp;quot;early-retired&amp;quot; some of its servers and network equipment, Olsavsky said, adding that this &amp;quot;accelerated depreciation&amp;quot; cost about $920 million and that the company expects it will decrease operating income in 2025 by about $600 million.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xlrz28xd"&gt; /u/xlrz28xd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhv57y/finally_some_good_news_for_older_hardware_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T09:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6jp</id>
    <title>Gemma3 is outperforming a ton of models on fine-tuning / world knowledge</title>
    <updated>2025-03-22T23:03:02+00:00</updated>
    <author>
      <name>/u/fluxwave</name>
      <uri>https://old.reddit.com/user/fluxwave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt; &lt;img alt="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" src="https://external-preview.redd.it/XifaOkXuUsJa3iGAsHuitV7h5kD9H1PhRfgSOnPUnbc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1663259cbca9e5b87b6c2d99b3d23f12f5a2118" title="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea"&gt;https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At fine-tuning they seem to be smashing evals -- see this tweet above from OpenPipe. &lt;/p&gt; &lt;p&gt;Then in world-knowledge (or at least this smaller task of identifying the gender of scholars across history) a 12B model beat OpenAI's gpt-4o-mini. This is using no fine-tuning. &lt;a href="https://thedataquarry.com/blog/using-llms-to-enrich-datasets/"&gt;https://thedataquarry.com/blog/using-llms-to-enrich-datasets/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p11ujen8nbqe1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897f8506ee01cffcbad459d11da436a2e1521501"&gt;Written by Prashanth Rao&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(disclaimer: Prashanth is a member of the BAML community -- our prompting DSL / toolchain &lt;a href="https://github.com/BoundaryML/baml"&gt;https://github.com/BoundaryML/baml&lt;/a&gt; , but he works at KuzuDB).&lt;/p&gt; &lt;p&gt;Has anyone else seen amazing results with Gemma3? Curious to see if people have tried it more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fluxwave"&gt; /u/fluxwave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhwr2p</id>
    <title>Next Gemma versions wishlist</title>
    <updated>2025-03-23T11:00:25+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Omar from the Gemma team. Few months ago, we &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hchoyy/open_models_wishlist/"&gt;asked for user feedback &lt;/a&gt;and incorporated it into Gemma 3: longer context, a smaller model, vision input, multilinguality, and so on, while doing a nice lmsys jump! We also made sure to collaborate with OS maintainers to have decent support at day-0 in your favorite tools, including vision in llama.cpp!&lt;/p&gt; &lt;p&gt;Now, it's time to look into the future. What would you like to see for future Gemma versions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhwr2p/next_gemma_versions_wishlist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T11:00:25+00:00</published>
  </entry>
</feed>
