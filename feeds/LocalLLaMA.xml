<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T05:37:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mz644g</id>
    <title>PCIe Bifurcation x4x4x4x4 Question</title>
    <updated>2025-08-24T20:12:17+00:00</updated>
    <author>
      <name>/u/ducksaysquackquack</name>
      <uri>https://old.reddit.com/user/ducksaysquackquack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: has anybody run into problems running pcie x16 to x4x4x4x4 on consumer hardware?&lt;/p&gt; &lt;p&gt;current setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;9800x3d (28 total pcie lanes, 24 usable lanes with 4 going to chipset)&lt;/li&gt; &lt;li&gt;64gb ddr5-6000&lt;/li&gt; &lt;li&gt;MSI x670e Mag Tomahawk WIFI board&lt;/li&gt; &lt;li&gt;5090 in pcie 5.0 x16 slot (cpu)&lt;/li&gt; &lt;li&gt;4090 in pcie 4.0 x4 slot (cpu)&lt;/li&gt; &lt;li&gt;3090ti in pcie 4.0 x2 slot (chipset)&lt;/li&gt; &lt;li&gt;Corsair HX1500i psu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i have two 3060 12gb that i have laying around and would like to add to the system, if anything just for the sake of using them instead of sitting in box. i would like to pick up two 3090 off fb market, but i'm not really trying to spend $500-$600 each for what folks are asking in my area. and since i already had these 3060 sitting around, why not use them. &lt;/p&gt; &lt;p&gt;i don't believe i'll have power issues since right now, aida64 sensor panel shows the hx1500i hitting max 950w during inference. psu connects via usb for power monitoring. i can't imagine the 3060 using more than 150w each, since they're only 1x8-pin each.&lt;/p&gt; &lt;p&gt;bios shows x16 slot can do either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;x8x8&lt;/li&gt; &lt;li&gt;x8x4x4&lt;/li&gt; &lt;li&gt;x4x4x4x4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;also, all i can find are $20-$50 bifurcation cards that are pcie 3.0, would dropping to gen3 be an issue during inference?&lt;/p&gt; &lt;p&gt;i'd like to have 5090/4090/3090ti/3060 on the bifurcation card and second 3060 on the pcie secondary x16 slot. hopefully add 3090 down the line if they price drop after the new supers release later this year.&lt;/p&gt; &lt;p&gt;if this is not worth it, then it's no biggie. i just like tinkering.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ducksaysquackquack"&gt; /u/ducksaysquackquack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfcby</id>
    <title>I Built a Separate Module for Qwen3 Coder – Outperforming Gemini 2.5 Pro now</title>
    <updated>2025-08-25T02:57:22+00:00</updated>
    <author>
      <name>/u/Ok-Pattern9779</name>
      <uri>https://old.reddit.com/user/Ok-Pattern9779</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I developed a separate module for Qwen3-Coder-480B after analyzing the issues with other editors.&lt;/p&gt; &lt;p&gt;The key focus is on full-file edits, which should be performed in the following format:&lt;/p&gt; &lt;p&gt;[FILE:path/to/filename.ext] (entire new file content on subsequent lines) [ENDFILE]&lt;/p&gt; &lt;p&gt;AnchorEdit mode should be limited to a maximum of 3 lines. If a tool call for AnchorEdit fails, the system should provide clear and proper feedback.&lt;/p&gt; &lt;p&gt;Each session should allow at least 30+ tool calls, and prompts should encourage reading the relevant files using read tool.&lt;/p&gt; &lt;p&gt;Currently, Qwen Coder outperforms Gemini 2.5 Pro, with fewer coding errors. The only advantage of Gemini is that it can sometimes understand my intentions without detailed explanations. However, consistently, Qwen Coder delivers better results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pattern9779"&gt; /u/Ok-Pattern9779 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfcby/i_built_a_separate_module_for_qwen3_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfcby/i_built_a_separate_module_for_qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfcby/i_built_a_separate_module_for_qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T02:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzd5m5</id>
    <title>Choosing between a single 3080TI; or dual 3060 12GBs</title>
    <updated>2025-08-25T01:11:12+00:00</updated>
    <author>
      <name>/u/DickFineman73</name>
      <uri>https://old.reddit.com/user/DickFineman73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title is self explanatory - but I'm adding a GPU to a home server for both locally hosted LLMs and Stable Diffusion; and originally I was just going to get a single 3080TI with 12GB of VRAM... but then I realized I can get two 3060s with 12GB of VRAM apiece for the same cost.&lt;/p&gt; &lt;p&gt;Does it make sense to pursue additional VRAM over the horsepower that the 3080TI would give me? Or would I be better off having the faster 3080TI without as much VRAM? &lt;/p&gt; &lt;p&gt;I don't have a direct use-case yet; I've got a CS degree and undergrad background in AI, so really I'm more &amp;quot;playing around&amp;quot; with this than anything else. So rather than having a specific usecase, I think the better question is: &amp;quot;If I have $500 to blow on a GPU, which way is the most flexible/extensible/interesting - and is there a third option I haven't considered?&amp;quot;&lt;/p&gt; &lt;p&gt;I also already have plenty of experience with self-hosted image generation tools like Automatic1111 - so I'm fine on that front; it's the LLM side that I'm more hesitant on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DickFineman73"&gt; /u/DickFineman73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd5m5/choosing_between_a_single_3080ti_or_dual_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd5m5/choosing_between_a_single_3080ti_or_dual_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd5m5/choosing_between_a_single_3080ti_or_dual_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T01:11:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzg1hq</id>
    <title>How to convert HF model to MLX without ram limitation</title>
    <updated>2025-08-25T03:33:33+00:00</updated>
    <author>
      <name>/u/Desperate-Sir-5088</name>
      <uri>https://old.reddit.com/user/Desperate-Sir-5088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently fine-tuning a large LLM model using MLX on the Apple M3 Ultra. The original tensor files recently released are larger than the M3's RAM (256GB), making it impossible to perform quantization locally using mlx_lm.convert. Additionally, it seems impossible to use HF's mlx-my-repo.&lt;/p&gt; &lt;p&gt;In summary, is there a way to perform quantization without memory restrictions by sequentially reading Deepseek v3.1 or KIMI K-2?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Sir-5088"&gt; /u/Desperate-Sir-5088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzg1hq/how_to_convert_hf_model_to_mlx_without_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzg1hq/how_to_convert_hf_model_to_mlx_without_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzg1hq/how_to_convert_hf_model_to_mlx_without_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzgarm</id>
    <title>RAG for financial fact checking</title>
    <updated>2025-08-25T03:47:26+00:00</updated>
    <author>
      <name>/u/Fast-Smoke-1387</name>
      <uri>https://old.reddit.com/user/Fast-Smoke-1387</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone here use LLM for multi class classification? I am using RAG by extracting top 30 docs from DuckDuckgo API, but the performance is measurable. &lt;/p&gt; &lt;p&gt;My dataset has 5 classes; True, Mostly True, Half True, False, Mostly false. It very often collapsed Between mostly true and true, it never predicted half-true. Rarely predicted true as well.&lt;/p&gt; &lt;p&gt;Any insight on this? Should I use LoRA for this kind of problem? I am new to this area, any help would be appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast-Smoke-1387"&gt; /u/Fast-Smoke-1387 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzgarm/rag_for_financial_fact_checking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzgarm/rag_for_financial_fact_checking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzgarm/rag_for_financial_fact_checking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mybft5</id>
    <title>grok 2 weights</title>
    <updated>2025-08-23T20:00:52+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt; &lt;img alt="grok 2 weights" src="https://external-preview.redd.it/4tfHT9vpFrwHCpX5cn0_tHyoUS8M6oeQ7jwWbePCicw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9576154cc1820a09f2c9b345d4d88427c3729b9a" title="grok 2 weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/xai-org/grok-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz0640</id>
    <title>Best small local llm for coding</title>
    <updated>2025-08-24T16:28:21+00:00</updated>
    <author>
      <name>/u/Low-Palpitation-4724</name>
      <uri>https://old.reddit.com/user/Low-Palpitation-4724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;br /&gt; I am looking for good small llm for coding. By small i mean somewhere around 10b parameters like gemma3:12b or codegemma. I like them both but first one is not specifically coding model and second one is a year old. Does anyone have some suggestions about other good models or a place that benchmarks those? I am talking about those small models because i use them on gpu with 12gb vram or even laptop with 8.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Palpitation-4724"&gt; /u/Low-Palpitation-4724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mypokb</id>
    <title>GPT OSS 20b is Impressive at Instruction Following</title>
    <updated>2025-08-24T07:56:56+00:00</updated>
    <author>
      <name>/u/crodjer</name>
      <uri>https://old.reddit.com/user/crodjer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found GPT OSS 20b to be consistently great at following complex instructions. For instance, it did performed perfectly with a test prompt I used: &lt;a href="https://github.com/crodjer/glaince/tree/main/cipher#results"&gt;https://github.com/crodjer/glaince/tree/main/cipher#results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All other models in the same size (Gemma 3, Qwen 3, Mistral Small) make the same mistake, resulting them to deviate from expectation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crodjer"&gt; /u/crodjer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T07:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzejbm</id>
    <title>Where do I go to see benchmark comparisons of local models?</title>
    <updated>2025-08-25T02:16:59+00:00</updated>
    <author>
      <name>/u/radioactive---banana</name>
      <uri>https://old.reddit.com/user/radioactive---banana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize if this is off topic, I can't find any good places that show a significant amount of locally hostable models and how they compare to the massive closed ones.&lt;/p&gt; &lt;p&gt;What should I do to get a general value assigned to how good models like gemma3 27b vs 12b, Qwen, etc are in comparison to each other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radioactive---banana"&gt; /u/radioactive---banana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzejbm/where_do_i_go_to_see_benchmark_comparisons_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzejbm/where_do_i_go_to_see_benchmark_comparisons_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzejbm/where_do_i_go_to_see_benchmark_comparisons_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T02:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytpf1</id>
    <title>Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)</title>
    <updated>2025-08-24T11:59:59+00:00</updated>
    <author>
      <name>/u/Mass2018</name>
      <uri>https://old.reddit.com/user/Mass2018</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt; &lt;img alt="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" src="https://b.thumbs.redditmedia.com/x4bULEyzYY3EEBanwjUi7dZiywWjI4EP8X8WNQAbYyk.jpg" title="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of discussion recently about the performance of the Apple studios with large models, so I thought I'd share actual data from about a month of usage in our household.&lt;/p&gt; &lt;p&gt;This is mainly used by the non-me part of our household, so it sits nice and stable and just runs Deepseek 24/7, where my personal rig is constantly being swapped between different things that I'm working on.&lt;/p&gt; &lt;p&gt;The Apple Studio replaced the 10xP100 rig I had previously built for this purpose, and I have to say for what we're using it for it's been a godsend. It's much, much faster, can load larger models, has a much lower power footprint, and it was just... so easy to get it up and running. Honestly, it felt a bit like cheating after the hell that the P100 rig put me through.&lt;/p&gt; &lt;p&gt;Anyway, actual numbers:&lt;/p&gt; &lt;p&gt;|| || |Total logged requests:|161| |Context Average:|643.72| |Average Prompt Eval Tokens/Second:|64.73 tokens/second| |Average Tokens Generated:|343.16| |Average Tokens Generated/Second:|13.97 tokens/second|&lt;/p&gt; &lt;p&gt;My personal opinion is if all you're going to do is inferencing, it's a great option. I absolutely loathe the Mac GUI, and my constant attempt to control-c/control-v is infuriating, but other than that... NO RAGRETS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mass2018"&gt; /u/Mass2018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mytpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzd0ik</id>
    <title>PSA: Filling those empty DIMM slots will slow down inference if you don’t have enough memory channels</title>
    <updated>2025-08-25T01:04:16+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 7900x on a x670e Pro RS mobo with 2x32GB &lt;a href="mailto:DDR5@5200"&gt;DDR5@5200&lt;/a&gt;. I really wanted to run GPT-OSS 120B with CPU moe but it wasn’t fully able to load. I obtained another pair of the same RAM (different batch, but same model/specs) and was able to run 120B, but only at 15 tk/s. I noticed that other models were slower as well. Then I realized that my RAM was running at 3600MTS as opposed to the 4800 it was at before. After digging into this issue it appears to be the grim reality with AMD AM5 boards that there isn’t much support for full throttle with DDR5 at 4 DIMMs. One would need an Intel build to get there apparently. In my case I think I’ll try to exchange for 2x48GB and sell my old RAM. &lt;/p&gt; &lt;p&gt;Does anyone know any way to use 4 slots at decent speeds and stability without buying a TR/EPYC?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzd0ik/psa_filling_those_empty_dimm_slots_will_slow_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T01:04:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4vwu</id>
    <title>What is the smallest model that rivals GPT-3.5?</title>
    <updated>2025-08-24T19:25:09+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I was recently looking at an old project of mine that i did as my bachelor's thesis back in Q2 2023 where i created a multi-agent system using one of the first versions of langchain and GPT-3.5. &lt;/p&gt; &lt;p&gt;This made me think about all the progress that we've made in the LLM world in such a short period of time, especially in the open-source space.&lt;/p&gt; &lt;p&gt;So, as the title suggests, What do you think is the smallest, open-source model that is &lt;em&gt;generally&lt;/em&gt; as good or better than GPT-3.5? I'm' not talking about a specific task, but general knowledge, intelligence and capability of completing a wide array of tasks. My guess would be something in the 30B parameter count, such as Qwen3-32B. Maybe with reasoning this number could go even lower, but i personally think it's a bit like cheating because we didn't have reasoning back in Q2 2023. &lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz2di5</id>
    <title>I tried fine-tuning Gemma-3-270m and prepared for deployments</title>
    <updated>2025-08-24T17:50:40+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google recently released &lt;strong&gt;Gemma3-270M&lt;/strong&gt; model, which is one of the smallest open models out there.&lt;br /&gt; Model weights are available on Hugging Face and its size is ~550MB and there were some testing where it was being used on phones.&lt;/p&gt; &lt;p&gt;It’s one of the perfect models for fine-tuning, so I put it to the test using the official Colab notebook and an NPC game dataset.&lt;/p&gt; &lt;p&gt;I put everything together as a written guide in my newsletter and also as a small demo video while performing the steps.&lt;/p&gt; &lt;p&gt;I have skipped the fine-tuning part in the guide because you can find the official notebook on the release blog to test using Hugging Face Transformers. I did the same locally on my notebook.&lt;/p&gt; &lt;p&gt;Gemma3-270M is so small that fine-tuning and testing were finished in just a few minutes (~15). Then I used a open source tool called KitOps to package it together for secure production deployments.&lt;/p&gt; &lt;p&gt;I was trying to see if fine-tuning this small model is fast and efficient enough to be used in production environments or not. The steps I covered are mainly for devs looking for secure deployment of these small models for real apps. (example covered is very basic and done on Mac mini M4)&lt;/p&gt; &lt;p&gt;Steps I took are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Importing a Hugging Face Model&lt;/li&gt; &lt;li&gt;Fine-Tuning the Model&lt;/li&gt; &lt;li&gt;Initializing the Model with KitOps&lt;/li&gt; &lt;li&gt;Packaging the model and related files after fine-tuning&lt;/li&gt; &lt;li&gt;Push to a Hub to get security scans done and container deployments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;watch the demo video – &lt;a href="https://youtu.be/8SKV_m5XV6o"&gt;here&lt;/a&gt;&lt;br /&gt; take a look at the guide – &lt;a href="https://mranand.substack.com/p/you-can-fine-tune-gemma3-270m-in"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T17:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1myjzmn</id>
    <title>There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)</title>
    <updated>2025-08-24T02:26:33+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt; &lt;img alt="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" src="https://preview.redd.it/2t25pwj6ovkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c8abd5ee1bf8381408ed5b298fc42879b01bd1" title="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And they have better licenses, less restrictions. What exactly is the point of Grok 2 then? I appreciate open source effort, but wouldn't it make more sense to open source a competitive model that can at least be run locally by most people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2t25pwj6ovkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz42eu</id>
    <title>Qwen3-Coder-480B Q4_0 on 6x7900xtx</title>
    <updated>2025-08-24T18:54:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt; &lt;img alt="Qwen3-Coder-480B Q4_0 on 6x7900xtx" src="https://a.thumbs.redditmedia.com/OtUhAjy3dNMyywvnZbc9ZIPzO4CmV_Rfiexy6H6qaR8.jpg" title="Qwen3-Coder-480B Q4_0 on 6x7900xtx" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-Coder-480B Q4_0 on 6x7900xtx with 7 token/s&lt;/strong&gt; output speed, did you have any suggestion or ideas to speed up it?&lt;/p&gt; &lt;p&gt;Maybe you know smart-offloading specific layers?&lt;/p&gt; &lt;p&gt;I launch it with this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./lama-hip-0608/build/bin/llama-server \ --model 480B-A35B_Q4_0/Qwen3-Coder-480B-A35B-Instruct-Q4_0-00001-of-00006.gguf \ --main-gpu 0 \ --temp 0.65 \ --top-k 20 \ --min-p 0.0 \ --top-p 0.95 \ --gpu-layers 48 \ --ctx-size 4000 \ --host 0.0.0.0 \ --port ${PORT} \ --parallel 1 \ --tensor-split 24,24,24,24,24,24 \ --jinja \ --mlock \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ -ot &amp;quot;.ffn_(down)_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T18:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1myx4l5</id>
    <title>Which local model are you currently using the most? What’s your main use case, and why do you find it good?</title>
    <updated>2025-08-24T14:32:16+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzaeee</id>
    <title>InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</title>
    <updated>2025-08-24T23:05:11+00:00</updated>
    <author>
      <name>/u/Dull-Ad-1708</name>
      <uri>https://old.reddit.com/user/Dull-Ad-1708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MeiGen-AI/InfiniteTalk"&gt;https://github.com/MeiGen-AI/InfiniteTalk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/vBgoXVW.mp4"&gt;https://i.imgur.com/vBgoXVW.mp4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Ad-1708"&gt; /u/Dull-Ad-1708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz59l</id>
    <title>Seed-OSS is insanely good</title>
    <updated>2025-08-24T15:50:03+00:00</updated>
    <author>
      <name>/u/I-cant_even</name>
      <uri>https://old.reddit.com/user/I-cant_even</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took a day for me to get it running but *wow* this model is good. I had been leaning heavily on a 4bit 72B Deepseek R1 Distill but it had some regularly frustrating failure modes.&lt;/p&gt; &lt;p&gt;I was prepping to finetune my own model to address my needs but now it's looking like I can remove refusals and run Seed-OSS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I-cant_even"&gt; /u/I-cant_even &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1myrdtb</id>
    <title>Mistral Large soon?</title>
    <updated>2025-08-24T09:45:24+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt; &lt;img alt="Mistral Large soon?" src="https://preview.redd.it/m9zk5bipuxkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ff9d33632d0268f989230460a6dbd3328b7244" title="Mistral Large soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source &lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;https://mistral.ai/news/mistral-medium-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9zk5bipuxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza0wy</id>
    <title>Made Chatterbox TTS a bit faster again on CUDA (155it/s on 3090)</title>
    <updated>2025-08-24T22:49:20+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/rsxdalv/chatterbox/tree/faster"&gt;https://github.com/rsxdalv/chatterbox/tree/faster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous version discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/&lt;/a&gt; (hopefully most of the old questions will become obsolete)&lt;/p&gt; &lt;p&gt;Disclaimer - for batched generation in dedicated deployments Chatterbox-VLLM should be the better choice.&lt;/p&gt; &lt;p&gt;I have mostly exhausted the options for speeding up almost vanilla HF Transformers' Llama with torch. Inductor, Triton, Max Autotune, different cache sizes etc, and they are available in the codebase. In the end, manually capturing cuda-graphs was the fastest. The model should be able to run around 230 it/s with fused kernels and better code. (I was unable to remedy the kv_cache code to enable cuda graph capture with torch.compile's max autotune.) Besides the speed, the main benefit is that setting a small cache size is no longer necessary, neither are max_new_tokens important. I plan to make it compile by default to facilitate drop-in use in other projects. Since the main effort is exhausted, I will keep on updating incrementally - for example, speeding up the s3gen (which is now a bottleneck).&lt;/p&gt; &lt;h1&gt;Results for 1500 cache size with BFloat16&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:02&amp;lt;00:04, 159.15it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 2.05 seconds 156.29 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:01&amp;lt;00:03, 170.52it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 1.88 seconds 170.87 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([2, 339, 1024]) Sampling: 62%|██████▏ | 620/1000 [00:04&amp;lt;00:02, 154.58it/s] Stopping at 621 because EOS token was generated Generated 621 tokens in 4.01 seconds 154.69 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([2, 46, 1024]) Sampling: 4%|▍ | 40/1000 [00:00&amp;lt;00:05, 182.08it/s] Stopping at 41 because EOS token was generated Generated 41 tokens in 0.22 seconds 184.94 it/s &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Disabling classifier free guidance (cfg_weight=0)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 169.38it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.89 seconds 158.95 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 194.04it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.55 seconds 193.66 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([1, 338, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 182.28it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.65 seconds 182.22 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([1, 45, 1024]) Sampling: 20%|██ | 60/300 [00:00&amp;lt;00:01, 208.54it/s] Stopping at 61 because EOS token was generated Generated 61 tokens in 0.29 seconds 210.54 it/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current code example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: ChatterboxTTS, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) torch.cuda.empty_cache() return model # Most new GPUs would work the fastest with this, but not all. t3_to(model, torch.bfloat16) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, warmup&amp;quot;) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, full speed&amp;quot;) # Extra options: audio = model.generate( text, t3_params={ # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;eager&amp;quot;, # slower - default # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # speeds up set up # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-manual&amp;quot;, # fastest - default # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;eager&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor-strided&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-strided&amp;quot;, # &amp;quot;stride_length&amp;quot;: 4, # &amp;quot;strided&amp;quot; options compile &amp;lt;1-2-3-4&amp;gt; iteration steps together, which improves performance by reducing memory copying issues in torch.compile # &amp;quot;skip_when_1&amp;quot;: True, # skips Top P when it's set to 1.0 # &amp;quot;benchmark_t3&amp;quot;: True, # Synchronizes CUDA to get the real it/s } ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T22:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz6f1</id>
    <title>Fast CUDA DFloat11 decoding kernel</title>
    <updated>2025-08-24T15:51:15+00:00</updated>
    <author>
      <name>/u/No_Dimension41</name>
      <uri>https://old.reddit.com/user/No_Dimension41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago, I came across the amazing work on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DFloat11&lt;/a&gt;, which achieves lossless output while shrinking models to 70% of their original size by compressing the exponent bits of BF16. It is a great work. However, I found a problem: it decompresses an entire tensor into VRAM, and then perform computations separately, which severely impacts the model's decoding speed. According to some &lt;a href="https://github.com/LeanModels/DFloat11/issues/7"&gt;issues&lt;/a&gt; on GitHub, it only reaches about 1/3 of the native BF16 speed. Furthermore, the author hasn't released the code for encoding the models, and the decoding kernel is provided in a nearly unreadable PTX format.&lt;/p&gt; &lt;p&gt;So, I decided to write my own implementation. I used the Huffman coding and LUT-based decoding algorithms described in their &lt;a href="https://arxiv.org/abs/2504.11651"&gt;paper&lt;/a&gt;, but I &lt;strong&gt;fused the Huffman decoding process and the GEMV operation into a single kernel&lt;/strong&gt;. This avoids unnecessary memory bandwidth overhead and dramatically speeds up decoding.&lt;/p&gt; &lt;p&gt;With a batch size of 1, my implementation can now reach about &lt;strong&gt;90% of native BF16 speed&lt;/strong&gt; on regular GPUs. On some VRAM bandwidth-constrained GPUs, like the RTX 4060 Ti, it can even &lt;strong&gt;surpass native BF16 speed&lt;/strong&gt; because the compressed weights reduce the demand on VRAM bandwidth.&lt;/p&gt; &lt;p&gt;Here's a simple benchmark for generating 256 tokens:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Raw BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Compressed BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Raw / Compressed Size&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 7B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;14.98s&lt;/td&gt; &lt;td align="left"&gt;13.02s&lt;/td&gt; &lt;td align="left"&gt;14.19 / 10.99 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;6.66s&lt;/td&gt; &lt;td align="left"&gt;7.23s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;14.11s&lt;/td&gt; &lt;td align="left"&gt;15.26 / 11.52 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;7.75s&lt;/td&gt; &lt;td align="left"&gt;8.24s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of course, there are still areas for improvement. Due to the extra padding required by the CUDA kernel's layout, the current compression rate is slightly lower than the original DFloat11, achieving around 75%-80%. Additionally, support for uncommon tensor shapes and batch sizes greater than 1 is currently limited.&lt;/p&gt; &lt;p&gt;For more information, please visit my GitHub repository: &lt;a href="https://github.com/lszxb/bf16_huffman_infer"&gt;https://github.com/lszxb/bf16_huffman_infer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Dimension41"&gt; /u/No_Dimension41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz6som</id>
    <title>Almost done with the dashboard for local llama.cpp agents</title>
    <updated>2025-08-24T20:38:43+00:00</updated>
    <author>
      <name>/u/PayBetter</name>
      <uri>https://old.reddit.com/user/PayBetter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt; &lt;img alt="Almost done with the dashboard for local llama.cpp agents" src="https://b.thumbs.redditmedia.com/7LaV7Jli4Sm51VyrQaQKuYWfN3-w_vEMntHaCP24k1w.jpg" title="Almost done with the dashboard for local llama.cpp agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This won't be for sale and will be released as open source with a non commercial license. No code will be released until after the hackathon I've entered is over next month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PayBetter"&gt; /u/PayBetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz6som"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzfh73</id>
    <title>Intel Granite Rapids CPU on sale at Newegg up to 65% off MSRP</title>
    <updated>2025-08-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very good news for people who want to run the huge MoE models nowadays.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;MSRP&lt;/th&gt; &lt;th align="left"&gt;newegg&lt;/th&gt; &lt;th align="left"&gt;% off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;6980P&lt;/td&gt; &lt;td align="left"&gt;$17800&lt;/td&gt; &lt;td align="left"&gt;$6179&lt;/td&gt; &lt;td align="left"&gt;65.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6972P&lt;/td&gt; &lt;td align="left"&gt;$14600&lt;/td&gt; &lt;td align="left"&gt;$5433.2&lt;/td&gt; &lt;td align="left"&gt;62.79%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6944P&lt;/td&gt; &lt;td align="left"&gt;$6850&lt;/td&gt; &lt;td align="left"&gt;$4208&lt;/td&gt; &lt;td align="left"&gt;38.57%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6781P&lt;/td&gt; &lt;td align="left"&gt;$8960&lt;/td&gt; &lt;td align="left"&gt;$7590&lt;/td&gt; &lt;td align="left"&gt;15.29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6761P&lt;/td&gt; &lt;td align="left"&gt;$6570&lt;/td&gt; &lt;td align="left"&gt;$6001&lt;/td&gt; &lt;td align="left"&gt;8.66%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6741P&lt;/td&gt; &lt;td align="left"&gt;$4421&lt;/td&gt; &lt;td align="left"&gt;$3900&lt;/td&gt; &lt;td align="left"&gt;11.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6731P&lt;/td&gt; &lt;td align="left"&gt;$2700&lt;/td&gt; &lt;td align="left"&gt;$2260.1&lt;/td&gt; &lt;td align="left"&gt;16,29%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6521P&lt;/td&gt; &lt;td align="left"&gt;$1250&lt;/td&gt; &lt;td align="left"&gt;$1208.2&lt;/td&gt; &lt;td align="left"&gt;3.34%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzfh73/intel_granite_rapids_cpu_on_sale_at_newegg_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
