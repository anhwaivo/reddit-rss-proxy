<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-24T20:06:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mytilm</id>
    <title>What are my best options for using Video Understanding Vision Language Models?</title>
    <updated>2025-08-24T11:49:47+00:00</updated>
    <author>
      <name>/u/LivingMNML</name>
      <uri>https://old.reddit.com/user/LivingMNML</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;I am working on a project that uses VLM models to analyse high fps tennis matches.&lt;/p&gt; &lt;p&gt;I am currently using Google Gemini 2.5 Pro, however they are limited to 1fps above 20mb and also I am not able to finetune it, I have been looking at benchmarks and have seen Salmonn 7b+ PEFT (on top of Qwen2.5), and now there is VLM 4.5, which I tried to use via the online demo but it didn't get good results, maybe it was confused with FPS etc.&lt;/p&gt; &lt;p&gt;What is the current best strategy for using a VLM to understand video at high FPS (5-10fps).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LivingMNML"&gt; /u/LivingMNML &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytilm/what_are_my_best_options_for_using_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:49:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4l65</id>
    <title>Local llm inside vsc</title>
    <updated>2025-08-24T19:13:49+00:00</updated>
    <author>
      <name>/u/kindkatz</name>
      <uri>https://old.reddit.com/user/kindkatz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have downloaded models and got them to run inside LM studio, but i am having problems getting those same models to run inside vsc extensions. I've tried roo code and cline, also with ollama. I think maybe i am skipping a step with cmds inside a terminal?&lt;/p&gt; &lt;p&gt;I am trying to run local llm free inside vsc without restrictions or limits. Most of the guides on youtube are using api providers through online services. I was trying to go this other route because i recently hit request cap on paid gemini a few days ago.&lt;/p&gt; &lt;p&gt;I know nothing of about coding, so yes a complete novice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindkatz"&gt; /u/kindkatz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4l65/local_llm_inside_vsc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4l65/local_llm_inside_vsc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4l65/local_llm_inside_vsc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:13:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mycmn2</id>
    <title>How long do you think it will take Chinese AI labs to respond to NanoBanana?</title>
    <updated>2025-08-23T20:48:55+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"&gt; &lt;img alt="How long do you think it will take Chinese AI labs to respond to NanoBanana?" src="https://preview.redd.it/gn3t9xnyztkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bf5c98d7901fa481076fe8c443580997af8c8d6" title="How long do you think it will take Chinese AI labs to respond to NanoBanana?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gn3t9xnyztkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mycmn2/how_long_do_you_think_it_will_take_chinese_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mysjww</id>
    <title>What is the Claude equivalent of DeepSeek v3.1 in coding ability?</title>
    <updated>2025-08-24T10:55:39+00:00</updated>
    <author>
      <name>/u/Livid-Self-5770</name>
      <uri>https://old.reddit.com/user/Livid-Self-5770</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been testing &lt;strong&gt;DeepSeek v3.1&lt;/strong&gt; for coding tasks and found it to be pretty solid so far. Out of curiosity, for those who have tried both, what would be the &lt;strong&gt;Claude model that‚Äôs roughly equivalent to DeepSeek v3.1&lt;/strong&gt; in terms of coding ability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid-Self-5770"&gt; /u/Livid-Self-5770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mysjww/what_is_the_claude_equivalent_of_deepseek_v31_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T10:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1myhawv</id>
    <title>Ever Wondered What‚Äôs Hiding in the ‚ÄúSystem Prompt‚Äù of Your Favorite AI Tool? I Scraped 10k+ Lines of Them</title>
    <updated>2025-08-24T00:11:14+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So‚Ä¶ turns out a lot of the magic in today‚Äôs ‚Äúsmart‚Äù AI tools isn‚Äôt just the model, it‚Äôs the system prompt quietly steering it behind the scenes. I‚Äôve been extracting these for months, and I published everything I found into a repo:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inside you‚Äôll find: - The hidden prompts from V0, Cursor, Manus, Lovable, Devin, Replit Agent, VSCode Agent, Windsor, Warp.dev, etc. - Over 10,000+ lines of text, showing how different companies structure reasoning, enforce rules, and sometimes‚Ä¶ straight-up contradict themselves.&lt;/p&gt; &lt;p&gt;It‚Äôs weirdly fascinating to see how varied these scaffolds are: some are verbose manifestos, others are brittle one-liners, some try to sound ‚Äúhuman,‚Äù and some read like legal contracts.&lt;/p&gt; &lt;p&gt;If you‚Äôre into red-teaming, agent design, prompt engineering, or just model anthropology, this repo is a candy store.&lt;/p&gt; &lt;p&gt;Curious which ones you find the most unhinged or overengineered, drop your favorite discoveries if you dig through.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myhawv/ever_wondered_whats_hiding_in_the_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T00:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1myigna</id>
    <title>"Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?"</title>
    <updated>2025-08-24T01:08:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt; &lt;img alt="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" src="https://external-preview.redd.it/amthMTBncThhdmtmMeYkHvQl6ANcbp9DAX5oa2nUyz5pQDo1cq9KjrP_m95D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23c6d3e5b22e0c35924b85610790740771d73e6b" title="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r0ym4gq8avkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T01:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1myvpqv</id>
    <title>Open Source Tool for Manga translation</title>
    <updated>2025-08-24T13:33:30+00:00</updated>
    <author>
      <name>/u/New_Blueberry9858</name>
      <uri>https://old.reddit.com/user/New_Blueberry9858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some paid tools for manga translation, like INKR studio, but turns out to be pretty expensive. Thus our team at curify-ai worked on our custom manga translation tool and decided to open source the prototype at : &lt;a href="https://huggingface.co/spaces/Curify/manga_translation"&gt;https://huggingface.co/spaces/Curify/manga_translation&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The prototype features the following:&lt;br /&gt; a. Horizontally cropping skinny manga images to improve its visibility.&lt;/p&gt; &lt;p&gt;b. Using PaddleOCR to detect text and use a polygon based approach for inpaint. Still need to improve OCR and inpainting method, Qwen might be a good candidate.&lt;/p&gt; &lt;p&gt;c. Translate with Microsoft translator and allow customization of translated text.&lt;/p&gt; &lt;p&gt;d. Render the translated image.&lt;/p&gt; &lt;p&gt;It's still work in progress, welcome to use and suggest improvements. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Blueberry9858"&gt; /u/New_Blueberry9858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T13:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1myb09v</id>
    <title>Google and Anthropic struggle to keep marketshare as everyone else catches up</title>
    <updated>2025-08-23T19:44:10+00:00</updated>
    <author>
      <name>/u/ObnoxiouslyVivid</name>
      <uri>https://old.reddit.com/user/ObnoxiouslyVivid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt; &lt;img alt="Google and Anthropic struggle to keep marketshare as everyone else catches up" src="https://preview.redd.it/35p1pim9ntkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e48d4b2543aa0cd859924de94edd03937a9fc35a" title="Google and Anthropic struggle to keep marketshare as everyone else catches up" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data from last 6 months on OpenRouter compared to now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObnoxiouslyVivid"&gt; /u/ObnoxiouslyVivid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p1pim9ntkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T19:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytkpp</id>
    <title>Do you still use mikupad or is there a replacement?</title>
    <updated>2025-08-24T11:52:58+00:00</updated>
    <author>
      <name>/u/aeroumbria</name>
      <uri>https://old.reddit.com/user/aeroumbria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mikupad was my go-to tool for generating text with the option to show alternative tokens. This is especially useful for getting a feel of a model's preferences, writing stories, hacking context, or just working with non-conversational tasks in general. However, it has not been updated for a while, and although still fully functional, I actually had to revert to an earlier commit to make alternative tokens work, as the last commit broke the function, and the prospect of this function breaking again with no fix is not reassuring. Has anyone found a good alternative for mikupad, or is it still the best tool we have for now? &lt;/p&gt; &lt;p&gt;In case this is not clear enough, by &amp;quot;alternative tokens&amp;quot; I mean the ability to see the top K options at each step of the generation, and in mikupad you can even click any of them and restart generation using the selected choice as the last input.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeroumbria"&gt; /u/aeroumbria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytbfz</id>
    <title>Accuracy recovery adapter with self-generated data (magpie-style)</title>
    <updated>2025-08-24T11:39:03+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLama"&gt;r/LocalLLama&lt;/a&gt;! Wanted to share a technique that's been working really well for recovering performance after INT4 quantization. &lt;/p&gt; &lt;p&gt;Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. &lt;/p&gt; &lt;p&gt;Last year Apple's foundational models paper (&lt;a href="https://arxiv.org/pdf/2407.21075"&gt;https://arxiv.org/pdf/2407.21075&lt;/a&gt;) had proposed a similar technique and found &amp;quot;By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.&amp;quot; (page 47). &lt;/p&gt; &lt;p&gt;We saw similar results on Qwen3-0.6B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Perplexity: 2.40 ‚Üí 2.09 (only 5.7% degradation from FP16 baseline)&lt;/li&gt; &lt;li&gt;Memory: Only 0.28GB vs 1.0GB for FP16 (75% reduction)&lt;/li&gt; &lt;li&gt;Speed: 3.0x faster inference than FP16&lt;/li&gt; &lt;li&gt;Quality: Generates correct, optimized code solutions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_1_Self_Distillation_For_Quantization_Recovery.ipynb"&gt;Colab notebook with full implementation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/codelion/Qwen3-0.6B-accuracy-recovery-lora"&gt;Pre-trained adapter on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/ellora"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the implementation or help anyone trying to replicate this. The key insight is that quantization errors are systematic and learnable - a small adapter can bridge the gap without negating the benefits of quantization.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with self-distillation for quantization recovery? Would love to hear about different approaches!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz0640</id>
    <title>Best small local llm for coding</title>
    <updated>2025-08-24T16:28:21+00:00</updated>
    <author>
      <name>/u/Low-Palpitation-4724</name>
      <uri>https://old.reddit.com/user/Low-Palpitation-4724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;br /&gt; I am looking for good small llm for coding. By small i mean somewhere around 10b parameters like gemma3:12b or codegemma. I like them both but first one is not specifically coding model and second one is a year old. Does anyone have some suggestions about other good models or a place that benchmarks those? I am talking about those small models because i use them on gpu with 12gb vram or even laptop with 8.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Palpitation-4724"&gt; /u/Low-Palpitation-4724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mymyfu</id>
    <title>A timeline of LLM Context Windows, Over the past 5 years. (done right this time)</title>
    <updated>2025-08-24T05:09:43+00:00</updated>
    <author>
      <name>/u/jack-ster</name>
      <uri>https://old.reddit.com/user/jack-ster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player"&gt;https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/CD9QEbCZ"&gt;https://pastebin.com/CD9QEbCZ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jack-ster"&gt; /u/jack-ster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T05:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz0tqc</id>
    <title>MALM: A Modular Adapter-based Language Model (paper + Hugging Face link)</title>
    <updated>2025-08-24T16:53:04+00:00</updated>
    <author>
      <name>/u/TimesLast_</name>
      <uri>https://old.reddit.com/user/TimesLast_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.&lt;/p&gt; &lt;p&gt;The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).&lt;/p&gt; &lt;p&gt;This means:&lt;/p&gt; &lt;p&gt;- Smaller, cheaper models&lt;/p&gt; &lt;p&gt;- Easy to add new languages&lt;/p&gt; &lt;p&gt;- Better for edge devices and low-resource settings&lt;/p&gt; &lt;p&gt;Example flow:&lt;br /&gt; ```&lt;br /&gt; User: &amp;quot;Translate 'my name is Adam' into German.&amp;quot;&lt;br /&gt; CLM ‚Üí &amp;lt;to:de&amp;gt; my name is Adam &amp;lt;/to&amp;gt;&lt;br /&gt; STA ‚Üí &amp;quot;Mein Name ist Adam&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Read the full paper here: &lt;a href="https://huggingface.co/TimesLast/MALM"&gt;https://huggingface.co/TimesLast/MALM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimesLast_"&gt; /u/TimesLast_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T16:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz42eu</id>
    <title>Qwen3-Coder-480B Q4_0 on 6x7900xtx</title>
    <updated>2025-08-24T18:54:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt; &lt;img alt="Qwen3-Coder-480B Q4_0 on 6x7900xtx" src="https://a.thumbs.redditmedia.com/OtUhAjy3dNMyywvnZbc9ZIPzO4CmV_Rfiexy6H6qaR8.jpg" title="Qwen3-Coder-480B Q4_0 on 6x7900xtx" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-Coder-480B Q4_0 on 6x7900xtx with 7 token/s&lt;/strong&gt; output speed, did you have any suggestion or ideas to speed up it?&lt;/p&gt; &lt;p&gt;Maybe you know smart-offloading specific layers?&lt;/p&gt; &lt;p&gt;I launch it with this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./lama-hip-0608/build/bin/llama-server \ --model 480B-A35B_Q4_0/Qwen3-Coder-480B-A35B-Instruct-Q4_0-00001-of-00006.gguf \ --main-gpu 0 \ --temp 0.65 \ --top-k 20 \ --min-p 0.0 \ --top-p 0.95 \ --gpu-layers 48 \ --ctx-size 4000 \ --host 0.0.0.0 \ --port ${PORT} \ --parallel 1 \ --tensor-split 24,24,24,24,24,24 \ --jinja \ --mlock \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ -ot &amp;quot;.ffn_(down)_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T18:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mybft5</id>
    <title>grok 2 weights</title>
    <updated>2025-08-23T20:00:52+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt; &lt;img alt="grok 2 weights" src="https://external-preview.redd.it/4tfHT9vpFrwHCpX5cn0_tHyoUS8M6oeQ7jwWbePCicw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9576154cc1820a09f2c9b345d4d88427c3729b9a" title="grok 2 weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/xai-org/grok-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mypokb</id>
    <title>GPT OSS 20b is Impressive at Instruction Following</title>
    <updated>2025-08-24T07:56:56+00:00</updated>
    <author>
      <name>/u/crodjer</name>
      <uri>https://old.reddit.com/user/crodjer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found GPT OSS 20b to be consistently great at following complex instructions. For instance, it did performed perfectly with a test prompt I used: &lt;a href="https://github.com/crodjer/glaince/tree/main/cipher#results"&gt;https://github.com/crodjer/glaince/tree/main/cipher#results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All other models in the same size (Gemma 3, Qwen 3, Mistral Small) make the same mistake, resulting them to deviate from expectation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crodjer"&gt; /u/crodjer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T07:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz2di5</id>
    <title>I tried fine-tuning Gemma-3-270m and prepared for deployments</title>
    <updated>2025-08-24T17:50:40+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google recently released &lt;strong&gt;Gemma3-270M&lt;/strong&gt; model, which is one of the smallest open models out there.&lt;br /&gt; Model weights are available on Hugging Face and its size is ~550MB and there were some testing where it was being used on phones.&lt;/p&gt; &lt;p&gt;It‚Äôs one of the perfect models for fine-tuning, so I put it to the test using the official Colab notebook and an NPC game dataset.&lt;/p&gt; &lt;p&gt;I put everything together as a written guide in my newsletter and also as a small demo video while performing the steps.&lt;/p&gt; &lt;p&gt;I have skipped the fine-tuning part in the guide because you can find the official notebook on the release blog to test using Hugging Face Transformers. I did the same locally on my notebook.&lt;/p&gt; &lt;p&gt;Gemma3-270M is so small that fine-tuning and testing were finished in just a few minutes (~15). Then I used a open source tool called KitOps to package it together for secure production deployments.&lt;/p&gt; &lt;p&gt;I was trying to see if fine-tuning this small model is fast and efficient enough to be used in production environments or not. The steps I covered are mainly for devs looking for secure deployment of these small models for real apps. (example covered is very basic)&lt;/p&gt; &lt;p&gt;Steps I took are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Importing a Hugging Face Model&lt;/li&gt; &lt;li&gt;Fine-Tuning the Model&lt;/li&gt; &lt;li&gt;Initializing the Model with KitOps&lt;/li&gt; &lt;li&gt;Packaging the model and related files after fine-tuning&lt;/li&gt; &lt;li&gt;Push to a Hub to get security scans done and container deployments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;watch the demo video ‚Äì &lt;a href="https://youtu.be/8SKV_m5XV6o"&gt;here&lt;/a&gt;&lt;br /&gt; take a look at the guide ‚Äì &lt;a href="https://mranand.substack.com/p/you-can-fine-tune-gemma3-270m-in"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T17:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytpf1</id>
    <title>Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)</title>
    <updated>2025-08-24T11:59:59+00:00</updated>
    <author>
      <name>/u/Mass2018</name>
      <uri>https://old.reddit.com/user/Mass2018</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt; &lt;img alt="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" src="https://b.thumbs.redditmedia.com/x4bULEyzYY3EEBanwjUi7dZiywWjI4EP8X8WNQAbYyk.jpg" title="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of discussion recently about the performance of the Apple studios with large models, so I thought I'd share actual data from about a month of usage in our household.&lt;/p&gt; &lt;p&gt;This is mainly used by the non-me part of our household, so it sits nice and stable and just runs Deepseek 24/7, where my personal rig is constantly being swapped between different things that I'm working on.&lt;/p&gt; &lt;p&gt;The Apple Studio replaced the 10xP100 rig I had previously built for this purpose, and I have to say for what we're using it for it's been a godsend. It's much, much faster, can load larger models, has a much lower power footprint, and it was just... so easy to get it up and running. Honestly, it felt a bit like cheating after the hell that the P100 rig put me through.&lt;/p&gt; &lt;p&gt;Anyway, actual numbers:&lt;/p&gt; &lt;p&gt;|| || |Total logged requests:|161| |Context Average:|643.72| |Average Prompt Eval Tokens/Second:|64.73 tokens/second| |Average Tokens Generated:|343.16| |Average Tokens Generated/Second:|13.97 tokens/second|&lt;/p&gt; &lt;p&gt;My personal opinion is if all you're going to do is inferencing, it's a great option. I absolutely loathe the Mac GUI, and my constant attempt to control-c/control-v is infuriating, but other than that... NO RAGRETS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mass2018"&gt; /u/Mass2018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mytpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1myx4l5</id>
    <title>Which local model are you currently using the most? What‚Äôs your main use case, and why do you find it good?</title>
    <updated>2025-08-24T14:32:16+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1myjzmn</id>
    <title>There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)</title>
    <updated>2025-08-24T02:26:33+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt; &lt;img alt="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" src="https://preview.redd.it/2t25pwj6ovkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c8abd5ee1bf8381408ed5b298fc42879b01bd1" title="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And they have better licenses, less restrictions. What exactly is the point of Grok 2 then? I appreciate open source effort, but wouldn't it make more sense to open source a competitive model that can at least be run locally by most people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2t25pwj6ovkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz59l</id>
    <title>Seed-OSS is insanely good</title>
    <updated>2025-08-24T15:50:03+00:00</updated>
    <author>
      <name>/u/I-cant_even</name>
      <uri>https://old.reddit.com/user/I-cant_even</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took a day for me to get it running but *wow* this model is good. I had been leaning heavily on a 4bit 72B Deepseek R1 Distill but it had some regularly frustrating failure modes.&lt;/p&gt; &lt;p&gt;I was prepping to finetune my own model to address my needs but now it's looking like I can remove refusals and run Seed-OSS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I-cant_even"&gt; /u/I-cant_even &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz6f1</id>
    <title>Fast CUDA DFloat11 decoding kernel</title>
    <updated>2025-08-24T15:51:15+00:00</updated>
    <author>
      <name>/u/No_Dimension41</name>
      <uri>https://old.reddit.com/user/No_Dimension41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago, I came across the amazing work on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DFloat11&lt;/a&gt;, which achieves lossless output while shrinking models to 70% of their original size by compressing the exponent bits of BF16. It is a great work. However, I found a problem: it decompresses an entire tensor into VRAM, and then perform computations separately, which severely impacts the model's decoding speed. According to some &lt;a href="https://github.com/LeanModels/DFloat11/issues/7"&gt;issues&lt;/a&gt; on GitHub, it only reaches about 1/3 of the native BF16 speed. Furthermore, the author hasn't released the code for encoding the models, and the decoding kernel is provided in a nearly unreadable PTX format.&lt;/p&gt; &lt;p&gt;So, I decided to write my own implementation. I used the Huffman coding and LUT-based decoding algorithms described in their &lt;a href="https://arxiv.org/abs/2504.11651"&gt;paper&lt;/a&gt;, but I &lt;strong&gt;fused the Huffman decoding process and the GEMV operation into a single kernel&lt;/strong&gt;. This avoids unnecessary memory bandwidth overhead and dramatically speeds up decoding.&lt;/p&gt; &lt;p&gt;With a batch size of 1, my implementation can now reach about &lt;strong&gt;90% of native BF16 speed&lt;/strong&gt; on regular GPUs. On some VRAM bandwidth-constrained GPUs, like the RTX 4060 Ti, it can even &lt;strong&gt;surpass native BF16 speed&lt;/strong&gt; because the compressed weights reduce the demand on VRAM bandwidth.&lt;/p&gt; &lt;p&gt;Here's a simple benchmark for generating 256 tokens:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Raw BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Compressed BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Raw / Compressed Size&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 7B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;14.98s&lt;/td&gt; &lt;td align="left"&gt;13.02s&lt;/td&gt; &lt;td align="left"&gt;14.19 / 10.99 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;6.66s&lt;/td&gt; &lt;td align="left"&gt;7.23s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;14.11s&lt;/td&gt; &lt;td align="left"&gt;15.26 / 11.52 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;7.75s&lt;/td&gt; &lt;td align="left"&gt;8.24s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of course, there are still areas for improvement. Due to the extra padding required by the CUDA kernel's layout, the current compression rate is slightly lower than the original DFloat11, achieving around 75%-80%. Additionally, support for uncommon tensor shapes and batch sizes greater than 1 is currently limited.&lt;/p&gt; &lt;p&gt;For more information, please visit my GitHub repository: &lt;a href="https://github.com/lszxb/bf16_huffman_infer"&gt;https://github.com/lszxb/bf16_huffman_infer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Dimension41"&gt; /u/No_Dimension41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1myrdtb</id>
    <title>Mistral Large soon?</title>
    <updated>2025-08-24T09:45:24+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt; &lt;img alt="Mistral Large soon?" src="https://preview.redd.it/m9zk5bipuxkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ff9d33632d0268f989230460a6dbd3328b7244" title="Mistral Large soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source &lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;https://mistral.ai/news/mistral-medium-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9zk5bipuxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
