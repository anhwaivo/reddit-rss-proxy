<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-25T06:26:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m7ts5g</id>
    <title>Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found</title>
    <updated>2025-07-24T03:30:49+00:00</updated>
    <author>
      <name>/u/West-Chocolate2977</name>
      <uri>https://old.reddit.com/user/West-Chocolate2977</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt; &lt;img alt="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" src="https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688" title="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt; &lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt; &lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt; &lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt; &lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt; &lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Chocolate2977"&gt; /u/West-Chocolate2977 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T03:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8oz07</id>
    <title>Why there is still no a proper or helpful inference for MOE models ?</title>
    <updated>2025-07-25T03:43:10+00:00</updated>
    <author>
      <name>/u/Highwaytothebeach</name>
      <uri>https://old.reddit.com/user/Highwaytothebeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It should be really easy to make something like: &lt;/p&gt; &lt;p&gt;Just MOE gatting network is initially loaded into RAM ( or offloaded to the GPU ) and stays there&lt;/p&gt; &lt;p&gt;Activation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input's characteristics.&lt;/p&gt; &lt;p&gt;Loading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice) for processing.&lt;/p&gt; &lt;p&gt;For the next prompt if gatting network decides different experts will be activated they are just replaced in RAM ( VRAM) . &lt;/p&gt; &lt;p&gt;There will be a little latency at the start but it is nothing compared to present clumsiness and huge processing time if not enough RAM or VRAM and memory swapping.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Highwaytothebeach"&gt; /u/Highwaytothebeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m82lwo</id>
    <title>Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs</title>
    <updated>2025-07-24T12:07:03+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"&gt; &lt;img alt="Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs" src="https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42380fe0539f546fdb60963fca95595cf9e80e4c" title="Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BI obtained an internal list of websites that could and couldn't be used for training Anthropic's latest AI models. &lt;/p&gt; &lt;p&gt;Anthropic's contractor Surge AI left the list fully public on Google Docs. &lt;/p&gt; &lt;p&gt;'Sites you can use' include Bloomberg, Harvard, &amp;amp; the Mayo Clinic.&lt;/p&gt; &lt;p&gt;Many of the whitelisted sources copyright or otherwise restrict their content. &lt;/p&gt; &lt;p&gt;At least 3 - the Mayo Clinic, Cornell University, &amp;amp; Morningstar - told BI they didn't have any AI training agreements with Anthropic.&lt;/p&gt; &lt;p&gt;The spreadsheet also includes a blacklist of websites that Surge AI's gig workers were &amp;quot;now disallowed&amp;quot; from using. &lt;/p&gt; &lt;p&gt;The blacklist includes companies like the NYT &amp;amp; Reddit which have sued AI startups for scraping without permission.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8qmd7</id>
    <title>Can you just have one expert from an MOE model</title>
    <updated>2025-07-25T05:12:27+00:00</updated>
    <author>
      <name>/u/opoot_</name>
      <uri>https://old.reddit.com/user/opoot_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.&lt;/p&gt; &lt;p&gt;If I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?&lt;/p&gt; &lt;p&gt;Wouldn’t this help with the required ram/vram amount?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opoot_"&gt; /u/opoot_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T05:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1m85v3a</id>
    <title>Running an LLM on the Wii</title>
    <updated>2025-07-24T14:27:00+00:00</updated>
    <author>
      <name>/u/leavesandautumn222</name>
      <uri>https://old.reddit.com/user/leavesandautumn222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"&gt; &lt;img alt="Running an LLM on the Wii" src="https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe0ab84525e2cc55b31752cd441c3263e798f588" title="Running an LLM on the Wii" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leavesandautumn222"&gt; /u/leavesandautumn222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8hvd0nnw0uef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8jy5y</id>
    <title>Had the Qwen3:1.7B model run on my Mac Mini!</title>
    <updated>2025-07-24T23:39:26+00:00</updated>
    <author>
      <name>/u/Nomadic_Seth</name>
      <uri>https://old.reddit.com/user/Nomadic_Seth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/"&gt; &lt;img alt="Had the Qwen3:1.7B model run on my Mac Mini!" src="https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a53dbacaafd5c021355e8c1205d343c0bf4bcac4" title="Had the Qwen3:1.7B model run on my Mac Mini!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty excited to see what the rest of 2025 holds tbh :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nomadic_Seth"&gt; /u/Nomadic_Seth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2af06x4irwef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T23:39:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7vlpn</id>
    <title>Anthropic’s New Research: Giving AI More "Thinking Time" Can Actually Make It Worse</title>
    <updated>2025-07-24T05:09:23+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt; &lt;img alt="Anthropic’s New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" src="https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69b7dca05f4a287acca18082926d12008127ef3d" title="Anthropic’s New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt; &lt;p&gt;Turns out, that’s not always true.&lt;/p&gt; &lt;p&gt;Their paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt; &lt;p&gt;So what’s going wrong?&lt;/p&gt; &lt;p&gt;The paper breaks it down across several models and tasks. Here's what they found:&lt;/p&gt; &lt;p&gt;🧠 More Thinking, More Problems&lt;/p&gt; &lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt; &lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt; &lt;p&gt;Overfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt; &lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt; &lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt; &lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.&lt;/p&gt; &lt;p&gt;Tasks Where This Shows Up&lt;/p&gt; &lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt; &lt;p&gt;Simple counting with distractors&lt;/p&gt; &lt;p&gt;Regression with spurious features&lt;/p&gt; &lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt; &lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt; &lt;p&gt;🧩 Why This Matters&lt;/p&gt; &lt;p&gt;This isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.&lt;/p&gt; &lt;p&gt;That’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.&lt;/p&gt; &lt;p&gt;⚠️ Bottom Line&lt;/p&gt; &lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.14417"&gt;Research Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/srk1p5og9ref1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T05:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8qtpd</id>
    <title>Fine-tuning qwen2.5 vl for Marathi OCR</title>
    <updated>2025-07-25T05:24:10+00:00</updated>
    <author>
      <name>/u/Rahul_Albus</name>
      <uri>https://old.reddit.com/user/Rahul_Albus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.&lt;br /&gt; However, after fine-tuning, the model performs &lt;strong&gt;significantly worse than the base model&lt;/strong&gt; — it struggles with basic OCR prompts and fails to recognize text it previously handled well.&lt;/p&gt; &lt;p&gt;Here’s how I configured the fine-tuning layers:&lt;br /&gt; finetune_vision_layers = True&lt;/p&gt; &lt;p&gt;finetune_language_layers = True&lt;/p&gt; &lt;p&gt;finetune_attention_modules = True&lt;/p&gt; &lt;p&gt;finetune_mlp_modules = False&lt;/p&gt; &lt;p&gt;Please suggest what can I do to improve it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rahul_Albus"&gt; /u/Rahul_Albus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T05:24:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m868na</id>
    <title>The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license.</title>
    <updated>2025-07-24T14:42:00+00:00</updated>
    <author>
      <name>/u/ru_cyber</name>
      <uri>https://old.reddit.com/user/ru_cyber</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt; &lt;img alt="The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license." src="https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7af7edb7da1fe811056d8c05b8f8d8acd8fdb89" title="The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just wanted to share some exciting news for anyone here who's into deep, long-form roleplaying. The team behind &lt;a href="https://astrsk.ai"&gt;Astrsk&lt;/a&gt;, a desktop app for RP that's been in development for about six months, has just announced they are going &lt;strong&gt;fully open source&lt;/strong&gt; under the GPL license!&lt;/p&gt; &lt;p&gt;As a fan of the project, I think this is a huge deal for the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The most important link first:&lt;/strong&gt; &lt;a href="https://github.com/astrskai/astrsk"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So, what is Astrsk and why is it interesting?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Astrsk is a UI for RP, but its main differentiator is the &lt;strong&gt;agentic workflow&lt;/strong&gt;. I've been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.&lt;/p&gt; &lt;p&gt;To make this concrete, let's look at the default workflow it comes with, called &lt;strong&gt;SAGA&lt;/strong&gt;. It's a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Step 1: The Analyzer Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the GM's logical brain. It looks at what your character just did and analyzes it against the current game state.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It answers the questions: &amp;quot;Is the player's action possible? What are the immediate consequences based on game rules or a dice roll?&amp;quot; It validates the action and determines the outcome.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 2: The Planner Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the creative storyteller. It takes the Analyzer's output and designs the narrative response.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It decides how NPCs will react to the player's action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 3: The Actor Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the performer. It takes the Planner's script and turns it into the actual text you read.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 4: The Formatter Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the final editor.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in italics, dialogue in &amp;quot;quotes&amp;quot;, and adds &lt;strong&gt;bold&lt;/strong&gt; for emphasis, making the final output clean and easy to read without changing the content.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it compare to the greats like SillyTavern / Agnaistic?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;From what I've seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent-based generation:&lt;/strong&gt; The core of Astrsk, designed for more coherent and long-term storytelling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sleek, Customizable UI:&lt;/strong&gt; A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-Agent Model Assignment:&lt;/strong&gt; This is a killer feature. You can assign a different LLM endpoint to each agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;True Cross-Platform Support:&lt;/strong&gt; The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it — no need to be an engineer or fight with dependencies to get started.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend Agnostic:&lt;/strong&gt; Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Open Source Move&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;According to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.&lt;/p&gt; &lt;p&gt;I'm not affiliated with the developers, just a user who is really excited about the project's potential and wanted to share it with a community that might appreciate the tech.&lt;/p&gt; &lt;p&gt;Definitely worth checking out the &lt;a href="https://github.com/astrskai/astrsk"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;, especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ru_cyber"&gt; /u/ru_cyber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m87q21</id>
    <title>Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!</title>
    <updated>2025-07-24T15:38:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"&gt; &lt;img alt="Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!" src="https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f3889ea17a471e18de4adc6fcaeee9fddc9d20" title="Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).&lt;/p&gt; &lt;p&gt;Important links: - Model: &lt;a href="https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX"&gt;https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX&lt;/a&gt; - Demo: &lt;a href="https://huggingface.co/spaces/webml-community/Voxtral-WebGPU"&gt;https://huggingface.co/spaces/webml-community/Voxtral-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9p0p7mqnbuef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T15:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8oc9j</id>
    <title>Stagnation in Knowledge Density</title>
    <updated>2025-07-25T03:10:19+00:00</updated>
    <author>
      <name>/u/Federal-Effective879</name>
      <uri>https://old.reddit.com/user/Federal-Effective879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.&lt;/p&gt; &lt;p&gt;I've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt; &lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt; &lt;li&gt;Mistral Large (2411)&lt;/li&gt; &lt;li&gt;Kimi K2&lt;/li&gt; &lt;li&gt;Cohere Command-A (2503)&lt;/li&gt; &lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt; &lt;li&gt;Llama 4 Maverick&lt;/li&gt; &lt;li&gt;Llama 3.3 70B&lt;/li&gt; &lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt; &lt;li&gt;Dots.LLM1&lt;/li&gt; &lt;li&gt;Gemma 3 27B&lt;/li&gt; &lt;li&gt;Mistral Small 3.2&lt;/li&gt; &lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt; &lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt; &lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.&lt;/p&gt; &lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt; &lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal-Effective879"&gt; /u/Federal-Effective879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ewlx</id>
    <title>Level1tech runs deepseek on am5 and it's not that bad!</title>
    <updated>2025-07-24T20:10:36+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"&gt; &lt;img alt="Level1tech runs deepseek on am5 and it's not that bad!" src="https://external-preview.redd.it/-htOFPXayCuXw6wsi6x_HzoPwXo6FB_FePd0EceoPtI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b8b07ebd60262e21a7df05d698d427003581ec8" title="Level1tech runs deepseek on am5 and it's not that bad!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am5 9000x3d 128gb ram (2*64) and a 3090&lt;/p&gt; &lt;p&gt;I promised i watch it but I couldn't get what exact quant nor speed.&lt;br /&gt; He said this was &amp;quot;compressed to 20% of the og model&amp;quot; so something like a q2.&lt;br /&gt; Regarding speed it seems very very descent&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/T17bpGItqXw?feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ewlx/level1tech_runs_deepseek_on_am5_and_its_not_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T20:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8aeh3</id>
    <title>Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness</title>
    <updated>2025-07-24T17:18:51+00:00</updated>
    <author>
      <name>/u/pheonis2</name>
      <uri>https://old.reddit.com/user/pheonis2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"&gt; &lt;img alt="Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness" src="https://external-preview.redd.it/c2RvemoyMGF2dWVmMUceqdxuAzoTIY7iz_8adXhwap77Psvz_mx_rNXgGzw3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b6aa3eabfc6cb4a722e695f90d9e84bf9d54a1f" title="Higgs Audio V2: A New Open-Source TTS Model with Voice Cloning and SOTA Expressiveness" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Boson AI has recently open-sourced the Higgs Audio V2 model.&lt;br /&gt; &lt;a href="https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The model demonstrates strong performance in automatic prosody adjustment and generating natural multi-speaker dialogues across languages . &lt;/p&gt; &lt;p&gt;Notably, it achieved a 75.7% win rate over GPT-4o-mini-tts in emotional expression on the EmergentTTS-Eval benchmark . The total parameter count for this model is approximately 5.8 billion (3.6B for the LLM and 2.2B for the Audio Dual FFN)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pheonis2"&gt; /u/pheonis2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rcsam20avuef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8aeh3/higgs_audio_v2_a_new_opensource_tts_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T17:18:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8bps2</id>
    <title>We just open sourced NeuralAgent: The AI Agent That Lives On Your Desktop and Uses It Like You Do!</title>
    <updated>2025-07-24T18:07:52+00:00</updated>
    <author>
      <name>/u/Nearby_Tart_9970</name>
      <uri>https://old.reddit.com/user/Nearby_Tart_9970</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeuralAgent lives on your desktop and takes action like a human, it clicks, types, scrolls, and navigates your apps to complete real tasks. Your computer, now working for you. It's now open source. &lt;/p&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/withneural/neuralagent"&gt;https://github.com/withneural/neuralagent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our website: &lt;a href="https://www.getneuralagent.com"&gt;https://www.getneuralagent.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Give us a star if you like the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nearby_Tart_9970"&gt; /u/Nearby_Tart_9970 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8bps2/we_just_open_sourced_neuralagent_the_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T18:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m80gsn</id>
    <title>GLM-4.5 Is About to Be Released</title>
    <updated>2025-07-24T10:10:17+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt; &lt;img alt="GLM-4.5 Is About to Be Released" src="https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e" title="GLM-4.5 Is About to Be Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vLLM commit: &lt;a href="https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;modelscope/ms-swift commit: &lt;a href="https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're going to get a 106B-A12B (Air) model and a 355B-A32B model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T10:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8qj9w</id>
    <title>Why I Forked Qwen Code</title>
    <updated>2025-07-25T05:07:40+00:00</updated>
    <author>
      <name>/u/ryanwang4thepeople</name>
      <uri>https://old.reddit.com/user/ryanwang4thepeople</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.&lt;/p&gt; &lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt; &lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt; &lt;li&gt;My priorities as a solo developer probably don't align with respective model companies.&lt;/li&gt; &lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt; &lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt; &lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt; &lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt; &lt;li&gt;Documentation&lt;/li&gt; &lt;li&gt;Multi-model support????&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Maybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryanwang4thepeople"&gt; /u/ryanwang4thepeople &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T05:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ozb0</id>
    <title>China's Bytedance releases Seed LiveInterpret simultaneous interpretation model</title>
    <updated>2025-07-25T03:43:35+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seed.bytedance.com/en/seed_liveinterpret"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m88s09</id>
    <title>Qwen's third bomb: Qwen3-MT</title>
    <updated>2025-07-24T16:17:55+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt; &lt;img alt="Qwen's third bomb: Qwen3-MT" src="https://b.thumbs.redditmedia.com/LksViWDcxO1eQ0ZQpLUVRXks4wbjVGa9UjqigE3hofA.jpg" title="Qwen's third bomb: Qwen3-MT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a translation model.&lt;/p&gt; &lt;p&gt;Key Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support for 92 Languages&lt;/strong&gt;: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Customizability&lt;/strong&gt;: The new version provides advanced translation capabilities such as terminology intervention, domain prompts and translation memory. By enabling customizable prompt engineering, it delivers optimized translation performance tailored to complex, domain-specific, and mission-critical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low Latency &amp;amp; Cost Efficiency&lt;/strong&gt;: By leveraging a lightweight Mixture of Experts (MoE) architecture, Qwen-MT achieves high translation performance with faster response times and significantly reduced API costs (as low as $0.5 per million output tokens). This is particularly well-suited for high-concurrency environments and latency-sensitive applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ebw46w8hkuef1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0652bf1ba1530779185f78006929ce89c53a2aaf"&gt;benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen-mt/"&gt;https://qwenlm.github.io/blog/qwen-mt/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m88s09/qwens_third_bomb_qwen3mt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m85vhw</id>
    <title>new mistralai/Magistral-Small-2507 !?</title>
    <updated>2025-07-24T14:27:29+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt; &lt;img alt="new mistralai/Magistral-Small-2507 !?" src="https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e" title="new mistralai/Magistral-Small-2507 !?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m83644</id>
    <title>China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks</title>
    <updated>2025-07-24T12:33:36+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt; &lt;img alt="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" src="https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa69848c81b950052de8eb2024c390e13024272" title="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dln1</id>
    <title>Qwen 3 Thinking is coming very soon</title>
    <updated>2025-07-24T19:19:39+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt; &lt;img alt="Qwen 3 Thinking is coming very soon" src="https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b99a58da488472ec93d5842e37998def1cbe76" title="Qwen 3 Thinking is coming very soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61i8pt44hvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dgfu</id>
    <title>Qwen3-235B-A22B-Thinking-2507 is about to be released</title>
    <updated>2025-07-24T19:14:14+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 is about to be released" src="https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f" title="Qwen3-235B-A22B-Thinking-2507 is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6l84nwc3gvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m88jdh</id>
    <title>Ok next big open source model also from China only ! Which is about to release</title>
    <updated>2025-07-24T16:08:57+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt; &lt;img alt="Ok next big open source model also from China only ! Which is about to release" src="https://preview.redd.it/j6rwug34juef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c" title="Ok next big open source model also from China only ! Which is about to release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6rwug34juef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T16:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8l648</id>
    <title>Executive Order: "Preventing Woke AI in the Federal Government"</title>
    <updated>2025-07-25T00:36:06+00:00</updated>
    <author>
      <name>/u/NunyaBuzor</name>
      <uri>https://old.reddit.com/user/NunyaBuzor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt; &lt;img alt="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunyaBuzor"&gt; /u/NunyaBuzor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T00:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8myxl</id>
    <title>Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.</title>
    <updated>2025-07-25T02:02:13+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt; &lt;img alt="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." src="https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d" title="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl9jgkkzgxef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T02:02:13+00:00</published>
  </entry>
</feed>
