<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-18T22:22:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i4i1q8</id>
    <title>I made a Huggingface Space to help build commands for OpenVINO model conversion.</title>
    <updated>2025-01-18T21:35:04+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Optimum-CLI-Tool_tool&lt;/h1&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I am launching &lt;a href="https://huggingface.co/spaces/Echo9Zulu/Optimum-CLI-Tool_tool"&gt;Echo9Zulu/Optimum-CLI-Tool_tool&lt;/a&gt;, a Huggingface Space, that &amp;quot;GUI-ifies&amp;quot; selecting conversion parameters with the Optimum-CLI tool for OpenVINO with Gradio. OpenVINO is a Inference engine offering hardware acceleration for Intel CPUs, GPUs, and NPUs. OpenVINO runtime, Optimum-Intel for Transformers and OpenVINO GenAI all require a deceptively simple model export to an Intermediate Representation format.&lt;/p&gt; &lt;p&gt;I made this tool to help me convert models efficently and know others may find it useful for keeping things organized.&lt;/p&gt; &lt;h1&gt;Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Build conversion commands from available parameters.&lt;/li&gt; &lt;li&gt;Respect positional arguments defined by Optimum.&lt;/li&gt; &lt;li&gt;Discussion of advanced usage for OpenVINO with Transformers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: At runtime your command references prebuilt configuration classes which don't support all models. However, if you open a PR the maintainers make it happen. See my issue &lt;a href="https://github.com/huggingface/optimum-intel/issues/1097"&gt;#1097&lt;/a&gt; and their PR &lt;a href="https://github.com/huggingface/optimum-intel/pull/1099"&gt;#1099&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Check your model architecture &lt;a href="https://huggingface.co/docs/optimum/main/en/intel/openvino/models"&gt;here&lt;/a&gt; to spare yourself a traceback.&lt;/p&gt; &lt;h1&gt;OpenVINO, briefly&lt;/h1&gt; &lt;p&gt;OpenVINO is a C++ runtime with Python APIs for managing inference deployments on Intel hardware from 6th gen forward. Using OpenVINO effectively requires new research for each usecase to discover facts about your model's architecture and the hardware you are using to choose the right settings. Choose your device type and use snippets &lt;a href="https://docs.openvino.ai/2024/about-openvino/compatibility-and-support/supported-devices.html"&gt;here&lt;/a&gt; to query device properties based on your hardware. This requires &lt;a href="https://docs.openvino.ai/2024/get-started/install-openvino.html?PACKAGE=OPENVINO_GENAI&amp;amp;VERSION=v_2024_6_0&amp;amp;OP_SYSTEM=LINUX&amp;amp;DISTRIBUTION=PIP"&gt;additional dependencies&lt;/a&gt; outside Optimum but is easier, faster and more reliable than using &lt;a href="https://www.intel.com/content/www/us/en/ark.html"&gt;Intel Ark&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here's an example Pytorch workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Train&lt;/li&gt; &lt;li&gt;OR finetune&lt;/li&gt; &lt;li&gt;OR quantize&lt;/li&gt; &lt;li&gt;Convert to OpenVINO Intermediate Representation&lt;/li&gt; &lt;li&gt;Test code until inference goes zoom&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: Models exported to OpenVINO IR cannot be trained.&lt;/p&gt; &lt;p&gt;Here are some useful resources to learn more about OpenVINO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks"&gt;openvino_notebooks&lt;/a&gt;: This is where the cutting edge stuff lives. Every other resource on the internet save the source code are 'behind' this repo. Most documentation points here and it's really easy to and much of the docs are outdated&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/docs/optimum/main/en/intel/openvino/export"&gt;openvino-export&lt;/a&gt;: Stored here are the parameters used in this tool. It's one to one.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.openvino.ai/2024/index.html"&gt;documentation&lt;/a&gt;: I have found that going straight to git can be more useful since non-Intel examples are scarce&lt;/p&gt; &lt;p&gt;Due to low adoption there aren't many projects which fully leverage the features offered by the Intel AI stack. So, starting with OpenVINO through Transformers is a good way to find out if it's even worth going deeper.&lt;/p&gt; &lt;p&gt;Thanks for checking this out and hopefully the tool is useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4i1q8/i_made_a_huggingface_space_to_help_build_commands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4iid0</id>
    <title>What LLM hardware should I buy for my business?</title>
    <updated>2025-01-18T21:56:41+00:00</updated>
    <author>
      <name>/u/Lost_Fox__</name>
      <uri>https://old.reddit.com/user/Lost_Fox__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a $7k limit, ideally spending less / closer to $3k, what should a buy for my business?&lt;/p&gt; &lt;p&gt;I need to do considerable website scraping, data extraction from documents, and content generation. Most of this can be done in the background, and isn't live / customer facing, so it seems like the cloud is unnecessary. I'm now looking into buying some hardware. The workload is probably close to endless.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I was considering buying a mac studio when the new M4 refresh comes out with maxed out RAM.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know the new 5090's are coming out, and I could camp out with my son and grab 2 of them if that was expected to be a good option.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know Nvidia is coming out with it's own hardware similar to a mac studio to run LLMs. Should I be looking at that?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I care about quality. I can build a machine, or multiple of that makes sense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost_Fox__"&gt; /u/Lost_Fox__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4iid0/what_llm_hardware_should_i_buy_for_my_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4iid0/what_llm_hardware_should_i_buy_for_my_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4iid0/what_llm_hardware_should_i_buy_for_my_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3xoyd</id>
    <title>What's the cheapest way to run Llama 3.x 8B class models with realtime-like (chatgpt speed) tokens per second?</title>
    <updated>2025-01-18T02:38:29+00:00</updated>
    <author>
      <name>/u/synexo</name>
      <uri>https://old.reddit.com/user/synexo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;fireworks.ai? spin up on runpod? build a home server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synexo"&gt; /u/synexo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T02:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4dswh</id>
    <title>The Best Animation Creator (Not Video Generator)?</title>
    <updated>2025-01-18T18:23:37+00:00</updated>
    <author>
      <name>/u/yukiarimo</name>
      <uri>https://old.reddit.com/user/yukiarimo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys! Do you know any good AI animation creators? I mean, to work like this:&lt;/p&gt; &lt;p&gt;I’m drawing like starting frame, ending frame, and a few in between, and similar to interpolation (but plain interpolation won’t work here because no video is ready) it will create enough frames to make from make few drawings an animated sequence?&lt;/p&gt; &lt;p&gt;Open-source only! Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yukiarimo"&gt; /u/yukiarimo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dswh/the_best_animation_creator_not_video_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T18:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3mybo</id>
    <title>LCLV: Real-time video analysis with Moondream 2B &amp; OLLama (open source, local). Anyone want a set up guide?</title>
    <updated>2025-01-17T18:21:33+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt; &lt;img alt="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" src="https://external-preview.redd.it/MXZ5aHh4bWZpbGRlMSTqk2DOPEdgmnDyQ8guvDBrE8AyiWMeqDE4BRKGe_SG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecd38999e371e083e545019f1eaf8d324a146b50" title="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3kcfymfilde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4dbg3</id>
    <title>Guide: Easiest way to run any vLLM model on AWS with autoscaling (scale down to 0)</title>
    <updated>2025-01-18T18:02:13+00:00</updated>
    <author>
      <name>/u/tempNull</name>
      <uri>https://old.reddit.com/user/tempNull</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of our customers have been finding our guide for vLLM deployment on their own private cloud super helpful. vLLM is super helpful and straightforward and provides the highest token throughput when compared against frameworks like LoRAX, TGI etc.&lt;/p&gt; &lt;p&gt;Please let me know your thoughts on whether the guide is helpful and has a positive contribution to your understanding of model deployments in general.&lt;/p&gt; &lt;p&gt;Find the guide here:- &lt;a href="https://tensorfuse.io/docs/guides/llama_guide"&gt;https://tensorfuse.io/docs/guides/llama_guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tempNull"&gt; /u/tempNull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4dbg3/guide_easiest_way_to_run_any_vllm_model_on_aws/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T18:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hppu</id>
    <title>Motherboard for 2 3090ti in pcie slots + a third on riser</title>
    <updated>2025-01-18T21:20:14+00:00</updated>
    <author>
      <name>/u/Better_Dress_8508</name>
      <uri>https://old.reddit.com/user/Better_Dress_8508</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you share what motherboard you used for such setup: 2 3090ti in pcie slots + a third on riser. The best I could accommodate was 1 3090ti + one 1080ti on board and another 3090ti on riser. I now want to replace the 1080ti with another 3090 or higher&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Dress_8508"&gt; /u/Better_Dress_8508 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hppu/motherboard_for_2_3090ti_in_pcie_slots_a_third_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hppu/motherboard_for_2_3090ti_in_pcie_slots_a_third_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hppu/motherboard_for_2_3090ti_in_pcie_slots_a_third_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3o7a8</id>
    <title>I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)</title>
    <updated>2025-01-17T19:14:56+00:00</updated>
    <author>
      <name>/u/yyjhao</name>
      <uri>https://old.reddit.com/user/yyjhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt; &lt;img alt="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" src="https://external-preview.redd.it/MGt0ZzN4Y3NzbGRlMeSgvI1GdDqWZSs569grdhgwadhN-F5M6UL9TiNWoaqW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c56ce3e01a5f41dcffc115930e49f7b1fee821" title="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yyjhao"&gt; /u/yyjhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n3fmqwcsslde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T19:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pexj</id>
    <title>DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench</title>
    <updated>2025-01-17T20:06:47+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt; &lt;img alt="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" src="https://external-preview.redd.it/RiXxcULN7VvmAA8zRKm9Hg6sMZIuDEZ9SdZM3h7z4e0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c43191d847a8866681673c575cc88d8e702dd05" title="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/WdpIkiy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4d7f3</id>
    <title>Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)</title>
    <updated>2025-01-18T17:57:29+00:00</updated>
    <author>
      <name>/u/s0n1cm0nk3y</name>
      <uri>https://old.reddit.com/user/s0n1cm0nk3y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt; &lt;img alt="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" src="https://b.thumbs.redditmedia.com/te1a0xEnqnHXgzGURJQZ5QsJ1nReFxta_RZWsEDD0xU.jpg" title="Success!: Tesla p40+1080GTX_Cooler in a Dell T420 :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, the money shot: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674"&gt;https://preview.redd.it/gg6j4ea9isde1.png?width=1347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=84d77db8a83b5481621d22deeedb3636a6efa674&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, I'm aware my PERCs are a bit close, I'm brainstorming on that. So the approach I took was following &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hozg2h/comment/m4di1mw/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;advice from FullStackSensei&lt;/a&gt; I acquired a used &lt;a href="https://www.youtube.com/watch?v=AM--NTHFBlI&amp;amp;lc=Ugzrysx0dPl-yrbO1yp4AaABAg.ADCmLC_52VxADCn-nMSdT-"&gt;GTX1080 dell reference card &lt;/a&gt;with issues. As the only things I needed were the fan and cooler, I wasn't too worried about it being for parts. It took some minor modifications, to include using a dremel and an oscillating cutter:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569"&gt;https://preview.redd.it/8g0vc0h5jsde1.png?width=467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02dc9789435917b0cf7fde5b109aec0157ba5569&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but as shown here, the temps are completely manageable, and the fan is barely blowing : &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958"&gt;https://preview.redd.it/4e7mwcdjjsde1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3d397688349dcfd222da0ae2722529e4d9be958&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Parts you'll need:&lt;/p&gt; &lt;p&gt;Links omitted to make sure I'm following guideines.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU fan adapter cable (look for &amp;quot;PWM GPU fan adapter cable&amp;quot;)&lt;/li&gt; &lt;li&gt;Thermal pads of varying sizes&lt;/li&gt; &lt;li&gt;PWM Fan Controller (I used the Coolerguys 12v PWM thermostat model)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps anyone having troubles like I was with all the 3d printed fan shrouds and their concern for noise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s0n1cm0nk3y"&gt; /u/s0n1cm0nk3y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4d7f3/success_tesla_p401080gtx_cooler_in_a_dell_t420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:57:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i46hrp</id>
    <title>Qualcomm AI hub</title>
    <updated>2025-01-18T12:25:57+00:00</updated>
    <author>
      <name>/u/Big-Ad1693</name>
      <uri>https://old.reddit.com/user/Big-Ad1693</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/quic/ai-hub-models?tab=readme-ov-file"&gt;https://github.com/quic/ai-hub-models?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I check every few months to see how things are going with the Snapdragon NPU, but I never find anything useful, until now&lt;/p&gt; &lt;p&gt;Maybe there are others out there who want to tinker a bit with Android and the NPU.&lt;/p&gt; &lt;p&gt;There also examples for Image Gen, LLM, whisper &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Ad1693"&gt; /u/Big-Ad1693 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i46hrp/qualcomm_ai_hub/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T12:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i48dmj</id>
    <title>Has anyone tried anything besides native Python to build Agents?</title>
    <updated>2025-01-18T14:14:04+00:00</updated>
    <author>
      <name>/u/QaeiouX</name>
      <uri>https://old.reddit.com/user/QaeiouX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, it's a very common question around here to ask. Actually I am working a project and have been using simple python to build my agentic workflow. But as it is expanding, I am facing some issues on keeping up with it. I am planning to use some framework and Pydantic AI is on my radar. I am also interested by Bee Agent Framework but, it's written in typescript predominantly. If you have any other suggestions, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QaeiouX"&gt; /u/QaeiouX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i48dmj/has_anyone_tried_anything_besides_native_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T14:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4a2by</id>
    <title>Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)</title>
    <updated>2025-01-18T15:36:46+00:00</updated>
    <author>
      <name>/u/Loya_3005</name>
      <uri>https://old.reddit.com/user/Loya_3005</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt; &lt;img alt="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" src="https://b.thumbs.redditmedia.com/QcFzNc573ISWHe0wjV3Yju32wsgz4dgJupoA0RiLPPc.jpg" title="Nuggt: Retrieve Information from the internet to be used as context for LLM (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/n6awgafpurde1.gif"&gt;Nuggt Demo GIF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; &lt;/p&gt; &lt;p&gt;We all understand that the quality of LLM output depends heavily on the context and prompt provided. For example, asking an LLM to generate a good blog article on a given topic (let's say &lt;em&gt;X&lt;/em&gt;) might result in a generic answer that may or may not meet your expectations. However, if you provide guidelines on how to write a good article and supply the LLM with additional relevant information about the topic, you significantly increase the chances of receiving a response that aligns with your needs.&lt;/p&gt; &lt;p&gt;With this in mind, I wanted to create a workspace that makes it easy to build and manage context for use with LLMs. I imagine there are many of us who might use LLMs in workflows similar to the following:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Let’s say you want to write an elevator pitch for your startup.&lt;br /&gt; &lt;strong&gt;Step 1&lt;/strong&gt;: Research how to write a good elevator pitch, then save the key points as context.&lt;br /&gt; &lt;strong&gt;Step 2&lt;/strong&gt;: Look up examples of effective elevator pitches and add these examples to your context.&lt;br /&gt; &lt;strong&gt;Step 3&lt;/strong&gt;: Pass this curated context to the LLM and ask it to craft an elevator pitch for your startup. Importantly, you expect transparency—ensuring the LLM uses your provided context as intended and shows how it informed the output.&lt;/p&gt; &lt;p&gt;If you find workflows like this appealing, I think you’ll enjoy this tool. Here are its key features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It integrates &lt;strong&gt;Tavily&lt;/strong&gt; and &lt;strong&gt;Firecrawl&lt;/strong&gt; to gather information on any topic from the internet.&lt;/li&gt; &lt;li&gt;You can highlight any important points, right-click, and save them as context.&lt;/li&gt; &lt;li&gt;You can pass this context to the LLM, which will use it to assist with your task. In its responses, the LLM will cite the relevant parts of the context so you can verify how your input was used and even trace it back to the original sources.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My hypothesis is that many of us would benefit from building strong context to complete our tasks. Of course, I could be wrong—perhaps this is just one of my idiosyncrasies, putting so much effort into creating detailed context! Who knows? The only way to find out is to post it here and see what the community thinks.&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback!&lt;/p&gt; &lt;p&gt;Here is the github repo: &lt;a href="https://github.com/shoibloya/nuggt-research"&gt;https://github.com/shoibloya/nuggt-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loya_3005"&gt; /u/Loya_3005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4a2by/nuggt_retrieve_information_from_the_internet_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T15:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4bfpo</id>
    <title>-Nevoria- LLama 3.3 70b</title>
    <updated>2025-01-18T16:39:34+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;TLDR: This is a merge focused on combining storytelling capabilities with detailed scene descriptions, while maintaining a balanced approach to maintain intelligence and useability and reducing positive bias. Currently ranked as the highest 70B on the UGI benchmark!&lt;/p&gt; &lt;p&gt;What went into this?&lt;/p&gt; &lt;p&gt;I took EVA-LLAMA 3.33 for its killer storytelling abilities and mixed it with EURYALE v2.3's detailed scene descriptions. Added Anubis v1 to enhance the prose details, and threw in some Negative_LLAMA to keep it from being too sunshine-and-rainbows. All this sitting on a Nemotron-lorablated base.&lt;/p&gt; &lt;p&gt;Subtracting the lorablated base during merging causes a &amp;quot;weight twisting&amp;quot; effect. If you've played with my previous Astoria models, you'll recognize this approach - it creates some really interesting balance in how the model responds.&lt;/p&gt; &lt;p&gt;As usual my goal is to keep the model Intelligent with a knack for storytelling and RP. &lt;/p&gt; &lt;p&gt;Benchmark Results:&lt;/p&gt; &lt;p&gt;- UGI Score: 56.75 (Currently #1 for 70B models and equal or better than 123b models!)&lt;/p&gt; &lt;p&gt;- Open LLM Average: 43.92% (while not as useful from people training on the questions, still useful)&lt;/p&gt; &lt;p&gt;- Solid scores across the board, especially in IFEval (69.63%) and BBH (56.60%)&lt;/p&gt; &lt;p&gt;Already got some quantized versions available: &lt;/p&gt; &lt;p&gt;Recommended template: LLam@ception by @.konnect&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B"&gt;https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and experiences with it! Your feedback helps make the next one even better. &lt;/p&gt; &lt;p&gt;Happy prompting! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4bfpo/nevoria_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hyn9</id>
    <title>The Case for Open Models</title>
    <updated>2025-01-18T21:31:18+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt; &lt;img alt="The Case for Open Models" src="https://external-preview.redd.it/9jCHWcqHBL_Q8CbrEzDT6EMKWVd07U_SmrG2Z0RXzj8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf06e18f281752c7c009c87125ee559793b0b851" title="The Case for Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amgadhasan.substack.com/p/the-case-for-open-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hyn9/the_case_for_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i46zfr</id>
    <title>Why can't LLMs be re-trained on the go with the conversation for infinite memory?</title>
    <updated>2025-01-18T12:56:46+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just trying to understand the technical limitations and is this something that's considered. &lt;/p&gt; &lt;p&gt;I think the context window should only exist for instructions, while maintaining an infinte memory. This could really put LLMs in the realms of writing a complete book series and effecively changing the world as w e know it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i46zfr/why_cant_llms_be_retrained_on_the_go_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T12:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h1qo</id>
    <title>Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant</title>
    <updated>2025-01-18T20:50:12+00:00</updated>
    <author>
      <name>/u/Voidmesmer</name>
      <uri>https://old.reddit.com/user/Voidmesmer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt; &lt;img alt="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" src="https://external-preview.redd.it/dqmU2zN05DBQIRTAk9vSzRwo09lhayQQe9rfQzGoyS8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3af3bcb4c5c2426d61848d63048912843cb91350" title="Kokoro TTS and GLaDOS make a low latency, realistic AI voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Voidmesmer"&gt; /u/Voidmesmer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=EBHwOCoGMKQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h1qo/kokoro_tts_and_glados_make_a_low_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4h3pp</id>
    <title>What would you do with free access to a 4x H100 server?</title>
    <updated>2025-01-18T20:52:42+00:00</updated>
    <author>
      <name>/u/SquareJordan</name>
      <uri>https://old.reddit.com/user/SquareJordan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long story short I have one in the lab and all that’s being run on it thus far are benchmarks. What should I do with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SquareJordan"&gt; /u/SquareJordan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4h3pp/what_would_you_do_with_free_access_to_a_4x_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T20:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i435so</id>
    <title>KoboldCpp 1.82 - Now supports OuteTTS v0.2+0.3 with speaker voice synthesis and XTTS/OpenAI speech API, TAESD for Flux &amp; SD3, multilingual whisper (plus RAG and WebSearch from v1.81)</title>
    <updated>2025-01-18T08:27:13+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's me Concedo, here again playing how-many-more-API-endpoints-can-koboldcpp-serve. &lt;/p&gt; &lt;p&gt;Today's release brings long awaited TTS support, which works on all versions of OuteTTS GGUFs including the newly released &lt;strong&gt;v0.3 500M and 1B&lt;/strong&gt; models. It also provides XTTS and OpenAI Speech compatible APIs, so it can work as a direct TTS drop-in for existing frontends that use those features. &lt;/p&gt; &lt;p&gt;There are also some pretty cool improvements, as well as many other features, so do check out the release notes if you haven't yet. Last release, we also added WebSearch and a simple browser based RAG, so check that out if you missed it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases"&gt;https://github.com/LostRuins/koboldcpp/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i435so/koboldcpp_182_now_supports_outetts_v0203_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T08:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4fmvy</id>
    <title>4080 16gb and my old 3070 8gb</title>
    <updated>2025-01-18T19:45:36+00:00</updated>
    <author>
      <name>/u/Glooves</name>
      <uri>https://old.reddit.com/user/Glooves</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt; &lt;img alt="4080 16gb and my old 3070 8gb" src="https://b.thumbs.redditmedia.com/_MB00WAkuuUTDJAJzS8IGuj3FCZdE56GfbQP0ShAflY.jpg" title="4080 16gb and my old 3070 8gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to throw my old 3070 in and an old set of ddr4 to see what happens. Now up to 24 gb of vram and 64 gb of dram with a 12700kf. I was worried about my 750 watt psu but it’s pulling under 400 watts at load and I’ll set some limits just in case. Got 22 tok/sec on gwen 2.5 32b q4_0. I’ll try a 70b later. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glooves"&gt; /u/Glooves &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i4fmvy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4fmvy/4080_16gb_and_my_old_3070_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:45:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4cfpz</id>
    <title>Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?</title>
    <updated>2025-01-18T17:23:32+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt; &lt;img alt="Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?" src="https://preview.redd.it/tr0h9qvkdsde1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=237cce46c19ab8ed30310b8c79fcf688f233dcf2" title="Llama 3.2 1B Instruct – What Are the Best Use Cases for Small LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tr0h9qvkdsde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T17:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4hb2l</id>
    <title>Theory: trying to use newer and more powerful LLMs to sound more human is likely moving in the wrong direction</title>
    <updated>2025-01-18T21:01:51+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like using more powerful LLMs to try to achieve human like speech is probably moving AWAY from the solution rather than towards it.&lt;/p&gt; &lt;p&gt;My thought process is this: what's the difference between Llama 3.3 and Llama 1? Besides technology, I mean. Lets go back in time, to the days when we only had a 2048 token context windows; before things like NTK RoPE scaling, when your only choices were linear RoPE scaling and/or a superHoT model. Forget all the technological differences, and think: what differentiated the models of those days to the models of today?&lt;/p&gt; &lt;p&gt;Synthetic data in the training, right? The models back then were trained on minimal, if any, synthetic data. Even most of the chat models were barely trained on user/assistant type interactions. The training data was mostly human text.&lt;/p&gt; &lt;p&gt;But today? Modern models have tons and tons of &amp;quot;GPTisms&amp;quot; baked into the training data, note even speaking about how bad the instruct tune can be. So of course when you talk to the thing, it's going to sound like what we now expect an AI Assistant to sound like. And the more powerful models we get, the more we're going to see this synthetic data further baked in there.&lt;/p&gt; &lt;p&gt;In my opinion, it makes a lot of sense to go back and try some of these older models for this purpose, such as using a workflow to have the final step use an old Llama 1 model to rewrite the output of the modern LLMs.&lt;/p&gt; &lt;p&gt;Just to test it, I updated one of my Wilmer workflows to use Mistral Small as the thinker and Guanaco 65b as the responder. I had mistral write the initial response to me trying to sound human, and gave Guanaco very simple instructions to take that response and rewrite it to also sound more human. I then did the same using ChatGPT and Llama 3.3 70b.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;em&gt;This was a very small and simple POC test, and the quality of the output could be far higher with proper prompting, but I was just curious what the difference would be.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I asked the question: &lt;strong&gt;&lt;em&gt;&amp;quot;Which do you like better: Java or C#?&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small response:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Well, it depends on what you're looking to do. Both Java and C# have their strengths. For example:&lt;/p&gt; &lt;p&gt;- **Java**: It’s great for building web applications, Android apps, and large-scale enterprise systems. It has a larger community and more libraries available.&lt;/p&gt; &lt;p&gt;- **C#**: It's fantastic for game development (thanks to Unity), building Windows applications, and integrating with other Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think they both have their use cases. Java might be more versatile for certain web projects, but C# can be incredibly efficient for desktop applications and games.&lt;/p&gt; &lt;p&gt;Do you have a specific type of project in mind that you want to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Obviously a very complete and well written answer, but also very indicative that an AI wrote it. Something about the way it responds is familiar to folks who would be using chatgpt and the like.&lt;/p&gt; &lt;p&gt;Now, look at &lt;strong&gt;Guanaco 65b's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I'd say Java and C# both have their pros and cons. Java is great for building websites, Android apps, and huge enterprise systems. But C# is amazing for making games with Unity, building Windows programs, and hooking into other Microsoft services. Personally, I'd use Java for web stuff and C# for desktop apps and games, but it depends on your project.&lt;/p&gt; &lt;p&gt;So, what're you trying to build? I can give you some pointers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The difference is subtle, but see how it feels a bit more... human? &lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;ChatGPT o1's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Honestly, it really depends on what you’re trying to accomplish. Both Java and C# have their own strengths:&lt;/p&gt; &lt;p&gt;* Java: Great for web applications, Android apps, and large-scale enterprise systems. It also has a huge community and tons of libraries.&lt;/p&gt; &lt;p&gt;* C#: Awesome for game development (especially with Unity), building Windows apps, and integrating with Microsoft services.&lt;/p&gt; &lt;p&gt;Personally, I think both are solid choices. Java might be more versatile for certain web-based projects, while C# shines when it comes to desktop applications and gaming.&lt;/p&gt; &lt;p&gt;Any specific type of project you’re looking to work on?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This one remains sounding like an AI assistant. Same instructions and prompt I gave Guanaco.&lt;/p&gt; &lt;p&gt;Here is &lt;strong&gt;Llama 3.3's rewrite&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;so you're wondering about c# and java, right? they both have their own things goin for 'em, and honestly it kinda comes down to what you're workin on and what you like. they're actually pretty similar, they're both object-oriented, they both run on virtual machines, c# uses .net and java uses the jvm, and they both got a ton of libraries and a huge community behind 'em.&lt;/p&gt; &lt;p&gt;so like, here's some differences, ok? c# is made by microsoft, so it works super well with windows and all that, and java's open-source and can run on pretty much anything that's got a jvm, so that's linux, mac, android, all that. and syntax-wise, they're a little different too, like c# uses these curly braces to define a block of code and java uses semicolons to end statements and all that.&lt;/p&gt; &lt;p&gt;personally, i'm cool with both, i mean c# is awesome for windows stuff, but java's got that whole cross-platform thing goin on, so yeah… what about you, got a preference or whatever?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Its... well, about what you'd expect lol. Llama also deciding rewriting like a human means no capitalization =D Also, it added some details of its own to the response, but that's ok.&lt;/p&gt; &lt;p&gt;Again, this is a very simplistic POC just to show what I mean, and I'm probably going to spend more time delving into the idea, but I really think that there's still value in these old models, especially when trying to write things that sound more &amp;quot;human&amp;quot;, like maybe article or email writing.&lt;/p&gt; &lt;p&gt;Anyhow, just a thought I'd figure might spark some conversation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4hb2l/theory_trying_to_use_newer_and_more_powerful_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T21:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4em80</id>
    <title>Interesting article on how DeepSeek has improved the architecture in DeepSeek V2 and V3.</title>
    <updated>2025-01-18T19:00:14+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://epoch.ai"&gt;epoch.ai&lt;/a&gt; has published an interesting article: &lt;a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture"&gt;https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It talks about MLA, MoE innovations and Multi-Token Prediction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4em80/interesting_article_on_how_deepseek_has_improved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T19:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i457gp</id>
    <title>Intel should release a 24GB version of the Arc B580</title>
    <updated>2025-01-18T10:58:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt; &lt;img alt="Intel should release a 24GB version of the Arc B580" src="https://external-preview.redd.it/KNNit46prWlA2v7rjsUV6TaIPMXvtB72RAGA4ZyQjNE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddd3f42144ca0c2a05d54cf349b57f74c2e13f0f" title="Intel should release a 24GB version of the Arc B580" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The B580 is already showing impressive performance for LLM inference, matching the RTX 3060 in Vulkan benchmarks (~36 tokens/sec on Qwen2 7B) while being more power efficient and $50 cheaper. But VRAM is the real bottleneck for running larger models locally.&lt;/p&gt; &lt;p&gt;With Intel's strong XMX matrix performance and the existing clamshell memory design validated in shipping docs, a 24GB variant is technically feasible. This would enable running 13B models quantized to 8-bit (most 13B models need ~14GB), existing models with larger context, etc.&lt;/p&gt; &lt;p&gt;It would have way better price/performance than RTX 4060 Ti 16GB, native Vulkan support without CUDA lock-in and more performance potential if OpenVINO is further optimized.&lt;/p&gt; &lt;p&gt;The regular B580's stellar price/performance ratio shows Intel can be aggressive on pricing. A ~$329 24GB variant would hit a sweet spot for local LLM enthusiasts building inference rigs.&lt;/p&gt; &lt;p&gt;This is Intel's chance to build mind- and marketshare among AI developers and enthusiasts who are tired of CUDA lock-in. They can grow a community around OpenVINO and their AI tooling. Every developer who builds with Intel's stack today helps their ecosystem forward. The MLPerf results show they have the performance - now they just need to get the hardware into developers' hands.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dec 16 '24: &lt;a href="https://www.pcgamer.com/hardware/graphics-cards/shipping-document-suggests-that-a-24-gb-version-of-intels-arc-b580-graphics-card-could-be-heading-to-market-though-not-for-gaming/"&gt;Shipping document suggests that a 24 GB version of Intel's Arc B580 graphics card could be heading to market, though not for gaming&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8"&gt;https://preview.redd.it/xaydqqjygqde1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d57bc47d8936ed555b725e7733a88541d20f6d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i457gp/intel_should_release_a_24gb_version_of_the_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T10:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4awir</id>
    <title>Have you truly replaced paid models(chatgpt, Claude etc) with self hosted ollama or hugging face ?</title>
    <updated>2025-01-18T16:14:58+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with locally hosted setups, but I keep finding myself coming back to ChatGPT for the ease and performance. For those of you who’ve managed to fully switch, do you still use services like ChatGPT occasionally? Do you use both? &lt;/p&gt; &lt;p&gt;Also, what kind of GPU setup is really needed to get that kind of seamless experience? My 16GB VRAM feels pretty inadequate in comparison to what these paid models offer. Would love to hear your thoughts and setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4awir/have_you_truly_replaced_paid_modelschatgpt_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T16:14:58+00:00</published>
  </entry>
</feed>
