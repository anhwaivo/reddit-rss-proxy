<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-04T18:25:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l2gv3a</id>
    <title>Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks</title>
    <updated>2025-06-03T16:46:59+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt; &lt;img alt="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/dgm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3boea</id>
    <title>Digitizing 30 Stacks of Uni Dokuments &amp; Feeding into a Local LLM</title>
    <updated>2025-06-04T17:31:45+00:00</updated>
    <author>
      <name>/u/SpitePractical8460</name>
      <uri>https://old.reddit.com/user/SpitePractical8460</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m embarking on a pretty ambitious project and could really use some advice. I have about 30 stacks of university notes – each stack is roughly 200 pages – that I want to digitize and then feed into a LLM for analysis. Basically, I'd love to be able to ask the LLM questions about my notes and get intelligent answers based on their content. Ideally, I’d also like to end up with editable Word-like documents containing the digitized text.&lt;/p&gt; &lt;p&gt;The biggest hurdle right now is the OCR (Optical Character Recognition) process. I've tried a few different methods already without much success. I've experimented with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tesseract OCR: Didn't produce great results, especially with my complex layouts.&lt;/li&gt; &lt;li&gt;PDF 24 OCR: Similar issues to Tesseract.&lt;/li&gt; &lt;li&gt;My Scanner’s Built-in Software: This was the best of the bunch so far, but it still struggles significantly. A lot of my notes contain tables and diagrams, and the OCR consistently messes those up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My goal is twofold: 1) To create a searchable knowledge base where I can ask questions about the content of my notes (e.g., &amp;quot;What were the key arguments regarding X?&amp;quot;), and 2) to have editable documents that I can add to or correct.&lt;/p&gt; &lt;p&gt;I'm relatively new to the world of LLMs, but I’ve been having fun experimenting with different models through Open WebUI connected to LM Studio. My setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: AMD Ryzen 7 5700X3D&lt;/li&gt; &lt;li&gt;GPU: RX 6700 XT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a bit concerned about whether my hardware will be sufficient. Also, I’m very new to programming – I don’t have any experience with Python or coding in general. I'm hoping there might be someone out there who can offer some guidance.&lt;/p&gt; &lt;p&gt;Specifically, I'd love to know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;OCR Recommendations: Are there any OCR engines or techniques that are particularly good at handling tables and complex layouts? (Ideally something that works well with AMD hardware).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Post-Processing: What’s the best way to clean up OCR output, especially when dealing with lots of tables? Are there any tools or libraries you recommend for correcting errors in bulk?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM Integration: Any suggestions on how to best integrate the digitized text into a local LLM (e.g., which models are good for question answering and knowledge retrieval)? I'm using Open WebUI/LM Studio currently (mainly because of LM Studios GPU Support), but open to other options.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Hardware Considerations: Is my AMD Ryzen 7 5700X3D and RX 6700 XT a reasonable setup for this kind of project?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help or suggestions would be greatly appreciated! I'm really excited about the potential of this project, but feeling a bit overwhelmed by the technical challenges.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;For anyone how is curious: I let gemma3 writes a good part of this post. On my own I just couldn’t keep it structured. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpitePractical8460"&gt; /u/SpitePractical8460 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3boea/digitizing_30_stacks_of_uni_dokuments_feeding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3boea/digitizing_30_stacks_of_uni_dokuments_feeding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3boea/digitizing_30_stacks_of_uni_dokuments_feeding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T17:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3csix</id>
    <title>Taskade MCP – Generate Claude/Cursor tools from any OpenAPI spec ⚡</title>
    <updated>2025-06-04T18:15:24+00:00</updated>
    <author>
      <name>/u/taskade</name>
      <uri>https://old.reddit.com/user/taskade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;We needed a faster way to wire AI agents (like Claude, Cursor) to real APIs using OpenAPI specs. So we built and open-sourced &lt;strong&gt;&lt;a href="https://github.com/taskade/mcp"&gt;Taskade MCP&lt;/a&gt;&lt;/strong&gt; — a codegen tool and local server that turns OpenAPI 3.x specs into Claude/Cursor-compatible MCP tools.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Auto-generates agent tools in seconds&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Compatible with MCP, Claude, Cursor&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Supports headers, fetch overrides, normalization&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Includes a local server&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-hostable or integrate into your workflow&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/taskade/mcp"&gt;https://github.com/taskade/mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More context: &lt;a href="https://www.taskade.com/blog/mcp/"&gt;https://www.taskade.com/blog/mcp/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks and welcome any feedback too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taskade"&gt; /u/taskade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3csix/taskade_mcp_generate_claudecursor_tools_from_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3csix/taskade_mcp_generate_claudecursor_tools_from_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3csix/taskade_mcp_generate_claudecursor_tools_from_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T18:15:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2uf6e</id>
    <title>Ecne AI Podcast Generator - Update</title>
    <updated>2025-06-04T02:29:53+00:00</updated>
    <author>
      <name>/u/Dundell</name>
      <uri>https://old.reddit.com/user/Dundell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt; &lt;img alt="Ecne AI Podcast Generator - Update" src="https://external-preview.redd.it/45EfjPVsgm1MYdHAKtLDE7-W5DynLHblAveFiC_Lni4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc854f0bdc436f59c7883ad04543499e14274e2e" title="Ecne AI Podcast Generator - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l1ttsivtlt4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cd68053e425cae46eaf174906c23e18539a2795"&gt;img&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I've been working more on one of my side projects, the &lt;a href="https://github.com/ETomberg391/Ecne-AI-Podcaster"&gt;Ecne-AI-Podcaster&lt;/a&gt; This was to automate as much as I can in a decent quality with as many free tools available to build some Automated Podcast videos. My project takes your Topic idea, some searching keywords you set, some guidance you'd like the podcast to use or follow, and then uses several techniques to automate researching the topic (Google/Brave API, Selenium, Newspaper4k, local pdf,docx,xlsx,xlsm,csv,txt files).&lt;/p&gt; &lt;p&gt;It will then compile a podcast script (Either Host/Guest or just Host in single speaker mode), along with an optional Report paper, and a Youtube Description generator in case you wanted such for posting. Once you have the script, you can then process it through the Podcast generator option, and it will generate segments of the audio for you to review, along with any tweaks and redo's you need to the text and TTS audio.&lt;/p&gt; &lt;p&gt;Overall the largest example I have done is a new video I've posted here: &lt;a href="https://youtu.be/zbZmEwGinoA?si=-SIWCyKdi8b94T9G"&gt; Dundell's Cyberspace - What are Game Emulators? &lt;/a&gt;which ended up with 173 sources used, distilled down to 89 with an acceptable relevance score based on the Topic, and then 78 segments of broken down TTS audio for a total 18 1/2 min video that took 2 hours (45 min script building + 45 min TTS generations + 30 min building the finalized video) along with 1 1/2 hours of manually fixing TTS audio ends with my built-in GUI for quality purposes.&lt;/p&gt; &lt;p&gt;Notes:&lt;br /&gt; - Installer is working but a huge mess. Taking some recommendations soon to either remove the sudo install requests and see if I an find a better solutions than using sudo for anything and just mention what the user needs to install beforehand like most other projects...&lt;/p&gt; &lt;p&gt;- Additionally looking into more options for the Docker backend. The backend TTS Server is entirely the &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;Orpheus-FastAPI Project&lt;/a&gt; and the models based on &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; which so far work the best for an all-in-one solution with very good quality audio in a nice FastAPI llama-server docker. I'd try out another TTS like Dia when I find a decent Dockerized FastAPI with similar functionality.&lt;/p&gt; &lt;p&gt;- Lastly I've been working on trying to get both Linux and Windows working, and so far I Can, but Windows takes a lot of reruns of the Installer, and again I am going to try to move away from anything Sudo or admin rights needed soon, or at least something more of Acknowledgement/consent for transparency.&lt;/p&gt; &lt;p&gt;If you have any questions let me know. I'm going to continue to look into developing this further. Fix up the Readme and requirements section and fix any additional bugs I can find.&lt;/p&gt; &lt;p&gt;Additional images of the project:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/66ujwcnlnt4f1.png?width=1510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=087b6bf7ac14be81c5d1348f199b8aafeb4eb111"&gt;Podcast TTS GUI (Still Pygame until I can rebuild into the WebGUI fully)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l745u1xxnt4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a2a15ae112155a6354240bd08660fd3bafc8f6e"&gt;Generating a Podcast TTS example&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3tiukfiot4f1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7a599e13242d6eb11b705cbbdece8caf6cc162"&gt;Generating Podcast Script Example&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dundell"&gt; /u/Dundell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2uf6e/ecne_ai_podcast_generator_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T02:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qv7z</id>
    <title>Help Me Understand MOE vs Dense</title>
    <updated>2025-06-03T23:33:16+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems SOTA LLMS are moving towards MOE architectures. The smartest models in the world &lt;a href="https://lmarena.ai/leaderboard"&gt;seem to be using it&lt;/a&gt;. But why? When you use a MOE model, only a fraction of parameters are actually active. Wouldn't the model be &amp;quot;smarter&amp;quot; if you just use all parameters? Efficiency is awesome, but there are many problems that the smartest models cannot solve (i.e., cancer, a bug in my code, etc.). So, are we moving towards MOE because we discovered some kind of intelligence scaling limit in dense models (for example, a dense 2T LLM could never outperform a well architected MOE 2T LLM) or is it just for efficiency, or both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2rwhu</id>
    <title>Secure Minions: private collaboration between Ollama and frontier models</title>
    <updated>2025-06-04T00:23:17+00:00</updated>
    <author>
      <name>/u/MediocreBye</name>
      <uri>https://old.reddit.com/user/MediocreBye</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"&gt; &lt;img alt="Secure Minions: private collaboration between Ollama and frontier models" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Secure Minions: private collaboration between Ollama and frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extremely interesting developments coming out of Hazy Research. Has anyone tested this yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MediocreBye"&gt; /u/MediocreBye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/blog/secureminions"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2rwhu/secure_minions_private_collaboration_between/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T00:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l33r5h</id>
    <title>Help me use AI for my game - specific case</title>
    <updated>2025-06-04T12:01:57+00:00</updated>
    <author>
      <name>/u/Salamander500</name>
      <uri>https://old.reddit.com/user/Salamander500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, hope this is the right place to ask.&lt;/p&gt; &lt;p&gt;I created a game to play myself in C# and C++ - its one of those hidden object games.&lt;/p&gt; &lt;p&gt;As I made it for myself I used assets from another game from a different genre. The studio that developed that game has since closed down in 2016, but I don't know who owns the copyright now, seems no one. The sprites I used from that game are distinctive and easily recognisable as coming from that game.&lt;/p&gt; &lt;p&gt;Now that I'm thinking of sharing my game with everyone, how can I use AI to recreate these images in a different but uniform style, to detach it from the original source.&lt;/p&gt; &lt;p&gt;Is there a way I can feed it the original sprites, plus examples of the style I want the new game to have, and for it to re-imagine the sprites?&lt;/p&gt; &lt;p&gt;Getting an artist to draw them is not an option as there are more than 10,000 sprites.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salamander500"&gt; /u/Salamander500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33r5h/help_me_use_ai_for_my_game_specific_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33r5h/help_me_use_ai_for_my_game_specific_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l33r5h/help_me_use_ai_for_my_game_specific_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T12:01:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l39mc2</id>
    <title>Is there any open source project leveraging genAI to run quality checks on tabular data ?</title>
    <updated>2025-06-04T16:12:22+00:00</updated>
    <author>
      <name>/u/Jazzlike_Tooth929</name>
      <uri>https://old.reddit.com/user/Jazzlike_Tooth929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, most of the work in the ML/data science/BI still relies on tabular data. Everybody who has worked on that knows data quality is where most of the work goes, and that’s super frustrating. &lt;/p&gt; &lt;p&gt;I used to use great expectations to run quality checks on dataframes, but that’s based on hard coded rules (you declare things like “column X needs to be between 0 and 10”). &lt;/p&gt; &lt;p&gt;Is there any open source project leveraging genAI to run these quality checks? Something where you tell what the columns mean and give business context, and the LLM creates tests and find data quality issues for you? &lt;/p&gt; &lt;p&gt;I tried deep research and openAI found nothing for me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jazzlike_Tooth929"&gt; /u/Jazzlike_Tooth929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39mc2/is_there_any_open_source_project_leveraging_genai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39mc2/is_there_any_open_source_project_leveraging_genai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l39mc2/is_there_any_open_source_project_leveraging_genai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T16:12:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l33bph</id>
    <title>Best model for data extraction from scanned documents</title>
    <updated>2025-06-04T11:39:19+00:00</updated>
    <author>
      <name>/u/Wintlink-</name>
      <uri>https://old.reddit.com/user/Wintlink-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building my little ocr tool to extract data from pdfs, mostly bank receipt, id cards, and stuff like that.&lt;br /&gt; I experimented with few models (running on ollama locally), and I found that gemma3:12b was the best choice I could get.&lt;br /&gt; I'm running on a 4070 laptop with 8Gb, but I have a desktop with a 5080 if the models really need more power and vram.&lt;br /&gt; Gemma3 is quite good especially with text data, but on the numbers it hallucinate a lot, even when the document is clearly readable.&lt;br /&gt; I tried Internvl2_5 4b, but it's not doing great at all, intervl3:8B is just responding &amp;quot;sorry&amp;quot;, so It's a bit broken in my use case.&lt;br /&gt; If you have any recommandation of models that could be great in my use case I would be interested :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wintlink-"&gt; /u/Wintlink- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l33bph/best_model_for_data_extraction_from_scanned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T11:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2gvar</id>
    <title>New META Paper - How much do language models memorize?</title>
    <updated>2025-06-03T16:47:12+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very interesting paper on dataset size, parameter size, and grokking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.24832"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l27g8d</id>
    <title>Google opensources DeepSearch stack</title>
    <updated>2025-06-03T09:25:47+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt; &lt;img alt="Google opensources DeepSearch stack" src="https://external-preview.redd.it/jtUtL7EqwS5bMEk8XfF81tFd6n1MgnQyQL0hQG-jzRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0633532a1f9e627adaa6246fd5a299a809f2654" title="Google opensources DeepSearch stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it's not evident if this is the exact same stack they use in the Gemini user app, it sure looks very promising! Seems to work with Gemini and Google Search. Maybe this can be adapted for any local model and SearXNG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T09:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2vrg2</id>
    <title>Fully offline verbal chat bot</title>
    <updated>2025-06-04T03:41:10+00:00</updated>
    <author>
      <name>/u/NonYa_exe</name>
      <uri>https://old.reddit.com/user/NonYa_exe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"&gt; &lt;img alt="Fully offline verbal chat bot" src="https://external-preview.redd.it/dWY5OGJ3aWl5dDRmMXZQOk9qxWLlo00dzciqndO7qSY-J3_oYBSBLg5Z6rT9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0facf9ba08505071c2b5d9e46bc075ab0b46dd5" title="Fully offline verbal chat bot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to get some feedback on my project at its current state. The goal is to have the program run in the background so that the LLM is always accessible with just a keybind. Right now I have it displaying a console for debugging, but it is capable of running fully in the background. This is written in Rust, and is set up to run fully offline. I'm using LM Studio to serve the model on an OpenAI compatable API, Piper TTS for the voice, and Whisper.cpp for the transcription.&lt;/p&gt; &lt;p&gt;Current ideas:&lt;br /&gt; - Find a better Piper model&lt;br /&gt; - Allow customization of hotkey via config file&lt;br /&gt; - Add a hotkey to insert the contents of the clipboard to the prompt&lt;br /&gt; - Add the ability to cut off the AI before it finishes&lt;/p&gt; &lt;p&gt;I'm not making the code available yet since at its current state its highly tailored to my specific computer. I will make it open source on GitHub once I fix that.&lt;/p&gt; &lt;p&gt;Please leave suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NonYa_exe"&gt; /u/NonYa_exe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cw4rpviiyt4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2vrg2/fully_offline_verbal_chat_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T03:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2oywk</id>
    <title>What GUI are you using for local LLMs? (AnythingLLM, LM Studio, etc.)</title>
    <updated>2025-06-03T22:09:03+00:00</updated>
    <author>
      <name>/u/Aaron_MLEngineer</name>
      <uri>https://old.reddit.com/user/Aaron_MLEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying out AnythingLLM and LM Studio lately to run models like LLaMA and Gemma locally. Curious what others here are using.&lt;/p&gt; &lt;p&gt;What’s been your experience with these or other GUI tools like GPT4All, Oobabooga, PrivateGPT, etc.?&lt;/p&gt; &lt;p&gt;What do you like, what’s missing, and what would you recommend for someone looking to do local inference with documents or RAG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaron_MLEngineer"&gt; /u/Aaron_MLEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T22:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l36s62</id>
    <title>Simple News Broadcast Generator Script using local LLM as "editor" EdgeTTS as narrator, using a list of RSS feeds you can curate yourself</title>
    <updated>2025-06-04T14:20:15+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l36s62/simple_news_broadcast_generator_script_using/"&gt; &lt;img alt="Simple News Broadcast Generator Script using local LLM as &amp;quot;editor&amp;quot; EdgeTTS as narrator, using a list of RSS feeds you can curate yourself" src="https://external-preview.redd.it/hcM8-O1ltv8-loyy7_68UOpQyrqYIJ-2Wv8L99rgmA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70fa2368d5859d9edd2fc09e22913d20ae1311ae" title="Simple News Broadcast Generator Script using local LLM as &amp;quot;editor&amp;quot; EdgeTTS as narrator, using a list of RSS feeds you can curate yourself" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this repo I built a simple python script which scrapes RSS feeds and generates a news broadcast mp3 narrated by a realistic voice, using Ollama, so local LLM, to generate the summaries and final composed broadcast. &lt;/p&gt; &lt;p&gt;You can specify whichever news sources you want in the feeds.yaml file, as well as the number of articles, as well as change the tone of the broadcast through editing the summary and broadcast generating prompts in the simple one file script. &lt;/p&gt; &lt;p&gt;All you need is Ollama installed and then pull whichever models you want or can run locally, I like mistral for this use case, and you can change out the models as well as the voice of the narrator, using edge tts, easily at the beginning of the script. &lt;/p&gt; &lt;p&gt;There is so much more you can do with this concept and build upon it. &lt;/p&gt; &lt;p&gt;I made a version the other day which had a full Vite/React frontend and FastAPI backend which displayed each of the news stories, summaries, links, sorting abilities as well as UI to change the sources and read or listen to the broadcast. &lt;/p&gt; &lt;p&gt;But I like the simplicity of this. Simply run the script and listen to the latest news in a brief broadcast from a myriad of viewpoints using your own choice of tone through editing the prompts. &lt;/p&gt; &lt;p&gt;This all originated on a post where someone said AI would lead to people being less informed and I argued that if you use AI correctly it would actually make you more informed. &lt;/p&gt; &lt;p&gt;So I decided to write a script which takes whichever news sources I want, in this case objectivity is my goal, as well I can alter the prompts which edit together the broadcast so that I do not have all of the interjected bias inherent in almost all news broadcasts nowadays. &lt;/p&gt; &lt;p&gt;So therefore I posit I can use AI to help people be more informed rather than less, through allowing an individual to construct their own news broadcasts free of the biases inherent with having a &amp;quot;human&amp;quot; editor of the news. &lt;/p&gt; &lt;p&gt;Soulless, but that is how I like my objective news content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kliewerdaniel/News02"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l36s62/simple_news_broadcast_generator_script_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l36s62/simple_news_broadcast_generator_script_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T14:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2ynsc</id>
    <title>Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say.</title>
    <updated>2025-06-04T06:36:26+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"&gt; &lt;img alt="Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say." src="https://external-preview.redd.it/rlOy3w1CoH2JdExOlsJT5MCZp4fqssksLcusxXxosAg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ce5032ae39d55f280b278a700a75157939738ba" title="Tried 10 models, all seem to refuse to write a 10,000 word story. Is there something bad with my prompt? I'm just doing some testing to learn and I can't figure out how to get the LLM to do as I say." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/uup3xQO.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2ynsc/tried_10_models_all_seem_to_refuse_to_write_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T06:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3btj3</id>
    <title>How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel</title>
    <updated>2025-06-04T17:37:22+00:00</updated>
    <author>
      <name>/u/Kapperfar</name>
      <uri>https://old.reddit.com/user/Kapperfar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"&gt; &lt;img alt="How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel" src="https://external-preview.redd.it/YWJhMzhuazA1eTRmMe_FxokcQpBsD-M8wgi3hLI8PHPTr7rVnpmgkef-dH9o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc4432caac062358ceae7b73921dc23d1f01261e" title="How does gemma3:4b-it-qat fare against OpenAI models on MMLU-Pro benchmark? Try for yourself in Excel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an Excel add-in that lets you run a prompt on thousands of rows of tasks. Might be useful for some of you to quickly benchmark new models when they come out. In the video I ran gemma3:4b-it-qat, gpt-4.1-mini, and o4-mini on a (admittedly tiny) subset of the MMLU Pro benchmark. I think I understand now why OpenAI didn't include MMLU Pro in their gpt-4.1-mini announcement blog post :D&lt;/p&gt; &lt;p&gt;To try for yourself, clone the git repo at &lt;a href="https://github.com/getcellm/cellm/"&gt;https://github.com/getcellm/cellm/&lt;/a&gt;, build with Visual Studio, and run the installer Cellm-AddIn-Release-x64.msi in src\Cellm.Installers\bin\x64\Release\en-US. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kapperfar"&gt; /u/Kapperfar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ye3ahlk05y4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3btj3/how_does_gemma34bitqat_fare_against_openai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T17:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2xpf5</id>
    <title>nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face</title>
    <updated>2025-06-04T05:34:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face" src="https://external-preview.redd.it/3iMGSgahr2EGqEFlPNVdelZ_zKKfIXKVuELuwxGt7R4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f881ace7e1b4f8039722557754569dbed2ca23ac" title="nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2xpf5/nvidiallama31nemotronnanovl8bv1_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T05:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l390xb</id>
    <title>Has anyone successfully built a coding assistant using local llama?</title>
    <updated>2025-06-04T15:49:06+00:00</updated>
    <author>
      <name>/u/rushblyatiful</name>
      <uri>https://old.reddit.com/user/rushblyatiful</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Something that's like Copilot, Kilocode, etc. &lt;/p&gt; &lt;p&gt;What model are you using? What pc specs do you have? How is the performance? &lt;/p&gt; &lt;p&gt;Lastly, is this even possible? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rushblyatiful"&gt; /u/rushblyatiful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l390xb/has_anyone_successfully_built_a_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T15:49:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1l35h5g</id>
    <title>KV Cache in nanoVLM</title>
    <updated>2025-06-04T13:23:57+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35h5g/kv_cache_in_nanovlm/"&gt; &lt;img alt="KV Cache in nanoVLM" src="https://external-preview.redd.it/fqfBX7C3jmtI84itPZTI_T_664vZv239TD6hn9XTKfY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47376a36a5552070fc30e5629099d856ecaa7080" title="KV Cache in nanoVLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought I had a fair amount of understanding about KV Cache before implementing it from scratch. I would like to dedicate this blog post to all of them who are really curious about KV Cache, think they know enough about the idea, but would love to implement it someday. &lt;/p&gt; &lt;p&gt;We discover a lot of things while working through it, and I have tried documenting it as much as I could. Hope you all will enjoy reading it. &lt;/p&gt; &lt;p&gt;We chose &lt;a href="https://github.com/huggingface/nanoVLM"&gt;nanoVLM&lt;/a&gt; to implement KV Cache so that it does not have too many abstractions and we could lay out the foundations better. &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="http://hf.co/blog/kv-cache"&gt;hf.co/blog/kv-cache&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rv93ilolvw4f1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd727fe98bf357bdccc6663a2183a50e5987c1c8"&gt;https://preview.redd.it/rv93ilolvw4f1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd727fe98bf357bdccc6663a2183a50e5987c1c8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35h5g/kv_cache_in_nanovlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35h5g/kv_cache_in_nanovlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l35h5g/kv_cache_in_nanovlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T13:23:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l3c8is</id>
    <title>GRMR-V3: A set of models for reliable grammar correction.</title>
    <updated>2025-06-04T17:53:41+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's face it: You don't need big models like 32B, or medium sized models like 8B for grammar correction. Smaller models, like &amp;lt;1B parameters, usually miss some grammatical nuances that require more context. So I've created a set of 1B-4B fine-tuned models specialized in just doing that: fixing grammar.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/qingy2024/grmr-v3-models-683e6a27b42e4eb0e950fbdd"&gt;Models&lt;/a&gt;: GRMR-V3 (1B, 1.2B, 1.7B, 3B, 4B, and 4.3B)&lt;br /&gt; &lt;a href="https://huggingface.co/collections/qingy2024/grmr-v3-ggufs-684083beb5be4b136e5fbc68"&gt;GGUFs here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Models don't really work with multiple messages, it just looks at your first message.&lt;br /&gt; - It works in llama.cpp, vllm, basically any inference engine.&lt;br /&gt; - Make sure you use the sampler settings in the model card, I know Open WebUI has different defaults.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Input/Output:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Original Text&lt;/th&gt; &lt;th align="left"&gt;Corrected Text&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;i dont know weather to bring a umbrella today&lt;/td&gt; &lt;td align="left"&gt;I don't know whether to bring an umbrella today.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l3c8is/grmrv3_a_set_of_models_for_reliable_grammar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T17:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2wvf3</id>
    <title>Python Pandas Ditches NumPy for Speedier PyArrow</title>
    <updated>2025-06-04T04:44:44+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"&gt; &lt;img alt="Python Pandas Ditches NumPy for Speedier PyArrow" src="https://external-preview.redd.it/YNruBwDE4glgMZxB7Rz7Dn9TVxQIuQU5bX9UAhvGW9I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0372e63d733e050c30987cb1c1b8b2b63ce28fc" title="Python Pandas Ditches NumPy for Speedier PyArrow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thenewstack.io/python-pandas-ditches-numpy-for-speedier-pyarrow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2wvf3/python_pandas_ditches_numpy_for_speedier_pyarrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T04:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l35rp1</id>
    <title>Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training</title>
    <updated>2025-06-04T13:37:13+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"&gt; &lt;img alt="Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training" src="https://preview.redd.it/l1wcpiqhyw4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09d23def47a360d64c26390174e009fd27fb722" title="Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Announcing the release of the official Common Corpus paper: a 20 page report detailing how we collected, processed and published 2 trillion tokens of reusable data for LLM pretraining.&amp;quot;&lt;/p&gt; &lt;p&gt;Thread by the first author: &lt;a href="https://x.com/Dorialexander/status/1930249894712717744"&gt;https://x.com/Dorialexander/status/1930249894712717744&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2506.01732"&gt;https://arxiv.org/abs/2506.01732&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l1wcpiqhyw4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l35rp1/common_corpus_the_largest_collection_of_ethical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T13:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l39ea3</id>
    <title>Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!</title>
    <updated>2025-06-04T16:03:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"&gt; &lt;img alt="Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!" src="https://external-preview.redd.it/v0smBaFAfIOYhWsjTXmZvmibfthD29DfOmGvXCsBLOk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21f1147df642a26ce4a38f94f61704c291faa086" title="Drummer's Cydonia 24B v3 - A Mistral 24B 2503 finetune!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Survey Time: I'm working on Skyfall v3 but need opinions on the upscale size. 31B sounds comfy for a 24GB setup? Do you have an upper/lower bound in mind for that range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l39ea3/drummers_cydonia_24b_v3_a_mistral_24b_2503/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T16:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l318di</id>
    <title>Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)</title>
    <updated>2025-06-04T09:32:34+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt; &lt;img alt="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" src="https://b.thumbs.redditmedia.com/72uFsuI12iQ2cSozyTM-SHrwJy10OoGPWLvGrCoRLbg.jpg" title="Shisa V2 405B: The strongest model ever built in Japan! (JA/EN)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, so we've released the latest member of our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2&lt;/a&gt; family of open bilingual (Japanes/English) models: &lt;a href="https://shisa.ai/posts/shisa-v2-405b/"&gt;Shisa V2 405B&lt;/a&gt;!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 405B Fine Tune, inherits the Llama 3.1 license&lt;/li&gt; &lt;li&gt;Not just our JA mix but also additional KO + ZH-TW to augment 405B's native multilingual&lt;/li&gt; &lt;li&gt;Beats GPT-4 &amp;amp; GPT-4 Turbo in JA/EN, matches latest GPT-4o and DeepSeek-V3 in JA MT-Bench (it's not a reasoning or code model, but 日本語上手!)&lt;/li&gt; &lt;li&gt;Based on our evals, it's is w/o a doubt the strongest model to ever be released from Japan, beating out the efforts of bigco's etc. Tiny teams can do great things leveraging open models!&lt;/li&gt; &lt;li&gt;Quants and end-point available for testing&lt;/li&gt; &lt;li&gt;Super cute doggos:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3suc49zzqv4f1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=732f3e703e207d4d9a4b1e750e3b793f061a811f"&gt;Shisa V2 405B 日本語上手！&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; crowd:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Of course full model weights at &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b"&gt;shisa-ai/shisa-v2-llama-3.1-405b&lt;/a&gt; but also a range of GGUFs in a repo as well: &lt;a href="https://huggingface.co/shisa-ai/shisa-v2-llama3.1-405b-GGUF"&gt;shisa-ai/shisa-v2-llama3.1-405b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;These GGUFs are all (except the Q8_0) imatrixed w/ a calibration set based on our (Apache 2.0, also available for download) core Shisa V2 SFT dataset. They range from 100GB for the IQ2_XXS to 402GB for the Q8_0. Thanks to ubergarm for the pointers for what the gguf quanting landscape looks like in 2025!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out our initially linked blog post for all the deets + a full set of overview slides in JA and EN versions. Explains how we did our testing, training, dataset creation, and all kinds of little fun tidbits like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vp7we685rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ebbb9ad61d82ad55b9bceb9db3493e4bc038d80"&gt;Top Notch Japanese&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xatqzpz7rv4f1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7a676b0479b34e2bca1af9d5d05d37b8cf32e7"&gt;When your model is significantly better than GPT 4 it just gives you 10s across the board 😂&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While I know these models are big and maybe not directly relevant to people here, we've now tested our dataset on a huge range of base models from 7B to 405B and can conclude it can basically make any model mo-betta' at Japanese (without negatively impacting English or other capabilities!).&lt;/p&gt; &lt;p&gt;This whole process has been basically my whole year, so happy to finally get it out there and of course, answer any questions anyone might have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T09:32:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1l352wk</id>
    <title>AMA – I’ve built 7 commercial RAG projects. Got tired of copy-pasting boilerplate, so we open-sourced our internal stack.</title>
    <updated>2025-06-04T13:06:03+00:00</updated>
    <author>
      <name>/u/Loud_Picture_1877</name>
      <uri>https://old.reddit.com/user/Loud_Picture_1877</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m a senior tech lead with 8+ years of experience, and for the last ~3 I’ve been knee-deep in building LLM-powered systems — RAG pipelines, agentic apps, text2SQL engines. We’ve shipped real products in manufacturing, sports analytics, NGOs, legal… you name it.&lt;/p&gt; &lt;p&gt;After doing this &lt;em&gt;again and again&lt;/em&gt;, I got tired of the same story: building ingestion from scratch, duct-taping vector DBs, dealing with prompt spaghetti, and debugging hallucinations without proper logs.&lt;/p&gt; &lt;p&gt;So we built &lt;a href="https://github.com/deepsense-ai/ragbits"&gt;&lt;strong&gt;ragbits&lt;/strong&gt;&lt;/a&gt; — a toolbox of reliable, type-safe, modular building blocks for GenAI apps. What started as an internal accelerator is now &lt;strong&gt;fully open-sourced (v1.0.0)&lt;/strong&gt; and ready to use.&lt;/p&gt; &lt;p&gt;Why we built it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We wanted &lt;em&gt;repeatability&lt;/em&gt;. RAG isn’t magic — but building it cleanly every time takes effort.&lt;/li&gt; &lt;li&gt;We needed to &lt;em&gt;move fast&lt;/em&gt; for PoCs, without sacrificing structure.&lt;/li&gt; &lt;li&gt;We hated black boxes — ragbits integrates easily with your observability stack (OpenTelemetry, CLI debugging, prompt testing).&lt;/li&gt; &lt;li&gt;And most importantly, we wanted to scale apps without turning the codebase into a dumpster fire.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m happy to answer questions about RAG, our approach, gotchas from real deployments, or the internals of ragbits. No fluff — just real lessons from shipping LLM systems in production.&lt;/p&gt; &lt;p&gt;We’re looking for feedback, contributors, and people who want to build better GenAI apps. If that sounds like you, take &lt;a href="https://github.com/deepsense-ai/ragbits"&gt;ragbits&lt;/a&gt; for a spin. &lt;/p&gt; &lt;p&gt;Let’s talk 👇&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Picture_1877"&gt; /u/Loud_Picture_1877 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l352wk/ama_ive_built_7_commercial_rag_projects_got_tired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-04T13:06:03+00:00</published>
  </entry>
</feed>
