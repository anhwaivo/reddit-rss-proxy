<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-22T06:25:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i6ku6i</id>
    <title>You can now use both R1 and Search Web (see the comparison with and without R1)</title>
    <updated>2025-01-21T15:16:32+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"&gt; &lt;img alt="You can now use both R1 and Search Web (see the comparison with and without R1)" src="https://b.thumbs.redditmedia.com/1p8-1CMzNGqsdKfM6gDHFVIvZPeHyEW7IhtpMIuEe9A.jpg" title="You can now use both R1 and Search Web (see the comparison with and without R1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i6ku6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6ku6i/you_can_now_use_both_r1_and_search_web_see_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73jcv</id>
    <title>AskDS - automatically send test failures and your repo to R1 to get actionable results to fix the failing test</title>
    <updated>2025-01-22T05:00:16+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73jcv/askds_automatically_send_test_failures_and_your/"&gt; &lt;img alt="AskDS - automatically send test failures and your repo to R1 to get actionable results to fix the failing test" src="https://external-preview.redd.it/dTB6cXZocXk4aGVlMZ6Ql2t-AEpYbSz_aL0unkgEeC92L3d6cUaq68j-jVoP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ccc7c45c0f19ed794e2b2be0d5b4756ca71d72b" title="AskDS - automatically send test failures and your repo to R1 to get actionable results to fix the failing test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l0zurgqy8hee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73jcv/askds_automatically_send_test_failures_and_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73jcv/askds_automatically_send_test_failures_and_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6kwy7</id>
    <title>just tell it to be logical</title>
    <updated>2025-01-21T15:19:52+00:00</updated>
    <author>
      <name>/u/spirobel</name>
      <uri>https://old.reddit.com/user/spirobel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"&gt; &lt;img alt="just tell it to be logical" src="https://preview.redd.it/j6ax54qh6dee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f2541b2247a45ef3e36854545cd09c866802e79" title="just tell it to be logical" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spirobel"&gt; /u/spirobel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6ax54qh6dee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6kwy7/just_tell_it_to_be_logical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i71j8q</id>
    <title>Difference between DeepSeek and OpenAI?</title>
    <updated>2025-01-22T03:16:29+00:00</updated>
    <author>
      <name>/u/devinak</name>
      <uri>https://old.reddit.com/user/devinak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How come OpenAI o1 + o1-mini has usage limits ($20 plus plan for 50 messages a week or $200 pro plan for unlimited) while DeepSeek's &amp;quot;DeepThink&amp;quot; is completely free to use on their site?&lt;/p&gt; &lt;p&gt;Am I missing something? Does DeepSeek's reasoning architecture have way less compute costs or does DeepSeek's reasoning not even come close to what o1 can do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devinak"&gt; /u/devinak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i71j8q/difference_between_deepseek_and_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i71j8q/difference_between_deepseek_and_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i71j8q/difference_between_deepseek_and_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T03:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6note</id>
    <title>Testing the new Deepseek models on my Cybersecurity test</title>
    <updated>2025-01-21T17:17:39+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran the new Deepseek models and distils through my multiple choice cyber security test:&lt;br /&gt; A good score requires heavy world knowledge and some reasoning.&lt;/p&gt; &lt;p&gt;1st - 01-preview - 95.72%&lt;br /&gt; &lt;strong&gt;2nd - Deepseek-R1-API - 94.06%&lt;/strong&gt;&lt;br /&gt; *** - Meta-Llama3.1-405b-FP8 - 94.06% (Modified dual prompt to allow CoT)&lt;br /&gt; 3rd - Claude-3.5-October - 92.92%&lt;br /&gt; 4th - O1-mini - 92.87%&lt;br /&gt; 5th - Meta-Llama3.1-405b-FP8 - 92.64%&lt;br /&gt; &lt;strong&gt;*** - Deepseek-v3-api - 92.64% (Modified dual prompt to allow CoT)&lt;/strong&gt;&lt;br /&gt; 6th - GPT-4o - 92.45%&lt;br /&gt; 7th - Mistral-Large-123b-2411-FP16 92.40%&lt;br /&gt; &lt;strong&gt;8th - Deepseek-v3-api - 91.92%&lt;/strong&gt;&lt;br /&gt; 9th - GPT-4o-mini - 91.75%&lt;br /&gt; *** - Sky-T1-32B-BF16 - 91.45&lt;br /&gt; *** - Qwen-QwQ-32b-AWQ - 90.74% (Modified dual prompt to allow CoT)&lt;br /&gt; 10th - DeepSeek-v2.5-1210-BF16 - 90.50%&lt;br /&gt; 11th - Meta-LLama3.3-70b-FP8 - 90.26%&lt;br /&gt; 11th - Qwen-2.5-72b-FP8 - 90.09%&lt;br /&gt; 13th - Meta-Llama3.1-70b-FP8 - 89.15%&lt;br /&gt; &lt;strong&gt;14th - DeepSeek-R1-Distill-Qwen-32B-FP16 - 89.31%&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;15th - DeepSeek-R1-Distill-Llama-70B-GGUF-Q5 - 89.07%&lt;/strong&gt;&lt;br /&gt; 16th - Phi-4-GGUF-Fixed-Q4 - 88.6%&lt;br /&gt; 16th - Hunyuan-Large-389b-FP8 - 88.60%&lt;br /&gt; &lt;strong&gt;18th - DeepSeek-R1-Distill-Qwen-32B-GGUF - 87.65%&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Fun fact not seen in the scores above, cost to run my ~420 question test&lt;br /&gt; DeepSeek V3 without COT: 3 Cents&lt;br /&gt; DeepSeek V3 with my COT: 9 Cents&lt;br /&gt; DeepSeek R1: 71 Cents&lt;br /&gt; O1 Mini: 196 Cents&lt;br /&gt; O1 Preview: 1600 Cents&lt;/p&gt; &lt;p&gt;Typically a score on my test drops by 0.5% or less going from full precision to Q4,&lt;br /&gt; Qwen going down 3% seems suspicious, wonder if there are GGUF issues?&lt;br /&gt; Llama distil scoring below llama 3.1 and 3.3 is also a little odd.&lt;br /&gt; I was running unsloth GGUF's in VLLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6note/testing_the_new_deepseek_models_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T17:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i703j8</id>
    <title>Now deploy via Transformers, Llama cpp, Ollama or integrate with XAI, OpenAI, Anthropic, Openrouter or custom endpoints! Local or OpenAI Embeddings CPU/MPS/CUDA Support Linux, Windows &amp; Mac. Fully open source.</title>
    <updated>2025-01-22T02:09:23+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i703j8/now_deploy_via_transformers_llama_cpp_ollama_or/"&gt; &lt;img alt="Now deploy via Transformers, Llama cpp, Ollama or integrate with XAI, OpenAI, Anthropic, Openrouter or custom endpoints! Local or OpenAI Embeddings CPU/MPS/CUDA Support Linux, Windows &amp;amp; Mac. Fully open source." src="https://external-preview.redd.it/F8-deTA8UPvQ15Oy12MGtJO13bYFuroRHIhDV-DD65s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3db535ac8a44bd516e80cc28284fb4eb89067335" title="Now deploy via Transformers, Llama cpp, Ollama or integrate with XAI, OpenAI, Anthropic, Openrouter or custom endpoints! Local or OpenAI Embeddings CPU/MPS/CUDA Support Linux, Windows &amp;amp; Mac. Fully open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://github.com/cntrlai/notate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i703j8/now_deploy_via_transformers_llama_cpp_ollama_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i703j8/now_deploy_via_transformers_llama_cpp_ollama_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T02:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n87h</id>
    <title>DeepSeek-R1 PlanBench benchmark results</title>
    <updated>2025-01-21T16:58:38+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"&gt; &lt;img alt="DeepSeek-R1 PlanBench benchmark results" src="https://preview.redd.it/qa5yh1w3odee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4bfe107587dff4e6f5da9478af9b4fd49c54d16" title="DeepSeek-R1 PlanBench benchmark results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qa5yh1w3odee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n87h/deepseekr1_planbench_benchmark_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6e2ni</id>
    <title>Trump Revokes Biden Executive Order on Addressing AI Risks</title>
    <updated>2025-01-21T08:24:46+00:00</updated>
    <author>
      <name>/u/logicchains</name>
      <uri>https://old.reddit.com/user/logicchains</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"&gt; &lt;img alt="Trump Revokes Biden Executive Order on Addressing AI Risks" src="https://external-preview.redd.it/kafi4nTKNA_q_kd_b8L_HlG2-8aXgOzhra9KW3niFio.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c38fa66355083c8c8e1692586840218d34f2bf17" title="Trump Revokes Biden Executive Order on Addressing AI Risks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logicchains"&gt; /u/logicchains &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.usnews.com/news/top-news/articles/2025-01-20/trump-revokes-biden-executive-order-on-addressing-ai-risks"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6e2ni/trump_revokes_biden_executive_order_on_addressing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T08:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zbsf</id>
    <title>The distilled R1 models likely work best in workflows, so now's a great time to learn those if you haven't already!</title>
    <updated>2025-01-22T01:31:21+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another member of our board recently pointed out that Deepseek's paper &lt;a href="https://kingy.ai/wp-content/uploads/2025/01/DeepSeek_R1.pdf"&gt;&amp;quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&amp;quot;&lt;/a&gt; said the following: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;R1, and likely all reasoning models, are best suited for a zero-shot &amp;quot;&lt;em&gt;please think through this specific problem&lt;/em&gt;&amp;quot; sort of prompts, and you'll likely get far better results doing that than having a multi-turn conversation jammed in there while it thinks.&lt;/p&gt; &lt;p&gt;So once again, I take the opportunity to say: workflows are your friend. I know I'm always harping about workflows, but this case is a slam dunk use-case for learning to use workflows, getting comfortable with them, etc.&lt;/p&gt; &lt;p&gt;You will likely get far better results out of R1, QwQ, the R1 Distilled models, etc if you were have a workflow that did something similar to the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Summarize the what the most recent message is saying and/or asking&lt;/li&gt; &lt;li&gt;Summarize any supporting context to assist in thinking about this&lt;/li&gt; &lt;li&gt;Pass 1 and 2 into the reasoning model, and let it think of a problem&lt;/li&gt; &lt;li&gt;Respond to the user using the output of 3.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There are 2 really valuable benefits of doing this- first: you only pass in a single scoped problem every time and second: you are hiding the full thinking logic of step 3, so that isn't kept within the conversation or agentic history.&lt;/p&gt; &lt;p&gt;It doesn't matter what workflow program you go with- n8n, langflow, wilmerai, omnichain, whatever. This is a great time to just try them out if you haven't already, and get used to working with them. I've been using workflows exclusively when using ai since at least May or June of last year, and I can't imagine going back. Many of you may end up not liking using them, but many of you might. Either way, you'll get the experience AND can use these distilled R1 models to their maximum benefit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zbsf/the_distilled_r1_models_likely_work_best_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6x1rz</id>
    <title>Deepseek-R1 is brittle</title>
    <updated>2025-01-21T23:45:27+00:00</updated>
    <author>
      <name>/u/girishsk</name>
      <uri>https://old.reddit.com/user/girishsk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt; &lt;img alt="Deepseek-R1 is brittle" src="https://b.thumbs.redditmedia.com/8NQUVLxGD7ZRf-EVvaa_9iF75S6_Ytn5AU8V2JqmcJg.jpg" title="Deepseek-R1 is brittle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86"&gt;https://preview.redd.it/w64gxy9sofee1.png?width=2005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ce8831dd1bd935250ebd75ccda5fdbf39ebe86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/girishsk"&gt; /u/girishsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x1rz/deepseekr1_is_brittle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T23:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1i72bip</id>
    <title>“Any Router” in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs</title>
    <updated>2025-01-22T03:53:08+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"&gt; &lt;img alt="“Any Router” in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs" src="https://preview.redd.it/ks10onr1xgee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61fada01dff806ec53c0bd3dc33cb257f94507e7" title="“Any Router” in v0.1.9 - unify access and observability to ollama-supported and API-based LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a new project; just a feature update on egress functionally to help developers unify access and observability for ollama- supported and API-based LLMs. Coincidentally out #1 feature request last month. &lt;/p&gt; &lt;p&gt;So if you want a simple way to access any LLM and get unified tracing and logs, then this update might be useful for you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Arch Gateway is an intelligent proxy server designed for prompts. Guides for egress LLM routing and ollama below &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw?tab=readme-ov-file#use-arch-gateway-as-llm-router"&gt;https://github.com/katanemo/archgw?tab=readme-ov-file#use-arch-gateway-as-llm-router&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/katanemo/archgw/tree/main/demos/currency_exchange_ollamau"&gt;https://github.com/katanemo/archgw/tree/main/demos/currency_exchange_ollamau&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ks10onr1xgee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i72bip/any_router_in_v019_unify_access_and_observability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T03:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6mjxv</id>
    <title>Deploy any LLM on Huggingface at 3-10x Speed</title>
    <updated>2025-01-21T16:30:20+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt; &lt;img alt="Deploy any LLM on Huggingface at 3-10x Speed" src="https://preview.redd.it/8dsnudtrhdee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bf725a99b4877d47c42a9a0a7d813a8339eb266" title="Deploy any LLM on Huggingface at 3-10x Speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dsnudtrhdee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6mjxv/deploy_any_llm_on_huggingface_at_310x_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vhzy</id>
    <title>Gemini Thinking experimental 01-21 is out!</title>
    <updated>2025-01-21T22:37:02+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"&gt; &lt;img alt="Gemini Thinking experimental 01-21 is out!" src="https://preview.redd.it/lizc4v8ncfee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d15911a5fea2f0b9809f3de42a8a8e34ce0a319" title="Gemini Thinking experimental 01-21 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lizc4v8ncfee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vhzy/gemini_thinking_experimental_0121_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6l3ms</id>
    <title>Thanks to DeepSeek other open model releases with "research" license will be laughable</title>
    <updated>2025-01-21T15:28:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine labs like Mistral, Cohere (do you remember them?) release open-weight model with so called research purposes only license. Comedy Central would call them for movie rights ;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6l3ms/thanks_to_deepseek_other_open_model_releases_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:28:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6pra7</id>
    <title>Spanish government releases some official models</title>
    <updated>2025-01-21T18:40:57+00:00</updated>
    <author>
      <name>/u/xdoso</name>
      <uri>https://old.reddit.com/user/xdoso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spanish government has fund the training of official and public LLMs, mainly trained on Spanish and co-official spanish languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main page&lt;/strong&gt;: &lt;a href="https://alia.gob.es/"&gt;https://alia.gob.es/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface models&lt;/strong&gt;: &lt;a href="https://huggingface.co/BSC-LT"&gt;https://huggingface.co/BSC-LT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The main released models are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Alia 40b&lt;/strong&gt; (base model still on training, published intermediate result; instruct version will be released in the future)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Salamandra 2b/7b&lt;/strong&gt; (base and instruct available)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main model has been trained using the Spanish Marenostrum 5 with a total of 2048 GPUs (H100 64Gb). They are all Apache 2.0 license and most datasets have been published also. They are mainly trained on European languages.&lt;/p&gt; &lt;p&gt;Also some translation models have been published:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Salamandra TA 2b:&lt;/strong&gt; translation between 30 main European languages directly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plume 256k, 128, 32k&lt;/strong&gt;: finetuning of gemma2 models for translation between Spanish languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aina models&lt;/strong&gt;: a list of 1to1 models for translation between Spanish languages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Alia 40b is the latest release and the most important one, although for the moments the results that we are seeing during the tests are pretty bad. &lt;/p&gt; &lt;p&gt;Posts about the results: &lt;/p&gt; &lt;p&gt;- &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i6qecq/spanish_alia_model_has_been_trained_with_porn_and/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdoso"&gt; /u/xdoso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6pra7/spanish_government_releases_some_official_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6x0od</id>
    <title>Local hero ports new minimax 456B model to llama.cpp</title>
    <updated>2025-01-21T23:44:01+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"&gt; &lt;img alt="Local hero ports new minimax 456B model to llama.cpp" src="https://external-preview.redd.it/XboNX-2aBFaEZb_pw6HGHDpi2wkmn6wYOlAdwzo9X-M.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32c1d2fa99a9246c99a12c019dd3fbea99b33755" title="Local hero ports new minimax 456B model to llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgflip.com/i/9hi08y"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6x0od/local_hero_ports_new_minimax_456b_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T23:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6n7jf</id>
    <title>Pretty sure OpenAI has their devs working 24/7 to not lose their throne to DeepSeek 😭</title>
    <updated>2025-01-21T16:57:48+00:00</updated>
    <author>
      <name>/u/Condomphobic</name>
      <uri>https://old.reddit.com/user/Condomphobic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And DeepSeek is making the same progress at a much faster pace than OpenAI is. They are definitely in a rock situation &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Condomphobic"&gt; /u/Condomphobic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6n7jf/pretty_sure_openai_has_their_devs_working_247_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T16:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6jbur</id>
    <title>DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!</title>
    <updated>2025-01-21T14:07:01+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt; &lt;img alt="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" src="https://external-preview.redd.it/8HwSiZPd8K_hder46_kXYrWF23xE0qYwa1myzoXXUfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32931922738bce5caa4226b968a442514cf96587" title="DeepSeek R1 (Qwen 32B Distill) is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6jbur/deepseek_r1_qwen_32b_distill_is_now_available_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T14:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6t08q</id>
    <title>DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH).</title>
    <updated>2025-01-21T20:53:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt; &lt;img alt="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." src="https://external-preview.redd.it/bHl5MDU0Yzl0ZWVlMQAQ0j2wFUvXTQrT52Nv81bl04kSZ1X_57NkDQOUMylE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277e9be2fe8997f1f81f1d7306a6ef378dd50407" title="DeepSeek-R1-Distill-Qwen-1.5B running 100% locally in-browser on WebGPU. Reportedly outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks (28.9% on AIME and 83.9% on MATH)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ei4j3c9teee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6t08q/deepseekr1distillqwen15b_running_100_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6lsgo</id>
    <title>From llama2 --&gt; DeepSeek R1 things have gone a long way in a 1 year</title>
    <updated>2025-01-21T15:58:08+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was blown away by llama2 70b when it came out. I felt so empowered having so much knowledge spun up locally on my M3 Max. &lt;/p&gt; &lt;p&gt;Just over a year, and DeepSeek R1 makes Llama 2 seem like a little child. It's crazy how good the outputs are, and how fast it spits out tokens in just 40GB.&lt;/p&gt; &lt;p&gt;Can't imagine where things will be in another year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6lsgo/from_llama2_deepseek_r1_things_have_gone_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T15:58:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6zid8</id>
    <title>Just a comparison of US $500B Stargate AI project to other tech projects</title>
    <updated>2025-01-22T01:40:19+00:00</updated>
    <author>
      <name>/u/Shir_man</name>
      <uri>https://old.reddit.com/user/Shir_man</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Manhattan Project&lt;/strong&gt; ~$30 billion in today’s dollars [~1.5% of US GDP in the mid-1940s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Apollo Program&lt;/strong&gt; ~$170–$180 billion in today’s dollars [~0.5% of US GDP in the mid-1960s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Space Shuttle Program&lt;/strong&gt; ~$275–$300 billion in today’s dollars [~0.2% of US GDP in the early 1980s]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interstate Highway System&lt;/strong&gt;, entire decades-long Interstate Highway System buildout, ~$500–$550 billion in today’s dollars [~0.2%–0.3% of GDP annually over multiple decades]&lt;/p&gt; &lt;p&gt;Stargate is huge AI project [~1.7% of US GDP 2024]&lt;/p&gt; &lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; rough GDP calculations added&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shir_man"&gt; /u/Shir_man &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6zid8/just_a_comparison_of_us_500b_stargate_ai_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T01:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6urjd</id>
    <title>Billions in proprietary AI? No more.</title>
    <updated>2025-01-21T22:06:11+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt; &lt;img alt="Billions in proprietary AI? No more." src="https://b.thumbs.redditmedia.com/9ar_9wVV9mMAYbxuIYJsMxrYp5TIPP2iGETnpq4RXng.jpg" title="Billions in proprietary AI? No more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI raised billions on promise of having and securing behind thick doors something no other is even remotely close to. The following tweet from just few days prior R1 release made me think they really have atomic bomb the world will knee for;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd"&gt;https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The truth is, they have nothing. o1-level, some say human-level reasoning is reproducible, can be privately hosted by anyone, anywhere. Can't be greedily priced.&lt;/p&gt; &lt;p&gt;MIT licensed open models is the future of AI. Zero dollars is the only right price for something made on all human knowledge. It is a sum of effort of the whole civilisation, spanning many generations. Just imagine, any book that landed in the pretraining dataset influences the whole model. There is no better way to honor any author contributing to the overall model performance, knowingly or not than to make a tool that help create new knowledge, available for anyone, at no cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6uviy</id>
    <title>R1 is mind blowing</title>
    <updated>2025-01-21T22:10:54+00:00</updated>
    <author>
      <name>/u/Not-The-Dark-Lord-7</name>
      <uri>https://old.reddit.com/user/Not-The-Dark-Lord-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gave it a problem from my graph theory course that’s reasonably nuanced. 4o gave me the wrong answer twice, but did manage to produce the correct answer once. R1 managed to get this problem right in one shot, and also held up under pressure when I asked it to justify its answer. It also gave a great explanation that showed it really understood the nuance of the problem. I feel pretty confident in saying that AI is smarter than me. Not just closed, flagship models, but smaller models that I could run on my MacBook are probably smarter than me at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not-The-Dark-Lord-7"&gt; /u/Not-The-Dark-Lord-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vnqc</id>
    <title>Trump announces a $500 billion AI infrastructure investment in the US</title>
    <updated>2025-01-21T22:43:49+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt; &lt;img alt="Trump announces a $500 billion AI infrastructure investment in the US" src="https://external-preview.redd.it/qFlenD3wOMEKpyf-2qth3Zo8oJQzBNIpBFiyCeVPdPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1661f472f059c05f183a4286b726667eb4724fc9" title="Trump announces a $500 billion AI infrastructure investment in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:43:49+00:00</published>
  </entry>
</feed>
