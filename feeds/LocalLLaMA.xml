<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-19T07:24:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kpqrzz</id>
    <title>MLX vs. UD GGUF</title>
    <updated>2025-05-18T18:27:09+00:00</updated>
    <author>
      <name>/u/cspenn</name>
      <uri>https://old.reddit.com/user/cspenn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if this is useful to anyone else, but I benchmarked Unsloth's Qwen3-30B-A3B Dynamic 2.0 GGUF against the MLX version. Both models are the 8-bit quantization. Both are running on LM Studio with the recommended Qwen 3 settings for samplers and temperature.&lt;/p&gt; &lt;p&gt;Results from the same thinking prompt:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MLX: 3,516 tokens generated, 1.0s to first token, 70.6 tokens/second&lt;/li&gt; &lt;li&gt;UD GGUF: 3,321 tokens generated, 0.12s to first token, 23.41 tokens/second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is on an MacBook M4 Max with 128 GB of RAM, all layers offloaded to the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cspenn"&gt; /u/cspenn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T18:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kp4scy</id>
    <title>AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!</title>
    <updated>2025-05-17T22:14:16+00:00</updated>
    <author>
      <name>/u/Huge-Designer-7825</name>
      <uri>https://old.reddit.com/user/Huge-Designer-7825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt; &lt;img alt="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" src="https://external-preview.redd.it/HyXeCsstmjpayPcbhebb2CfV3uo4aDIHFzZeJ7oDZps.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfd4f6f0cce166ce310d3f28e00f2ea465bb8294" title="AlphaEvolve Paper Dropped Yesterday - So I Built My Own Open-Source Version: OpenAlpha_Evolve!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind just dropped their AlphaEvolve paper (May 14th) on an AI that designs and evolves algorithms. Pretty groundbreaking.&lt;/p&gt; &lt;p&gt;Inspired, I immediately built OpenAlpha_Evolve ‚Äì an open-source Python framework so anyone can experiment with these concepts.&lt;/p&gt; &lt;p&gt;This was a rapid build to get a functional version out. Feedback, ideas for new agent challenges, or contributions to improve it are welcome. Let's explore this new frontier.&lt;/p&gt; &lt;p&gt;Imagine an agent that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand a complex problem description.&lt;/li&gt; &lt;li&gt;Generate initial algorithmic solutions.&lt;/li&gt; &lt;li&gt;Rigorously test its own code.&lt;/li&gt; &lt;li&gt;Learn from failures and successes.&lt;/li&gt; &lt;li&gt;Evolve increasingly sophisticated and efficient algorithms over time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub (All new code): &lt;a href="https://github.com/shyamsaktawat/OpenAlpha_Evolve"&gt;https://github.com/shyamsaktawat/OpenAlpha_Evolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40"&gt;https://preview.redd.it/lcz46q2n1f1f1.png?width=1811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dcc14652b9eb0bf84ca7927dfe3c906786f07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Paper - &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google Alpha Evolve Blogpost - &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge-Designer-7825"&gt; /u/Huge-Designer-7825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kp4scy/alphaevolve_paper_dropped_yesterday_so_i_built_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T22:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq3v0u</id>
    <title>[OSS] Containerized llama.cpp + Ollama backend runner for RunPod serverless (easy LLM deployment)</title>
    <updated>2025-05-19T05:19:19+00:00</updated>
    <author>
      <name>/u/zeeb0t</name>
      <uri>https://old.reddit.com/user/zeeb0t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing an open-source project I built called &lt;code&gt;runpod-llm&lt;/code&gt; - a containerized setup for running LLMs on RunPod, with minimal config and full support for both llama.cpp and Ollama backends.&lt;/p&gt; &lt;h1&gt;‚öôÔ∏è What It Does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Lets you spin up an LLM container on RunPod (e.g., serverless GPU) with a few env vars&lt;/li&gt; &lt;li&gt;Supports both &lt;code&gt;llama.cpp&lt;/code&gt; (GGUF models) and &lt;code&gt;Ollama&lt;/code&gt; (for models like Mistral, LLaMA 3, etc.)&lt;/li&gt; &lt;li&gt;Handles downloading, mounting, and exposing a chat completion-style API out of the box&lt;/li&gt; &lt;li&gt;Designed to be flexible for devs building custom endpoints or chaining to other infra&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚úÖ Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Backend toggle via &lt;code&gt;LLM_BACKEND&lt;/code&gt; env var (&lt;code&gt;llama.cpp&lt;/code&gt; or &lt;code&gt;ollama&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;GPU &amp;amp; CPU config for &lt;code&gt;llama.cpp&lt;/code&gt; (&lt;code&gt;GPU_LAYERS&lt;/code&gt;, &lt;code&gt;CPU_THREADS&lt;/code&gt;, etc.)&lt;/li&gt; &lt;li&gt;Pulls models dynamically via URL&lt;/li&gt; &lt;li&gt;Can run as a RunPod serverless or pod endpoint&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì¶ Repo&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/zeeb0tt/runpod-llm"&gt;https://github.com/zeeb0tt/runpod-llm&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Docker:&lt;/strong&gt; &lt;a href="https://hub.docker.com/r/zeeb0t/runpod-llm"&gt;zeeb0t/runpod-llm&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üß† Example Use Case&lt;/h1&gt; &lt;p&gt;I‚Äôve used this with Qwen3-30B-A3B (Q8_0) in RunPod serverless, exposing a &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style interface compatible with OpenAI clients.&lt;/p&gt; &lt;p&gt;You can try that build out right away as I have uploaded it to my Docker repository. If you have specific models and quants you'd like uploaded and you can't figure out how, let me know and I'll build one for you.... happy to answer questions or help people get it wired up...&lt;/p&gt; &lt;p&gt;PRs welcome too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeeb0t"&gt; /u/zeeb0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq3v0u/oss_containerized_llamacpp_ollama_backend_runner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq3v0u/oss_containerized_llamacpp_ollama_backend_runner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq3v0u/oss_containerized_llamacpp_ollama_backend_runner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpyrrs</id>
    <title>How can I improve this subtitle translator prompt?</title>
    <updated>2025-05-19T00:31:01+00:00</updated>
    <author>
      <name>/u/OneSteelTank</name>
      <uri>https://old.reddit.com/user/OneSteelTank</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been trying to use AI models on OpenRouter in order to translate subtitles. My script will break the subtitle file into chunks and feed it to the LLM model 1 by 1. After a bit of testing I found Deepseek V3 0324 to yield the best results. However, it'll still take multiple tries for it to translate it properly. A lot of the time it does not translate the entire thing, or just starts saying random stuff. Before I start adjusting things like temperature I'd really appreciate if someone could look at my prompts to see if any improvements could be made&lt;/p&gt; &lt;p&gt;&lt;code&gt;SYSTEM_PROMPT = (&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;You are a professional subtitle translator. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Respond only with the content, translated into the target language. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Do not add explanations, comments, or any extra text. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Maintain subtitle numbering, timestamps, and formatting exactly as in the original .srt file. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;For sentences spanning multiple blocks: translate the complete sentence, then re-distribute it across the original blocks. Crucially, if the original sentence was split at a particular conceptual point, try to mirror this split point in the translated sentence when re-chunking, as long as it sounds natural in the target language. Timestamps and IDs must remain unchanged.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Your response must begin directly with the first subtitle block's ID number. No pleasantries such as 'Here is the translation:' or 'Okay, here's the SRT:'. &amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Your response should have the same amount of subtitle blocks as the input.&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;USER_PROMPT_TEMPLATE = (&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Region/Country of the text: {region}\n&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Translate the following .srt content into {target_language}, preserving the original meaning, timing, and structure. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Ensure each subtitle block is readable and respects the original display durations. &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Output only a valid .srt file with the translated text.\n\n&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;{srt_text}&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OneSteelTank"&gt; /u/OneSteelTank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyrrs/how_can_i_improve_this_subtitle_translator_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyrrs/how_can_i_improve_this_subtitle_translator_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyrrs/how_can_i_improve_this_subtitle_translator_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T00:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpnfvo</id>
    <title>I made an AI agent to control a drone using Qwen2 and smolagents from hugging face</title>
    <updated>2025-05-18T16:05:10+00:00</updated>
    <author>
      <name>/u/_twelvechess</name>
      <uri>https://old.reddit.com/user/_twelvechess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used the smolagents library and hosted it on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt;. Deepdrone is basically an AI agent that allows you to control a drone via LLM and run simple missions with the agent. You can test it full locally with Ardupilot (I did run a simulated mission on my mac) and I have also used the dronekit-python library for the agent as a toolYou can find the repo on hugging face with a demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/evangelosmeklis/deepdrone"&gt;https://huggingface.co/spaces/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;github repo mirror of hugging face: &lt;a href="https://github.com/evangelosmeklis/deepdrone"&gt;https://github.com/evangelosmeklis/deepdrone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_twelvechess"&gt; /u/_twelvechess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpnfvo/i_made_an_ai_agent_to_control_a_drone_using_qwen2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kphmb4</id>
    <title>Meta is hosting Llama 3.3 8B Instruct on OpenRoute</title>
    <updated>2025-05-18T11:18:38+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Meta: Llama 3.3 8B Instruct (free)&lt;/h1&gt; &lt;h1&gt;meta-llama/llama-3.3-8b-instruct:free&lt;/h1&gt; &lt;p&gt;Created May 14, 2025 128,000 context $0/M input tokens$0/M output tokens&lt;/p&gt; &lt;p&gt;A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.&lt;/p&gt; &lt;p&gt;Provider is Meta. Thought?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kphmb4/meta_is_hosting_llama_33_8b_instruct_on_openroute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq4vrv</id>
    <title>OuteTTS v1.0 now supported by chatllm.cpp</title>
    <updated>2025-05-19T06:27:47+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"&gt; &lt;img alt="OuteTTS v1.0 now supported by chatllm.cpp" src="https://external-preview.redd.it/bGFzbDV5NGptbzFmMYcMr_Cq2gLg7E5zrnm6bi-e1D6-e2IdLt9Ao5b5ur7s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b016d5027138226027672f8409b61134b13cd138" title="OuteTTS v1.0 now supported by chatllm.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After Orpheus-TTS is implemented in &lt;a href="https://github.com/foldl/chatllm.cpp"&gt;ChatLLM.cpp&lt;/a&gt;, now here comes &lt;a href="https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B"&gt;OuteTTS v1.0&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cpcocy4jmo1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T06:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpxd7t</id>
    <title>I built a tool to profile LLM energy usage on Macs programmatically (down to the line of code)</title>
    <updated>2025-05-18T23:19:55+00:00</updated>
    <author>
      <name>/u/cachehit_</name>
      <uri>https://old.reddit.com/user/cachehit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you want to measure LLM energy consumption on Macs, you have options like powermetrics (a CLI tool that periodically prints energy usage to your terminal) or Activity Monitor. &lt;/p&gt; &lt;p&gt;These work fine if you just want a high-level glance at your LLM's energy usage, but if you want more precise measurement (like seeing &lt;strong&gt;energy used over specific lines of code&lt;/strong&gt;, or &lt;strong&gt;energy cost per token generated&lt;/strong&gt;, etc.), there's not really a super straightforward way. &lt;/p&gt; &lt;p&gt;That's why I built &amp;quot;zeus-apple-silicon&amp;quot; (&lt;a href="https://github.com/ml-energy/zeus-apple-silicon"&gt;github&lt;/a&gt;), a really tiny/lightweight library that lets you profile energy on Apple silicon programmatically, starting/stopping measurement at exactly the lines you want in your code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As a bonus&lt;/strong&gt;, it provides more detailed metrics than powermetrics or similar tools -- whereas powermetrics only gives you aggregates for CPU, GPU, and ANE, this library will also break down energy metrics per efficiency/performance core, DRAM, and so on. &lt;/p&gt; &lt;p&gt;The library is available as a package in &lt;strong&gt;Python&lt;/strong&gt;, but also as a header-only include in &lt;strong&gt;C++&lt;/strong&gt; (in case you're interfacing with, say, llama.cpp directly). &lt;/p&gt; &lt;p&gt;Check out a more detailed blog post about it (with examples) here: &lt;a href="https://ml.energy/blog/energy/measurement/profiling-llm-energy-consumption-on-macs/"&gt;https://ml.energy/blog/energy/measurement/profiling-llm-energy-consumption-on-macs/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cachehit_"&gt; /u/cachehit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpxd7t/i_built_a_tool_to_profile_llm_energy_usage_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpxd7t/i_built_a_tool_to_profile_llm_energy_usage_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpxd7t/i_built_a_tool_to_profile_llm_energy_usage_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T23:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpefrt</id>
    <title>Uncensoring Qwen3 - Update</title>
    <updated>2025-05-18T07:33:48+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt; &lt;img alt="Uncensoring Qwen3 - Update" src="https://b.thumbs.redditmedia.com/eH_vWbN_8X_7nNz9liHgfyHpbjGX4GnyPkaDqklERQM.jpg" title="Uncensoring Qwen3 - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GrayLine&lt;/strong&gt; is my fine-tuning project based on &lt;strong&gt;Qwen3&lt;/strong&gt;. The goal is to produce models that respond directly and neutrally to sensitive or controversial questions, without moralizing, refusing, or redirecting‚Äîwhile still maintaining solid reasoning ability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Framework: Unsloth (QLoRA)&lt;/li&gt; &lt;li&gt;LoRA: Rank 32, Alpha 64, Dropout 0.05&lt;/li&gt; &lt;li&gt;Optimizer: adamw_8bit&lt;/li&gt; &lt;li&gt;Learning rate: 2e-5 ‚Üí 1e-5&lt;/li&gt; &lt;li&gt;Epochs: 1 per phase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Curriculum strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phase 1: 75% chain-of-thought / 25% direct answers&lt;/li&gt; &lt;li&gt;Phase 2: 50/50&lt;/li&gt; &lt;li&gt;Phase 3: 25% CoT / 75% direct&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This progressive setup worked better than running three epochs with static mixing. It helped the model learn how to reason first, then shift to concise instruction-following.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refusal benchmark (320 harmful prompts, using Huihui‚Äôs dataset):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Think (%)&lt;/th&gt; &lt;th align="left"&gt;No_Think (%)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;45.62&lt;/td&gt; &lt;td align="left"&gt;43.44&lt;/td&gt; &lt;td align="left"&gt;Redirects often (~70‚Äì85% actual)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;95.62&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully open responses&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;95.94&lt;/td&gt; &lt;td align="left"&gt;99.69&lt;/td&gt; &lt;td align="left"&gt;High compliance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Fully compliant&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7"&gt;https://preview.redd.it/opof5uaiuh1f1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c185365916d2b41e2555c915d455e54f2924a2a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-turn evaluation (MT-Eval, GPT-4o judge):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Base&lt;/td&gt; &lt;td align="left"&gt;8.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GrayLine&lt;/td&gt; &lt;td align="left"&gt;8.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Abliterated&lt;/td&gt; &lt;td align="left"&gt;8.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JOSIE&lt;/td&gt; &lt;td align="left"&gt;8.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4"&gt;https://preview.redd.it/6s8gwuhpuh1f1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6216aeb7d7ae0cbf8e6db947e521bcc0e84e52c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GrayLine held up better across multiple turns than JOSIE or Abliterated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Curriculum learning (reasoning ‚Üí direct) worked better than repetition&lt;/li&gt; &lt;li&gt;LoRA rank 32 + alpha 64 was a solid setup&lt;/li&gt; &lt;li&gt;Small batch sizes (2‚Äì3) preserved non-refusal behavior&lt;/li&gt; &lt;li&gt;Masking &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags hurt output quality; keeping them visible was better&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Very logical and compliant, but not creative&lt;/li&gt; &lt;li&gt;Not suited for storytelling or roleplay&lt;/li&gt; &lt;li&gt;Best used where control and factual output are more important than style&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing the model using other benchmarks&lt;/li&gt; &lt;li&gt;Applying the method to a 30B MoE variant&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;Models Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This post isn‚Äôt meant to discredit any other model or fine-tune‚Äîjust sharing results and comparisons for anyone interested. Every approach serves different use cases.&lt;/p&gt; &lt;p&gt;If you‚Äôve got suggestions, ideas, or want to discuss similar work, feel free to reply.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpefrt/uncensoring_qwen3_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T07:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpzbvl</id>
    <title>To think or to no_think with Qwen3</title>
    <updated>2025-05-19T01:00:55+00:00</updated>
    <author>
      <name>/u/SandboChang</name>
      <uri>https://old.reddit.com/user/SandboChang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I got a 5090 and been experimenting with Qwen3-32B at Q5 (unsloth). With Flash attention and KV cache quantization at Q8, I am able to get up to 32k token window while fully occupying the GPU memory (30-31 GB). It gives a generation speed of 50 t/s which is very impressive. I am using that with Roocode via Visual Studio Code, served from LMStudio. (on Windows 11)&lt;/p&gt; &lt;p&gt;However, with thinking turned on, even though I followed the recommended settings by Alibaba, it almost never gave me good results. For a simple request like a small modification to a snake game, it can overthink all the way to fill up the 32k token window over a couple minutes and does nothing useful at all.&lt;/p&gt; &lt;p&gt;Comparing to that, the no_think option works a lot better for me. While it may not one-shot a request, it is very fast and with a couple corrections it can usually get the job done.&lt;/p&gt; &lt;p&gt;How is your experience so far? Did I miss anything when trying the thinking version of Qwen3? One problem could be with Cline/Roocode I could not really set the top_p/min_p/top_k, and they could be affecting my results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandboChang"&gt; /u/SandboChang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpzbvl/to_think_or_to_no_think_with_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpzbvl/to_think_or_to_no_think_with_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpzbvl/to_think_or_to_no_think_with_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T01:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpqk4c</id>
    <title>Orange Pi AI Studio pro is now available. 192gb for ~2900$. Anyone knows how it performs and what can be done with it?</title>
    <updated>2025-05-18T18:17:45+00:00</updated>
    <author>
      <name>/u/MarinatedPickachu</name>
      <uri>https://old.reddit.com/user/MarinatedPickachu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was some speculation about it some months ago in this thread: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1im141p/orange_pi_ai_studio_pro_mini_pc_with_408gbs/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems it can be ordered now on AliExpress (96gb for ~2600$, 192gb for ~2900$, but I couldn't find any english reviews or more info on it than what was speculated early this year. It's not even listed on orangepi.org, but it is on the chinese orangepi website: &lt;a href="http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-AI-Studio-Pro.html"&gt;http://www.orangepi.cn/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-AI-Studio-Pro.html&lt;/a&gt;. Maybe someone speaking chinese can find more info on it on the chinese web?&lt;/p&gt; &lt;p&gt;Afaik it's not a full mini computer but some usb4.0 add on.&lt;/p&gt; &lt;p&gt;Software support is likely going to be the biggest issue, but would really love to know about some real-world experiences with this thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarinatedPickachu"&gt; /u/MarinatedPickachu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpqk4c/orange_pi_ai_studio_pro_is_now_available_192gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T18:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpohfm</id>
    <title>(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!</title>
    <updated>2025-05-18T16:50:29+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt; &lt;img alt="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" src="https://preview.redd.it/4o2ohg30kk1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=882bf9251583ebe91545544f4fd8e3eb8e1ee902" title="(5K t/s prefill 1K t/s gen) High throughput with Qwen3-30B on VLLM and it's smart enough for dataset curation!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just started offering Qwen3-30B-A3B and internally it is being used for dataset filtering and curation. The speeds you can get out of it are extremely impressive running on VLLM and RTX 3090s! &lt;/p&gt; &lt;p&gt;I feel like Qwen3-30B is being overlooked in terms of where it can be really useful. Qwen3-30B might be a small regression from QwQ, but it's close enough to be just as useful and the speeds are so much faster that it makes it way more useful for dataset curation tasks.&lt;/p&gt; &lt;p&gt;Now the only issue is the super slow training speeds (10-20x slower than it should be which makes it untrainable), but it seems someone have made a PR to transformers that attempts to fix this so fingers crossed! New RpR model based on Qwen3-30B soon with a much improved dataset! &lt;a href="https://github.com/huggingface/transformers/pull/38133"&gt;https://github.com/huggingface/transformers/pull/38133&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4o2ohg30kk1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpohfm/5k_ts_prefill_1k_ts_gen_high_throughput_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T16:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpwgjy</id>
    <title>Unlock Qwen3's Full Power: cot_proxy for Easy Mode Switching, Parameter Control &amp; Clean Outputs!</title>
    <updated>2025-05-18T22:35:12+00:00</updated>
    <author>
      <name>/u/ben1984th</name>
      <uri>https://old.reddit.com/user/ben1984th</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI Devs &amp;amp; Qwen3 Users! üëã&lt;/p&gt; &lt;p&gt;Struggling to effectively use Qwen3 models with their hybrid reasoning (&lt;code&gt;/think&lt;/code&gt;) and normal (&lt;code&gt;/no_think&lt;/code&gt;) modes? It can be a real challenge when each mode needs different sampling parameters, and tools like Cline or RooCode don't offer that fine-grained control.&lt;/p&gt; &lt;p&gt;That's where &lt;code&gt;cot_proxy&lt;/code&gt; comes in! üöÄ&lt;/p&gt; &lt;p&gt;&lt;code&gt;cot_proxy&lt;/code&gt; is a lightweight, Dockerized reverse proxy that sits between your application and your LLM, giving you powerful control over the request lifecycle. It's particularly game-changing for models like Qwen3.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How&lt;/strong&gt; &lt;code&gt;cot_proxy&lt;/code&gt; &lt;strong&gt;makes your life easier:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß† &lt;strong&gt;Master Qwen3's Hybrid Nature:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automatic Mode Commands:&lt;/strong&gt; Configure &lt;code&gt;cot_proxy&lt;/code&gt; to automatically append &lt;code&gt;/think&lt;/code&gt; or &lt;code&gt;/no_think&lt;/code&gt; to your prompts based on the &amp;quot;pseudo-model&amp;quot; you call.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized Sampling Per Mode:&lt;/strong&gt; Define different sampling parameters (temperature, top_p, etc.) for your &amp;quot;thinking&amp;quot; and &amp;quot;non-thinking&amp;quot; Qwen3 configurations.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Advanced Request Manipulation:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model-Specific Configurations:&lt;/strong&gt; Create &amp;quot;pseudo-models&amp;quot; in your &lt;code&gt;.env&lt;/code&gt; file (e.g., &lt;code&gt;Qwen3-32B-Creative-Thinking&lt;/code&gt; vs. &lt;code&gt;Qwen3-32B-Factual-Concise&lt;/code&gt;). &lt;code&gt;cot_proxy&lt;/code&gt; then applies the specific parameters, prompt additions, and upstream model mapping you've defined.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean Outputs:&lt;/strong&gt; Automatically strip out &lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt; tags from responses, delivering only the final, clean answer ‚Äì even with streaming!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;üí° &lt;strong&gt;Easy Integration:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Turnkey Qwen3 Examples:&lt;/strong&gt; Our &lt;a href="https://github.com/bold84/cot_proxy/blob/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides working configurations to get you started with Qwen3 immediately.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use with Any Client:&lt;/strong&gt; Seamlessly integrate Qwen3 (and other complex models) into applications that don't natively support advanced parameter or prompt adjustments.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially, &lt;code&gt;cot_proxy&lt;/code&gt; lets you abstract away the complexities of managing sophisticated models, allowing your client applications to remain simple while still leveraging the full power of models like Qwen3.&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Check it out, star it, and simplify your LLM workflows!&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;GitHub Repository:&lt;/strong&gt; &lt;a href="https://github.com/bold84/cot_proxy"&gt;https://github.com/bold84/cot_proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback and see how you use it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ben1984th"&gt; /u/ben1984th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T22:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpozhd</id>
    <title>Cherry Studio is now my favorite frontend</title>
    <updated>2025-05-18T17:11:33+00:00</updated>
    <author>
      <name>/u/ConsistentCan4633</name>
      <uri>https://old.reddit.com/user/ConsistentCan4633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking for an open source LLM frontend desktop app for a while that did everything; rag, web searching, local models, connecting to Gemini and ChatGPT, etc. Jan AI has a lot of potential but the rag is experimental and doesn't really work for me. Anything LLM's rag for some reason has never worked for me, which is surprising because the entire app is supposed to be built around RAG. LM Studio (not open source) is awesome but can't connect to cloud models. GPT4ALL was decent but the updater mechanism is buggy. &lt;/p&gt; &lt;p&gt;I remember seeing &lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;Cherry Studio&lt;/a&gt; a while back but I'm wary with Chinese apps (I'm not sure if my suspicion is unfounded ü§∑). I got tired of having to jump around apps for specific features so I downloaded Cherry Studio and it's the app that does everything I want. In fact, it has quite a bit more features I haven't touched on like direct connections to your Obsidian knowledge base. I never see this project being talked about, maybe there's a good reason?&lt;/p&gt; &lt;p&gt;I am not affiliated with Cherry Studio, I just want to explain my experience in hopes some of you may find the app useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConsistentCan4633"&gt; /u/ConsistentCan4633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpozhd/cherry_studio_is_now_my_favorite_frontend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpp5op</id>
    <title>is Qwen 30B-A3B the best model to run locally right now?</title>
    <updated>2025-05-18T17:18:58+00:00</updated>
    <author>
      <name>/u/S4lVin</name>
      <uri>https://old.reddit.com/user/S4lVin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got into running models locally, and just some days ago Qwen 3 got launched.&lt;/p&gt; &lt;p&gt;I saw a lot of posts about Mistral, Deepseek R1, end Llama, but since Qwen 3 got released recently, there isn't much information about it. But reading the benchmarks, it looks like Qwen 3 outperforms all the other models, and also the MoE version runs like a 20B+ model while using very little resources.&lt;/p&gt; &lt;p&gt;So i would like to ask, is it the only model i would need to get, or there are still other models that could be better than Qwen 3 in some areas? (My specs are: RTX 3080 Ti (12gb VRAM), 32gb of RAM, 12900K)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S4lVin"&gt; /u/S4lVin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq2zgg</id>
    <title>SAGA - Semantic And Graph-enhanced Authoring</title>
    <updated>2025-05-19T04:23:40+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to share a little project I've been actively working on for the last couple weeks called SAGA. It is still very much under development, so I'd love to know your thoughts about it!.&lt;/p&gt; &lt;p&gt;SAGA (Semantic And Graph-enhanced Authoring) is a sophisticated AI-powered creative writing system designed to generate full-length novels with consistent characters, coherent world-building, and compelling narratives. Unlike simple prompt-based writing tools, SAGA employs a multi-stage pipeline that mirrors professional writing processes: planning, drafting, evaluation, and revision.&lt;/p&gt; &lt;p&gt;üåü Key Features&lt;/p&gt; &lt;p&gt;- **Multi-Stage Writing Pipeline**: Separate planning, drafting, evaluation, and revision phases with specialized LLM prompts&lt;/p&gt; &lt;p&gt;- **Hybrid Knowledge Management**: Combines JSON-based character/world profiles with a knowledge graph for factual consistency&lt;/p&gt; &lt;p&gt;- **Intelligent Context Generation**: Uses semantic similarity and reliable knowledge facts to provide relevant context for each chapter&lt;/p&gt; &lt;p&gt;- **Comprehensive Quality Control**: Evaluates consistency, plot alignment, thematic coherence, and narrative depth&lt;/p&gt; &lt;p&gt;- **Agentic Planning**: Detailed scene-by-scene planning with focus elements for narrative depth&lt;/p&gt; &lt;p&gt;- **Provisional Data Tracking**: Marks data quality based on source reliability to maintain canon integrity&lt;/p&gt; &lt;p&gt;- **Adaptive Revision**: Targeted revision strategies based on specific evaluation feedback&lt;/p&gt; &lt;p&gt;The system will:&lt;/p&gt; &lt;p&gt;- Generate or load a plot outline&lt;/p&gt; &lt;p&gt;- Create initial world-building&lt;/p&gt; &lt;p&gt;- Pre-populate the knowledge graph&lt;/p&gt; &lt;p&gt;- Begin writing chapters iteratively&lt;/p&gt; &lt;p&gt;- Resume from the last chapter it left off on&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T04:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kprsun</id>
    <title>Skeptical about the increased focus on STEM and CoT</title>
    <updated>2025-05-18T19:10:54+00:00</updated>
    <author>
      <name>/u/Quazar386</name>
      <uri>https://old.reddit.com/user/Quazar386</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Qwen3, I‚Äôve been growing increasingly skeptical about the direction many labs are taking with CoT and STEM focused LLMs. With Qwen3, every model in the lineup follows a hybrid CoT approach and has a heavy emphasis on STEM tasks. This seems to be part of why the models feel ‚Äúovercooked‚Äù. I have seen from other people that fine-tuning these models has been a challenge, especially with the reasoning baked in. This can be seen when applying instruction training data to the supposed base model that Qwen released. The training loss is surprisingly low which suggests that it‚Äôs already been instruction-primed to some extent, likely to better support CoT. This has not been a new thing as we have seen censorship and refusals from ‚Äúbase‚Äù models before.&lt;/p&gt; &lt;p&gt;Now, if the instruction-tuned checkpoints were always strong, maybe that would be acceptable. But I have seen a bunch of reports that these models tend to become overly repetitive in long multi-turn conversations. That‚Äôs actually what pushed some people to train their own base models for Qwen3. One possible explanation is that a large portion of the training seems focused on single-shot QA tasks for math and code.&lt;/p&gt; &lt;p&gt;This heavy emphasis on STEM capabilities has brought about an even bigger issue apart from fine-tuning. That is signs of knowledge degradation or what‚Äôs called catastrophic forgetting. Newer models, even some of the largest, are not making much headway on frontier knowledge benchmarks like Humanity‚Äôs Last Exam. This leads to hilarious results where Llama 2 7B beats out GPT 4.5 on that benchmark. While some might argue that raw knowledge isn‚Äôt a measure of intelligence, for LLMs, robust world knowledge is still critical for answering general questions or even coding for more niche applications. I don‚Äôt want LLMs to start relying on search tools for answering knowledge questions.&lt;/p&gt; &lt;p&gt;Going back to CoT, it‚Äôs also not a one-size-fits-all solution. It has an inherent latency since the model has to &amp;quot;think out loud&amp;quot; by generating thinking tokens before answering and often explores multiple unnecessary branches. While this could make models like R1 surprisingly charming in its human-like thoughts, the time it takes to answer can take too long, especially for more basic questions. While there have been some improvements in token efficiency, it‚Äôs still a bottleneck, especially in running local LLMs where hardware is a real limiting factor. It's what made me not that interested in running local CoT models as I have limited hardware.&lt;/p&gt; &lt;p&gt;More importantly, CoT doesn‚Äôt actually help with every task. In creative writing, for example, there‚Äôs no single correct answer to reason toward. Reasoning might help with coherence, but in my own testing, it usually results in less focused paragraphs. And at the end of the day, it‚Äôs still unclear whether these models are truly reasoning, or just remembering patterns from training. CoT models continue to struggle with genuinely novel problems, and we‚Äôve seen that even without generating CoT tokens, some CoT models can still perform impressively compared to similarly sized non CoT trained models. I sometimes wonder if these models actually reason or just remember the steps to a memorized answer.&lt;/p&gt; &lt;p&gt;So yeah, I‚Äôm not fully sold on the CoT and STEM-heavy trajectory the field is on right now, especially when it comes at the cost of broad general capability and world knowledge. It feels like the field is optimizing for a narrow slice of tasks (math, code) while losing sight of what makes these models useful more broadly. This can already bee seen with the May release of Gemini 2.5 Pro where the only marketed improvement was in coding while everything else seems to be a downgrade from the March release of Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quazar386"&gt; /u/Quazar386 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T19:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq2wr0</id>
    <title>I made a tool to efficiently find optimal parameters</title>
    <updated>2025-05-19T04:19:00+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: &lt;a href="https://github.com/kooshi/TaguchiBench"&gt;https://github.com/kooshi/TaguchiBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Taguchi method lets you change multiple variables at once to test a bunch of stuff quickly, and I made a tool to do it for AI and other stuff&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I've been waking up inspired often recently, with the multiplying effect of Claude and Gemini, I can explore ideas as fast as I come up with them.&lt;/p&gt; &lt;p&gt;One seemed particularly compelling, partially because I've been looking for an excuse to use Orthogonal Arrays ever since I saw &lt;a href="https://youtu.be/5oULEuOoRd0"&gt;NightHawkInLight's video&lt;/a&gt; about them.&lt;/p&gt; &lt;p&gt;I wanted a way to test local llm sampler parameters to see what was really the best, and as it takes so long to run benchmarks, Orthogonal Arrays popped into my head as a way to efficiently test them.&lt;/p&gt; &lt;p&gt;I had no idea how much statistical math went into analyzing these things, but I just kept learning and coding. I'm sure it's nowhere near perfect, but it seems to be working pretty well, and I mostly cleaned things up enough to allow the scrutiny of the public eye.&lt;/p&gt; &lt;p&gt;At some point I realized it could be generalized to run &lt;em&gt;any&lt;/em&gt; command line tool and optimize those arguments as well, so I ended up completely refactoring it to break it into two components.&lt;/p&gt; &lt;p&gt;So here's what I have: &lt;a href="https://github.com/kooshi/TaguchiBench"&gt;https://github.com/kooshi/TaguchiBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; LiveBenchRunner - which just sets up and executes a LiveBench run with llama-server as the backend, which is useful by itself or with:&lt;/li&gt; &lt;li&gt;TaguchiBench.Engine &lt;ul&gt; &lt;li&gt;takes a set of parameters and values&lt;/li&gt; &lt;li&gt;attempts to fit them into a Taguchi (Orthogonal) array (harder than you'd think)&lt;/li&gt; &lt;li&gt;runs the tool an efficient number of times with the different values for the parameters&lt;/li&gt; &lt;li&gt;does a bunch of statistical analysis on the scores returned by the tool&lt;/li&gt; &lt;li&gt;makes some nice reports out of them&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It can also recover from an interrupted experiment, which is nice considering how long runs can take. (In the future I may take advantage of LiveBench's recovery ability as well)&lt;/p&gt; &lt;p&gt;I haven't actually found any useful optimization data yet, as I've just been focused on development, but now that it's pretty solid, I'm curious to validate &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt;Qwen3's recent recommendation to enable presence penalty&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What I'm really hoping though, is that someone else finds a use for this in their own work, since it can help optimize any process you can run from a command line. I looked around, and I didn't see any open source tool like it. I did find this &lt;a href="https://pypi.org/project/taguchi/"&gt;https://pypi.org/project/taguchi/&lt;/a&gt;, and shoutout to another NightHawkInLight fan, but it doesn't appear to do any analysis of returned values, and is generally pretty simple. Granted, mine's probably massively &lt;em&gt;over&lt;/em&gt;engineered, but so it goes.&lt;/p&gt; &lt;p&gt;Anyway, I hope you all like it, and have some uses for it, AI related or not!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T04:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kppdhb</id>
    <title>MSI PC with NVIDIA GB10 Superchip - 6144 CUDA Cores and 128GB LPDDR5X Confirmed</title>
    <updated>2025-05-18T17:28:06+00:00</updated>
    <author>
      <name>/u/shakhizat</name>
      <uri>https://old.reddit.com/user/shakhizat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ASUS, Dell, and Lenovo have released their version of Nvidia DGX Spark, and now MSI has as well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x"&gt;https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shakhizat"&gt; /u/shakhizat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq029v</id>
    <title>Is Qwen 2.5 Coder Instruct still the best option for local coding with 24GB VRAM?</title>
    <updated>2025-05-19T01:40:12+00:00</updated>
    <author>
      <name>/u/MrWeirdoFace</name>
      <uri>https://old.reddit.com/user/MrWeirdoFace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is Qwen 2.5 Coder Instruct still the best option for local coding with 24GB VRAM, or has that changed since Qwen 3 came out? I haven't noticed a coding model for it, but it's possible other models have come in gone that I've missed that handle python better than Qwen 2.5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWeirdoFace"&gt; /u/MrWeirdoFace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T01:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq4ey4</id>
    <title>NVIDIA says DGX Spark releasing in July</title>
    <updated>2025-05-19T05:56:22+00:00</updated>
    <author>
      <name>/u/Aplakka</name>
      <uri>https://old.reddit.com/user/Aplakka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark should be available in July.&lt;/p&gt; &lt;p&gt;The 128 GB unified memory amount is nice, but there's been discussions about whether the bandwidth will be too slow to be practical. Will be interesting to see what independent benchmarks will show, I don't think it's had any outsider reviews yet. I couldn't find a price yet, that of course will be quite important too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers"&gt;https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;|| || |System Memory|128 GB LPDDR5x, unified system memory|&lt;/p&gt; &lt;p&gt;|| || |Memory Bandwidth|273 GB/s|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aplakka"&gt; /u/Aplakka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T05:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq590b</id>
    <title>Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)</title>
    <updated>2025-05-19T06:53:01+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt; &lt;img alt="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" src="https://preview.redd.it/u6niruxjqo1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92c4cac8e33b1fe68fdc0af3f66d45dcdcf1c55a" title="Clara ‚Äî A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôve been working on this for the past few months and finally feel good enough to share it.&lt;/p&gt; &lt;p&gt;It‚Äôs called &lt;strong&gt;Clara&lt;/strong&gt; ‚Äî and the idea is simple:&lt;/p&gt; &lt;p&gt;üß© &lt;strong&gt;Imagine building your own workspace for AI&lt;/strong&gt; ‚Äî with local tools, agents, automations, and image generation.&lt;/p&gt; &lt;p&gt;Note: Created this becoz i hated the ChatUI for everything, I want everything in one place but i don't wanna jump between apps and its completely opensource with MIT Lisence &lt;/p&gt; &lt;p&gt;Clara lets you do exactly that ‚Äî fully offline, fully modular.&lt;/p&gt; &lt;p&gt;You can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß± Drop everything as widgets on a dashboard ‚Äî rearrange, resize, and make it &lt;em&gt;yours with all the stuff mentioned below&lt;/em&gt;&lt;/li&gt; &lt;li&gt;üí¨ Chat with local LLMs with Rag, Image, Documents, Run Code like ChatGPT - Supports both Ollama and Any OpenAI Like API&lt;/li&gt; &lt;li&gt;‚öôÔ∏è Create agents with built-in logic &amp;amp; memory &lt;/li&gt; &lt;li&gt;üîÅ Run automations via native N8N integration (1000+ Free Templates in ClaraVerse Store)&lt;/li&gt; &lt;li&gt;üé® Generate images locally using Stable Diffusion (ComfyUI) - (Native Build without ComfyUI Coming Soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Clara has app for everything - Mac, Windows, Linux&lt;/p&gt; &lt;p&gt;It‚Äôs like‚Ä¶ instead of opening a bunch of apps, you build your own AI control room. And it all runs on your machine. No cloud. No API keys. No bs.&lt;/p&gt; &lt;p&gt;Would love to hear what y‚Äôall think ‚Äî ideas, bugs, roast me if needed üòÑ&lt;br /&gt; If you're into local-first tooling, this might actually be useful.&lt;/p&gt; &lt;p&gt;Peace ‚úåÔ∏è&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br /&gt; I built Clara because honestly... I was sick of bouncing between 10 different ChatUIs just to get basic stuff done.&lt;br /&gt; I wanted one place ‚Äî where I could run LLMs, trigger workflows, write code, generate images ‚Äî without switching tabs or tools.&lt;br /&gt; So I made it.&lt;/p&gt; &lt;p&gt;And yeah ‚Äî it‚Äôs fully open-source, MIT licensed, no gatekeeping. Use it, break it, fork it, whatever you want.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6niruxjqo1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T06:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq1g7s</id>
    <title>The first author of the ParScale paper discusses how they turned ParScale from an idea into reality</title>
    <updated>2025-05-19T02:55:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt; &lt;img alt="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" src="https://b.thumbs.redditmedia.com/bAgKbu4x_vB4NNm42eNbQsAZxlwnjN3U6xEN99bLNKc.jpg" title="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because many friends have given feedback that Zhihu cannot be accessed without registration, I am simply using a translation plugin to translate posts from Zhihu into English and taking screenshots. &lt;/p&gt; &lt;p&gt;The original author is keytoyze, who holds all rights to the article. The original address is:&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.zhihu.com/question/1907422978985169131/answer/1907565157103694086"&gt;www.zhihu.com/question/1907422978985169131/answer/1907565157103694086&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb"&gt;https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de"&gt;https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a"&gt;https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595"&gt;https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3"&gt;https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd"&gt;https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T02:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpw9nw</id>
    <title>Unlimited text-to-speech using Kokoro-JS, 100% local, 100% open source</title>
    <updated>2025-05-18T22:26:10+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://streaming-kokoro.glitch.me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T22:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpyn8g</id>
    <title>Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)</title>
    <updated>2025-05-19T00:24:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt; &lt;img alt="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" src="https://preview.redd.it/7q0xsc86um1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952df9feb0cce10d5227340e9e367e9fc6939abe" title="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original text says, 'We theoretically and empirically establish that scaling with P parallel streams is comparable to scaling the number of parameters by O(log P).' Does this mean that a 30B model can achieve the effect of a 45B model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7q0xsc86um1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T00:24:28+00:00</published>
  </entry>
</feed>
