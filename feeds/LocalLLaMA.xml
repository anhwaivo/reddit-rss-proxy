<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-20T04:09:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m3wogu</id>
    <title>Are there any quants of larger models 48 VRAM + 96 RAM can run, which are better than just 32B models?</title>
    <updated>2025-07-19T13:48:31+00:00</updated>
    <author>
      <name>/u/West_Investigator258</name>
      <uri>https://old.reddit.com/user/West_Investigator258</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. &lt;/p&gt; &lt;p&gt;However most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can't even shove more RAM into it to run 200gb quants. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West_Investigator258"&gt; /u/West_Investigator258 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m44tnz</id>
    <title>Keras vs Transformers fine tuning</title>
    <updated>2025-07-19T19:30:35+00:00</updated>
    <author>
      <name>/u/Ok-Refrigerator6609</name>
      <uri>https://old.reddit.com/user/Ok-Refrigerator6609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to ML and fine tuning.&lt;/p&gt; &lt;p&gt;Recently I've tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn't convert it to pytorch because the conversion script by Keras doesn't support the gemma 3 yet and so I abandoned this project because of that.&lt;/p&gt; &lt;p&gt;I then tried fine tuning with transformers and even though I've tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.&lt;/p&gt; &lt;p&gt;I learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.&lt;/p&gt; &lt;p&gt;I'm wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.&lt;/p&gt; &lt;p&gt;Thank you in advance&lt;/p&gt; &lt;p&gt;Sorry, this post &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Refrigerator6609"&gt; /u/Ok-Refrigerator6609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T19:30:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m390kj</id>
    <title>DGAF if it’s dumber. It’s mine.</title>
    <updated>2025-07-18T17:48:14+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt; &lt;img alt="DGAF if it’s dumber. It’s mine." src="https://preview.redd.it/8dnb7bl76odf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca95154378d607f250ca4e5e26488394250116bf" title="DGAF if it’s dumber. It’s mine." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dnb7bl76odf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T17:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qpxz</id>
    <title>What are the most intriguing AI papers of 2025</title>
    <updated>2025-07-19T08:00:18+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been keeping up with AI research in 2025, and DeepSeek R1 really stands out to me as game-changing. What other papers from this year do you consider to be truly revolutionary?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T08:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4al6m</id>
    <title>Which model is best for vision fitting 24gb vram</title>
    <updated>2025-07-19T23:46:34+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model is best for vision fitting 24gb vram? Trying to do nsfw categorization for user uploaded images. Gemma3 24b is quite good but is there any other, opinnions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4al6m/which_model_is_best_for_vision_fitting_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3tk92</id>
    <title>I love local models</title>
    <updated>2025-07-19T11:06:55+00:00</updated>
    <author>
      <name>/u/TweeMansLeger</name>
      <uri>https://old.reddit.com/user/TweeMansLeger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"&gt; &lt;img alt="I love local models" src="https://preview.redd.it/k7ebpl1nctdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22285a6e5642636636a349ad5e51158d2aa60f71" title="I love local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TweeMansLeger"&gt; /u/TweeMansLeger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k7ebpl1nctdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3tk92/i_love_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T11:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m48ffs</id>
    <title>OCR and GenAI: Key Trends from H1 2025</title>
    <updated>2025-07-19T22:06:28+00:00</updated>
    <author>
      <name>/u/Careless_Bed_5075</name>
      <uri>https://old.reddit.com/user/Careless_Bed_5075</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/"&gt; &lt;img alt="OCR and GenAI: Key Trends from H1 2025" src="https://b.thumbs.redditmedia.com/Lq7zM4DsEFII19bc0yxSQM-QEZerpL9N_AQPJZj_7fk.jpg" title="OCR and GenAI: Key Trends from H1 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve noticed plenty of questions and great insights in Reddit threads about the latest OCR and document-AI tools. After learning a lot from those discussions—and adding lessons from my own enterprise projects —I pulled together a brief mid-2025 summary: key VLM releases, specialist models, pipeline updates, new benchmarks and intresting findings.&lt;/p&gt; &lt;p&gt;If you work with OCR or RAG, the 5-minute read might help you catch up. I’d love to swap notes and hear what I’ve missed.&lt;/p&gt; &lt;p&gt;Link &lt;a href="https://www.linkedin.com/pulse/ocr-genai-key-trends-from-h1-2025-igor-galitskiy-lldie/?trackingId=BbmlpEfVIzeh2jXrWnUcdw%3D%3D"&gt;here&lt;/a&gt; (LinkedIn)&lt;/p&gt; &lt;p&gt;Thanks, looking forward to the discussion&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Bed_5075"&gt; /u/Careless_Bed_5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m48ffs/ocr_and_genai_key_trends_from_h1_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T22:06:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3iv6s</id>
    <title>any idea how to open source that?</title>
    <updated>2025-07-19T00:42:15+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt; &lt;img alt="any idea how to open source that?" src="https://preview.redd.it/x9e7q7z59qdf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd978d7cb888d92fdfc0a24134a57d1d3821cd08" title="any idea how to open source that?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9e7q7z59qdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T00:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m39uqi</id>
    <title>I made a 1000 hour NSFW TTS dataset</title>
    <updated>2025-07-18T18:20:34+00:00</updated>
    <author>
      <name>/u/hotroaches4liferz</name>
      <uri>https://old.reddit.com/user/hotroaches4liferz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can find and listen to the dataset on huggingface: &lt;a href="https://huggingface.co/datasets/setfunctionenvironment/testnew"&gt;https://huggingface.co/datasets/setfunctionenvironment/testnew&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The sample rate of all audio is 24,000 kHz&lt;/p&gt; &lt;p&gt;Stats:&lt;/p&gt; &lt;p&gt;Total audio files/samples: 556,667&lt;/p&gt; &lt;p&gt;Total duration: 1024.71 hours (3688949 seconds)&lt;/p&gt; &lt;p&gt;Average duration: 6.63 seconds&lt;/p&gt; &lt;p&gt;Shortest clip: 0.41 seconds&lt;/p&gt; &lt;p&gt;Longest clip: 44.97 seconds (all audio &amp;gt;45 seconds removed)&lt;/p&gt; &lt;p&gt;more and more TTS models are releasing and improving, the size of these models are decreasing some even being 0.5b 0.7b or 0.1b parameters but unfortunately they all dont have NSFW capability. It is a shame there are so many NSFW LLM finetunes out there but none exist for text to speech, so if anyone at all has the compute to finetune one of the existing TTS models (kokoro, zonos, F5, chatterbox, orpheus) on my dataset that would be very appreciated as I would like to try it 🙏🙏🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hotroaches4liferz"&gt; /u/hotroaches4liferz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T18:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3ssb2</id>
    <title>ARC AGI 3 is stupid</title>
    <updated>2025-07-19T10:17:53+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the first game, first level of 8, I completed the level after wasting a lot of time trying to figure out what functionality the spacebar and mouse clicks had. None, it turned out. On the second level, I got completely stuck, then read in another thread that you have to move on and off the first shape several times to loop through available shapes until hitting the target shape. I would never in a millioin years have figured this out because I would never consider anyone would make an intelligence test this stupid.&lt;/p&gt; &lt;p&gt;ARC AGI 1 and 2 were fine, well designed. But this 3 version is a test of stupid persistence, not intelligence.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T10:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xp21</id>
    <title>ChatSong, a lightweight, local LLM chat tool that's a single executable file</title>
    <updated>2025-07-19T14:33:28+00:00</updated>
    <author>
      <name>/u/Suitable-Patience916</name>
      <uri>https://old.reddit.com/user/Suitable-Patience916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"&gt; &lt;img alt="ChatSong, a lightweight, local LLM chat tool that's a single executable file" src="https://preview.redd.it/jcc7hsejdudf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b46aa1f23372af44a62d97c0c4858e30eac7768" title="ChatSong, a lightweight, local LLM chat tool that's a single executable file" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built a lightweight LLM API invocation tool that requires no installation, just a single executable file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Truly Portable: It's a single executable file, no installation required.&lt;/li&gt; &lt;li&gt;Bring Your Own Model: Customize models and prompts easily through a config file.&lt;/li&gt; &lt;li&gt;Save &amp;amp; Share: Export entire conversations as clean, single-file HTML pages.&lt;/li&gt; &lt;li&gt;Model Hopping: Switch between models in the same conversation.&lt;/li&gt; &lt;li&gt;Web-Aware: Can perform a web search or pull text from a URL to use as context for its answers.&lt;/li&gt; &lt;li&gt;File Upload: Drop in a PDF, TXT, or even a ZIP file to chat with your documents.&lt;/li&gt; &lt;li&gt;Code-Friendly: Proper Markdown rendering and syntax highlighting for code blocks.&lt;/li&gt; &lt;li&gt;Cost-Aware: Tracks token usage and lets you limit the conversation history sent with each request, which is a huge token saver.&lt;/li&gt; &lt;li&gt;Incognito Mode: For all your top-secret conversations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/jingangdidi/chatsong"&gt;https://github.com/jingangdidi/chatsong&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Patience916"&gt; /u/Suitable-Patience916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcc7hsejdudf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xp21/chatsong_a_lightweight_local_llm_chat_tool_thats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:33:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3wnnm</id>
    <title>What's New in Agent Leaderboard v2?</title>
    <updated>2025-07-19T13:47:25+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"&gt; &lt;img alt="What's New in Agent Leaderboard v2?" src="https://preview.redd.it/bwu8hq345udf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=994610c0a05c8b64924cfefbf8e0691a9c5619ef" title="What's New in Agent Leaderboard v2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Here is a quick TL;DR 👇&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;GPT-4.1&lt;/strong&gt; tops with 62% Action Completion (AC) overall.&lt;br /&gt; ⚡ &lt;strong&gt;Gemini 2.5&lt;/strong&gt; Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).&lt;br /&gt; 💸 &lt;strong&gt;GPT-4.1&lt;/strong&gt;-mini is &lt;em&gt;most cost-effective&lt;/em&gt; at $0.014/session vs. GPT-4.1’s $0.068.&lt;br /&gt; 🏭 No single model dominates across industries.&lt;br /&gt; 🤖 &lt;strong&gt;Grok 4&lt;/strong&gt; didn't lead in any metric.&lt;br /&gt; 🧩 Reasoning models &lt;em&gt;underperform&lt;/em&gt; compared to non-reasoning ones.&lt;br /&gt; 🆕 &lt;strong&gt;Kimi’s K2&lt;/strong&gt; leads &lt;em&gt;open-source models&lt;/em&gt; with 0.53 AC, 0.90 TSQ, and $0.039/session.&lt;/p&gt; &lt;p&gt;Link Below:&lt;/p&gt; &lt;p&gt;[Blog]: &lt;a href="https://galileo.ai/blog/agent-leaderboard-v2"&gt;https://galileo.ai/blog/agent-leaderboard-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Agent v2 Live Leaderboard]: &lt;a href="https://huggingface.co/spaces/galileo-ai/agent-leaderboard"&gt;https://huggingface.co/spaces/galileo-ai/agent-leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwu8hq345udf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3wnnm/whats_new_in_agent_leaderboard_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:47:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m48v53</id>
    <title>Looking for diarization model better than Pyannote</title>
    <updated>2025-07-19T22:26:49+00:00</updated>
    <author>
      <name>/u/bluedragon102</name>
      <uri>https://old.reddit.com/user/bluedragon102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently i’m using whisperX, which uses whisper + pyannote for transcription + diarization of audio but I find the speaker recognition quite lackluster. It’s often wrong at labeling the speakers. Any better alternatives to this?&lt;/p&gt; &lt;p&gt;I tried Eleven Labs but they only offer an API and dont make the models available and the API is quite expensive. Their quality is VERY good though.&lt;/p&gt; &lt;p&gt;In trying to find alternatives i’ve found Nvidia Nemo + titanet but it seems that is english only. I would prefer a model trained on multiple languages. Anyone have some recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bluedragon102"&gt; /u/bluedragon102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m48v53/looking_for_diarization_model_better_than_pyannote/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T22:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4b8ji</id>
    <title>NSFW AI Local</title>
    <updated>2025-07-20T00:18:03+00:00</updated>
    <author>
      <name>/u/TheGodOfCarrot</name>
      <uri>https://old.reddit.com/user/TheGodOfCarrot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an AI template or GUI(?) I can use locally for free that generates nsfw art of already existing characters. I mean images similar to those on the green site. I know little to nothing about AI but my computer is pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGodOfCarrot"&gt; /u/TheGodOfCarrot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4b8ji/nsfw_ai_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T00:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4cil7</id>
    <title>Getting into local ai. Photo restoration.</title>
    <updated>2025-07-20T01:23:05+00:00</updated>
    <author>
      <name>/u/lokito50</name>
      <uri>https://old.reddit.com/user/lokito50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm pretty new to this AI stuff but have a system I think can handle some localLLama. 3090Ti 12900K. So I'm looking for a model I can give it an old photo and ask it to restore it and possibly add coloration. Any guidance will be much appreciated. TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lokito50"&gt; /u/lokito50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4cil7/getting_into_local_ai_photo_restoration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T01:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3sgr1</id>
    <title>WordPecker: Open Source Personalized Duolingo</title>
    <updated>2025-07-19T09:57:11+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt; &lt;img alt="WordPecker: Open Source Personalized Duolingo" src="https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b297127eab67178a51943bc8535bfc9dfb9f671" title="WordPecker: Open Source Personalized Duolingo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/baturyilmaz/wordpecker-app"&gt;https://github.com/baturyilmaz/wordpecker-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5fximscazsdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T09:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3n89p</id>
    <title>(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models</title>
    <updated>2025-07-19T04:28:21+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt; &lt;img alt="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" src="https://preview.redd.it/edxmilbhdrdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed39c860785fb34d8104df720311441abac8087" title="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.&lt;/p&gt; &lt;p&gt;“Text data generated by the model is NOT considered as a derivative work.”&lt;/p&gt; &lt;p&gt;Hopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edxmilbhdrdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3yzes</id>
    <title>Localllama’s (first?) IFTA - I’ll Fine-Tune Anything</title>
    <updated>2025-07-19T15:28:11+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.&lt;/p&gt; &lt;p&gt;Premise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. &lt;/p&gt; &lt;p&gt;Rules are:&lt;/p&gt; &lt;p&gt;Has to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. &lt;/p&gt; &lt;p&gt;The output of the training will be shared publicly on HF for the benefit of the community. &lt;/p&gt; &lt;p&gt;What do you say, interested? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4djo6</id>
    <title>Made a local C++ utility to calculate RAM needed to fit a quantized model</title>
    <updated>2025-07-20T02:15:52+00:00</updated>
    <author>
      <name>/u/philetairus_socius</name>
      <uri>https://old.reddit.com/user/philetairus_socius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using &lt;a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator"&gt;NyxKrage's VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt; &lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt; &lt;p&gt;There are two modes:&lt;/p&gt; &lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt; &lt;p&gt;I'm planning to add functionality like calculating parameters, letting you use it without a `config.json`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/71cj34/llmcalculator"&gt;Link Here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philetairus_socius"&gt; /u/philetairus_socius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T02:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41bj1</id>
    <title>A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created</title>
    <updated>2025-07-19T17:05:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt; &lt;img alt="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" src="https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c89159d8d8c6aba292f37a10b0a43f8493d0366" title="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the extensie discussion &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;about UTCP&lt;/a&gt; last week, the authors of UTCP created an RFC for it.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.&lt;/p&gt; &lt;p&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.&lt;/p&gt; &lt;p&gt;UTCP does this by specifying a &amp;quot;manual&amp;quot;, where a tool provider publishes a standardized description of its &amp;quot;tools&amp;quot; together with the necessary information to call them (named in the following &amp;quot;transport&amp;quot;, previously known as &amp;quot;provider&amp;quot;).&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Discussion issue: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Current RFC: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46gtn</id>
    <title>Can we finally "index" a code project?</title>
    <updated>2025-07-19T20:40:18+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I understand how &amp;quot;tooling&amp;quot; works w/ newer LLMs now, I can take a large code project and &amp;quot;index&amp;quot; it in such a way that an LLM can &amp;quot;search&amp;quot; it like a database and answer questions regarding the source code?&lt;/p&gt; &lt;p&gt;This is my #1 need at the moment, being able to get quick answers about my code base that's quite large. I don't need a coder so much as I need a local LLM that can be API and Source-Code &amp;quot;aware&amp;quot; and can help me in the biggest bottlenecks that myself and most senior engineers face: &amp;quot;Now where the @#$% did that line of code that does that one thing??&amp;quot; or &amp;quot;Given the class names i've used so far, what's a name for this NEW class that stays consistent with the other names&amp;quot; and finally &amp;quot;What's the thousand-mile view of this class/script's purpose?&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks in advance! I'm fairly new so my terminology could certainly be outdated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xgjo</id>
    <title>Dual GPU set up was surprisingly easy</title>
    <updated>2025-07-19T14:23:11+00:00</updated>
    <author>
      <name>/u/m-gethen</name>
      <uri>https://old.reddit.com/user/m-gethen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt; &lt;img alt="Dual GPU set up was surprisingly easy" src="https://a.thumbs.redditmedia.com/FZ5L51GTZo6IrqOEds48bUmd3srrQbWvmNjPPEfS1l0.jpg" title="Dual GPU set up was surprisingly easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. &lt;/p&gt; &lt;p&gt;Both in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. &lt;/p&gt; &lt;p&gt;Current spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold&lt;/p&gt; &lt;p&gt;Will be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.&lt;/p&gt; &lt;p&gt;Any tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m-gethen"&gt; /u/m-gethen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m3xgjo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3vqom</id>
    <title>A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality</title>
    <updated>2025-07-19T13:03:43+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.11851"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ag6u</id>
    <title>Hackers are never sleeping</title>
    <updated>2025-07-19T23:40:20+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt; &lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt; &lt;p&gt;Don't let your guard down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:40:20+00:00</published>
  </entry>
</feed>
