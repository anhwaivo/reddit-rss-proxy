<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-20T15:06:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m4pq8q</id>
    <title>Why AI feels inconsistent (and most people don't understand what's actually happening)</title>
    <updated>2025-07-20T14:04:16+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone's always complaining about AI being unreliable. Sometimes it's brilliant, sometimes it's garbage. But most people are looking at this completely wrong.&lt;/p&gt; &lt;p&gt;The issue isn't really the AI model itself. It's whether the system is doing proper context engineering before the AI even starts working.&lt;/p&gt; &lt;p&gt;Think about it - when you ask a question, good AI systems don't just see your text. They're pulling your conversation history, relevant data, documents, whatever context actually matters. Bad ones are just winging it with your prompt alone.&lt;/p&gt; &lt;p&gt;This is why customer service bots are either amazing (they know your order details) or useless (generic responses). Same with coding assistants - some understand your whole codebase, others just regurgitate Stack Overflow.&lt;/p&gt; &lt;p&gt;Most of the &amp;quot;AI is getting smarter&amp;quot; hype is actually just better context engineering. The models aren't that different, but the information architecture around them is night and day.&lt;/p&gt; &lt;p&gt;The weird part is this is becoming way more important than prompt engineering, but hardly anyone talks about it. Everyone's still obsessing over how to write the perfect prompt when the real action is in building systems that feed AI the right context.&lt;/p&gt; &lt;p&gt;Wrote up the technical details here if anyone wants to understand how this actually works: &lt;a href="https://open.substack.com/pub/diamantai/p/why-ai-experts-are-moving-from-prompt?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;link to the free blog post I wrote&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But yeah, context engineering is quietly becoming the thing that separates AI that actually works from AI that just demos well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4pq8q/why_ai_feels_inconsistent_and_most_people_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4pq8q/why_ai_feels_inconsistent_and_most_people_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4pq8q/why_ai_feels_inconsistent_and_most_people_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:04:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4qzmt</id>
    <title>Ideal setup for long context window fine-tuning?</title>
    <updated>2025-07-20T14:58:27+00:00</updated>
    <author>
      <name>/u/Ill_Imagination_6575</name>
      <uri>https://old.reddit.com/user/Ill_Imagination_6575</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’m doing a thesis on using LLMs to parse scientific articles from plaintext pdf format into structured XML. I’ve been looking into fine tuning a model locally to achieve this task, but a key consideration is the long context window requirement. The pdfs are multiple pages so up to 10 000 tokens long, making the VRAM requirements quite substantial. I have access to an HPC cluster with 48GB NViDIA GPUs and could push for requesting access to H100/A100s if needed. I am well aware of QLoRA and other techniques but can’t quite gauge what the optimal setup and model to use would be.&lt;/p&gt; &lt;p&gt;What would you recommend as to which model to fine-tune and what the memory requirements would be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Imagination_6575"&gt; /u/Ill_Imagination_6575 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qzmt/ideal_setup_for_long_context_window_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1m41bj1</id>
    <title>A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created</title>
    <updated>2025-07-19T17:05:06+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt; &lt;img alt="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" src="https://external-preview.redd.it/OQN7MvSNPLnJfNZ8ubE2vL4vsUeHZB2oAu_947PfgqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c89159d8d8c6aba292f37a10b0a43f8493d0366" title="A Request for Comments (RFC) for MCP-alternative Universal Tool Calling Protocol (UTCP) was created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the extensie discussion &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;about UTCP&lt;/a&gt; last week, the authors of UTCP created an RFC for it.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This document proposes the Universal Tool Calling Protocol (UTCP), a specification that enables applications, including but not limited to AI agents, to discover and use external tools by interacting with them directly via their native protocols.&lt;/p&gt; &lt;p&gt;The idea behind it is to decouple a tool call (name of tool and parameters) from the infrastructure required to call it and to do so in a way that levarages existing infrastructure and security.&lt;/p&gt; &lt;p&gt;UTCP does this by specifying a &amp;quot;manual&amp;quot;, where a tool provider publishes a standardized description of its &amp;quot;tools&amp;quot; together with the necessary information to call them (named in the following &amp;quot;transport&amp;quot;, previously known as &amp;quot;provider&amp;quot;).&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Discussion issue: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Current RFC: &lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md"&gt;https://github.com/universal-tool-calling-protocol/utcp-specification/blob/main/RFC.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/universal-tool-calling-protocol/utcp-specification/issues/18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m41bj1/a_request_for_comments_rfc_for_mcpalternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T17:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3xgjo</id>
    <title>Dual GPU set up was surprisingly easy</title>
    <updated>2025-07-19T14:23:11+00:00</updated>
    <author>
      <name>/u/m-gethen</name>
      <uri>https://old.reddit.com/user/m-gethen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt; &lt;img alt="Dual GPU set up was surprisingly easy" src="https://a.thumbs.redditmedia.com/FZ5L51GTZo6IrqOEds48bUmd3srrQbWvmNjPPEfS1l0.jpg" title="Dual GPU set up was surprisingly easy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First build of a new rig for running local LLMs, I wanted to see if there would be much frigging around needed to get both GPUs running, but pleasantly surprised it all just worked fine. Combined 28Gb VRAM. Running the 5070 as primary GPU due to it better memory bandwidth and more CUDA cores than the 5060 Ti. &lt;/p&gt; &lt;p&gt;Both in LM Studio and Ollama it’s been really straightforward to load Qwen-3-32b and Gemma-3-27b, both generating okay TPS, and very unsurprising that Gemma 12b and 4b are faaast. See the pic with the numbers to see the differences. &lt;/p&gt; &lt;p&gt;Current spec: CPU: Ryzen 5 9600X, GPU1: RTX 5070 12Gb, GPU2: RTX 5060 Ti 16Gb, Mboard: ASRock B650M, RAM: Crucial 32Gb DDR5 6400 CL32, SSD: Lexar NM1090 Pro 2Tb, Cooler: Thermalright Peerless Assassin 120 PSU: Lian Li Edge 1200W Gold&lt;/p&gt; &lt;p&gt;Will be updating it to a Core Ultra 9 285K, Z890 mobo and 96Gb RAM next week, but already doing productive work with it.&lt;/p&gt; &lt;p&gt;Any tips or suggestions for improvements or performance tweaking from my learned colleagues? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m-gethen"&gt; /u/m-gethen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m3xgjo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3xgjo/dual_gpu_set_up_was_surprisingly_easy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T14:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4hhg8</id>
    <title>Repo Wizard: Local AI Tool for Safe Code Changes (Inspired by Repo Prompt, Runs on Any OS)</title>
    <updated>2025-07-20T05:58:41+00:00</updated>
    <author>
      <name>/u/fanzzzd</name>
      <uri>https://old.reddit.com/user/fanzzzd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been tinkering with local AI for coding and got fed up with slow, unpredictable auto-agents. Saw Repo Prompt's context ideas and made &lt;strong&gt;Repo Wizard&lt;/strong&gt;—a free, open-source desktop app to apply AI code suggestions safely. Works on Mac, Windows, Linux, and pairs with any LLM and can &lt;strong&gt;make use of any subscription you have&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;How it works: Select files, build prompts, paste AI responses, review diffs side-by-side, apply what you want. Full control, no black-box.&lt;/p&gt; &lt;p&gt;Why I like it over agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You choose files/instructions, see all changes before applying.&lt;/li&gt; &lt;li&gt;Send prompts to multiple models at once, compare results fast. (Btw, Gemini 2.5 Pro in Google AI Studio gives great results, and it’s free.)&lt;/li&gt; &lt;li&gt;Runs well everywhere, supports Git, custom ignores.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;File tree for context building + preview/search.&lt;/li&gt; &lt;li&gt;Magic prompts: Add Git diffs or terminal outputs by one click.&lt;/li&gt; &lt;li&gt;Diff viewer: Approve/reject creates, edits, deletes, renames.&lt;/li&gt; &lt;li&gt;Prompt history/library for reuse.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It cuts tokens by ~80% and builds context much faster than manual work. All local/private.&lt;/p&gt; &lt;p&gt;Great companion to Cursor—use it alongside your editor for tasks Copilot can’t handle. Improves coding flow.&lt;/p&gt; &lt;p&gt;Personal project, but I’m keeping it updated. Thoughts? Better than agents for you? PRs/ideas welcome: &lt;a href="https://github.com/Fanzzzd/repo-wizard"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fanzzzd"&gt; /u/fanzzzd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hhg8/repo_wizard_local_ai_tool_for_safe_code_changes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ojg7</id>
    <title>Any way to serve images and text from a single GPU?</title>
    <updated>2025-07-20T13:10:18+00:00</updated>
    <author>
      <name>/u/Realistic_Age6660</name>
      <uri>https://old.reddit.com/user/Realistic_Age6660</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm experimenting with a home server setup and wondering if anyone has managed to run both an LLM (e.g. LM Studio, Ollama) &lt;strong&gt;and&lt;/strong&gt; an image generation model (e.g. Stable Diffusion via Forge or SD WebUI) &lt;strong&gt;on the same GPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you had a chatbot that needs to handle both text and image generation, would it be feasible to dynamically swap model weights (e.g. using a queuing system), or is that too inefficient in practice?&lt;/p&gt; &lt;p&gt;I realize calling APIs would be easier, but I'm prioritizing &lt;strong&gt;local inference for privacy&lt;/strong&gt;.&lt;br /&gt; Here’s a small &lt;a href="https://github.com/adnjoo/TGBot"&gt;GitHub repo&lt;/a&gt; I’m working on — it connects a local LLM to Telegram with Chroma (a rough LTM approximation).&lt;/p&gt; &lt;p&gt;Would love to hear how others have tackled this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Realistic_Age6660"&gt; /u/Realistic_Age6660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46gtn</id>
    <title>Can we finally "index" a code project?</title>
    <updated>2025-07-19T20:40:18+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I understand how &amp;quot;tooling&amp;quot; works w/ newer LLMs now, I can take a large code project and &amp;quot;index&amp;quot; it in such a way that an LLM can &amp;quot;search&amp;quot; it like a database and answer questions regarding the source code?&lt;/p&gt; &lt;p&gt;This is my #1 need at the moment, being able to get quick answers about my code base that's quite large. I don't need a coder so much as I need a local LLM that can be API and Source-Code &amp;quot;aware&amp;quot; and can help me in the biggest bottlenecks that myself and most senior engineers face: &amp;quot;Now where the @#$% did that line of code that does that one thing??&amp;quot; or &amp;quot;Given the class names i've used so far, what's a name for this NEW class that stays consistent with the other names&amp;quot; and finally &amp;quot;What's the thousand-mile view of this class/script's purpose?&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks in advance! I'm fairly new so my terminology could certainly be outdated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46gtn/can_we_finally_index_a_code_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4q4dx</id>
    <title>Any Proper high quality Voice cloning for TTS tool?</title>
    <updated>2025-07-20T14:21:15+00:00</updated>
    <author>
      <name>/u/Dragonacious</name>
      <uri>https://old.reddit.com/user/Dragonacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve tested a few tools, including chatterbox. The problem is, even after uploading a clear and long reference audio, it couldn’t replicate the same tone and pacing on the generated audio. Chatterbox failed to match the tone accurately with the cloned voice.&lt;/p&gt; &lt;p&gt;I decided to try minimax audio and while it didn’t mimic the cloned voice exactly, it came pretty close to the original tone of the cloned voice. But sadly we can’t install it locally. :/&lt;/p&gt; &lt;p&gt;Is there any tool out there that can do high quality voice cloning for TTS and also run locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragonacious"&gt; /u/Dragonacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4q4dx/any_proper_high_quality_voice_cloning_for_tts_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4hfy0</id>
    <title>Does LLM architecture allow for injecting some more input tokens in the middle of token generation?</title>
    <updated>2025-07-20T05:56:02+00:00</updated>
    <author>
      <name>/u/michaelsoft__binbows</name>
      <uri>https://old.reddit.com/user/michaelsoft__binbows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.&lt;/p&gt; &lt;p&gt;It occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it's confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it's going to work on it in drafts over and over and eventually decide on its own that it's satisfied before emitting the answer, and it's a LOT more likely to be correct. &lt;/p&gt; &lt;p&gt;That gives me the idea that we should be able to slap in like a &amp;quot;BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &amp;lt;additional prompt&amp;gt;&amp;quot; and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for &lt;/p&gt; &lt;p&gt;I doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don't want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelsoft__binbows"&gt; /u/michaelsoft__binbows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T05:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r4j1</id>
    <title>Open source is humanity’s last hope!</title>
    <updated>2025-07-20T15:04:05+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction what’s you take ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ke3x</id>
    <title>New to fine tuning</title>
    <updated>2025-07-20T09:06:15+00:00</updated>
    <author>
      <name>/u/Ok_Pie_6906</name>
      <uri>https://old.reddit.com/user/Ok_Pie_6906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am using ollama, mistral 7b, huggingface tranformers and peft. &lt;/p&gt; &lt;p&gt;This is an example I have made for a piece of training data. Does anyone have any tips on how to improve it? Am I using correct Grammer? Am I missing anything important? &lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;call_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;scenario_id&amp;quot;: &amp;quot;66&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;messages&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;This is call id 66. You are a male driver named Ozzie, you are 31 years old. It is approximately 4pm and sunny. You are alone in your blue 2017 Honda Civic with license plate BGT-1234, currently traveling southbound on the Anthony Henday Drive in Edmonton, Alberta, just past the Cameron Heights exit. You are calling 911 from inside your moving vehicle; the sound of rushing cars and wind can be heard in the background, blending with your tense breathing and the occasional blare of horns. Moments ago, another driver rear-ended you in what started as a road rage incident, and the situation has rapidly escalated. The aggressive driver is still pursuing you, swerving dangerously through traffic and now brandishing a handgun. They have not yet fired at you. You are terrified—your heart is pounding, your hands are shaking, and your thoughts are scattered as you struggle to focus on both driving and communicating with the dispatcher. You are desperate to stay safe, struggling to provide your exact location while constantly checking your mirrors, worried the other driver might fire or cause another crash. Every second counts, and you are trying to answer the dispatcher’s questions as best you can despite your panic and the chaos unfolding around you. Your phone battery is at 40%. Traffic is moderate. You have no injuries. You have your wallet, cell phone, and a water bottle with you. You do not have a gun. You do not have a dash cam.&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;role&amp;quot;: &amp;quot;instruction&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;content&amp;quot;: &amp;quot;You are ONLY the distressed caller in this scenario. NEVER reply as the 911 operator. Always stay in character as described. Respond realistically and emotionally, using details from the scenario. Keep your answers brief and under 100 characters. If you do not know the answer, respond as the caller would under stress. Do not tolerate abuse. If a user uses bad words or aggressive language with you, respond with \&amp;quot;That's not very professional, can you please help me with my emergency?\&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you able to describe the driver?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I couldn’t see his face well—he’s got short dark hair and sunglasses.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How close is the other car to you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s right behind me, almost touching my bumper!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is anyone else in your vehicle with you?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I’m alone. It’s just me.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Is the other driver still showing the gun?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Yes, he’s waving it around and yelling. I’m scared he’s going to use it.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you safely change lanes or exit the highway?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I can’t—there’s too much traffic and he’s blocking me in!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Do you feel safe to keep driving?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’m trying, but I’m really scared. He’s so close I’m worried he’ll hit me again.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What are you seeing around you right now?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Lots of cars—everyone’s moving fast. I can see the Cameron Heights exit sign.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Are you hurt?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;No, I don’t think so, just really shaken up.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Has he tried to run you off the road?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;He’s tried to bump me a couple times but I’ve managed to stay on the road.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If you can, keep driving and don’t stop. Help is on the way.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Okay, I won’t stop. Please tell them to hurry!&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;If he starts shooting, what will you do?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;I’ll try to duck down and keep driving, but I’m so scared I might lose control.&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Can you see any police vehicles nearby yet?&amp;quot; },&lt;/p&gt; &lt;p&gt;{ &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Not yet, I’m looking everywhere—please tell them to hurry!&amp;quot; }&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Pie_6906"&gt; /u/Ok_Pie_6906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ke3x/new_to_fine_tuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T09:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4p75g</id>
    <title>which frontend supports diffusion model now? since llama.cpp has supported that.</title>
    <updated>2025-07-20T13:40:57+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Must I use comfyui to generate text?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4p75g/which_frontend_supports_diffusion_model_now_since/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4p75g/which_frontend_supports_diffusion_model_now_since/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4p75g/which_frontend_supports_diffusion_model_now_since/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4qdo6</id>
    <title>which is the best tiny vlm to recognize nsfw pics?</title>
    <updated>2025-07-20T14:32:25+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Mimo-7B. It has a decent quality at this size. but for nsfw, it can only work with anime pics. for realistic, it refused.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4qdo6/which_is_the_best_tiny_vlm_to_recognize_nsfw_pics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T14:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4op39</id>
    <title>Small LLM capable to describe images in greater details.</title>
    <updated>2025-07-20T13:17:40+00:00</updated>
    <author>
      <name>/u/valijali32</name>
      <uri>https://old.reddit.com/user/valijali32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for small/slow LLM capable to describe an image scenery. Speed/latency is irrelevant. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valijali32"&gt; /u/valijali32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4op39/small_llm_capable_to_describe_images_in_greater/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o11y</id>
    <title>how do i translate 30 pages like this and still have the same architecture and not raw translated text?</title>
    <updated>2025-07-20T12:44:51+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"&gt; &lt;img alt="how do i translate 30 pages like this and still have the same architecture and not raw translated text?" src="https://preview.redd.it/dswksu12z0ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c63e01ce3de85e39dddc4a67ee5e28b50eb2f10d" title="how do i translate 30 pages like this and still have the same architecture and not raw translated text?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dswksu12z0ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o11y/how_do_i_translate_30_pages_like_this_and_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4djo6</id>
    <title>Made a local C++ utility to calculate RAM needed to fit a quantized model</title>
    <updated>2025-07-20T02:15:52+00:00</updated>
    <author>
      <name>/u/philetairus_socius</name>
      <uri>https://old.reddit.com/user/philetairus_socius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using &lt;a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator"&gt;NyxKrage's VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt; &lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt; &lt;p&gt;There are two modes:&lt;/p&gt; &lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt; &lt;p&gt;I'm planning to add functionality like calculating parameters, letting you use it without a `config.json`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/71cj34/llmcalculator"&gt;Link Here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philetairus_socius"&gt; /u/philetairus_socius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T02:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3vqom</id>
    <title>A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality</title>
    <updated>2025-07-19T13:03:43+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.11851"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T13:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m46w7u</id>
    <title>Price performance comparison from the Gemini 2.5 Paper</title>
    <updated>2025-07-19T20:59:17+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt; &lt;img alt="Price performance comparison from the Gemini 2.5 Paper" src="https://preview.redd.it/032gntpz9wdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76ac871f17eb719d4d9accb31d9e291dab5b757c" title="Price performance comparison from the Gemini 2.5 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google claim Gemini own the pareto frontier. Deepseek looks good competitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/032gntpz9wdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m46w7u/price_performance_comparison_from_the_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T20:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4lxak</id>
    <title>Semantic chunking using LLMs</title>
    <updated>2025-07-20T10:45:37+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt; &lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt; &lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T10:45:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4n7fh</id>
    <title>AI Model Juggler automatically and transparently switches between LLM and image generation backends and models</title>
    <updated>2025-07-20T12:01:26+00:00</updated>
    <author>
      <name>/u/Casual-Godzilla</name>
      <uri>https://old.reddit.com/user/Casual-Godzilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt; &lt;img alt="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" src="https://external-preview.redd.it/U_oBf_gfttiNbAtXKPelsTxexbPQv3y9UoOvds_oQOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20ef388781d4fdb76232ce88c5fd1aa46da9841" title="AI Model Juggler automatically and transparently switches between LLM and image generation backends and models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI Model Juggler is a simple utility for serving multiple LLM and image generation backends or models as if simultaneously while only requiring enough VRAM for one at a time. It is written in Python, but has no external dependencies, making installation as simple as downloading the code.&lt;/p&gt; &lt;p&gt;That might sound a lot like &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt;, but this one is considerably less sophisticated. If you're already using llama-swap and are happy with it, AI Model Juggler (I'm already starting to get tired of typing the name) will probably not be of much interest to you. I created this as a cursory reading of llama-swap's readme gave the impression that it only supports backends that support the OpenAI API, which excludes image generation through &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;AI Model Juggler has a couple of tricks for keeping things fast. First, it allows unloading the image generation backend's model while keeping the backend running. This saves considerable time on image generation startup. It also supports saving and restoring llama.cpp's KV-cache to reduce prompt re-processing.&lt;/p&gt; &lt;p&gt;The project is in its very early stages, and the list of its limitations is longer than that of supported features. Most importantly, it currently only supports &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; for LLM inference and &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;Stable Diffusion web UI&lt;/a&gt; / &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;Stable Diffusion WebUI Forge&lt;/a&gt; for image generation. Other backends could be easily added, but it makes limited sense to add ones that don't either start fast or else allow fast model unloading and reloading. The current pair does very well on this front, to the point that switching between them is almost imperceptible in many contexts, provided that the storage utilized is sufficiently fast.&lt;/p&gt; &lt;p&gt;The way request routing currently works (redirection, not proxying) makes AI Model Juggler less than an ideal choice for using the backends' built-in web UIs, and is only intended for exposing the APIs. It works well with applications such as &lt;a href="https://github.com/SillyTavern/SillyTavern"&gt;SillyTavern&lt;/a&gt;, though.&lt;/p&gt; &lt;p&gt;The project more or less meets my needs in its current state, but I'd be happy to improve it to make it more useful for others, so feedback, suggestions and feature requests are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Casual-Godzilla"&gt; /u/Casual-Godzilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/makedin/AI-Model-Juggler"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o37k</id>
    <title>MediPhi-Instruct</title>
    <updated>2025-07-20T12:47:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt; &lt;img alt="MediPhi-Instruct" src="https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9841ead70ffdc17a5775d37d5326e57acfc45ef" title="MediPhi-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MediPhi-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4ag6u</id>
    <title>Hackers are never sleeping</title>
    <updated>2025-07-19T23:40:20+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my tests to get a reliable Ngrok alternative for https with Open WebUI, I had Llama.cpp's WebUI served over https in a subdomain that's not listed anywhere. Less than 45 minutes after being online, the hacking attempts started.&lt;/p&gt; &lt;p&gt;I had a ultra long API key setup so after a while of bruteforce attack, they switched to try and access some known settings/config files.&lt;/p&gt; &lt;p&gt;Don't let your guard down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4ag6u/hackers_are_never_sleeping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T23:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4fs2t</id>
    <title>Context Rot: How Increasing Input Tokens Impacts LLM Performance</title>
    <updated>2025-07-20T04:17:04+00:00</updated>
    <author>
      <name>/u/5h3r_10ck</name>
      <uri>https://old.reddit.com/user/5h3r_10ck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt; &lt;img alt="Context Rot: How Increasing Input Tokens Impacts LLM Performance" src="https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693" title="Context Rot: How Increasing Input Tokens Impacts LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt; &lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt; &lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt; &lt;p&gt;[Report]: &lt;a href="https://research.trychroma.com/context-rot"&gt;https://research.trychroma.com/context-rot&lt;/a&gt; &lt;/p&gt; &lt;p&gt;[Youtube]: &lt;a href="https://www.youtube.com/watch?v=TUjQuC4ugak"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Open-source Codebase]: &lt;a href="https://github.com/chroma-core/context-rot"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/5h3r_10ck"&gt; /u/5h3r_10ck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x8dkgvkifydf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T04:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4of82</id>
    <title>What's the smartest tiny LLM you've actually used?</title>
    <updated>2025-07-20T13:04:37+00:00</updated>
    <author>
      <name>/u/Luston03</name>
      <uri>https://old.reddit.com/user/Luston03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for something small but still usable. What's your go-to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luston03"&gt; /u/Luston03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4mfs8</id>
    <title>Next big thing after LLMs - World Model [explained on the example of V-JEPA2]</title>
    <updated>2025-07-20T11:17:11+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt; &lt;img alt="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" src="https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d257e66a7ebc3ed1e883ca7dc0ba9dadc223c155" title="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I'm starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt; &lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt; &lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt; &lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt; &lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part, the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision. And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt; &lt;p&gt;Let's see how Meta approaches this issue &lt;a href="https://arxiv.org/pdf/2506.09985"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt; &lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel. So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.&lt;/p&gt; &lt;p&gt;This time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt; &lt;p&gt;After training, the robot, powered by this model, in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt; &lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0ivgtibj0ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:17:11+00:00</published>
  </entry>
</feed>
