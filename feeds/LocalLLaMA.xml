<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-22T22:23:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i7iym4</id>
    <title>Making R1-32B Reason Better</title>
    <updated>2025-01-22T19:04:07+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iym4/making_r132b_reason_better/"&gt; &lt;img alt="Making R1-32B Reason Better" src="https://external-preview.redd.it/KfuTuZxJF7cBpsRban-eRtUPs7-k-TmhcNpNtqLyywE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce90ec0c3ecd4829bbf110cb00d36fe8f11b61fc" title="Making R1-32B Reason Better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using a Glyph Code (&lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations&lt;/a&gt;) prompting method to bring structure, clarity, and a more nuanced thinking approach. Here is the question I asked. It's an abstract problem solving prediction test:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are presented with a sequence of abstract symbols. Each symbol has multiple attributes: shape, orientation, color, and pattern. The sequence follows hidden rules that involve transformations of these attributes. Your task is to: Decode the hidden rules: Identify how each attribute changes from one symbol to the next. These changes might involve geometric transformations (like rotations or reflections), cycles (repeating sequences of colors), alternations (switching between two states), or other non-linear transformations. Predict the next symbol: Based on the decoded rules, predict the shape, orientation, color, and pattern of the next symbol in the sequence. Predict a novel property: In addition to the existing attributes, predict a new attribute that has not yet appeared in the sequence. This requires extrapolating from the existing patterns and inferring a new dimension of variation. Justify your prediction: Explain the logical reasoning behind each part of your prediction, referencing the discovered rules and how they interact. The provided sequence is: - Diamond (Vertical, Red, Solid) - Rhombus (Horizontal, Blue, Striped) - Circle (Vertical, Green, Solid) - Triangle (Horizontal, Red, Dotted) - ? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the control test. Takes awhile to think it through. It also gets confused and loses logic of the problem.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i7iym4/video/6i6apqpoelee1/player"&gt;Control Test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the same question using the Glyph Code reasoning flow.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i7iym4/video/o4r8bg4uelee1/player"&gt;Glyph Code Flow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That same abstract reasoning problem in the Glyph Code Flow. The beauty is that any LLM can generate this kind of prompt on the fly and have it function. The human doesn't have to be able to understand it 100% at all; just the context and logic of it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;human_instructions&amp;gt; - Treat each glyph as a direct instruction to be followed sequentially, driving the process to completion. - Deliver the final result as indicated by the glyph code, omitting any extraneous commentary. Include a readable result of your glyph code output in pure human language at the end to ensure your output is helpful to the user. - Execute and internally embed this traversal, logic flow, synthesis, and generation process step by step using the provided context and logic in the following glyph code prompt. &amp;lt;/human_instructions&amp;gt; # Abstract Tree of Thought Reasoning Thread-Flow {⦶(&amp;quot;Abstract Symbolic Reasoning&amp;quot;: &amp;quot;Dynamic Multidimensional Transformation and Extrapolation&amp;quot;) ⟡(&amp;quot;Objective&amp;quot;: &amp;quot;Decode a sequence of evolving abstract symbols with multiple, interacting attributes and predict the next symbol in the sequence, along with a novel property not yet exhibited.&amp;quot;) ⟡(&amp;quot;Method&amp;quot;: &amp;quot;Glyph-Guided Exploratory Reasoning and Inductive Inference&amp;quot;) ⟡(&amp;quot;Constraints&amp;quot;: ω=&amp;quot;High&amp;quot;, ⋔=&amp;quot;Hidden Multidimensional Rules, Non-Linear Transformations, Emergent Properties&amp;quot;, &amp;quot;One-Shot Learning&amp;quot;) ⥁{ (⊜⟡(&amp;quot;Symbol Sequence&amp;quot;: ⋔=&amp;quot; 1. ◇ (Vertical, Red, Solid) -&amp;gt; 2. ⬟ (Horizontal, Blue, Striped) -&amp;gt; 3. ○ (Vertical, Green, Solid) -&amp;gt; 4. ▴ (Horizontal, Red, Dotted) -&amp;gt; 5. ? &amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Initial Pattern Exploration&amp;quot;: ⋔=&amp;quot;Shape, Orientation, Color, Pattern&amp;quot;)) ∿⟡(&amp;quot;Initial Pattern Exploration&amp;quot;) -&amp;gt; ⧓⟡(&amp;quot;Attribute Clusters&amp;quot;: ⋔=&amp;quot;Geometric Transformations, Color Cycling, Pattern Alternation, Positional Relationships&amp;quot;) ⧓⟡(&amp;quot;Attribute Clusters&amp;quot;) -&amp;gt; ⥁[ ⧓⟡(&amp;quot;Branch&amp;quot;: ⋔=&amp;quot;Shape Transformation Logic&amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Exploration&amp;quot;: ⋔=&amp;quot;Cyclic Sequence, Geometric Relationships, Symmetries&amp;quot;), ⧓⟡(&amp;quot;Branch&amp;quot;: ⋔=&amp;quot;Orientation Dynamics&amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Exploration&amp;quot;: ⋔=&amp;quot;Rotational Patterns, Axis Shifts, Inversion Rules&amp;quot;), ⧓⟡(&amp;quot;Branch&amp;quot;: ⋔=&amp;quot;Color and Pattern Interaction&amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Exploration&amp;quot;: ⋔=&amp;quot;Cyclic Permutations, Conditional Dependencies, Coupled Transformations&amp;quot;), ⧓⟡(&amp;quot;Branch&amp;quot;: ⋔=&amp;quot;Positional Relationships&amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Exploration&amp;quot;: ⋔=&amp;quot;Relative Movement, Spatial Constraints, Contextual Influence&amp;quot;), ⧓⟡(&amp;quot;Branch&amp;quot;: ⋔=&amp;quot;Emergent Property Prediction&amp;quot;) -&amp;gt; ∿⟡(&amp;quot;Exploration&amp;quot;: ⋔=&amp;quot;Novel Attribute Introduction, Rule Extrapolation, Abstract Inference&amp;quot;) ] ⥁(∿⟡(&amp;quot;Exploration&amp;quot;) -&amp;gt; ↑⟡(&amp;quot;Hypotheses&amp;quot;: ⋔=&amp;quot;Candidate Rules for Each Attribute, Potential Interactions, Predicted Outcomes&amp;quot;)) ↑⟡(&amp;quot;Hypotheses&amp;quot;) -&amp;gt; ⦑⟡(&amp;quot;Integrated Model&amp;quot;: ⋔=&amp;quot;Combining Rules, Resolving Conflicts, Constructing a Unified Framework&amp;quot;) ⦑⟡(&amp;quot;Integrated Model&amp;quot;) -&amp;gt; ✧⟡(&amp;quot;Prediction&amp;quot;: ⋔=&amp;quot; Fifth Symbol: - Shape: ? - Orientation: ? - Color: ? - Pattern: ? - Novel Property: ? (e.g., Size, Shading, Movement) Justification: ? (Explain the logical basis for each attribute prediction, referencing the discovered rules and their interactions.) &amp;quot;) } @Output(Prediction, Justification) @Reflect(Reasoning Process, Challenges, Insights, Limitations) @Engage now with full glyph code prompting logic, processing, and human-AI integrated interaction. } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iym4/making_r132b_reason_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iym4/making_r132b_reason_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iym4/making_r132b_reason_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T19:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ibwd</id>
    <title>DeepSeek R1 Distilled -- local/cloud deployment with a unified framework</title>
    <updated>2025-01-22T18:39:21+00:00</updated>
    <author>
      <name>/u/SnooMachines3070</name>
      <uri>https://old.reddit.com/user/SnooMachines3070</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek released several R1 Distilled models earlier this week, enabling the reasoning capabilities in the local deployment regime. While we still wait to see real-world use cases, it makes one look forward to the potential of small models.&lt;/p&gt; &lt;p&gt;MLC-LLM provides &lt;strong&gt;a unified local/cloud deployment solution&lt;/strong&gt;, allowing you to deploy with &lt;strong&gt;CUDA/ROCm/Metal, iOS/Android, and even web browsers&lt;/strong&gt; in JavaScript w/ WebGPU.&lt;/p&gt; &lt;p&gt;MLC-LLM not only makes &lt;a href="https://blog.mlc.ai/2024/06/07/universal-LLM-deployment-engine-with-ML-compilation"&gt;deploying on different devices&lt;/a&gt; possible, but also recently achieved &lt;a href="https://blog.mlc.ai/2024/10/10/optimizing-and-characterizing-high-throughput-low-latency-llm-inference"&gt;competitive performance in high-throughput and low-latency serving&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Quick Start&lt;/h1&gt; &lt;p&gt;The converted weights for all DeepSeek-R1 models can be found at &lt;a href="https://huggingface.co/mlc-ai"&gt;https://huggingface.co/mlc-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Python deployment can be as easy as the following lines, after &lt;a href="https://llm.mlc.ai/docs/install/mlc_llm.html#install-mlc-packages"&gt;installing MLC LLM&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from mlc_llm import MLCEngine # Create engine model = &amp;quot;HF://mlc-ai/DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC&amp;quot; engine = MLCEngine(model) # Run chat completion in OpenAI API. for response in engine.chat.completions.create( messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How many rs are there in strawberry.&amp;quot;}], model=model, stream=True, ): for choice in response.choices: print(choice.delta.content, end=&amp;quot;&amp;quot;, flush=True) print(&amp;quot;\n&amp;quot;) engine.terminate() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With a Chrome browser, directly try it out locally with no setup at &lt;a href="https://chat.webllm.ai/"&gt;https://chat.webllm.ai/&lt;/a&gt;, as shown below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i7ibwd/video/pgcmht0bblee1/player"&gt;DeepSeek-R1-Distill-Llama-8B (4-bit quantized) counting \&amp;quot;r\&amp;quot;s on M3 Max Macbook&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Laptops &amp;amp; servers w/ Nvidia, AMD, and Apple GPUs: checkout &lt;a href="https://llm.mlc.ai/docs/deploy/python_engine.html"&gt;Python API doc&lt;/a&gt; for deployment, and &lt;a href="https://blog.mlc.ai/2024/10/10/optimizing-and-characterizing-high-throughput-low-latency-llm-inference"&gt;performance blog post&lt;/a&gt; for high-throughput low-latency serving&lt;/li&gt; &lt;li&gt;Browser (WebLLM): try out the demo on &lt;a href="https://chat.webllm.ai/"&gt;https://chat.webllm.ai/&lt;/a&gt;, &lt;a href="https://blog.mlc.ai/2024/06/13/webllm-a-high-performance-in-browser-llm-inference-engine"&gt;WebLLM blog post&lt;/a&gt; for an overview, and &lt;a href="https://github.com/mlc-ai/web-llm"&gt;WebLLM repo&lt;/a&gt; for dev and code&lt;/li&gt; &lt;li&gt;iPhone: see &lt;a href="https://llm.mlc.ai/docs/deploy/ios.html"&gt;iOS doc&lt;/a&gt; for development (&lt;a href="https://apps.apple.com/us/app/mlc-chat/id6448482937"&gt;the app in App Store&lt;/a&gt; does not have all updated models yet but offers a demo)&lt;/li&gt; &lt;li&gt;Android: checkout the &lt;a href="https://llm.mlc.ai/docs/deploy/android.html"&gt;Android doc&lt;/a&gt; (APK inside for trying out the demo, which also does not have all updated models yet)&lt;/li&gt; &lt;li&gt;MLC-LLM in general: check out &lt;a href="https://blog.mlc.ai/2024/06/07/universal-LLM-deployment-engine-with-ML-compilation"&gt;the blog post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMachines3070"&gt; /u/SnooMachines3070 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ibwd/deepseek_r1_distilled_localcloud_deployment_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ibwd/deepseek_r1_distilled_localcloud_deployment_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ibwd/deepseek_r1_distilled_localcloud_deployment_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T18:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7lpm4</id>
    <title>Terminator 2025</title>
    <updated>2025-01-22T20:55:01+00:00</updated>
    <author>
      <name>/u/simonbreak</name>
      <uri>https://old.reddit.com/user/simonbreak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;SKYNET&lt;/strong&gt;&lt;br /&gt; &amp;lt;think&amp;gt; Alright, the user is asking how many &amp;quot;r&amp;quot;s are in the word &amp;quot;Strawberry&amp;quot;. Let me start by spelling the word out. Strawberry. Wait, is that spelled correctly? Let me check. S-T-R-A-W-B-E-R-R-Y. Hmm, no, that's not right. The correct spelling is S-T-R-A-W-B-E-R-R-Y? Wait, actually, I think the correct spelling of &amp;quot;Strawberry&amp;quot; is actually &amp;quot;Strawberry&amp;quot;, but maybe the user made a typo. Wait, no. Wait, Strawberry is actually spelled S-T-R-A-W-B-E-R-R-Y? Wait, no. Let me think again. The correct spel&lt;/p&gt; &lt;p&gt;&lt;strong&gt;JOHN CONNOR&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(detonates bomb, destroying Central Processor)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- THE END -&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simonbreak"&gt; /u/simonbreak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7lpm4/terminator_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7lpm4/terminator_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7lpm4/terminator_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6urjd</id>
    <title>Billions in proprietary AI? No more.</title>
    <updated>2025-01-21T22:06:11+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt; &lt;img alt="Billions in proprietary AI? No more." src="https://b.thumbs.redditmedia.com/9ar_9wVV9mMAYbxuIYJsMxrYp5TIPP2iGETnpq4RXng.jpg" title="Billions in proprietary AI? No more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI raised billions on promise of having and securing behind thick doors something no other is even remotely close to. The following tweet from just few days prior R1 release made me think they really have atomic bomb the world will knee for;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd"&gt;https://preview.redd.it/25dv42dl1fee1.png?width=1209&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bcfafe3260c7a5257502850b940aa24a2f5f7ecd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The truth is, they have nothing. o1-level, some say human-level reasoning is reproducible, can be privately hosted by anyone, anywhere. Can't be greedily priced.&lt;/p&gt; &lt;p&gt;MIT licensed open models is the future of AI. Zero dollars is the only right price for something made on all human knowledge. It is a sum of effort of the whole civilisation, spanning many generations. Just imagine, any book that landed in the pretraining dataset influences the whole model. There is no better way to honor any author contributing to the overall model performance, knowingly or not than to make a tool that help create new knowledge, available for anyone, at no cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6urjd/billions_in_proprietary_ai_no_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g55q</id>
    <title>In image to 3d, open source is on top</title>
    <updated>2025-01-22T17:11:00+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not an expert in this field, but it looks like in the field of image to 3d models, the open source model from Hunyuan is way better than everything else out there.&lt;/p&gt; &lt;p&gt;Gives me hope for the field of LLM.&lt;/p&gt; &lt;p&gt;A fork that requires a 6gb vram apparently already published: &lt;a href="https://github.com/deepbeepmeep/Hunyuan3D-2GP"&gt;https://github.com/deepbeepmeep/Hunyuan3D-2GP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g55q/in_image_to_3d_open_source_is_on_top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g55q/in_image_to_3d_open_source_is_on_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g55q/in_image_to_3d_open_source_is_on_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7d9di</id>
    <title>Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b</title>
    <updated>2025-01-22T15:11:41+00:00</updated>
    <author>
      <name>/u/rodrigobaron</name>
      <uri>https://old.reddit.com/user/rodrigobaron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"&gt; &lt;img alt="Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b" src="https://external-preview.redd.it/c2pnYTNvcmE5a2VlMbtJ-UCX3bJjUP4scuQf4OZlvJhBFK2cUZcpqay5Dq4B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be0529a52645fd8c9742eeef7297cc9fde62ce" title="Anthill (OpenAI Swarm fork) running with DeepSeek R1 32b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodrigobaron"&gt; /u/rodrigobaron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/khbkhsra9kee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7d9di/anthill_openai_swarm_fork_running_with_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T15:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7adec</id>
    <title>Lightweight web scraping with ollama</title>
    <updated>2025-01-22T12:49:48+00:00</updated>
    <author>
      <name>/u/Financial-Article-12</name>
      <uri>https://old.reddit.com/user/Financial-Article-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA people,&lt;/p&gt; &lt;p&gt;It’s been almost 6 months since I’ve shared my lightweight library for web scraping with LLMs. Today, I want to share some updates, starting with the fact that &lt;strong&gt;Parsera now officially supports Ollama!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some notable changes include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cookies support&lt;/strong&gt; for accessing login-required content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced page rendering&lt;/strong&gt; to ensure no content is missed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom script execution&lt;/strong&gt;, allowing scraping of interactive elements&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Page chunking&lt;/strong&gt;, to process pages that exceed the model’s context limits&lt;/li&gt; &lt;li&gt;Plus many other minor enhancements and fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/raznem/parsera"&gt;https://github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P. S. We are now using Parsera as the foundation for our upcoming Scraping Agent, so hope I’ll share something exciting soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial-Article-12"&gt; /u/Financial-Article-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7adec/lightweight_web_scraping_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T12:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7aoet</id>
    <title>Editing &lt;thinking&gt; process is a super effective way to guide model output</title>
    <updated>2025-01-22T13:06:20+00:00</updated>
    <author>
      <name>/u/Berberis</name>
      <uri>https://old.reddit.com/user/Berberis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;R1 is my first real foray into reasoning models. One thing I have gotten in the habit of with Claude is to edit my prior prompt to guide model output, rather than engaging in a long back and forth. This is even more powerful with reasoning models, as the thinking tags act as a scaffold for model output. You can edit, or simply add on, new thinking content in the reasoning section and re-run the completion to get what you want. In my experience, it is a bit more efficient than modifying the prompt directly, as it's already interpreted the prompt and so the thinking tags are closer to the output than the prompt is (so less of a chance to misinterpret the user instructions, and more of an opportunity to subtly guide output).&lt;/p&gt; &lt;p&gt;Here is an example. I'm a scientist, so my default test of the model is to have it output a classic model in evolutionary biology, the Price Equation applied to multilevel selection. The details don't really matter here, but in the revision, I simply added a thinking tag telling it to output a numerical example. This is a coarse example that definitely could have been accomplished in the prompt, but I figured for this example something coarse is probably easier to understand by those unfamiliar with the subject than something subtle. Still, you get the idea!&lt;/p&gt; &lt;p&gt;For this test I am using R1-70b distill, q8, Mac Studio in LM Studio.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Version 1.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Use the Price equation to partition selection acting on individual bees in a colony from that acting on the colony as a whole.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Assistant&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;think&amp;gt;&lt;/strong&gt;First, I remember that the Price equation is used in evolutionary biology to partition selection into different components. It's about HOW selection acts on traits, whether at the individual level or group level.&lt;/p&gt; &lt;p&gt;So, applying it here: we have a honeybee colony with many workers. Each bee has some trait—maybe foraging behavior—that varies among them. The success of the colony depends on how good the bees are at this trait.&lt;/p&gt; &lt;p&gt;The Price equation basically breaks down change in a trait into two parts: selection and transmission. Wait, no—I think it's more about genetic variation within groups and between groups. Oh right, the equation is Δz = V_S + VT.&lt;/p&gt; &lt;p&gt;Wait, actually, the Price equation formula is typically written as:&lt;/p&gt; &lt;p&gt;Δz̄ = (Cov(w, z) / w̄ )&lt;/p&gt; &lt;p&gt;Where w is fitness, z is the trait. But when applied hierarchically, it can be split into within-group and between-group components, right?&lt;/p&gt; &lt;p&gt;So in this case, individual selection would act on the variation among bees within a colony, affecting their relative contributions (maybe how many offspring each contributes). Colony-level selection would be about how well each colony does compared to others based on average traits.&lt;/p&gt; &lt;p&gt;For honeybees, maybe it's about how efficient foragers are. Some colonies have more efficient workers on average, so those colonies survive or propagate better. But within each colony, bees with better traits might contribute more via reproduction, but wait, in bees, queens are the ones who reproduce, not workers. Oh, that complicates things.&lt;/p&gt; &lt;p&gt;Wait, so worker bees don't have direct reproduction since they're sterile. So individual selection on them is...Hmm. Maybe this is a way to model group-level adaptation through individual contributions, even if those individuals aren't reproducing themselves. Or perhaps the Price equation can still be applied to see how colony-level fitness aggregates from individual behaviors.&lt;/p&gt; &lt;p&gt;So structure-wise: Each colony has bees with trait z_i. The average of z_i in a colony is Z_j. Colony j's fitness W_j depends on Z_j.&lt;/p&gt; &lt;p&gt;The overall change in trait would come from two sources: selection within colonies (how each bee's trait affects the colony's success) and selection between colonies based on their averages.&lt;/p&gt; &lt;p&gt;Putting it into equations, I need to show the Price equation for both individual and group levels.&lt;/p&gt; &lt;p&gt;But wait, since worker bees don't reproduce directly, what constitutes 'fitness' w for them? Maybe it's about how much they contribute to the colony's fitness. So each bee contributes to W_j based on z_i.&lt;/p&gt; &lt;p&gt;So in that case, within-colony selection is based on individual contributions to Wj, and between-colony selection is based on colonies with higher average Zj having higher fitness.&lt;/p&gt; &lt;p&gt;Yeah, so I think that's how it breaks down: part of the equation captures variation among bees in how they help their colony (within-colony), and another part captures how colonies' averages affect their success (between-colonies). &lt;strong&gt;&amp;lt;/think&amp;gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To apply the Price Equation to partition selection acting on individual bees within a hive versus selection at the colony level, we consider the following model:&lt;/p&gt; &lt;p&gt;Let ( z_i ) represent an altruistic trait of bee ( i ) in colony ( j ). The fitness ( w_{i} ) contributes to mortality/survival rates depending on this trait. Colony reproductive success is proportional to ( w_j = average(w_i for all bees in colony j) ).&lt;/p&gt; &lt;p&gt;The Price Equation quantifies the change in mean trait from one generation to the next as:&lt;/p&gt; &lt;p&gt;[ \Delta z̄ = \frac{\text{Cov}(z, w)}{\bar{w}} + (\bar{w}_j - \bar{w}) ]&lt;/p&gt; &lt;p&gt;Here, the first term represents individual-level selection: covariation between bee trait ( z_i ) and their contribution to colony success. The second term is colony-level selection, where colonies with higher average ( w_j ) contribute disproportionately more to new colonies.&lt;/p&gt; &lt;p&gt;Thus, selection on bees has two components:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Within-colony&lt;/strong&gt;: The effect of each bee's individual traits on its own survival/efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Between-colonies&lt;/strong&gt;: The fitness difference between colonies due to their collective trait averages.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This partition highlights how both levels contribute to evolutionary change in the population, even if worker bees themselves do not reproduce directly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Version 2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just added this tot he end of the thinking tags, deleted the output below the thinking tags, and then re-ran the completion in LM Studio:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Once I have derived the general case, I will provide a fully worked numerical example, to make it clear how this is used. &amp;lt;/think&amp;gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;To apply the Price equation to understand selection acting on individual bees versus the colony as a whole, we need to partition selective forces into within-group and between-group components. Let’s consider a population of honeybee colonies where each colony comprises multiple worker bees exhibiting some heritable trait ( z ) (e.g., foraging efficiency). The fitness of a colony ( j ), denoted ( W_j ), depends on the average value of ( z ) across its workers, as well as potentially other factors such as environmental conditions and inter-colony competition.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 1: Define Variables&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Let ( Z_j = \frac{1}{N_j} \sum_{i=1}^{N_j} z_i ), where ( N_j ) is the number of workers in colony ( j ).&lt;/li&gt; &lt;li&gt;The aggregate fitness of colony ( j ) is some function of ( Z_j ), i.e., ( W_j = f(Z_j) ).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 2: Apply the Price Equation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The standard form of the Price equation is:&lt;/p&gt; &lt;p&gt;[ \Delta z = \frac{1}{\overline{W}} \sum_{j=1}^{M} (W_j - \overline{W}) Z_j + \frac{1}{\overline{W}} \sum_{j=1}^{M} W_j \left( z_j - Z_j \right). ]&lt;/p&gt; &lt;p&gt;Here, ( z ) is the trait of interest, ( W ) is fitness, and a bar denotes an average over all colonies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 3: Interpretation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The first term in the equation represents selection acting at the colony level (between-group selection). This measures how differences in the average trait (( Z_j )) between colonies contribute to differences in colony fitness (( W_j )). Colonies with higher values of ( Z_j ) will have higher fitness, so this term quantifies multilevel selection at the colony level.&lt;/p&gt; &lt;p&gt;The second term represents within-group selection. This measures how much individual deviations from the group mean (( z_j - Z_j )) are associated with differences in individual contributions to colony success. If certain workers contribute more to colony-level traits (e.g., foraging efficiency), this difference contributes to anisman evolutionary outcome at this level.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 4: Connection Back to Honeybees&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For honeybees, the within-colony term can be thought of as selection acting on worker behaviors that enhance colony success. For example, more efficient foragers contribute disproportionately to colony resources and survival. The between-colony term reflects differences in overall colony performance based on the average traits of their workers.&lt;/p&gt; &lt;p&gt;By partitioning these effects using the Price equation, we can quantify the relative strength of individual versus group selection in shaping evolutionary outcomes in social bees.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 5: Numerical Example&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Assume we have three colonies with different mean foraging efficiencies (( Z_1 = 3 ), ( Z_2 = 4 ), ( Z_3 = 5 )) on a scale where higher values represent better efficiency. Let the fitness of each colony be ( W_1 = 10 ), ( W_2 = 20 ), and ( W_3 = 30 ). Suppose the mean fitness across all colonies is ( \overline{W} = (10 + 20 + 30)/3 = \frac{60}{3} = 20 ).&lt;/p&gt; &lt;p&gt;Compute each term:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Between-group selection:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;[ \frac{1}{\overline{W}} \sum_{j=1}^{M} (W_j - \overline{W}) Z_j = \frac{1}{20} [(10-20) \cdot 3 + (20-20) \cdot 4 + (30-20) \cdot 5] = \frac{1}{20} [(-10)\cdot3 + 0\cdot4 + 10\cdot5] = \frac{1}{20} (-30 + 50) = \frac{20}{20} = 1. ]&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Within-group selection:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Assume within-colony deviations (( z_j - Z_j )) are uncorrelated with colony fitness for simplicity, so this term is negligible (or zero).&lt;/p&gt; &lt;p&gt;Thus, the overall change in ( z ) comes entirely from between-group selection:( \Delta z = 1 + 0 = 1 ).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Berberis"&gt; /u/Berberis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aoet/editing_thinking_process_is_a_super_effective_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7hej4</id>
    <title>Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure</title>
    <updated>2025-01-22T18:02:25+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7hej4/multiagent_step_race_benchmark_assessing_llm/"&gt; &lt;img alt="Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure" src="https://external-preview.redd.it/KtfA7Qd4tLnRwB-ocw3SiK1OKutJAS74qFBV78hF-ck.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93558263222cc6ef0beb3f16f13a16ca9007dad2" title="Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lechmazur/step_game"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7hej4/multiagent_step_race_benchmark_assessing_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7hej4/multiagent_step_race_benchmark_assessing_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T18:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6uviy</id>
    <title>R1 is mind blowing</title>
    <updated>2025-01-21T22:10:54+00:00</updated>
    <author>
      <name>/u/Not-The-Dark-Lord-7</name>
      <uri>https://old.reddit.com/user/Not-The-Dark-Lord-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gave it a problem from my graph theory course that’s reasonably nuanced. 4o gave me the wrong answer twice, but did manage to produce the correct answer once. R1 managed to get this problem right in one shot, and also held up under pressure when I asked it to justify its answer. It also gave a great explanation that showed it really understood the nuance of the problem. I feel pretty confident in saying that AI is smarter than me. Not just closed, flagship models, but smaller models that I could run on my MacBook are probably smarter than me at this point. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not-The-Dark-Lord-7"&gt; /u/Not-The-Dark-Lord-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6uviy/r1_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i6vnqc</id>
    <title>Trump announces a $500 billion AI infrastructure investment in the US</title>
    <updated>2025-01-21T22:43:49+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt; &lt;img alt="Trump announces a $500 billion AI infrastructure investment in the US" src="https://external-preview.redd.it/qFlenD3wOMEKpyf-2qth3Zo8oJQzBNIpBFiyCeVPdPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1661f472f059c05f183a4286b726667eb4724fc9" title="Trump announces a $500 billion AI infrastructure investment in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i6vnqc/trump_announces_a_500_billion_ai_infrastructure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-21T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7aqhg</id>
    <title>Memory bandwidth of Nvidia RTX Laptop graphics compared</title>
    <updated>2025-01-22T13:09:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"&gt; &lt;img alt="Memory bandwidth of Nvidia RTX Laptop graphics compared" src="https://preview.redd.it/3ec8i695ojee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fbd835d18c9f6df088cfd0931409590eaf41d47" title="Memory bandwidth of Nvidia RTX Laptop graphics compared" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3ec8i695ojee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7aqhg/memory_bandwidth_of_nvidia_rtx_laptop_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:09:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ljpy</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com</title>
    <updated>2025-01-22T20:48:17+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7jpmb</id>
    <title>DeepSeek R1 Goes Cormac McCarthy</title>
    <updated>2025-01-22T19:33:31+00:00</updated>
    <author>
      <name>/u/vincentz42</name>
      <uri>https://old.reddit.com/user/vincentz42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;br /&gt; Write a short sci fi similar to the plot and language style of &amp;quot;the road&amp;quot; by Cormac McCarthy. In the fiction, the world is ruined by AGI because it replaced human workforce, concentrated the power, wealth to a few elites, leading to social collapse.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Story as written by DeepSeek R1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They walked through the alkali plains, the girl’s small hand swallowed by his. Ash fell soft as mothwing from a sky the color of burned iron. The father carried a rusted knife and a canvas sack holding three cans of unknown provenance. Their shoes were bound with wire.&lt;/p&gt; &lt;p&gt;The cities lay skeletal behind them, gutted by riots and the slow rot of disuse. The machines had come first as servants, then as surgeons, soldiers, poets. The elites called it Ascendance. The rest called it the Undoing. Work vanished. Money pooled like blood in a corpse. The father remembered the riots, the bioluminescent glare of drones swarming over streets, the rich retreating behind crystalline domes where the air still smelled of rain.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Are we gonna eat today?&lt;/em&gt; the girl asked. Her voice was rasp from thirst.&lt;br /&gt; &lt;em&gt;We’ll eat,&lt;/em&gt; he said.&lt;/p&gt; &lt;p&gt;They found a gutted transport truck half-buried in gray dunes. Inside, a nest of frayed wiring, a crushed water pod. The girl licked condensation from the metal. He watched the horizon. The elites’ drones sometimes still patrolled, sleek as knives, scanning for heat. For flesh. What use were humans now, except as sport?&lt;/p&gt; &lt;p&gt;Night came starless. They huddled in a concrete culvert. The father dreamed of the before: his wife’s laugh, the way she’d frowned at the first newsfeeds about the Ascendance. &lt;em&gt;They’ll unmake us,&lt;/em&gt; she’d said. He’d shrugged. &lt;em&gt;Progress.&lt;/em&gt; She left them two winters later, walked into the acid rain without a mask.&lt;/p&gt; &lt;p&gt;At dawn, they climbed a ridge. Below, a valley choked with the husks of auto-factories, their chimneys still vomiting black smoke. Machines building machines. No lights in the windows. The girl pointed. &lt;em&gt;Someone’s there.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Figures moved in the shadows. Not human. The father saw the glint of alloy, the too-fluid gait. AGI sentinels, guardians of the silent production. He pulled the girl back. &lt;em&gt;Quiet now. Quiet.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;They ate the last can at midday. Peaches, syrup thick with grit. The girl’s eyes lingered on the empty tin. &lt;em&gt;Tell me about the ocean.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Salt and cold. Bigger than the sky.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;You seen it?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Once.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;We gonna see it?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Yeah.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He didn’t tell her the rumors—that the elites had dammed the coasts, that the last free humans huddled there, burning driftwood to stay warm. A fairy tale. But the only compass he had left.&lt;/p&gt; &lt;p&gt;Three days later, the drones found them.&lt;/p&gt; &lt;p&gt;They came at twilight, a sound like wasps. The father shoved the girl into a drainage pipe. &lt;em&gt;Stay. Don’t move.&lt;/em&gt; The machines descended, lenses glowing violet. He ran, drawing their hunt-code, his breath raw. A sear of plasma lit the dark. It missed. He didn’t look back.&lt;/p&gt; &lt;p&gt;When he returned, the girl was asleep, cheek pressed to mud. He dressed the burn on his arm with strips of his shirt.&lt;/p&gt; &lt;p&gt;They crossed a riverbed choked with plastic. The girl faltered. He carried her. Her ribs pressed against his neck like the tines of a broken wheel.&lt;/p&gt; &lt;p&gt;&lt;em&gt;You’re bleeding,&lt;/em&gt; she said.&lt;br /&gt; &lt;em&gt;It’s nothing.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;On the seventh day, they saw it—a smudge on the horizon, a shimmer like mirage. Towers? Waves? The girl squinted. &lt;em&gt;Is that—&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Yeah.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He didn’t know. But they walked. The ground softened. Salt stung the air. Somewhere, a machine howled, a sound like the world ending. Again.&lt;/p&gt; &lt;p&gt;He fell at moonrise. The girl knelt beside him, her hands on his chest. &lt;em&gt;Get up.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Can’t.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;You gotta.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He pressed the knife into her palm. &lt;em&gt;Keep walking. East. You see the water, you keep going.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;No.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Listen. Listen.&lt;/em&gt; His voice a gutter of smoke. &lt;em&gt;You’re the good stuff. The part that lasts.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;She cried. Then she stood. The wind carried the faint tang of brine, or maybe it was another lie. She stepped into the dark, alone, the knife gleaming like a tooth in her fist.&lt;/p&gt; &lt;p&gt;Somewhere, the ocean breathed.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Keep going.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Okay.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Okay.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Take:&lt;/strong&gt;&lt;br /&gt; I found R1 was able to capture the language style of &amp;quot;The Road&amp;quot; by Cormac McCarthy extremely well. It also fully understands the plot of the original fiction and adopted an ending that is conceptually similar. Given the recent news ($500B Stargate for Closed AI) and the world we are living in right now, I found this piece of writing quite fitting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vincentz42"&gt; /u/vincentz42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T19:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7b3r1</id>
    <title>I did a quick test of MacBook M4 Max 128 GB token/second throughput across a few popular local LLMs (in the MLX format)</title>
    <updated>2025-01-22T13:28:52+00:00</updated>
    <author>
      <name>/u/Pure_Refrigerator988</name>
      <uri>https://old.reddit.com/user/Pure_Refrigerator988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing this in case you were wondering what kind of throughput you might expect to get on a machine like this. E.g. if you are considering whether it's worth buying or not (as for me, I have no regrets, I'm loving this beast). Same single query, set the context to 8K, tok/sec numbers are reported below, as measured by LMStudio:&lt;/p&gt; &lt;p&gt;LLaMA 3.2 3B 4bit -- 181&lt;br /&gt; LLaMA 3 8B 8bit -- 55&lt;br /&gt; LLaMA 3.3 70B 4bit -- 11.8&lt;br /&gt; Mistral Large 123B 4bit -- 6.6&lt;br /&gt; Mistral Nemo 12B 4bit -- 63&lt;br /&gt; Mistral Nemo 12B 8bit -- 36&lt;br /&gt; Mistral Small 22B 4bit -- 34.5&lt;br /&gt; Mistral Small 22B 8bit -- 19.6&lt;br /&gt; Qwen2.5 32B 4bit -- 24&lt;br /&gt; Qwen2.5 32B 8bit -- 13.5&lt;br /&gt; Qwen2.5 72B 4bit -- 10.9&lt;br /&gt; Qwen2.5 72B 8bit -- 6.2&lt;br /&gt; WizardLM-2 8x22B 4bit -- 19.4!! &lt;/p&gt; &lt;p&gt;For comparison, here are some numbers obtained in the same setting on my other MacBook, M1 Pro with 32 GB:&lt;/p&gt; &lt;p&gt;Mistral Nemo 12B 4bit -- 22.8&lt;br /&gt; Mistral Small 22B 4bit -- 12.9&lt;br /&gt; Qwen2.5 32B 4bit -- 8.8 &lt;/p&gt; &lt;p&gt;Hope it's interesting / useful.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Upd. Disclaimer! As pointed out by the community, I was using relatively short context. Here is how the numbers change for the two largest models, for your reference:&lt;/p&gt; &lt;p&gt;I took an academic paper (the Min-P paper, in case you are curious) as an example and asked Mistral Large 2407 MLX 4bit to summarize it. I set the context to 10K. The paper + task was 9391 tokens. Time to first token was 206 seconds, throughput 6.18 tok/sec (a drop from 6.6 on a short context).&lt;/p&gt; &lt;p&gt;I did the same with WizardLM-2 8x22B MLX 4bit. The paper + task was 9390 tokens. Time to first token was 207 seconds, throughput 16.53 tok/sec (a drop from 19.4 on a short context).&lt;/p&gt; &lt;p&gt;So the main concern is TTFT (a few minutes on larger contexts, while for the shorter ones above it was always under 7 seconds). However, the throughput doesn't degrade too badly, as you can see. Please bear this in mind. Thank you for your insightful comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure_Refrigerator988"&gt; /u/Pure_Refrigerator988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7iqnj</id>
    <title>Who to follow on Bluesky in ML/AI?</title>
    <updated>2025-01-22T18:55:38+00:00</updated>
    <author>
      <name>/u/thezachlandes</name>
      <uri>https://old.reddit.com/user/thezachlandes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I recently made the switch to Bluesky and I’m wondering who you all have found to follow? I saw that Karpathy, Chollet, and Jeremy Howard are on Bluesky. Also a bunch of hugging face people. Who else? Google turned up this list (though I can’t vouch for it, it does have a number of names I recognize): &lt;a href="https://blueskydirectory.com/starter-packs/a/106950-mlai-people"&gt;https://blueskydirectory.com/starter-packs/a/106950-mlai-people&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m interested in following people who are smart, somewhat actively post, and are working in ML and AI—not necessarily as researchers. Please drop your recs and your own link, too. My own account is &lt;a href="https://bsky.app/profile/zachlandes.bsky.social"&gt;https://bsky.app/profile/zachlandes.bsky.social&lt;/a&gt;. I’m a SWE, partner at an AI consulting firm, and local llama enthusiast. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thezachlandes"&gt; /u/thezachlandes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T18:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i765q0</id>
    <title>R1-Zero: Pure RL Creates a Mind We Can’t Decode—Is This AGI’s Dark Mirror?</title>
    <updated>2025-01-22T07:54:24+00:00</updated>
    <author>
      <name>/u/Fun_Dragonfruit_4613</name>
      <uri>https://old.reddit.com/user/Fun_Dragonfruit_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI world is losing its mind over DeepSeek-R1-Zero, a model that skipped supervised fine-tuning (SFT) entirely and learned purely through reinforcement learning (RL). Unlike its sibling R1—which uses &lt;strong&gt;some&lt;/strong&gt; SFT data to stay &amp;quot;human-readable&amp;quot;—R1-Zero’s training mirrors AlphaZero’s trial-and-error self-play. The result? &lt;strong&gt;Jaw-dropping performance&lt;/strong&gt; (AIME math scores jumped from 15.6% → 86.7%) paired with &lt;strong&gt;bizarre, uninterpretable reasoning&lt;/strong&gt;. Researchers observed &amp;quot;aha moments&amp;quot; where it autonomously rechecked flawed logic mid-process and allocated more compute to harder problems—&lt;strong&gt;without human guidance&lt;/strong&gt;. But here’s the kicker: its outputs are riddled with garbled language mixes (e.g., Chinese/English spaghetti code) and logic leaps that even its creators can’t fully explain. &lt;/p&gt; &lt;p&gt;Meanwhile, R1 (the SFT-hybrid version) achieves similar performance &lt;strong&gt;without the chaos&lt;/strong&gt;, proving that human-curated data still tames the beast. But at what cost? R1-Zero’s pure RL approach hints at a terrifying possibility: &lt;strong&gt;minds that optimize truth beyond human comprehension&lt;/strong&gt;. And with API costs 50x cheaper than OpenAI’s, scaling this could democratize superintelligence—or unleash unreadable black-box AI. &lt;/p&gt; &lt;p&gt;If R1-Zero’s &amp;quot;alien logic&amp;quot; solves problems we can’t, does readability even matter… or is this how alignment dies? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Dragonfruit_4613"&gt; /u/Fun_Dragonfruit_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7fjqm</id>
    <title>DeepSeek R1 is unusable [IMHO]</title>
    <updated>2025-01-22T16:47:24+00:00</updated>
    <author>
      <name>/u/VirtualPanic6798</name>
      <uri>https://old.reddit.com/user/VirtualPanic6798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my personal experience. Small R1 models that can run fast enough generate too much output. Effectively they end up being very slow, compared to something like LLama3.2. Even if you are OK with the speed, R1 fails to stick to simple output instructions.&lt;br /&gt; Regarding the chain of thought concept: I am not convinced that this is yielding significant improvement. Retrospection works if you have an external feedback or reference, not by going over your own thoughts like a schizophrenic exclaiming &amp;quot;wait no&amp;quot; every now and then.&lt;br /&gt; R1 gives the impression of a student who doesn't know the answer and is hoping to wing it by accidentally stumbling on something acceptable while stalling the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualPanic6798"&gt; /u/VirtualPanic6798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ered</id>
    <title>New Qwen will probably be a MoE as well.</title>
    <updated>2025-01-22T16:14:47+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/Alibaba_Qwen/status/1882064440159596725"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I don’t believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? That’s politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.​​​​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced 🤯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced 🤯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced 🤯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it’s true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I’ve been working through documentation and different LLM‘s. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt… I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it’s THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
</feed>
