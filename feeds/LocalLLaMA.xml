<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-18T04:24:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i3b1jb</id>
    <title>New framework aims to mimic human thinking for writing long-form content (OmniThink)</title>
    <updated>2025-01-17T07:22:39+00:00</updated>
    <author>
      <name>/u/emanuilov</name>
      <uri>https://old.reddit.com/user/emanuilov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt; &lt;img alt="New framework aims to mimic human thinking for writing long-form content (OmniThink)" src="https://external-preview.redd.it/P3wPulsj-vbHIfL8pdoJemWboTREaTu--SoaotPYjzU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=098b6934967a73dbc796419d5bd3b3397ed04814" title="New framework aims to mimic human thinking for writing long-form content (OmniThink)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a paper about OmniThink - an approach that tries to replicate how humans write long-form content. The framework focuses on continuous reflection and exploration, similar to how we gather information and refine our understanding when writing detailed articles.&lt;/p&gt; &lt;p&gt;(Not affiliated with the authors)&lt;/p&gt; &lt;p&gt;The paper's style reminds me of Google Deep Research's functionality. I couldn't get their online demo to work, but the ideas in the paper are worth checking out, IMO. I will spend some time on their repo to see if that will work out of the box.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2501.09751"&gt;https://huggingface.co/papers/2501.09751&lt;/a&gt;&lt;br /&gt; Project page: &lt;a href="https://zjunlp.github.io/project/OmniThink/"&gt;https://zjunlp.github.io/project/OmniThink/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/zjunlp/OmniThink"&gt;https://github.com/zjunlp/OmniThink&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb"&gt;https://preview.redd.it/alrt6fyh9ide1.png?width=3875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a41e77eac565e5bf61deeaae9c0de535fb45feb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emanuilov"&gt; /u/emanuilov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3b1jb/new_framework_aims_to_mimic_human_thinking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:22:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3yh5o</id>
    <title>What llm do you use for your local agents.</title>
    <updated>2025-01-18T03:21:44+00:00</updated>
    <author>
      <name>/u/rhaastt-ai</name>
      <uri>https://old.reddit.com/user/rhaastt-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using run pod cloud hosting. My preference is using autogen. A while ago I remember trying to get local models to use autogen and it would just be a shit show. Wouldn't really work. Only thing that worked consistently was gpt4. What local model have you found to be better then others when it comes to local agents. 94gb -140gb vram is what I have available &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhaastt-ai"&gt; /u/rhaastt-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yh5o/what_llm_do_you_use_for_your_local_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yh5o/what_llm_do_you_use_for_your_local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yh5o/what_llm_do_you_use_for_your_local_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T03:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3yw8d</id>
    <title>How do you guys use Open Source models in your workplace? I wish to start using them at my workplace.</title>
    <updated>2025-01-18T03:45:06+00:00</updated>
    <author>
      <name>/u/Existing-Pay7076</name>
      <uri>https://old.reddit.com/user/Existing-Pay7076</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am the only AI guy in our workplace. I have built some fine GenAI applications for the company but using Openai's API.&lt;/p&gt; &lt;p&gt;We got some credits for Scaleway, so we are free to play around with GPU for a month.&lt;/p&gt; &lt;p&gt;Btw there best gpu is good enough to run non quantised version of qwen-32b.&lt;/p&gt; &lt;p&gt;Now here is where my doubt arises, scaleway also provides access to some open source models api keys(most of them being costlier than their gpu itself). This made me question if there is a drawback in locally downloding the models from huggingface.&lt;/p&gt; &lt;p&gt;This is why I am asking you guys how you all leverage open source models in your company. I am very eager to explore the real AI part of my job, some guidance will be appreciated. Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Existing-Pay7076"&gt; /u/Existing-Pay7076 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yw8d/how_do_you_guys_use_open_source_models_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yw8d/how_do_you_guys_use_open_source_models_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yw8d/how_do_you_guys_use_open_source_models_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T03:45:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3yzoc</id>
    <title>I don't think AI will kill programming, but it will change it in a few big ways.</title>
    <updated>2025-01-18T03:50:36+00:00</updated>
    <author>
      <name>/u/malformed-packet</name>
      <uri>https://old.reddit.com/user/malformed-packet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think it will kill websites and frontends. I think companies will start having their own internal tools that their agents can use, but somebody still has to code those. And I think those will be about a billion times more fun to code than another stuffy react app. I can see an app store for tools that you can embed.&lt;/p&gt; &lt;p&gt;Think of all the stupid things you have had to code that needed just enough interface to be easier to use than the command line, but not quite a full app or page.&lt;/p&gt; &lt;p&gt;The real winner we have here is natural language processing that honestly doesn't suck any more, and that is achievable with even some of the simpler models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/malformed-packet"&gt; /u/malformed-packet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yzoc/i_dont_think_ai_will_kill_programming_but_it_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yzoc/i_dont_think_ai_will_kill_programming_but_it_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3yzoc/i_dont_think_ai_will_kill_programming_but_it_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T03:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3z6cb</id>
    <title>Grokking at the Edge of Numerical Stability</title>
    <updated>2025-01-18T04:01:09+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04697"&gt;https://arxiv.org/abs/2501.04697&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the naïve loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and ⊥Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at this https URL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3z6cb/grokking_at_the_edge_of_numerical_stability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T04:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3itva</id>
    <title>"I/We/They Couldn't Help But..." Repeating LLM Phrasing?</title>
    <updated>2025-01-17T15:27:12+00:00</updated>
    <author>
      <name>/u/Jattoe</name>
      <uri>https://old.reddit.com/user/Jattoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The spacecraft's sensors detected a safe landing spot near a lush forest, and the pilot navigated the ship towards the area. As they approached, they couldn't help but notice the array of exotic flora that thrived in the region.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;To those that use LLMs often, I imagine you too have noticed the same phrases being used, and in very odd ways (why stress helplessness to notice an array of exotic flora in the region?)&lt;br /&gt; I've actually added &amp;quot;Don't use the words '&lt;em&gt;I couldn't help but&lt;/em&gt;' in your output&amp;quot; and have still had the LLM put the phrase in there, almost like it worked like the &amp;quot;don't think of an elephant,&amp;quot; concept for humans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jattoe"&gt; /u/Jattoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3itva/iwethey_couldnt_help_but_repeating_llm_phrasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3r6iu</id>
    <title>Function calling in llama.cpp?</title>
    <updated>2025-01-17T21:25:35+00:00</updated>
    <author>
      <name>/u/Few_Acanthisitta_858</name>
      <uri>https://old.reddit.com/user/Few_Acanthisitta_858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are you using function calling in llama.cpp? I tried few things but it doesn't really seem to work 😕 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Acanthisitta_858"&gt; /u/Few_Acanthisitta_858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3r6iu/function_calling_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3px18</id>
    <title>Current SoTA for local speech to text + diarization?</title>
    <updated>2025-01-17T20:29:12+00:00</updated>
    <author>
      <name>/u/dat09</name>
      <uri>https://old.reddit.com/user/dat09</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the current sota for local speech to text + diarization? Is it still whisper + pyannote? feel like it’s been 1yr+ without any significant jumps in performance/ efficiency.&lt;/p&gt; &lt;p&gt;Wondering if anyone else has found a step change since?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat09"&gt; /u/dat09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3px18/current_sota_for_local_speech_to_text_diarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31ji5</id>
    <title>What is ElevenLabs doing? How is it so good?</title>
    <updated>2025-01-16T22:42:26+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. What's their trick? On everything but voice, local models are pretty good for what they are, but ElevenLabs just blows everyone out of the water. &lt;/p&gt; &lt;p&gt;Is it full Transformer? Some sort of Diffuser? Do they model the human anatomy to add accuracy to the model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3ilu3</id>
    <title>[REPOST]Linux 6.14 will have amdxdna! The Ryzen AI NPU driver</title>
    <updated>2025-01-17T15:17:23+00:00</updated>
    <author>
      <name>/u/KillerX629</name>
      <uri>https://old.reddit.com/user/KillerX629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What will this mean for amd cards and AI inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillerX629"&gt; /u/KillerX629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3ilu3/repostlinux_614_will_have_amdxdna_the_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T15:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3xoyd</id>
    <title>What's the cheapest way to run Llama 3.x 8B class models with realtime-like (chatgpt speed) tokens per second?</title>
    <updated>2025-01-18T02:38:29+00:00</updated>
    <author>
      <name>/u/synexo</name>
      <uri>https://old.reddit.com/user/synexo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;fireworks.ai? spin up on runpod? build a home server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synexo"&gt; /u/synexo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3xoyd/whats_the_cheapest_way_to_run_llama_3x_8b_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T02:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3fli7</id>
    <title>Laptop LLM performance - beware of the power settings!</title>
    <updated>2025-01-17T12:48:10+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's pity that I did such a lame negligence, but want to share with you, in case someone struggles with the same issue.&lt;/p&gt; &lt;p&gt;Both me and the wife have Lenovo gaming laptops:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Rizen 5, 16GB DDR5 RAM, 3050ti 4GB&lt;/li&gt; &lt;li&gt;i5, 16GB DDR5 RAM, 4060 8GB&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Logically, if a model fits entirely in the VRAM, the machine 2 runs it noticeble faster. BUT, everything beyond 7B which is partially offloaded in VRAM, (like Qwen 2.5 14B, 26/49 layers offloaded to GPU) practically goes with less than 0.2T/s and takes 2-3 minutes to output the first token on the machine 2! While machine 1 runs the same Qwen 2.5 (14B, 9/49 layers offloaded to GPU) quite acceptable with around 2T/s.&lt;/p&gt; &lt;p&gt;I was changing nVidia/CUDA drivers, settings of llama.cpp - nothing helped. Till I checked the &amp;quot;power settings&amp;quot; of Windows and changed the presets from &amp;quot;balanced&amp;quot; to &amp;quot;performance&amp;quot;. It was the CPU/RAM of the machine which killed all the fun. Now I get 5-10 T/s with 14B model and 26/49 layers to GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3fli7/laptop_llm_performance_beware_of_the_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T12:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3h7hs</id>
    <title>Attend - Proof of Concept</title>
    <updated>2025-01-17T14:12:36+00:00</updated>
    <author>
      <name>/u/Pedalnomica</name>
      <uri>https://old.reddit.com/user/Pedalnomica</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've gotten fed up with hoping on the computer to do one thing, and doing other stuff instead.&lt;/p&gt; &lt;p&gt;I'm building Attend so that our devices can help us dedicate our time and attention on what matters to us, instead of what some algorithm was optimized for.&lt;/p&gt; &lt;p&gt;Right now, it is a voice assistant that uses a vision LLM to &amp;quot;watch&amp;quot; your screen and help you get back on track if what you're doing isn't aligned with what you said you wanted to do.&lt;/p&gt; &lt;p&gt;I've got some work to do on the workflows and prompts to reduce false positives, but it &amp;quot;works&amp;quot; and I'm very excited about it!&lt;/p&gt; &lt;p&gt;I'd like to get this down to a single 3090, but two seems pretty feasible. Part of the problem is most open weight vision language models are garbage with 4K images/screenshots. Qwen2-VL seems to be an exception, but it (especially the 7B) is garbage when it comes to driving the workflows behind Attend. So, I've just been using Qwen2-VL-7B-Instruct and Llama-3.3 at 8-bit as I get it working. I'd love to hear suggestions for minimizing the VRAM required (Intern2_5-VL also seems to handle 4K alright, but I haven't tested it enough on the workflows).&lt;/p&gt; &lt;p&gt;Attend interfaces with all models using OpenAI compatable API calls. So, you should be able to use the cloud, if you're into that kinda thing... You could also take a hybrid approach. I think you could get the STT and vision LLM into 16GB VRAM and run that locally. Piper TTS runs well on CPU. You could then use a cloud model just for the text LLM and STT and keep the most sensitive stuff (screenshots!) local.&lt;/p&gt; &lt;p&gt;Check out the open-source code &lt;a href="https://github.com/hyperfocAIs/Attend/"&gt;https://github.com/hyperfocAIs/Attend/&lt;/a&gt; and a proof of concept video &lt;a href="https://youtu.be/PETrY540zMM"&gt;https://youtu.be/PETrY540zMM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Typos, clarified that this project is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pedalnomica"&gt; /u/Pedalnomica &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3h7hs/attend_proof_of_concept/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T14:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qfgy</id>
    <title>AI Research</title>
    <updated>2025-01-17T20:52:03+00:00</updated>
    <author>
      <name>/u/ASI-Enjoyer</name>
      <uri>https://old.reddit.com/user/ASI-Enjoyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do we still need AI research, or is ASI just a matter of scaling? I'm 17 years old and I want to become an AI researcher. I want to know your opinion/get advice&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASI-Enjoyer"&gt; /u/ASI-Enjoyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qfgy/ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nbb7</id>
    <title>Any "mainstream" apps with genuinely useful local AI features?</title>
    <updated>2025-01-17T18:36:57+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if any of you actually regularly use features in apps with local AI processing?&lt;/p&gt; &lt;p&gt;When I say &amp;quot;mainstream app&amp;quot;, I mean more like PyCharm from JetBrains (i.e. making lots of money, large teams behind them, etc.) than an open-source/indie dev app.&lt;/p&gt; &lt;p&gt;And I'm more talking about a feature in an app (which does a bunch of things other than that AI feature), as opposed to an app that's entirely about using AI locally, like Ollama, LMStudio, etc.&lt;/p&gt; &lt;p&gt;I'm also not talking about OS features, e.g. auto-complete on iPhones. More interested in apps that you've downloaded.&lt;/p&gt; &lt;p&gt;Currently, the only thing I can think of in my day-to-day is &lt;a href="https://blog.jetbrains.com/ai/2024/11/jetbrains-ai-assistant-2024-3/"&gt;code completion in PyCharm&lt;/a&gt;, but even that is now some kind of hybrid local/cloud thing.&lt;/p&gt; &lt;p&gt;EDIT: Not necessarily just talking about LLM stuff. Realized that I also use some photo editing apps every now and then with local ML models (but that's all pretty old tech, e.g. interactive background removal/segmentation)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nbb7/any_mainstream_apps_with_genuinely_useful_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3qzom</id>
    <title>5090 OpenCL &amp; Vulkan leaks</title>
    <updated>2025-01-17T21:17:14+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ack, not crushing 4090.&lt;br /&gt; &lt;a href="https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks"&gt;https://videocardz.com/newz/nvidia-geforce-rtx-5090-appears-in-first-geekbench-opencl-vulkan-leaks&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3qzom/5090_opencl_vulkan_leaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3as1m</id>
    <title>OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)</title>
    <updated>2025-01-17T07:02:43+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt; &lt;img alt="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" src="https://a.thumbs.redditmedia.com/11a6AQbm8PHTqUzymosrsOz6WrQ1h5fnohaqQF7icF0.jpg" title="OpenWebUI Canvas Implementation -- Coming Soon! (Better Artifacts)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ytezb1q05ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93364222443da5f695a745265842c91ee604d9e5"&gt;C# and XML View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1ttzjm4s5ide1.png?width=1862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd00eb16ef20e090d9f5ebee0d69f48c4f3b8bf0"&gt;Design View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tj92xav5ide1.png?width=1749&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81d8f9dec9bd3575fb4fc4ea8d399627b2eacd4a"&gt;Code View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all! I'm implementing Canvas (beefing up Artifacts) on OpenWebUI.&lt;/p&gt; &lt;p&gt;This was my only issue ever with OpenWebUI, just the very limited canvas feature (only restricted to HTML, CSS, JavaScript and SVG).&lt;/p&gt; &lt;p&gt;I've expanded support for the following languages:&lt;/p&gt; &lt;p&gt;C#, Python, Java, PHP, Ruby, Bash, Shell, AppleScript, SQL, JSON, XML, YAML, Markdown, HTML&lt;/p&gt; &lt;p&gt;If I'm missing one feel free to comment it! It's super easy to add at this point.&lt;/p&gt; &lt;p&gt;Another notable feature I'm adding is to switch between Design view and Code view for web design.&lt;/p&gt; &lt;p&gt;I'm super close to finishing! I just need to clean it up and visualize/track changes between revisions. Expect my pull request it in the next couple of weeks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3as1m/openwebui_canvas_implementation_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T07:02:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3rpsh</id>
    <title>The “apple” test - Why aren’t newer reasoning models doing better on this basic benchmark? (and yes, I know token prediction mechanics play a role)</title>
    <updated>2025-01-17T21:49:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of you are probably familiar with the infamous LLM “apple test” benchmark.&lt;/p&gt; &lt;p&gt;If you’re not, here it is, you give an LLM the following seemingly simple instruction prompt:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write 10 sentences that end in the word “apple”.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sadly, most open source (and even a lot of frontier models fail miserably at this task. I’ve read that it has a lot to do with the way token prediction works, but some models can actually pass this test easily.&lt;/p&gt; &lt;p&gt;Models that I’ve tested that pass or fail on this test:&lt;/p&gt; &lt;p&gt;LLMs that PASS the apple test:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.3:70b (Q4KM)&lt;/li&gt; &lt;li&gt;Athene-V2 (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron (Q4KM)&lt;/li&gt; &lt;li&gt;Qwen 2.5:72b (Q4KM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LLMs that FAIL the apple test (most are newer models) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14b (FP16)&lt;/li&gt; &lt;li&gt;InternLM3 (FP16)&lt;/li&gt; &lt;li&gt;Falcon 3 10b (FP16)&lt;/li&gt; &lt;li&gt;Granite 3 Dense (FP16)&lt;/li&gt; &lt;li&gt;QwQ 32b (Q_8)&lt;/li&gt; &lt;li&gt;GLM-4 8b (FP16)&lt;/li&gt; &lt;li&gt;Command-R (Q4KM)&lt;/li&gt; &lt;li&gt;MiniCPM 8b v2.6 (FP16)&lt;/li&gt; &lt;li&gt;Mistral Small 22b (Q4KM)&lt;/li&gt; &lt;li&gt;Nemotron Mini 4b (FP16)&lt;/li&gt; &lt;li&gt;Qwen 2.5 7b (FP16) &lt;/li&gt; &lt;li&gt;WizardLM2 7b (FP16)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FAILED but with an honorable mention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Olmo2 14b (FP16) - this model is lightning fast and got 8 of 10 consistently correct and was able to fix its mistake after a second shot at it (most models won’t do better with more chances). &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This task seems to be challenging for models under 70b to complete. Even the newer reasoning models with higher test time compute capabilities don’t seem to do well at all. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why haven’t newer models gotten better at this task over time? &lt;/li&gt; &lt;li&gt;Is the underlying mechanism of token prediction still preventing success? &lt;/li&gt; &lt;li&gt;Are the models that this works with just cheating by training to pass the specific benchmark? &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone found an open source model under 70b that can pass the apple test consistently? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3rpsh/the_apple_test_why_arent_newer_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T21:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3kv1n</id>
    <title>[Magnum/SE] LLama 3.3 70b</title>
    <updated>2025-01-17T16:54:07+00:00</updated>
    <author>
      <name>/u/lucyknada</name>
      <uri>https://old.reddit.com/user/lucyknada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, folks!&lt;/p&gt; &lt;p&gt;We've got something a little different to share this time. It's not a full release or a new series as of yet, but more like an epilogue to the v4 series we released a few months back. DoctorShotgun wasn't entirely satisfied with how the large models in the series turned out, so he spent some more time in the lab - this time on the newer llama 3.3 model for a change:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE"&gt;https://huggingface.co/Doctor-Shotgun/L3.3-70B-Magnum-v4-SE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This time, the model was trained as an rslora with recommendations from Gryphe of Mythomax fame, and it comes with the full set of adapter checkpoints for mergers and other experimenters to play around with (&lt;a href="https://huggingface.co/Doctor-Shotgun/Magnum-v4-SE-70B-LoRA"&gt;available here&lt;/a&gt;). Preliminary testing suggests that rslora adequately style-transfers the classic Claude-y flavor of magnum to the llama 3.3 model.&lt;/p&gt; &lt;p&gt;In terms of changes to the data, the model doesn't deviate too far from the v4 series. The dataset includes some further cleaning of the RP log dataset used in v4, as well as the re-introduction of a subset of the data used in the v2 and earlier models. As per usual, the training config is linked from the model card in the spirit of open source.&lt;/p&gt; &lt;p&gt;No first-party quants are available at this time, but links to those created by well-known quanters are linked in the model description.&lt;/p&gt; &lt;p&gt;Hope you enjoy this belated New Years present, and stay tuned for what's to come!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lucyknada"&gt; /u/lucyknada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3kv1n/magnumse_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T16:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3w7ao</id>
    <title>[2403.09919] Recurrent Drafter for Fast Speculative Decoding in Large Language Models</title>
    <updated>2025-01-18T01:20:34+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2403.09919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3w7ao/240309919_recurrent_drafter_for_fast_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-18T01:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pup0</id>
    <title>Beating cuBLAS in SGEMM from Scratch</title>
    <updated>2025-01-17T20:26:10+00:00</updated>
    <author>
      <name>/u/salykova</name>
      <uri>https://old.reddit.com/user/salykova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt; &lt;img alt="Beating cuBLAS in SGEMM from Scratch" src="https://a.thumbs.redditmedia.com/5VtAEp46vu6qAMyyshFSAQ0PS4VyO1ibLsIEkWU_HY0.jpg" title="Beating cuBLAS in SGEMM from Scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while ago, I shared my article here about optimizing matrix multiplication on CPUs - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1dt3rqc/beating_numpys_matrix_multiplication_in_150_lines/"&gt;Beating NumPy's matrix multiplication in 150 lines of C code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I received positive feedback from you, and today I'm excited to share my second blog post. This one focuses on an SGEMM (Single-precision GEneral Matrix Multiply) that outperforms NVIDIA's implementation from cuBLAS library with its (modified?) CUTLASS kernel across a wide range of matrix sizes. This project primarily targets &lt;strong&gt;CUDA-learners&lt;/strong&gt; and aims to bridge the gap between the SGEMM implementations explained in books/blogs and those used in NVIDIA’s BLAS libraries. The blog delves into benchmarking code on CUDA devices and explains the algorithm's design along with optimization techniques. These include inlined PTX, asynchronous memory copies, double-buffering, avoiding shared memory bank conflicts, and efficient coalesced storage through shared memory.&lt;/p&gt; &lt;p&gt;The code is super easy to tweak, so you can customize it for your projects with kernel fusion or just drop it into your libraries as-is. Below, I've included performance comparisons against cuBLAS and Simon Boehm’s highly cited work, which is now integrated into llamafile aka tinyBLAS.&lt;/p&gt; &lt;p&gt;P.S. The next blog post will cover implementing HGEMM (FP16 GEMM) and HGEMV (FP16 Matrix-Vector Multiplication) on Tensor Cores achieving performance comparable to cuBLAS (or maybe even faster? let's see). If you enjoy educational content like this and would like to see more, please share the article. If you have any questions, feel free to comment or send me a direct message - I'd love to hear your feedback and answer any questions you may have!&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://salykova.github.io/sgemm-gpu"&gt;https://salykova.github.io/sgemm-gpu&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/salykova/sgemm.cu"&gt;https://github.com/salykova/sgemm.cu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f"&gt;https://preview.redd.it/uq14ysfvamde1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d3998a50e61643c76e82ff048d1dd20703e3a65f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salykova"&gt; /u/salykova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pup0/beating_cublas_in_sgemm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3mybo</id>
    <title>LCLV: Real-time video analysis with Moondream 2B &amp; OLLama (open source, local). Anyone want a set up guide?</title>
    <updated>2025-01-17T18:21:33+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt; &lt;img alt="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" src="https://external-preview.redd.it/MXZ5aHh4bWZpbGRlMSTqk2DOPEdgmnDyQ8guvDBrE8AyiWMeqDE4BRKGe_SG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecd38999e371e083e545019f1eaf8d324a146b50" title="LCLV: Real-time video analysis with Moondream 2B &amp;amp; OLLama (open source, local). Anyone want a set up guide?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3kcfymfilde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3mybo/lclv_realtime_video_analysis_with_moondream_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3o7a8</id>
    <title>I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)</title>
    <updated>2025-01-17T19:14:56+00:00</updated>
    <author>
      <name>/u/yyjhao</name>
      <uri>https://old.reddit.com/user/yyjhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt; &lt;img alt="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" src="https://external-preview.redd.it/MGt0ZzN4Y3NzbGRlMeSgvI1GdDqWZSs569grdhgwadhN-F5M6UL9TiNWoaqW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c56ce3e01a5f41dcffc115930e49f7b1fee821" title="I am open sourcing a smart text editor that runs completely in-browser using WebLLM + LLAMA (requires Chrome + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yyjhao"&gt; /u/yyjhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n3fmqwcsslde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3o7a8/i_am_open_sourcing_a_smart_text_editor_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T19:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3pexj</id>
    <title>DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench</title>
    <updated>2025-01-17T20:06:47+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt; &lt;img alt="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" src="https://external-preview.redd.it/RiXxcULN7VvmAA8zRKm9Hg6sMZIuDEZ9SdZM3h7z4e0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c43191d847a8866681673c575cc88d8e702dd05" title="DeepSeek-R1 (Preview) Benchmarked on LiveCodeBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/WdpIkiy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3pexj/deepseekr1_preview_benchmarked_on_livecodebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T20:06:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i3nsbx</id>
    <title>Realtime speaker diarization</title>
    <updated>2025-01-17T18:57:16+00:00</updated>
    <author>
      <name>/u/Lonligrin</name>
      <uri>https://old.reddit.com/user/Lonligrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt; &lt;img alt="Realtime speaker diarization" src="https://external-preview.redd.it/JO_yTxc06ktYf5LFR-Rn-h9sKgRJ8XcsPo1m_3iqmLE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a91a5195fd0960c0d708c2f400bd55c115bba5a" title="Realtime speaker diarization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lonligrin"&gt; /u/Lonligrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=-zpyi1KHOUk&amp;amp;si=qzksOIhsLjo9J8Zp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i3nsbx/realtime_speaker_diarization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T18:57:16+00:00</published>
  </entry>
</feed>
