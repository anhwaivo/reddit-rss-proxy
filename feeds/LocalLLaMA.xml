<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-17T02:58:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ko0d4w</id>
    <title>ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps</title>
    <updated>2025-05-16T13:05:35+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"&gt; &lt;img alt="ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps" src="https://external-preview.redd.it/pEL8WVAo4mDFDpBgOq60y0m4cdA7556rb7t3GIF-8DM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b2baa5a8ddd53ef6c8acdfc70e71cf12f8444d6" title="ValiantLabs/Qwen3-14B-Esper3 reasoning finetune focused on coding, architecture, and DevOps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ValiantLabs/Qwen3-14B-Esper3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0d4w/valiantlabsqwen314besper3_reasoning_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko4be2</id>
    <title>OpenAI Healthbench in MEDIC</title>
    <updated>2025-05-16T15:53:02+00:00</updated>
    <author>
      <name>/u/clechristophe</name>
      <uri>https://old.reddit.com/user/clechristophe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4be2/openai_healthbench_in_medic/"&gt; &lt;img alt="OpenAI Healthbench in MEDIC" src="https://preview.redd.it/b0i7tlhe161f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c679c5052106bbb1daa71e844764210a3db66dc" title="OpenAI Healthbench in MEDIC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the release of OpenAI Healthbench earlier this week, we integrated it into MEDIC framework. Qwen3 models are showing incredible results for their size!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clechristophe"&gt; /u/clechristophe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b0i7tlhe161f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4be2/openai_healthbench_in_medic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4be2/openai_healthbench_in_medic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T15:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko8ltz</id>
    <title>What Makes a Good RP Model?</title>
    <updated>2025-05-16T18:47:54+00:00</updated>
    <author>
      <name>/u/AccomplishedAir769</name>
      <uri>https://old.reddit.com/user/AccomplishedAir769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a roleplay and writing LLM and I’d love to hear what &lt;em&gt;y&lt;/em&gt;ou guys think makes a good RP model. &lt;/p&gt; &lt;p&gt;Before I actually do this, I wanted to ask the RP community here: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any annoying habits you wish RP/creative writing models would finally ditch?&lt;/li&gt; &lt;li&gt;Are there any traits, behaviors, or writing styles you wish more RP/creative writing models had (or avoided)?&lt;/li&gt; &lt;li&gt;What &lt;em&gt;actually&lt;/em&gt; makes a roleplay/creative writing model good, in your opinion? Is it tone, character consistency, memory simulation, creativity, emotional depth? How do you test if a model “feels right” for RP?&lt;/li&gt; &lt;li&gt;Are there any open-source RP/creative writing models or datasets you think set the gold standard?&lt;/li&gt; &lt;li&gt;What are the signs that a model is overfitted vs. well-tuned for RP/creative writing?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m also open to hearing about dataset tips, prompt tricks, or just general thoughts on how to avoid the “sterile LLM voice” and get something that feels alive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedAir769"&gt; /u/AccomplishedAir769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko8ltz/what_makes_a_good_rp_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko8ltz/what_makes_a_good_rp_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko8ltz/what_makes_a_good_rp_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T18:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko387o</id>
    <title>Open source MCP course on GitHub</title>
    <updated>2025-05-16T15:08:40+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The MCP course is free, open source, and with Apache 2 license. &lt;/p&gt; &lt;p&gt;So if you’re working on MCP you can do any of this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;take the course and reuse it for your own educational/ dev advocacy projects&lt;/li&gt; &lt;li&gt;collaborate with us on new units about your projects or interests&lt;/li&gt; &lt;li&gt;star the repo on github so more devs hear about it and join in&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note, some of these options are cooler than others.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/huggingface/mcp-course"&gt;https://github.com/huggingface/mcp-course&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko387o/open_source_mcp_course_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko387o/open_source_mcp_course_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko387o/open_source_mcp_course_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T15:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1knqap9</id>
    <title>Are we finally hitting THE wall right now?</title>
    <updated>2025-05-16T02:41:06+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw in multiple articles today that Llama Behemoth is delayed: &lt;a href="https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html"&gt;https://finance.yahoo.com/news/looks-meta-just-hit-big-214000047.html&lt;/a&gt; . I tried the open models from Llama 4 and felt not that great progress. I am also getting underwhelming vibes from the qwen 3, compared to qwen 2.5. Qwen team used 36 trillion tokens to train these models, which even had trillions of STEM tokens in mid-training and did all sorts of post training, the models are good, but not that great of a jump as we expected. &lt;/p&gt; &lt;p&gt;With RL we definitely got a new paradigm on making the models think before speaking and this has led to great models like Deepseek R1, OpenAI O1, O3 and possibly the next ones are even greater, but the jump from O1 to O3 seems to be not that much, me being only a plus user and have not even tried the Pro tier. Anthropic Claude Sonnet 3.7 is not better than Sonnet 3.5, where the latest version seems to be good but mainly for programming and web development. I feel the same for Google where Gemini 2.5 Pro 1 seemed to be a level above the rest of the models, I finally felt that I could rely on a model and company, then they also rug pulled the model totally with Gemini 2.5 Pro 2 where I do not know how to access the version 1 and they are field testing a lot in lmsys arena which makes me wonder that they are not seeing those crazy jumps as they were touting.&lt;/p&gt; &lt;p&gt;I think Deepseek R2 will show us the ultimate conclusion on this, whether scaling this RL paradigm even further will make models smarter.&lt;/p&gt; &lt;p&gt;Do we really need a new paradigm? Or do we need to go back to architectures like T5? Or totally novel like JEPA from Yann Lecunn, twitter has hated him for not agreeing that the autoregressors can actually lead to AGI, but sometimes I feel it too with even the latest and greatest models do make very apparent mistakes and makes me wonder what would it take to actually have really smart and reliable models.&lt;/p&gt; &lt;p&gt;I love training models using SFT and RL especially GRPO, my favorite, I have even published some work on it and making pipelines for clients, but seems like when used in production for longer, the customer sentiment seems to always go down and not even maintain as well.&lt;/p&gt; &lt;p&gt;What do you think? Is my thinking in this saturation of RL for Autoregressor LLMs somehow flawed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knqap9/are_we_finally_hitting_the_wall_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T02:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1tg5</id>
    <title>If you are comparing models, please state the task you are using them for!</title>
    <updated>2025-05-16T14:10:07+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The amount of posts like &amp;quot;Why is deepseek so much better than qwen 235,&amp;quot; with no information about the task that the poster is comparing the models on, is maddening. ALL models' performance levels vary across domains, and many models are highly domain specific. Some people are creating waifus, some are coding, some are conducting medical research, etc. &lt;/p&gt; &lt;p&gt;The posts read like &amp;quot;The Miata is the absolute superior vehicle over the Cessna Skyhawk. It has been the best driving experience since I used my Rolls Royce as a submarine&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1tg5/if_you_are_comparing_models_please_state_the_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:10:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2gq1</id>
    <title>AM-Thinking-v1</title>
    <updated>2025-05-16T14:37:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt; &lt;img alt="AM-Thinking-v1" src="https://external-preview.redd.it/hpID4tGlVXcsccCrPaCo5PJZ9uVrqES2Dr7pBVPNMCc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d9a6b67fdd30d72dd266f22d019598feded92a1" title="AM-Thinking-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/a-m-team/AM-Thinking-v1"&gt;https://huggingface.co/a-m-team/AM-Thinking-v1&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We release &lt;strong&gt;AM-Thinking‑v1&lt;/strong&gt;, a 32B dense language model focused on enhancing reasoning capabilities. Built on Qwen 2.5‑32B‑Base, AM-Thinking‑v1 shows strong performance on reasoning benchmarks, comparable to much larger MoE models like &lt;strong&gt;DeepSeek‑R1&lt;/strong&gt;, &lt;strong&gt;Qwen3‑235B‑A22B&lt;/strong&gt;, &lt;strong&gt;Seed1.5-Thinking&lt;/strong&gt;, and larger dense model like &lt;strong&gt;Nemotron-Ultra-253B-v1&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.08311"&gt;https://arxiv.org/abs/2505.08311&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://a-m-team.github.io/am-thinking-v1/"&gt;https://a-m-team.github.io/am-thinking-v1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/79z2klmbn51f1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18a3b5a0d06b75e6712891b7c19853ec1de3e737"&gt;https://preview.redd.it/79z2klmbn51f1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18a3b5a0d06b75e6712891b7c19853ec1de3e737&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;I'm not affiliated with the model provider, just sharing the news.&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;System prompt &amp;amp; generation_config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are a helpful assistant. To answer the user’s question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; and &amp;lt;answer&amp;gt; &amp;lt;/answer&amp;gt; tags, respectively, i.e., &amp;lt;think&amp;gt; reasoning process here &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; answer here &amp;lt;/answer&amp;gt;. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;---&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95, &amp;quot;repetition_penalty&amp;quot;: 1.0 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2gq1/amthinkingv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1knv4bq</id>
    <title>Falcon-E: A series of powerful, fine-tunable and universal BitNet models</title>
    <updated>2025-05-16T07:38:42+00:00</updated>
    <author>
      <name>/u/JingweiZUO</name>
      <uri>https://old.reddit.com/user/JingweiZUO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TII announced today the release of Falcon-Edge, a set of compact language models with 1B and 3B parameters, sized at 600MB and 900MB respectively. They can also be reverted back to bfloat16 with little performance degradation.&lt;br /&gt; Initial results show solid performance: better than other small models (SmolLMs, Microsoft bitnet, Qwen3-0.6B) and comparable to Qwen3-1.7B, with 1/4 memory footprint.&lt;br /&gt; They also released a fine-tuning library, &lt;code&gt;onebitllms&lt;/code&gt;: &lt;a href="https://github.com/tiiuae/onebitllms"&gt;https://github.com/tiiuae/onebitllms&lt;/a&gt;&lt;br /&gt; Blogposts: &lt;a href="https://huggingface.co/blog/tiiuae/falcon-edge"&gt;https://huggingface.co/blog/tiiuae/falcon-edge&lt;/a&gt; / &lt;a href="https://falcon-lm.github.io/blog/falcon-edge/"&gt;https://falcon-lm.github.io/blog/falcon-edge/&lt;/a&gt;&lt;br /&gt; HF collection: &lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JingweiZUO"&gt; /u/JingweiZUO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knv4bq/falcone_a_series_of_powerful_finetunable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T07:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko4jsb</id>
    <title>Fastgen - Simple high-throughput inference</title>
    <updated>2025-05-16T16:02:29+00:00</updated>
    <author>
      <name>/u/_mpu</name>
      <uri>https://old.reddit.com/user/_mpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4jsb/fastgen_simple_highthroughput_inference/"&gt; &lt;img alt="Fastgen - Simple high-throughput inference" src="https://external-preview.redd.it/CP0e7IZ7SIX4P9NyHqNMl-joiEybFlPwga41CzF0mCk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38527e7a1ac1e3de5c0c41c1ab00e1a334a68c83" title="Fastgen - Simple high-throughput inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released a tiny (~3kloc) Python library that implements state-of-the-art inference algorithms on GPU and provides performance similar to vLLM. We believe it's a great learning vehicle for inference techniques and the code is quite easy to hack on!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_mpu"&gt; /u/_mpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/facebookresearch/fastgen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4jsb/fastgen_simple_highthroughput_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4jsb/fastgen_simple_highthroughput_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T16:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1koggmm</id>
    <title>My voice dataset creator is now on Colab with a GUI</title>
    <updated>2025-05-17T00:43:45+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"&gt; &lt;img alt="My voice dataset creator is now on Colab with a GUI" src="https://external-preview.redd.it/0-fRWqjlLadVXj5pfYp4_Oe3xgBWE-_rdjVSn7hlohI.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2817183828c9747b960cb2e55c59cfa41f4f9ded" title="My voice dataset creator is now on Colab with a GUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My voice extractor tool is now on Google Colab with a GUI interface. Tested it with one minute of audio and it processed in about 5 minutes on Colab's CPU - much slower than with a GPU, but still works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://colab.research.google.com/github/ReisCook/Voice_Extractor_Colab/blob/main/Voice_Extractor_Colab.ipynb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koggmm/my_voice_dataset_creator_is_now_on_colab_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T00:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko86xz</id>
    <title>Claude Code and Openai Codex Will Increase Demand for Software Engineers</title>
    <updated>2025-05-16T18:30:51+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, everyone who is selling API or selling interfaces, such as OpenAI, Google and Anthropic have been telling that the software engineering jobs will soon be extinct in a few years. I would say that this will not be the case and it might even have the opposite effect in that it will lead to increment and not only increment but even better paid.&lt;/p&gt; &lt;p&gt;We recently saw that Klarna CEO fired tons of people saying that AI will do everything and we are more efficient and so on, but now they are hiring again, and in great numbers. Google is saying that they will create agents that will &amp;quot;vibe code&amp;quot; apps, makes me feel weird to hear from Sir Demis Hassabis, a noble laureate who knows himself the flaws of these autoregressive models deeply. People are fearing, that software engineers and data scientists will lose jobs because the models will be so much better that everyone will code websites in a day.&lt;/p&gt; &lt;p&gt;Recently an acquaintance of mine created an app for his small startups for chefs, another one for a RAG like app but for crypto to help with some document filling stuff. They said that now they can become &amp;quot;vibe coders&amp;quot; and now do not need any technical people, both of these are business graduates and no technical background. After creating the app, I saw their frustration of not being able to change the borders of the boxes that Sonnet 3.7 made for them as they do not know what the border radius is. They subsequently hired people to help with this, and this not only led to weekly projects and high payments, for which they could have asked a well taught and well experienced front end person, they paid more than they should have starting from the beginning. I can imagine that the low hanging fruit is available to everyone now, no doubt, but vibe coding will &amp;quot;hit a wall&amp;quot; of experience and actual field knowledge.&lt;/p&gt; &lt;p&gt;Self driving will not mean that you do not need to drive anymore, but that you can drive better and can be more relaxed as there is another artificial intelligence to help you. In my humble opinion, a researcher working with LLMs, a lot of people will need to hire software engineers and will be willing to pay more than they originally had to as they do not know what they are doing. But in the short term there will definitely be job losses, but the creative and actual specialization knowledge people will not only be safe but thrive. With open source, we all can compliment our specializations.&lt;/p&gt; &lt;p&gt;A few jobs that in my opinion will thrive: data scientists, researchers, optimizers, front end developers, backend developers, LLM developers and teachers of each of these fields. These models will be a blessing to learn easily, if people use them for learning and not just directly vibe coding, and will definitely be a positive sum for the scociety. But after seeing the people next to me, I think that high quality software engineers will not only be in demand, but actively sought after with high salaries and per hourly rates.&lt;/p&gt; &lt;p&gt;I definitely maybe flawed in some senses in my thinking here, please point out so. I am more than happy to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko86xz/claude_code_and_openai_codex_will_increase_demand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko86xz/claude_code_and_openai_codex_will_increase_demand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko86xz/claude_code_and_openai_codex_will_increase_demand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T18:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko4oor</id>
    <title>Qwen: Parallel Scaling Law for Language Models</title>
    <updated>2025-05-16T16:07:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.10475"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4oor/qwen_parallel_scaling_law_for_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4oor/qwen_parallel_scaling_law_for_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T16:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko7v3l</id>
    <title>Style Control will be the default view on the LMArena leaderboard</title>
    <updated>2025-05-16T18:17:08+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko7v3l/style_control_will_be_the_default_view_on_the/"&gt; &lt;img alt="Style Control will be the default view on the LMArena leaderboard" src="https://b.thumbs.redditmedia.com/NAGwp7IHCuytBe6Yq5inwAD1VfyzzoIobYxT9i2YvRk.jpg" title="Style Control will be the default view on the LMArena leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ko7v3l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko7v3l/style_control_will_be_the_default_view_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko7v3l/style_control_will_be_the_default_view_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T18:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1koagwh</id>
    <title>Offline real-time voice conversations with custom chatbots using AI Runner</title>
    <updated>2025-05-16T20:06:51+00:00</updated>
    <author>
      <name>/u/w00fl35</name>
      <uri>https://old.reddit.com/user/w00fl35</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"&gt; &lt;img alt="Offline real-time voice conversations with custom chatbots using AI Runner" src="https://external-preview.redd.it/cSHoRfgGCxZ0WRaZwhiy2L4dmB2Ncgy7iEYxgxHsJTE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=506daa7e710caccec20b3c8724df06d9e0cf2a5c" title="Offline real-time voice conversations with custom chatbots using AI Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w00fl35"&gt; /u/w00fl35 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/n0SaEkXmeaA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koagwh/offline_realtime_voice_conversations_with_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T20:06:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1koak4w</id>
    <title>Don't Sleep on BitNet</title>
    <updated>2025-05-16T20:10:39+00:00</updated>
    <author>
      <name>/u/Arcuru</name>
      <uri>https://old.reddit.com/user/Arcuru</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arcuru"&gt; /u/Arcuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://jackson.dev/post/dont-sleep-on-bitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koak4w/dont_sleep_on_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koak4w/dont_sleep_on_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T20:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kofuse</id>
    <title>ArchGW 0.2.8 is out 🚀 - unifying repeated "low-level" functionality in building LLM apps via a local proxy.</title>
    <updated>2025-05-17T00:11:49+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"&gt; &lt;img alt="ArchGW 0.2.8 is out 🚀 - unifying repeated &amp;quot;low-level&amp;quot; functionality in building LLM apps via a local proxy." src="https://preview.redd.it/gap0dbz2h81f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e218b966553c521564274d6134d94b9521e83f61" title="ArchGW 0.2.8 is out 🚀 - unifying repeated &amp;quot;low-level&amp;quot; functionality in building LLM apps via a local proxy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am thrilled about our latest release: &lt;a href="https://github.com/katanemo/archgw"&gt;Arch 0.2.8&lt;/a&gt;. Initially we handled calls made to LLMs - to unify key management, track spending consistently, improve resiliency and improve model choice - but we just added support for an ingress listener (on the same running process) to handle both ingress an egress functionality that is common and repeated in application code today - now managed by an intelligent local proxy (in a framework and language agnostic way) that makes building AI applications faster, safer and more consistently between teams. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in 0.2.8.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Added support for bi-directional traffic as a first step to support Google's A2A&lt;/li&gt; &lt;li&gt;Improved &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat 3B&lt;/a&gt; LLM for fast routing and common tool calling scenarios&lt;/li&gt; &lt;li&gt;Support for LLMs hosted on Groq&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;🚦 Ro&lt;/code&gt;uting. Engineered with purpose-built &lt;a href="https://huggingface.co/collections/katanemo/arch-function-66f209a693ea8df14317ad68"&gt;LLMs&lt;/a&gt; for fast (&amp;lt;100ms) agent routing and hand-off&lt;/li&gt; &lt;li&gt;&lt;code&gt;⚡ Tools Use&lt;/code&gt;: For common agentic scenarios Arch clarifies prompts and makes tools calls&lt;/li&gt; &lt;li&gt;&lt;code&gt;⛨ Guardrails&lt;/code&gt;: Centrally configure and prevent harmful outcomes and enable safe interactions&lt;/li&gt; &lt;li&gt;&lt;code&gt;🔗 Access t&lt;/code&gt;o LLMs: Centralize access and traffic to LLMs with smart retries&lt;/li&gt; &lt;li&gt;&lt;code&gt;🕵 Observab&lt;/code&gt;ility: W3C compatible request tracing and LLM metrics&lt;/li&gt; &lt;li&gt;&lt;code&gt;🧱 Built on&lt;/code&gt; Envoy: Arch runs alongside app servers as a containerized process, and builds on top of &lt;a href="https://envoyproxy.io"&gt;Envoy's&lt;/a&gt; proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gap0dbz2h81f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kofuse/archgw_028_is_out_unifying_repeated_lowlevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-17T00:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko4gjh</id>
    <title>Drummer's Big Alice 28B v1 - A 100 layer upscale working together to give you the finest creative experience!</title>
    <updated>2025-05-16T15:59:01+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4gjh/drummers_big_alice_28b_v1_a_100_layer_upscale/"&gt; &lt;img alt="Drummer's Big Alice 28B v1 - A 100 layer upscale working together to give you the finest creative experience!" src="https://external-preview.redd.it/RaXPiHRNTSUjMvQHqKyyr5p4_KJP9L2YFlPLat5z5Po.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=261065cb239472639351be9a147285bf23a5bfc3" title="Drummer's Big Alice 28B v1 - A 100 layer upscale working together to give you the finest creative experience!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Big-Alice-28B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4gjh/drummers_big_alice_28b_v1_a_100_layer_upscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko4gjh/drummers_big_alice_28b_v1_a_100_layer_upscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T15:59:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kof8ni</id>
    <title>On the universality of BitNet models</title>
    <updated>2025-05-16T23:41:01+00:00</updated>
    <author>
      <name>/u/Automatic_Truth_6666</name>
      <uri>https://old.reddit.com/user/Automatic_Truth_6666</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt; &lt;img alt="On the universality of BitNet models" src="https://b.thumbs.redditmedia.com/OsDAcsjTSwHMN5CuEUlkPyEBrJv-5Ly3N_qkaYzNlas.jpg" title="On the universality of BitNet models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mrig6j2bc81f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=715576fa0b82337c43c5150bb06950c9a39e45d0"&gt;https://preview.redd.it/mrig6j2bc81f1.png?width=1872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=715576fa0b82337c43c5150bb06950c9a39e45d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the &amp;quot;novelty&amp;quot; of the recent Falcon-E release is that the checkpoints are universal, meaning they can be reverted back to bfloat16 format, llama compatible, with almost no performance degradation. e.g. you can test the 3B bf16 here: &lt;a href="https://chat.falconllm.tii.ae/"&gt;https://chat.falconllm.tii.ae/&lt;/a&gt; and the quality is very decent from our experience (especially on math questions)&lt;br /&gt; This also means in a single pre-training run you can get at the same time the bf16 model and the bitnet counterpart.&lt;br /&gt; This can be interesting from the pre-training perspective and also adoption perspective (not all people want bitnet format), to what extend do you think this &amp;quot;property&amp;quot; of Bitnet models can be useful for the community?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Truth_6666"&gt; /u/Automatic_Truth_6666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kof8ni/on_the_universality_of_bitnet_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T23:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko2mq7</id>
    <title>what happened to Stanford</title>
    <updated>2025-05-16T14:44:17+00:00</updated>
    <author>
      <name>/u/BoringAd6806</name>
      <uri>https://old.reddit.com/user/BoringAd6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt; &lt;img alt="what happened to Stanford" src="https://preview.redd.it/l9ap08t4p51f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99406294d0642388d4c739930b9569d685129d1" title="what happened to Stanford" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoringAd6806"&gt; /u/BoringAd6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l9ap08t4p51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko2mq7/what_happened_to_stanford/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1koccyx</id>
    <title>I just to give love to Mistral ❤️🥐</title>
    <updated>2025-05-16T21:27:29+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Of all the open models, Mistral's offerings (particularly Mistral Small) has to be the one of the most consistent in terms of just getting the task done. &lt;/p&gt; &lt;p&gt;Yesterday wanted to turn a 214 row, 4 column row into a list. Tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Flash 2.5 - worked but stopped short a few times&lt;/li&gt; &lt;li&gt;Chatgpt 4.1 - asked a few questions to clarify,started and stopped&lt;/li&gt; &lt;li&gt;Meta llama 4 - did a good job, but stopped just slight short&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hit up Lè Chat , paste in CSV , seconds later , list done. &lt;/p&gt; &lt;p&gt;In my own experience, I have defaulted to Mistral Small in my chrome extension PromptPaul, and Small handles tools, requests and just about any of the circa 100 small jobs I throw it each day with ease.&lt;/p&gt; &lt;p&gt;Thank you Mistral.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1koccyx/i_just_to_give_love_to_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T21:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1v1k</id>
    <title>I built a tiny Linux OS to make your LLMs actually useful on your machine</title>
    <updated>2025-05-16T14:12:00+00:00</updated>
    <author>
      <name>/u/iluxu</name>
      <uri>https://old.reddit.com/user/iluxu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt; &lt;img alt="I built a tiny Linux OS to make your LLMs actually useful on your machine" src="https://external-preview.redd.it/Opn0lWenfUSxX1FlZaKUoyxIpn8_sSk-rxtkMoj2byo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62958b7668163a6b32bc9aa0eddc4ec07f59c982" title="I built a tiny Linux OS to make your LLMs actually useful on your machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks — I’ve been working on llmbasedos, a minimal Arch-based Linux distro that turns your local environment into a first-class citizen for any LLM frontend (like Claude Desktop, VS Code, ChatGPT+browser, etc).&lt;/p&gt; &lt;p&gt;The problem: every AI app has to reinvent the wheel — file pickers, OAuth flows, plugins, sandboxing… The idea: expose local capabilities (files, mail, sync, agents) via a clean, JSON-RPC protocol called MCP (Model Context Protocol).&lt;/p&gt; &lt;p&gt;What you get: • An MCP gateway (FastAPI) that routes requests • Small Python daemons that expose specific features (FS, mail, sync, agents) • Auto-discovery via .cap.json — your new feature shows up everywhere • Optional offline mode (llama.cpp included), or plug into GPT-4o, Claude, etc.&lt;/p&gt; &lt;p&gt;It’s meant to be dev-first. Add a new capability in under 50 lines. Zero plugins, zero hacks — just a clean system-wide interface for your AI.&lt;/p&gt; &lt;p&gt;Open-core, Apache-2.0 license.&lt;/p&gt; &lt;p&gt;Curious to hear what features you’d build with it — happy to collab if anyone’s down!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iluxu"&gt; /u/iluxu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iluxu/llmbasedos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1v1k/i_built_a_tiny_linux_os_to_make_your_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko6hy7</id>
    <title>When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane.</title>
    <updated>2025-05-16T17:21:55+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"&gt; &lt;img alt="When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane." src="https://preview.redd.it/1fwbjz4zf61f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c05a4e8bffe7ec3f4e56031dc110d91c80808d7b" title="When did small models get so smart? I get really good outputs with Qwen3 4B, it's kinda insane." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can remember, like a few months ago, I ran some of the smaller models with &amp;lt;7B parameters and couldn't even get coherent sentences. This 4B model runs super fast and answered this question perfectly. To be fair, it probably has seen a lot of these examples in it's training data but nonetheless - it's crazy. I only ran this prompt in English to show it here but initially it was in German. Also there, got very well expressed explanations for my question. Crazy that this comes from a 2.6GB file of structured numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1fwbjz4zf61f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko6hy7/when_did_small_models_get_so_smart_i_get_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T17:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko0khr</id>
    <title>Stanford has dropped AGI</title>
    <updated>2025-05-16T13:15:17+00:00</updated>
    <author>
      <name>/u/Abject-Huckleberry13</name>
      <uri>https://old.reddit.com/user/Abject-Huckleberry13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt; &lt;img alt="Stanford has dropped AGI" src="https://external-preview.redd.it/RLiqoJrn4RdLs0J4_egpcYM7T2LlLp_klpSUS3M3qFg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa5717d8c431adaa645d19436f3ab2adbc6cfc8" title="Stanford has dropped AGI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Huckleberry13"&gt; /u/Abject-Huckleberry13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Stanford/Rivermind-AGI-12B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko0khr/stanford_has_dropped_agi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko1iob</id>
    <title>Ollama violating llama.cpp license for over a year</title>
    <updated>2025-05-16T13:57:38+00:00</updated>
    <author>
      <name>/u/op_loves_boobs</name>
      <uri>https://old.reddit.com/user/op_loves_boobs</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/op_loves_boobs"&gt; /u/op_loves_boobs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44003741"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko1iob/ollama_violating_llamacpp_license_for_over_a_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T13:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ko27bi</id>
    <title>Did Standford HuggingFace account got Hacked?</title>
    <updated>2025-05-16T14:26:25+00:00</updated>
    <author>
      <name>/u/ObscuraMirage</name>
      <uri>https://old.reddit.com/user/ObscuraMirage</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObscuraMirage"&gt; /u/ObscuraMirage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0j4j7z8yl51f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ko27bi/did_standford_huggingface_account_got_hacked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-16T14:26:25+00:00</published>
  </entry>
</feed>
