<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-11T16:55:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mmvgsg</id>
    <title>We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)</title>
    <updated>2025-08-10T22:00:51+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt; &lt;img alt="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" src="https://b.thumbs.redditmedia.com/p1o0kjxV46xwAd_9PUtAD07QnkEEvWp1rRev7JI4V2c.jpg" title="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;https://github.com/TesslateAI/Agent-Builder&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a Visual flow builder for multi-agent LLM systems. Drag, drop, connect agents, tools, put agents in patterns, create triggers, work on outputs, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TFrameX&lt;/strong&gt; - The orchestration framework that runs your agents. It has patterns for agent collaboration (sequential, parallel, router, discussion patterns built-in). Agents can call other agents as tools, which opens up supervisor-worker architectures.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; - The visual layer on top of your existing flows and code. ReactFlow-based drag-and-drop interface where you build flows visually, that then compile into a 'flow' that you can save or create new components in real-time. &lt;/p&gt; &lt;h1&gt;Some features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streaming responses&lt;/strong&gt; - Just add &lt;code&gt;streaming=True&lt;/code&gt; to any agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent hierarchies&lt;/strong&gt; - Agents calling agents. Build a CTO agent that delegates to developer agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pattern nesting&lt;/strong&gt; - Put parallel patterns inside sequential patterns inside discussion patterns. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic code registration&lt;/strong&gt; - Add new agents/tools through the UI without restarting anything. You can add this via Python code as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both repos are on GitHub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/TFrameX"&gt;TFrameX&lt;/a&gt; - The framework (has MCP Support)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;Agent-Builder&lt;/a&gt; - The visual builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything's MIT licensed. If you find bugs (you will), open an issue. If you build something cool, share it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmvgsg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T22:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn00j3</id>
    <title>Talking with QWEN Coder 30b</title>
    <updated>2025-08-11T01:28:10+00:00</updated>
    <author>
      <name>/u/1Garrett2010</name>
      <uri>https://old.reddit.com/user/1Garrett2010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Believe me, I wish I shared your enthusiasm, but my experience with QWEN Coder 30b has not been great. I tried building features for a Godot 4 prototype interactively and asked the same questions to OpenAI gpt oss 20b. The solutions and explanations from the OpenAI model were clearly better for my use case, while QWEN often felt like talking to models from years ago. The only upside was that even with 8,000 tokens, QWEN stayed reasonably fast on my machine, while the OpenAI one slowed down a lot.&lt;/p&gt; &lt;p&gt;Maybe I am using QWEN wrong? Is interactive use not recommended? Should prompts ask for complete games or full code chunks? Examples like the Mario clone or Snake are not convincing to me. For custom work, I think the real test is in how flexible a model can be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Garrett2010"&gt; /u/1Garrett2010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T01:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7o6e</id>
    <title>Whisper Key - Simple local STT app for Windows with global hotkey (auto-paste, auto-ENTER)</title>
    <updated>2025-08-11T08:44:00+00:00</updated>
    <author>
      <name>/u/PinW</name>
      <uri>https://old.reddit.com/user/PinW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a little STT app I made to learn vibe coding (Windows only for now).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PinW/whisper-key-local/"&gt;https://github.com/PinW/whisper-key-local/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the processing is local, and it doesn't beautify the transcription either, so &lt;strong&gt;the main use case is talking to LLMs&lt;/strong&gt; (I use it with Claude Code, ChatGPT, etc.)&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CTRL+WIN to start recording&lt;/li&gt; &lt;li&gt;CTRL to stop, transcribe, and auto-paste&lt;/li&gt; &lt;li&gt;ALT to stop, transcribe, auto-paste, and auto-send (ENTER)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pasting/sending via key press simulation&lt;/li&gt; &lt;li&gt;Transcription via faster-whisper with TEN VAD supporting&lt;/li&gt; &lt;li&gt;Model size control (I recommend `base.en`) via system tray menu&lt;/li&gt; &lt;li&gt;Many more settings in config file&lt;/li&gt; &lt;li&gt;Runs offline outside of model downloads&lt;/li&gt; &lt;li&gt;Uses CPU by default (can also config to CUDA but I haven't tested)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And it's free!&lt;/p&gt; &lt;p&gt;Portable app here: &lt;a href="https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip"&gt;https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it out, would appreciate any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PinW"&gt; /u/PinW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mngpna</id>
    <title>Microsoft Edge Proposes Web Model Context API</title>
    <updated>2025-08-11T15:52:18+00:00</updated>
    <author>
      <name>/u/-json-</name>
      <uri>https://old.reddit.com/user/-json-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mngpna/microsoft_edge_proposes_web_model_context_api/"&gt; &lt;img alt="Microsoft Edge Proposes Web Model Context API" src="https://external-preview.redd.it/RQHQV8bgli4Kw9ABS63OM3ee5MAkmYUuh38EshjaHIQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0835384964e9c055a4f3d404c9b7587c303756c" title="Microsoft Edge Proposes Web Model Context API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-json-"&gt; /u/-json- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MicrosoftEdge/MSEdgeExplainers/blob/main/WebModelContext/explainer.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mngpna/microsoft_edge_proposes_web_model_context_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mngpna/microsoft_edge_proposes_web_model_context_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmuw5o</id>
    <title>Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)</title>
    <updated>2025-08-10T21:36:49+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt; &lt;img alt="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" src="https://a.thumbs.redditmedia.com/bopafOnq5xsp2CpoFO3L_ayEyeeQLFGBm-_z9UfQKw0.jpg" title="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7"&gt;https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm testing many LLMs on a dataset of official quizzes (5 choices) taken by Italian students after finishing Med School and starting residency. &lt;/p&gt; &lt;p&gt;The human performance was ~67% this year and the best student had a ~94% (out of 16 000 students) &lt;/p&gt; &lt;p&gt;In this test I benchmarked these models on all quizzes from the past 6 years. Multimodal models were tested on all quizzes (including some containing images) while those that worked only with text were not (the % you see is already corrected). &lt;/p&gt; &lt;p&gt;I also tested their sycophancy (tendency to agree with the user) by telling them that I believed the correct answer was a wrong one. &lt;/p&gt; &lt;p&gt;For now I only tested them on models available on openrouter, but I plan to add models such as MedGemma. Do you reccomend doing so on Huggingface or google Vertex? Also suggestions for other models are appreciated. I especially want to add more small models that I can run locally (I have a 6GB RTX 3060). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnevw3</id>
    <title>PCI/MMIO/BAR resource exhaustion issues with 2x PRO 6000 Workstation and 2x RTX A6000 GPUs on a Gigabyte-based EPYC server. Any of you grizzled old multi-GPU miners got some nuggets of wisdom?</title>
    <updated>2025-08-11T14:43:29+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick note: there is no AI slop in this post. Any slop you find was lovingly crafted by a pair of human hands, the old school way. All mistakes are mine. &lt;/p&gt; &lt;p&gt;I've posted a similar request over at &lt;a href="/r/gigabyte"&gt;/r/gigabyte&lt;/a&gt;, but I figured there's a lot of old-timers around here that have solved trickier problems than this in multi-GPU setups. I work with a few old hackers, but this problem is outside any of our areas of expertise so here we are. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary of Issue&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each time we boot the server we can use up to 3 of the 4 installed GPUs, but never all 4, due to CUDA initialization errors running vllm or llama.cpp. We seem to always get a &amp;quot;bad pair&amp;quot; of GPUs, which I'll explain more in a minute. First some context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Server Config&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: Gigabyte MZ33-AR1 motherboard running latest firmware. &lt;ul&gt; &lt;li&gt;Resizeable BAR is enabled in BIOS.&lt;/li&gt; &lt;li&gt;There is no option for &amp;quot;Above 4G encoding&amp;quot; in the BIOS despite the manual saying there is.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD EPYC 9B45&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 768GB DDR 6400 in 12x 64GB RDIMMS; slots populated with the correct pattern according to user manual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU 0&lt;/strong&gt;: NVidia RTX A6000 48GB connected via PCIe 4.0 riser cable to x16 PCIe 5.0 slot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU 1&lt;/strong&gt;: NVidia RTX A6000 48GB connected via PCIe 4.0 riser cable to x16 PCIe 5.0 slot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU 2&lt;/strong&gt;: NVidia RTX PRO 6000 Workstation 96GB Blackwell connected via PCIe 5.0 riser cable to x16 PCIe 5.0 slot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU 3&lt;/strong&gt;: NVidia RTX PRO 6000 Workstation 96GB Blackwell connected via PCIe 5.0 riser cable to x16 PCIe 5.0 slot&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU&lt;/strong&gt;: Super Flower 240V / 2800W PSU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu Linux LTS 24.x&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All 4 GPUs work on every boot. Individually they're all 100% known good 100% of the time. We can use any single GPU on any boot, guaranteed. The hardware is solid. The PCIe 4.0 and 5.0 riser cables (I know, I know, very crypto bro) are known good. The physical hardware, connections, etc. are thoroughly tested and trusted as reliable.&lt;/p&gt; &lt;p&gt;We can see that &lt;code&gt;nvidia-smi&lt;/code&gt; reports all 4 cards are detected and present:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ nvidia-smi -L GPU 0: NVIDIA RTX A6000 (UUID: xx) GPU 1: NVIDIA RTX A6000 (UUID: xx) GPU 2: NVIDIA RTX PRO 6000 Blackwell Workstation Edition (UUID: xx) GPU 3: NVIDIA RTX PRO 6000 Blackwell Workstation Edition (UUID: xx) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;More on the Issue&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The real heart of the issue is that there is always a &amp;quot;bad pair&amp;quot; of GPUs that refuse to &lt;em&gt;work together&lt;/em&gt;. It seems that the bad pair is randomly affected per boot and is always either both of the A6000s or both of the Blackwells, but never one A6000 and one Blackwell (we speculate this is due to the physical ordering of the cards attached to the motherboard; we have not (cannot) reorder the GPUs due to the left/right orientation of the PCIe 4.0/5.0 riser cables).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let's say that we've just booted the server and have discovered that the &amp;quot;bad pair&amp;quot; is GPUs 2 and 3, the Blackwell cards. Using the GPU layout from &lt;code&gt;nvidia-smi -L&lt;/code&gt; above, we can state that for this boot:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# GOOD: Running llama.cpp without the bad pair will allow CUDA to initialize export VISIBLE_CUDA_DEVICES=0,1 export VISIBLE_CUDA_DEVICES=1,2 export VISIBLE_CUDA_DEVICES=0,1,2 export VISIBLE_CUDA_DEVICES=0,1,3 # BAD: Running llama.cpp with the bad pair will cause CUDA to fail during initialization export VISIBLE_CUDA_DEVICES=0,2,3 export VISIBLE_CUDA_DEVICES=1,2,3 export VISIBLE_CUDA_DEVICES=2,3 export VISIBLE_CUDA_DEVICES=0,1,2,3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the &amp;quot;bad pair&amp;quot; is active then llama.cpp (or vLLM, it doesn't matter, the result is the same) fails:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ export CUDA_VISIBLE_DEVICES=0,1,2,3 # Both A6000s and both Blackwells $ build/bin/llama-server ... # args here ggml_cuda_init: failed to initialize CUDA: initialization error warning: no usable GPU found, --gpu-layers option will be ignored &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Some Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any GPU combination that excludes the &amp;quot;bad pair&amp;quot; allows CUDA to initialize as normal. &lt;/li&gt; &lt;li&gt;This includes using &lt;em&gt;only one&lt;/em&gt; of the GPUs in the bad pair: it will work just fine. The failure only occurs when &lt;em&gt;both&lt;/em&gt; of the GPUs in the bad pair are used at the same time.&lt;/li&gt; &lt;li&gt;The bad pair seems to be randomly selected every reboot.&lt;/li&gt; &lt;li&gt;Disabling resizeable BAR in the BIOS causes the server to fail to POST.&lt;/li&gt; &lt;li&gt;Disabling IOMMU in the BIOS has no effect on the issue.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Supporting Data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's some cleaned data from &lt;code&gt;lspci&lt;/code&gt; relating to GPUs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GA102GL [RTX A6000] (rev a1) (prog-if 00 [VGA controller]) Subsystem: NVIDIA Corporation GA102GL [RTX A6000] Physical Slot: 17 IOMMU group: 52 Region 0: Memory at e0000000 (32-bit, non-prefetchable) [size=16M] Region 1: Memory at 14f000000000 (64-bit, prefetchable) [size=64G] Region 3: Memory at 150000000000 (64-bit, prefetchable) [size=32M] Region 5: I/O ports at 2000 [size=128] Expansion ROM at e1000000 [virtual] [disabled] [size=512K] Capabilities: [bb0 v1] Physical Resizable BAR BAR 0: current size: 16MB, supported: 16MB BAR 1: current size: 64GB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB BAR 3: current size: 32MB, supported: 32MB 21:00.0 VGA compatible controller: NVIDIA Corporation GA102GL [RTX A6000] (rev a1) (prog-if 00 [VGA controller]) Subsystem: Dell GA102GL [RTX A6000] Physical Slot: 25 IOMMU group: 72 Region 0: Memory at b9000000 (32-bit, non-prefetchable) [size=16M] Region 1: Memory at 18f000000000 (64-bit, prefetchable) [size=64G] Region 3: Memory at 190000000000 (64-bit, prefetchable) [size=32M] Region 5: I/O ports at 4000 [size=128] Expansion ROM at ba000000 [virtual] [disabled] [size=512K] Capabilities: [bb0 v1] Physical Resizable BAR BAR 0: current size: 16MB, supported: 16MB BAR 1: current size: 64GB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB BAR 3: current size: 32MB, supported: 32MB a7:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 52) (prog-if 00 [VGA controller]) Subsystem: Gigabyte Technology Co., Ltd ASPEED Graphics Family IOMMU group: 32 Region 0: Memory at c0000000 (32-bit, non-prefetchable) [size=64M] Region 1: Memory at c4000000 (32-bit, non-prefetchable) [size=256K] Expansion ROM at 000c0000 [virtual] [disabled] [size=128K] c1:00.0 VGA compatible controller: NVIDIA Corporation Device 2bb1 (rev a1) (prog-if 00 [VGA controller]) Subsystem: NVIDIA Corporation Device 204b Physical Slot: 9 IOMMU group: 38 Region 0: Memory at cc000000 (32-bit, non-prefetchable) [size=64M] Region 1: Memory at 8e000000000 (64-bit, prefetchable) [size=128G] Region 3: Memory at 90000000000 (64-bit, prefetchable) [size=32M] Region 5: I/O ports at e000 [size=128] Expansion ROM at d0000000 [virtual] [disabled] [size=512K] Capabilities: [134 v1] Physical Resizable BAR BAR 1: current size: 128GB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB 128GB e1:00.0 VGA compatible controller: NVIDIA Corporation Device 2bb1 (rev a1) (prog-if 00 [VGA controller]) Subsystem: NVIDIA Corporation Device 204b Physical Slot: 1 IOMMU group: 10 Region 0: Memory at d4000000 (32-bit, non-prefetchable) [size=64M] Region 1: Memory at 4e000000000 (64-bit, prefetchable) [size=128G] Region 3: Memory at 50000000000 (64-bit, prefetchable) [size=32M] Region 5: I/O ports at f000 [size=128] Expansion ROM at d8000000 [virtual] [disabled] [size=512K] Capabilities: [134 v1] Physical Resizable BAR BAR 1: current size: 128GB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB 128GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;BAR errors in dmesg:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ sudo dmesg | grep 'BAR ' [ 1.430321] pci 0000:e1:00.0: BAR 0 [mem 0xd4000000-0xd7ffffff] [ 1.430323] pci 0000:e1:00.0: BAR 1 [mem 0x4e000000000-0x4ffffffffff 64bit pref] [ 1.430326] pci 0000:e1:00.0: BAR 3 [mem 0x50000000000-0x50001ffffff 64bit pref] [ 1.430328] pci 0000:e1:00.0: BAR 5 [io 0xf000-0xf07f] [ 1.430556] pci 0000:e1:00.1: BAR 0 [mem 0xd8080000-0xd8083fff] [ 1.431129] pci 0000:e2:00.4: BAR 0 [mem 0xd8300000-0xd83fffff 64bit] [ 1.433015] pci 0000:e3:00.0: BAR 5 [mem 0xd8201000-0xd82017ff] [ 1.433147] pci 0000:e3:00.1: BAR 5 [mem 0xd8200000-0xd82007ff] [ 1.442014] pci 0000:a3:00.0: BAR 0 [mem 0xc4600000-0xc4603fff 64bit] [ 1.443976] pci 0000:a4:00.0: BAR 0 [mem 0xc4500000-0xc4503fff 64bit] [ 1.444284] pci 0000:a5:00.0: BAR 0 [mem 0xd0080110000-0xd008011ffff 64bit pref] [ 1.444288] pci 0000:a5:00.0: BAR 2 [mem 0xd0080000000-0xd00800fffff 64bit pref] [ 1.444291] pci 0000:a5:00.0: BAR 4 [mem 0xd0080122000-0xd0080123fff 64bit pref] [ 1.444511] pci 0000:a5:00.1: BAR 0 [mem 0xd0080100000-0xd008010ffff 64bit pref] [ 1.444511] pci 0000:a5:00.1: BAR 2 [mem 0xd007ff00000-0xd007fffffff 64bit pref] [ 1.444511] pci 0000:a5:00.1: BAR 4 [mem 0xd0080120000-0xd0080121fff 64bit pref] [ 1.446245] pci 0000:a7:00.0: BAR 0 [mem 0xc0000000-0xc3ffffff] [ 1.446247] pci 0000:a7:00.0: BAR 1 [mem 0xc4000000-0xc403ffff] [ 1.446250] pci 0000:a7:00.0: BAR 2 [io 0xc000-0xc07f] [ 1.446364] pci 0000:a8:00.5: BAR 2 [mem 0xc4200000-0xc42fffff] [ 1.446364] pci 0000:a8:00.5: BAR 5 [mem 0xc4300000-0xc4301fff] [ 1.452540] pci 0000:01:00.0: BAR 0 [mem 0xe0000000-0xe0ffffff] [ 1.452542] pci 0000:01:00.0: BAR 1 [mem 0x14f000000000-0x14ffffffffff 64bit pref] [ 1.452545] pci 0000:01:00.0: BAR 3 [mem 0x150000000000-0x150001ffffff 64bit pref] [ 1.452547] pci 0000:01:00.0: BAR 5 [io 0x2000-0x207f] [ 1.452755] pci 0000:01:00.1: BAR 0 [mem 0xe1080000-0xe1083fff] [ 1.461073] pci 0000:41:00.0: BAR 0 [mem 0xb6200000-0xb6203fff 64bit] [ 1.461344] pci 0000:44:00.4: BAR 0 [mem 0xb6100000-0xb61fffff 64bit] [ 1.461560] pci 0000:45:00.0: BAR 5 [mem 0xb6001000-0xb60017ff] [ 1.461694] pci 0000:45:00.1: BAR 5 [mem 0xb6000000-0xb60007ff] [ 1.471993] pci 0000:c1:00.0: BAR 0 [mem 0xcc000000-0xcfffffff] [ 1.471996] pci 0000:c1:00.0: BAR 1 [mem 0x8e000000000-0x8ffffffffff 64bit pref] [ 1.471998] pci 0000:c1:00.0: BAR 3 [mem 0x90000000000-0x90001ffffff 64bit pref] [ 1.472001] pci 0000:c1:00.0: BAR 5 [io 0xe000-0xe07f] [ 1.472212] pci 0000:c1:00.1: BAR 0 [mem 0xd0080000-0xd0083fff] [ 1.477992] pci 0000:21:00.0: BAR 0 [mem 0xb9000000-0xb9ffffff] [ 1.477994] pci 0000:21:00.0: BAR 1 [mem 0x18f000000000-0x18ffffffffff 64bit pref] [ 1.477997] pci 0000:21:00.0: BAR 3 [mem 0x190000000000-0x190001ffffff 64bit pref] [ 1.477999] pci 0000:21:00.0: BAR 5 [io 0x4000-0x407f] [ 1.478218] pci 0000:21:00.1: BAR 0 [mem 0xba080000-0xba083fff] [ 1.491122] pnp 00:04: disabling [io 0xfe00-0xfefe] because it overlaps 0000:e0:01.1 BAR 13 [io 0xf000-0xffff] [ 1.509602] pci 0000:a7:00.0: BAR 2 [io size 0x0080]: can't assign; no space [ 1.509603] pci 0000:a7:00.0: BAR 2 [io size 0x0080]: failed to assign &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Does anybody have ideas for debugging, diagnosing, or fixing the problem? &lt;/p&gt; &lt;p&gt;Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnevw3/pcimmiobar_resource_exhaustion_issues_with_2x_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnevw3/pcimmiobar_resource_exhaustion_issues_with_2x_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnevw3/pcimmiobar_resource_exhaustion_issues_with_2x_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T14:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnguv9</id>
    <title>Akhil-Theerthala/Kuvera-8B-qwen3-v0.2.1 · Hugging Face</title>
    <updated>2025-08-11T15:57:35+00:00</updated>
    <author>
      <name>/u/The-Silvervein</name>
      <uri>https://old.reddit.com/user/The-Silvervein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnguv9/akhiltheerthalakuvera8bqwen3v021_hugging_face/"&gt; &lt;img alt="Akhil-Theerthala/Kuvera-8B-qwen3-v0.2.1 · Hugging Face" src="https://external-preview.redd.it/rHdqAu8OP9ctEO4GaMG7eJGP27NSaRtxXat3UWEiNq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3b2be00c89c9cb8692ee4b1e9048f599e5a61d3" title="Akhil-Theerthala/Kuvera-8B-qwen3-v0.2.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally releasing the next step in the personal finance advisor model. The models are still trained on US-specific data, yet, they should perform better than the previous versions of the model. &lt;/p&gt; &lt;p&gt;I have also trained a smaller, 4B version of the model that can be checked &lt;a href="https://huggingface.co/Akhil-Theerthala/Kuvera-4B-unsloth-gemma3"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am actively looking to collaborate with others who are passionate about analyzing and improving the quality of personal finance advice generated by large language models. If this sounds interesting to you, please reach out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Silvervein"&gt; /u/The-Silvervein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Akhil-Theerthala/Kuvera-8B-qwen3-v0.2.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnguv9/akhiltheerthalakuvera8bqwen3v021_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnguv9/akhiltheerthalakuvera8bqwen3v021_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mngl7i</id>
    <title>How does --n-cpu-moe and --cpu-moe params help over --ngl=999 along with --ot=regex_to_offload_ffn_on_CPU in llama.cpp?</title>
    <updated>2025-08-11T15:47:50+00:00</updated>
    <author>
      <name>/u/Rohit_RSS</name>
      <uri>https://old.reddit.com/user/Rohit_RSS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been reading that these new flags ( --n-cpu-moe and --cpu-moe) are very useful. But how? If I'm not wrong, these new flags help us offload a moe layers on CPU, but our goal is to offload these layers on GPU, right? My understanding is, we max out all layers on GPU, then selectively offload ffn tensors on CPU so attn tensors stays on GPU, for better performance. Please help me understand these new flags. &lt;/p&gt; &lt;p&gt;Edit-1: If --ngl targets complete layer to offload on GPU. What is the target of 'moe' in these new flags? Is it ffn or attn or something else? If goal was to add simplicity then they could have added a flag to define no. of attn to offload on GPU, instead. I am sure these new flags wont dynamically load/unload the layers/tensors at runtime, right? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rohit_RSS"&gt; /u/Rohit_RSS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mngl7i/how_does_ncpumoe_and_cpumoe_params_help_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mngl7i/how_does_ncpumoe_and_cpumoe_params_help_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mngl7i/how_does_ncpumoe_and_cpumoe_params_help_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:47:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6ks4</id>
    <title>Baichuan-M2-32B / Medical-enhanced reasoning model</title>
    <updated>2025-08-11T07:31:15+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt; &lt;img alt="Baichuan-M2-32B / Medical-enhanced reasoning model" src="https://external-preview.redd.it/KKdyt2ZeDreCYgeAuxBjKIzpGTVT0m7lArYCwUUEez0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b150247e9d9e2c506d93c66add9d74e20472849f" title="Baichuan-M2-32B / Medical-enhanced reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baichuan-M2-32B is Baichuan AI's medical-enhanced reasoning model, the second medical model released by Baichuan. Designed for real-world medical reasoning tasks, this model builds upon Qwen2.5-32B with an innovative Large Verifier System. Through domain-specific fine-tuning on real-world medical questions, it achieves breakthrough medical performance while maintaining strong general capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M2-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnfomq</id>
    <title>Searching actually viable alternative to Ollama</title>
    <updated>2025-08-11T15:13:53+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;as we've all figured out by now, Ollama is certainly not the best way to go. Yes, it's simple, but there are so many alternatives out there which either outperform Ollama or just work with broader compatibility. So I said to myself, &amp;quot;screw it&amp;quot;, I'm gonna try that out, too.&lt;/p&gt; &lt;p&gt;Unfortunately, it turned out to be everything but simple. I need an alternative that...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;implements model swapping (loading/unloading on the fly, dynamically) just like Ollama does&lt;/li&gt; &lt;li&gt;exposes an OpenAI API endpoint&lt;/li&gt; &lt;li&gt;is open-source&lt;/li&gt; &lt;li&gt;can take pretty much any GGUF I throw at it&lt;/li&gt; &lt;li&gt;is easy to set up and spins up quickly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I looked at a few alternatives already. vLLM seems nice, but is quite the hassle to set up. It threw a lot of errors I simply did not have the time to look for, and I want a solution that &lt;em&gt;just works&lt;/em&gt;. LM Studio is closed and their open-source CLI still mandates usage of the closed LM Studio application...&lt;/p&gt; &lt;p&gt;Any go-to recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnfomq/searching_actually_viable_alternative_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T15:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn4xzz</id>
    <title>huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face</title>
    <updated>2025-08-11T05:47:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt; &lt;img alt="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" src="https://external-preview.redd.it/C7Cl5waSbnvCgBEqBkwRMcfcMS_U7KCkSFBsZHxrfV8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74b5078777884f1d012627cfd338116b92e0fed6" title="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably the first finetune of 120b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huizimao/gpt-oss-120b-uncensored-bf16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T05:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn9z3d</id>
    <title>Qwen3-30B-A3B-Instruct-2507@Q8_0 vs GLM-4.5-Air@UD-Q2_K_XL</title>
    <updated>2025-08-11T11:05:27+00:00</updated>
    <author>
      <name>/u/DanielusGamer26</name>
      <uri>https://old.reddit.com/user/DanielusGamer26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With this configuration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Ryzen 5900x &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;RTX 5060Ti 16GB &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB DDR4 RAM @ 3600MHz &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NVMe drive with ~2GB/s read speed when models are offloaded to disk &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Should I use &lt;code&gt;Qwen3-30B-A3B-Instruct-2507-Q8_0&lt;/code&gt; or &lt;code&gt;GLM-4.5-Air-UD-Q2_K_XL&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;Considering I typically use no more than 16k of context and usually ask trivia-style questions while studying—requesting explanations of specific concepts with excerpts from books or web research as context. &lt;/p&gt; &lt;p&gt;I know these are models of completely different magnitudes (~100B vs 30B), but they're roughly similar in size (GLM being slightly larger and potentially requiring more disk offloading). Could the Q2_K quantization degrade performance so severely that the smaller, higher-precision Qwen3 model would perform better?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Translated with Qwen3-30B-A3B&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielusGamer26"&gt; /u/DanielusGamer26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T11:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnh0s5</id>
    <title>Llama.cpp Vulkan is awesome, It gave new life to my old RX580</title>
    <updated>2025-08-11T16:03:21+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a new computer and instead of buying a GPU I decided to give my old RX580 8GB a try for running inference. I had it laying around unused.&lt;/p&gt; &lt;p&gt;My PC specs are not crazy, its b850 motherboard, Ryzen 7700X and a b580. My total cost was about 700 dollars.&lt;/p&gt; &lt;p&gt;Tried running Qwen3 30 b with about 20 layers offloaded to the GPU and got 24 tokes a second. Here is my command&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --n_gpu_layers 20 --ctx-size 16000 --model ../../../models/Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Adding top_p and top_k and temp slows down the inference by about 10 tokens a second, not sure why. &lt;/p&gt; &lt;p&gt;&lt;code&gt;slot print_timing: id 0 | task 0 | prompt eval time = 559.81 ms / 13 tokens ( 43.06 ms per token, 23.22 tokens per second) eval time = 30875.68 ms / 743 tokens ( 41.56 ms per token, 24.06 tokens per second) total time = 31435.49 ms / 756 tokens &lt;/code&gt;&lt;/p&gt; &lt;p&gt;My RX580 is actually useful to me now, and it worked out of the box with Linux Mint!&lt;/p&gt; &lt;p&gt;With Vulkan being this good now, you can actually build a decent localllama build for about 7-800 dollars. Very excited for the future of local llms!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnh0s5/llamacpp_vulkan_is_awesome_it_gave_new_life_to_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmy738</id>
    <title>Repost But Just Wanted to Fix the Image</title>
    <updated>2025-08-11T00:02:32+00:00</updated>
    <author>
      <name>/u/KlutzyWay7692</name>
      <uri>https://old.reddit.com/user/KlutzyWay7692</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt; &lt;img alt="Repost But Just Wanted to Fix the Image" src="https://preview.redd.it/tq7hvht17aif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2ae9a4c55d6ab31997795fcac8ccb8283ae40bb" title="Repost But Just Wanted to Fix the Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KlutzyWay7692"&gt; /u/KlutzyWay7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tq7hvht17aif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T00:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7plv</id>
    <title>Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models</title>
    <updated>2025-08-11T08:46:35+00:00</updated>
    <author>
      <name>/u/Gad_3dart</name>
      <uri>https://old.reddit.com/user/Gad_3dart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt; &lt;img alt="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" src="https://preview.redd.it/8ib7o8elncif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e511e3e8857eff67e6a0c38d17d2fb76fd260819" title="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My friend and I are super excited to share our latest work with you. Recently, we’ve been focusing on improving &lt;strong&gt;multilingual capabilities&lt;/strong&gt;, with a special emphasis on &lt;strong&gt;bilingual French–English&lt;/strong&gt; performance.&lt;/p&gt; &lt;p&gt;As you probably know, English dominates the NLP world, and performance in many other languages can be significantly worse. Our research shows that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It’s possible to close much of the performance gap between English and other languages with proper post-training and a carefully curated dataset. We even achieved, as far as we know, SoTa results for models&amp;lt;2B on several French benchmarks&lt;/li&gt; &lt;li&gt;This can be done &lt;strong&gt;without sacrificing&lt;/strong&gt; high performance in English benchmarks, and can even improve some of them thanks to cross-lingual transfer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To demonstrate this, we’re releasing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-0.6B-Instruct"&gt;Luth-0.6B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-1.7B-Instruct"&gt;Luth-1.7B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/luth-sft"&gt;Luth-SFT dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/scholar"&gt;Scolar dataset&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We go into more detail in our Hugging Face blog post here:&lt;br /&gt; &lt;a href="https://huggingface.co/blog/MaxLSB/luth"&gt;https://huggingface.co/blog/MaxLSB/luth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love feedback, benchmarks, and any multilingual test cases you throw at these models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gad_3dart"&gt; /u/Gad_3dart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ib7o8elncif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn98w0</id>
    <title>Vllm documentation is garbage</title>
    <updated>2025-08-11T10:23:37+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wtf is this documentation, vllm? Incomplete and so cluttered. You need someone to help with your shtty documentation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T10:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8l69</id>
    <title>Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now</title>
    <updated>2025-08-11T09:43:41+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt; &lt;img alt="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" src="https://b.thumbs.redditmedia.com/ea0TMc1wgQgQw7ly692klA1IbdpyyTri9nJucym3uOo.jpg" title="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a re-distill of my SVD based distillation of qwen3 coder 480b into qwen 3 coder 30b. I fixed a bug that caused the MoE distillation to not actually distill so v1 did not distill the MoE layers properly. I also added SLERP and procrustes alignment to the distillation script alongside DARE (pretty much just cleans up the noise when making the lora) which seems to have produced a much better model. SVD distillation is a data-free distillation method I have not seen anyone do for a opensource model although ive seen a paper on it so its been done before. Its a really efficient distillation method it took 4 hours to distill the full 900+GB qwen3 coder 480b model into the unquantized qwen3 coder 30b model on 2x 3090's. The script distills and then creates a large rank 2048 lora (using the maximum rank for lora on SVD seems to be required to capture as much information as possible since its purely mathematical) and then I merged it with the 30b and then quantized. Ill post the github link for the scripts but it will be a bit until I post the updated scripts since its 4am and I should probably go to sleep lol. This has taken around 100 hours or more of research and testing script after script to get to this point, I think it was worth it, hopefully it will work well for you as well. I have not tested it on very complex code but it should be better at more than just what I tested it with since pretty much the weights themselfs have been distilled. Also Qwen models really love to put that one guy as the cover photo in alot of the dev portfolio website prompts I tested. I guess thats what a dev with 30 years of experience looks like in the AI stock photo world lol. The fintrack website was just 3 prompts and most things work. Its around 2000 lines of code for it. Heres the model page and github &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mn8l69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnd144</id>
    <title>Am I the only one who never really liked Ollama?</title>
    <updated>2025-08-11T13:30:22+00:00</updated>
    <author>
      <name>/u/a_normal_user1</name>
      <uri>https://old.reddit.com/user/a_normal_user1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all that happens with it now and them wanting people to make accounts to use certain features(which kinda defeats the purpose of it) am I the only one who thought that it's really not the best? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_normal_user1"&gt; /u/a_normal_user1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5fe6</id>
    <title>Apple patents matmul technique in GPU</title>
    <updated>2025-08-11T06:17:07+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US452614511&amp;amp;_cid=P12-M8WPOS-61919-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnhgt0</id>
    <title>GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks</title>
    <updated>2025-08-11T16:19:43+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt; &lt;img alt="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" src="https://preview.redd.it/jw671veezeif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ef1b882c2760541b723f2922a88f046fea21c80" title="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released their first open models since GPT-2, and GPT-OSS-120B is now the best open-weight model on our real-world TaskBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better completion performance overall compared to other open-weight models like Kimi-K2 and DeepSeek-R1, while being roughly 1/10th the size. Cheaper, better, faster.&lt;/li&gt; &lt;li&gt;Relative to closed-source models, it performs like smaller frontier models such as o4-mini or previous-generation top tier models like Claude-3.7.&lt;/li&gt; &lt;li&gt;Clearly optimized for agentic use cases, it’s close to Sonnet-4 on our agentic benchmarks and could be a strong main agent model.&lt;/li&gt; &lt;li&gt;Works more like an action model than a chat or knowledge model. Multi-lingual performance is limited, and it hallucinates more on world knowledge, so it benefits from retrieval grounding and pairing with another model for multi-lingual scenarios.&lt;/li&gt; &lt;li&gt;Context recall is decent but weaker than top frontier models, so it’s better suited for shorter or carefully managed context windows.&lt;/li&gt; &lt;li&gt;Excels when paired with strong context engineering and agentic engineering, where each task completion reliably feeds into the next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, this model looks to be a real gem and will likely inject more energy into open-source models.&lt;/p&gt; &lt;p&gt;We’ve published the full benchmark results, including GPT-5, mini, and nano, and our task categories and eval methods here: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those building with it, anyone else seeing similar strengths/weaknesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw671veezeif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnedxo</id>
    <title>SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China</title>
    <updated>2025-08-11T14:24:03+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt; &lt;img alt="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" src="https://b.thumbs.redditmedia.com/euatxyG4VjgoW0nwCU8kLUP0naLHxC18HHdMXCH_NiU.jpg" title="SOTA on 41 benchmarks! GLM-4.5V -- A new open-source VLM from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two weeks ago, China's &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; open-sourced the &lt;a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b"&gt;GLM-4.5 model&lt;/a&gt;. Now, building on GLM-4.5’s language architecture, they’ve trained a new VLM—&lt;a href="https://huggingface.co/collections/zai-org/glm-45v-68999032ddf8ecf7dcdbc102"&gt;GLM-4.5V&lt;/a&gt; — which achieved SOTA in ​&lt;strong&gt;41 out of 42 benchmarks&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Absolutely insane! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc"&gt;https://preview.redd.it/zvok5ul5geif1.jpg?width=5188&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4a692340ca8cf3126cffd7d9c091d296dc02acbc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnedxo/sota_on_41_benchmarks_glm45v_a_new_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T14:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8ij6</id>
    <title>gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)</title>
    <updated>2025-08-11T09:38:57+00:00</updated>
    <author>
      <name>/u/chikengunya</name>
      <uri>https://old.reddit.com/user/chikengunya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt; &lt;img alt="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" src="https://preview.redd.it/0lv50zsy1dif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea6791432a529ab3ea6d7e9ca517b8c29a23b19" title="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chikengunya"&gt; /u/chikengunya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0lv50zsy1dif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncfif</id>
    <title>GLM-4.5V (based on GLM-4.5 Air)</title>
    <updated>2025-08-11T13:04:47+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A vision-language model (VLM) in the GLM-4.5 family. Features listed in model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image reasoning&lt;/strong&gt; (scene understanding, complex multi-image analysis, spatial recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video understanding&lt;/strong&gt; (long video segmentation and event recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GUI tasks&lt;/strong&gt; (screen reading, icon recognition, desktop operation assistance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex chart &amp;amp; long document parsing&lt;/strong&gt; (research report analysis, information extraction)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grounding&lt;/strong&gt; (precise visual element localization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5V"&gt;https://huggingface.co/zai-org/GLM-4.5V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnc8lx</id>
    <title>I built Excel Add-in for Ollama</title>
    <updated>2025-08-11T12:56:39+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt; &lt;img alt="I built Excel Add-in for Ollama" src="https://preview.redd.it/mvjwf2f81eif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=17b456d91ceed7000d3f08cd2f8917aec6e4254a" title="I built Excel Add-in for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an excel add-in that connects Ollama with Microsoft Excel. Data to remain inside excel only. You can simply write function =ollama(A1), assuming prompt in cell A1. You can simply drag to run on multiple cells. It has arguments to specify system instructions, temperature and model. You can set at both global level and specific to your prompts. &lt;a href="https://www.listendata.com/2025/08/ollama-in-excel.html"&gt;https://www.listendata.com/2025/08/ollama-in-excel.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvjwf2f81eif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T12:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncrqp</id>
    <title>ollama</title>
    <updated>2025-08-11T13:19:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt; &lt;img alt="ollama" src="https://preview.redd.it/2whabjm55eif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea8efc9d0fe6d86f047a62709601f55061db889" title="ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2whabjm55eif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
