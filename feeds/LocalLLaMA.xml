<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-29T16:24:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lneb9h</id>
    <title>I built Coretx to manage AI amnesia - 90 second demo</title>
    <updated>2025-06-29T13:10:02+00:00</updated>
    <author>
      <name>/u/nontrepreneur_</name>
      <uri>https://old.reddit.com/user/nontrepreneur_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you get tired of re-explaining things when switching between AIs, or returning to one later? I did. So I built Coretx and now I don't work without it.&lt;/p&gt; &lt;p&gt;AIs connect via MCP, can import from Claude/ChatGPT, and runs completely local with encrypted storage. No sign up required.&lt;/p&gt; &lt;p&gt;I've been using it while building it for about a month now, and I can't go back to working without it.&lt;/p&gt; &lt;p&gt;I'd love feedback from fellow power-users.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nontrepreneur_"&gt; /u/nontrepreneur_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getcoretx.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lneb9h/i_built_coretx_to_manage_ai_amnesia_90_second_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lneb9h/i_built_coretx_to_manage_ai_amnesia_90_second_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln10a8</id>
    <title>A bunch of LLM FPHAM Python scripts I've added to my GitHub in recent days</title>
    <updated>2025-06-28T23:57:43+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feel free to downvote me into the gutter, but these are some of the latest Stupid FPHAM Crap (S-FPHAM_C) python scripts that I came up:&lt;/p&gt; &lt;p&gt;merge_lora_CPU&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/merge_lora_CPU"&gt;https://github.com/FartyPants/merge_lora_CPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LoRA merging with a base model, primarily designed for CPU&lt;/p&gt; &lt;p&gt;This script allows you to merge a PEFT (Parameter-Efficient Fine-Tuning) LoRA adapter with a base Hugging Face model. It can also be used to simply resave a base model, potentially changing its format (e.g., to SafeTensors) or data type.&lt;br /&gt; Oy, and it goes around the Tied Weights in safetensors which was introduced after the &amp;quot;recent Transformers happy update.&amp;quot;&lt;/p&gt; &lt;h1&gt;chonker&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/chonker"&gt;https://github.com/FartyPants/chonker&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Smart Text Chunker&lt;/h1&gt; &lt;p&gt;A &amp;quot;sophisticated&amp;quot; Python command-line tool for splitting large text files into smaller, more manageable chunks of, shall we say, semantic relevance. It's designed for preparing text datasets for training and fine-tuning Large Language Models (LLMs).&lt;/p&gt; &lt;h1&gt;mass_rewriter&lt;/h1&gt; &lt;p&gt;Extension for oobabooga WebUI&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/mass_rewriter"&gt;https://github.com/FartyPants/mass_rewriter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Version 2.0, now with better logic is here!&lt;br /&gt; This tool helps you automate the process of modifying text in bulk using an AI model. You can load plain text files or JSON datasets, apply various transformations, and then save the rewritten content.&lt;/p&gt; &lt;h1&gt;Axolotl_Loss_Graph&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/FartyPants/Axolotl_Loss_Graph"&gt;https://github.com/FartyPants/Axolotl_Loss_Graph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A handy, dinky-doo graph of your Axolotl training progress.&lt;br /&gt; It takes the data copied from the terminal output and makes a nice little&lt;br /&gt; loss graph in a PNG format that you can easily send to your friends&lt;br /&gt; showing them how training your Axolotl is going so well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln10a8/a_bunch_of_llm_fpham_python_scripts_ive_added_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T23:57:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmfiu9</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-</title>
    <updated>2025-06-28T05:57:46+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" src="https://external-preview.redd.it/lSrPd1MMz7blRmLYLnruRoJd4XS5NpPXF_maDibWecs.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d68c4413ac33077ccb1f955a9767daec572c1df8" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All feedback is welcome! I am learning how to do better everyday.&lt;/p&gt; &lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others… not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time**)**&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question!&lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2–3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest – consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; – often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;I did think of creating a control set of answers. I could tell the mdoel this is the perfect answer basis this rate others. But I did not because it would need support from a lot of people- creating perfect answer, which still can have a bias. I read a few answers and found most of them decent except math. So I tried to find which model's evaluation scores were closest to the average to determine a decent model for evaluation tasks(check last image)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn’t everything – some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Best Performers (My Picks)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Why&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 1B&lt;/td&gt; &lt;td align="left"&gt;Fast &amp;amp; relevant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;Gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;Fast, accurate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;Generates numerical scores and evaluations closest to model average&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Worst Surprises&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Problem&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;Qwen3 4B&lt;/td&gt; &lt;td align="left"&gt;Took 486s to generate 1 question&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;Slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;DeepSeek-R1 1.5B&lt;/td&gt; &lt;td align="left"&gt;Inconsistent, skipped scores&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) – if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ≠ performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;5 Models have a self bais, they rate their own answers higher than average scores. attaching screen shot of a table. Diagonal is their own evaluation, last column is average.&lt;/li&gt; &lt;li&gt;Models' evaluation has high variance! Every model has a unique distribution of the scores it gave.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer.&lt;/p&gt; &lt;p&gt;Happy to share more data if you need.&lt;/p&gt; &lt;p&gt;Open to collaborate on interesting projects! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lmfiu9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T05:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmp3en</id>
    <title>support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp</title>
    <updated>2025-06-28T15:10:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt; &lt;img alt="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" src="https://external-preview.redd.it/STjjFmknxf7nBEMMInmMUB27ROh3VGJuNDaQ8cvttgc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fba0076b0b7c05a00a73da6e0fe0aa6d24a9166" title="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baidu has announced that it will officially release the ERNIE 4.5 models as open source on June 30, 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T15:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnh84u</id>
    <title>Detecting if an image contains a table, performance comparsion</title>
    <updated>2025-06-29T15:19:56+00:00</updated>
    <author>
      <name>/u/Gr33nLight</name>
      <uri>https://old.reddit.com/user/Gr33nLight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I'm building a tool that integrates a table extraction functionality from images.&lt;/p&gt; &lt;p&gt;I already have the main flow going with AWS Textract, to convert table images to a HTMl table and pass it to the llm model to answer questions.&lt;/p&gt; &lt;p&gt;My question is on the step before that, I need to be able to detect if a passed image contains a table, and redirect the request to the proper flow. &lt;/p&gt; &lt;p&gt;What would be the best method to do this? In terms of speed and cost?&lt;/p&gt; &lt;p&gt;I currently am trying to use all mistral models (because the platform is using EU-based models and infrastructure), so I the idea was to have a simple prompt to Pixtral or mistral-small and ask it if the image contains a table, would this be a correct solution?&lt;/p&gt; &lt;p&gt;Between pixtral and mistral-small what would be the best model for this specific use case? (Just determining if an image contains a table) ? &lt;/p&gt; &lt;p&gt;Or if you think you have better solutions, I'm all ears, thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gr33nLight"&gt; /u/Gr33nLight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnh84u/detecting_if_an_image_contains_a_table/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnh84u/detecting_if_an_image_contains_a_table/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnh84u/detecting_if_an_image_contains_a_table/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T15:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmqsru</id>
    <title>deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt</title>
    <updated>2025-06-28T16:21:43+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt; &lt;img alt="deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt" src="https://b.thumbs.redditmedia.com/GzycAOTsFOjTvMP8TcwlR9GvTE-MtmC--vwGaVrlpzQ.jpg" title="deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open weights model matching the best from closed AI. Seems quite impressive to me. What do you think? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mgu6oo7n1p9f1.png?width=2249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d375709b8e115ace177d0510bec0a16ad31d568e"&gt;https://preview.redd.it/mgu6oo7n1p9f1.png?width=2249&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d375709b8e115ace177d0510bec0a16ad31d568e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T16:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmni3q</id>
    <title>What framework are you using to build AI Agents?</title>
    <updated>2025-06-28T14:00:09+00:00</updated>
    <author>
      <name>/u/PleasantInspection12</name>
      <uri>https://old.reddit.com/user/PleasantInspection12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, if anyone here is building AI Agents for production what framework are you using? For research and building leisure projects, I personally use langgraph. I wanted to also know if you are not using langgraph, what was the reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantInspection12"&gt; /u/PleasantInspection12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T14:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lniowu</id>
    <title>Best local set up for getting writing critique/talking about the characters?</title>
    <updated>2025-06-29T16:21:21+00:00</updated>
    <author>
      <name>/u/Vast_Description_206</name>
      <uri>https://old.reddit.com/user/Vast_Description_206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I have a RTX 3060 with 12 Gb vram gpu. A fairly alright computer for entry level AI stuff.&lt;br /&gt; I've been experimenting with LM Studio, GT4ALL, AnythingLLM and Dot.&lt;/p&gt; &lt;p&gt;My use case is that I want to upload chapters of a book I'm writing for fun, get critiques, have it tell me strengths and weaknesses in my writing and also learn about the characters so it can help me think of stuff about them. My characters are quite fleshed out, but I enjoy the idea of &amp;quot;discovery&amp;quot; when say asking &amp;quot;What type of drinks based on the story and info you know about Kevin do you think he'd like?&amp;quot; kind of stuff, so both a critique assistant as well as a talk about the project in general.&lt;/p&gt; &lt;p&gt;I need long term persistent memory (as much as my rig will allow) and a good way to reference back to uploads/conversations with the bot. So far I've been using AnythingLLM because it has a workspace and I can tell it what model to use, currently it's Deep Seek AI R1 Distill Qwen 14B Q6_K which is about the upper limit to run with out too many issues.&lt;/p&gt; &lt;p&gt;So are there any better models I could use and does anyone have any thoughts on which LLM interface would be best for what I want to use it for?&lt;/p&gt; &lt;p&gt;Note: I've used ChatGPT and Claude, but both are limited or lost the thread. Otherwise it was pretty helpful for concurrent issues I have in my writing, like I use too much purple prose and don't trust the reader to know what's going on through physical action and instead explain the characters inner thoughts too much. I'm not looking for flattery, more strength, highlights, weaknesses, crucial fixes etc type critique. GPT tended to flattery till I told it to stop and Claude has a built in writers help function, but I only got one chapter in.&lt;/p&gt; &lt;p&gt;I also don't mind if it's slow, so long as it's accurate and less likely to lose details or get confused. In addition, I'm also not super fussed about my stuff being used as future model improvements/scrapping but it's nice to have something online more for personal privacy than contributing to anonymous data in a pool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Description_206"&gt; /u/Vast_Description_206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lniowu/best_local_set_up_for_getting_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lniowu/best_local_set_up_for_getting_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lniowu/best_local_set_up_for_getting_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T16:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lniptg</id>
    <title>Best foss LLMs for analysing PTE essay for potato system</title>
    <updated>2025-06-29T16:22:28+00:00</updated>
    <author>
      <name>/u/UnknownSh00ter</name>
      <uri>https://old.reddit.com/user/UnknownSh00ter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I'm developing a PTE essay generation and evaluation (scoring, giving feedback, etc.) tool for learning the AI and LLMs using python and ollama.&lt;/p&gt; &lt;p&gt;The problem is my potato system. (6GB Usable RAM outof 8GB with No GPU)&lt;/p&gt; &lt;p&gt;Which are the best FOSS LLMs out there for this scenario? (Which are the best if I've CHAD 💪🏋️ system?)&lt;/p&gt; &lt;p&gt;Any tips and ideas for the tool (if you're interested to share your thoughts)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnknownSh00ter"&gt; /u/UnknownSh00ter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lniptg/best_foss_llms_for_analysing_pte_essay_for_potato/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lniptg/best_foss_llms_for_analysing_pte_essay_for_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lniptg/best_foss_llms_for_analysing_pte_essay_for_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T16:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln5l6b</id>
    <title>Training Open models on my data for replacing RAG</title>
    <updated>2025-06-29T04:06:25+00:00</updated>
    <author>
      <name>/u/help_all</name>
      <uri>https://old.reddit.com/user/help_all</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have RAG based solution for search on my products and domain knowledge data. we are right now using open AI api to do the search but cost is slowly becoming a concern. I want to see if this can be a good idea if I take a LLama model or some other open model and train it on our own data. Has anyone had success while doing this. Also please point me to effective documentation about on how it should be done. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/help_all"&gt; /u/help_all &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln5l6b/training_open_models_on_my_data_for_replacing_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T04:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmk2dj</id>
    <title>Progress stalled in non-reasoning open-source models?</title>
    <updated>2025-06-28T10:58:35+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt; &lt;img alt="Progress stalled in non-reasoning open-source models?" src="https://preview.redd.it/q53t8do2fn9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbf6bcfd1d93bd65c875ca994b48c3b38839c958" title="Progress stalled in non-reasoning open-source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if you've noticed, but a lot of model providers no longer explicitly note that their models are reasoning models (on benchmarks in particular). Reasoning models aren't ideal for every application.&lt;/p&gt; &lt;p&gt;I looked at the non-reasoning benchmarks on &lt;a href="https://artificialanalysis.ai/models/llama-4-maverick?model-filters=open-source%2Cnon-reasoning-models#artificial-analysis-intelligence-index-by-model-type"&gt;Artificial Analysis&lt;/a&gt; today and the top 2 models (performing comparable) are DeepSeek v3 and Llama 4 Maverick (which I heard was a flop?). I was surprised to see these 2 at the top.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q53t8do2fn9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T10:58:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnesft</id>
    <title>What's the best way to summarize or chat with website content?</title>
    <updated>2025-06-29T13:32:52+00:00</updated>
    <author>
      <name>/u/Sandzaun</name>
      <uri>https://old.reddit.com/user/Sandzaun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using kobold and it would be nice if my Firefox browser could talk with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sandzaun"&gt; /u/Sandzaun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnesft/whats_the_best_way_to_summarize_or_chat_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnesft/whats_the_best_way_to_summarize_or_chat_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnesft/whats_the_best_way_to_summarize_or_chat_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnacbb</id>
    <title>Why the local Llama-3.2-1B-Instruct is not as smart as the one provided on Hugging Face?</title>
    <updated>2025-06-29T09:13:12+00:00</updated>
    <author>
      <name>/u/OkLengthiness2286</name>
      <uri>https://old.reddit.com/user/OkLengthiness2286</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/"&gt; &lt;img alt="Why the local Llama-3.2-1B-Instruct is not as smart as the one provided on Hugging Face?" src="https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2bfb801d9de9bf9739d847c79dca9f8dfe661ec0" title="Why the local Llama-3.2-1B-Instruct is not as smart as the one provided on Hugging Face?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the website of &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"&gt;https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct&lt;/a&gt;, there is an &amp;quot;Inference Providers&amp;quot; section where I can chat with Llama-3.2-1B-Instruct. It gives reasonable responses like the following.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440"&gt;https://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, when I download and run the model with the following code, it does not run properly. I have asked the same questions, but got bad responses. &lt;/p&gt; &lt;p&gt;I am new to LLMs and wondering what causes the difference. Do I use the model not in the right way?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from transformers import AutoModelForCausalLM, AutoTokenizer import torch import ipdb model_name = &amp;quot;Llama-3.2-1B-Instruct&amp;quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, device_map=&amp;quot;cuda&amp;quot;, torch_dtype=torch.float16,) def format_prompt(instruction: str, system_prompt: str = &amp;quot;You are a helpful assistant.&amp;quot;): if system_prompt: return f&amp;quot;&amp;lt;s&amp;gt;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;\n{system_prompt}\n&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;\n\n{instruction.strip()} [/INST]&amp;quot; else: return f&amp;quot;&amp;lt;s&amp;gt;[INST] {instruction.strip()} [/INST]&amp;quot; def generate_response(prompt, max_new_tokens=256): inputs = tokenizer(prompt, return_tensors=&amp;quot;pt&amp;quot;).to(model.device) with torch.no_grad(): outputs = model.generate( input_ids=inputs[&amp;quot;input_ids&amp;quot;], attention_mask=inputs[&amp;quot;attention_mask&amp;quot;], max_new_tokens=max_new_tokens, temperature=0.7, top_p=0.9, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id ) decoded = tokenizer.decode(outputs[0], skip_special_tokens=True) response = decoded.split(&amp;quot;[/INST]&amp;quot;)[-1].strip() return response if __name__ == &amp;quot;__main__&amp;quot;: print(&amp;quot;Chat with LLaMA-3.2-1B-Instruct. Type 'exit' to stop.&amp;quot;) while True: user_input = input(&amp;quot;You: &amp;quot;) if user_input.lower() in [&amp;quot;exit&amp;quot;, &amp;quot;quit&amp;quot;]: break prompt = format_prompt(user_input) response = generate_response(prompt) print(&amp;quot;LLaMA:&amp;quot;, response) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd"&gt;https://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkLengthiness2286"&gt; /u/OkLengthiness2286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T09:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln8uqb</id>
    <title>LM Studio vision models???</title>
    <updated>2025-06-29T07:32:07+00:00</updated>
    <author>
      <name>/u/BP_Ray</name>
      <uri>https://old.reddit.com/user/BP_Ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, so I'm brand new to local LLMs, and as such I'm using LM Studio since It's easy to use.&lt;/p&gt; &lt;p&gt;But the thing is I need to use vision models, and while LM Studio has some, for the most part every one I try to use doesn't actually allow me to upload images as in doesn't give me the option at all. I'm mainly trying to use uncensored models, so the main staff-picked ones aren't suitable for my purpose.&lt;/p&gt; &lt;p&gt;Is there some reason why most of these don't work on LM Studio? Am I doing something wrong or is it LM Studio that is the problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BP_Ray"&gt; /u/BP_Ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln8uqb/lm_studio_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T07:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnin1x</id>
    <title>AI coding agents...what am I doing wrong?</title>
    <updated>2025-06-29T16:19:12+00:00</updated>
    <author>
      <name>/u/furyfuryfury</name>
      <uri>https://old.reddit.com/user/furyfuryfury</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are other people having such good luck with ai coding agents and I can't even get mine to write a simple comment block at the top of a 400 line file?&lt;/p&gt; &lt;p&gt;The common refrain is it's like having a junior engineer to pass a coding task off to...well, I've never had a junior engineer scroll 1/3rd of the way through a file and then decide it's too big for it to work with. It frequently just gets stuck in a loop reading through the file looking for where it's supposed to edit and then giving up part way through and saying it's reached a token limit. How many tokens do I need for a 300-500 line C/C++ file? Most of mine are about this big, I try to split them up if they get much bigger because even my own brain can't fathom my old 20k line files very well anymore...&lt;/p&gt; &lt;p&gt;Tell me what I'm doing wrong?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LM Studio on a Mac M4 max with 128 gigglebytes of RAM&lt;/li&gt; &lt;li&gt;Qwen3 30b A3B, supports up to 40k tokens&lt;/li&gt; &lt;li&gt;VS Code with Continue extension pointed to the local LM Studio instance (I've also tried through OpenWebUI's OpenAI endpoint in case API differences were the culprit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do I need a beefier model? Something with more tokens? Different extension? More gigglebytes? Why can't I just give it 10 million tokens if I otherwise have enough RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/furyfuryfury"&gt; /u/furyfuryfury &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T16:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln1a6u</id>
    <title>What's it currently like for people here running AMD GPUs with AI?</title>
    <updated>2025-06-29T00:11:26+00:00</updated>
    <author>
      <name>/u/83yWasTaken</name>
      <uri>https://old.reddit.com/user/83yWasTaken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is the support?&lt;br /&gt; What is the performance loss?&lt;/p&gt; &lt;p&gt;I only really use LLM's with a RTX 3060 Ti, I was want to switch to AMD due to their open source drivers, I'll be using a mix of Linux &amp;amp; Windows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/83yWasTaken"&gt; /u/83yWasTaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T00:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnf00q</id>
    <title>GUI for Writing Long Stories with LLMs?</title>
    <updated>2025-06-29T13:42:45+00:00</updated>
    <author>
      <name>/u/BlacksmithRadiant322</name>
      <uri>https://old.reddit.com/user/BlacksmithRadiant322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a GUI that can assist in writing long stories, similar to Perchance's story generator. Perchance allows you to write what happens next, generates the subsequent passage, let's you edit what it generates and automatically makes summaries of previous passages to keep everything within the context window.&lt;/p&gt; &lt;p&gt;I'm wondering if there are any similar programs with a user interface that can be connected to Ollama or another LLM to help write long, coherent stories. Any recommendations or suggestions would be greatly appreciated!&lt;/p&gt; &lt;p&gt;The only resource about this topic that I've found is the awesome story generation github page. I haven't even been able to find a Discord server for writing enthusiasts that try using AI to help with their writing. At this pace book to movie is going to arrive before AI is capable of writing a lengthy story of any substance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlacksmithRadiant322"&gt; /u/BlacksmithRadiant322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf00q/gui_for_writing_long_stories_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lndmzj</id>
    <title>Mistral Small 3.2 can't generate tables, and stops generation altogether</title>
    <updated>2025-06-29T12:35:38+00:00</updated>
    <author>
      <name>/u/MQuarneti</name>
      <uri>https://old.reddit.com/user/MQuarneti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;```&lt;/p&gt; &lt;h3&gt;Analisi del Testo&lt;/h3&gt; &lt;h4&gt;📌 &lt;strong&gt;Introduzione&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Il testo analizza le traiettorie di vita di tre individui bangladesi, esplorando come la mobilità e l'immobilità siano influenzate da poteri esterni, come gli apparati burocratico-polizieschi e le forze economiche. I soggetti studiati sono definiti &amp;quot;probashi&amp;quot;, un termine che indica persone al contempo cosmopolite e profondamente radicate in un luogo, mobili e sedentarie.&lt;/p&gt; &lt;h4&gt;📌 &lt;strong&gt;Termini Chiave&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;| &lt;strong&gt;Termine&lt;/strong&gt; | &lt;strong&gt;Definizione&lt;/strong&gt; ```&lt;/p&gt; &lt;p&gt;I'm using Mistral-Small-3.2-24B-Instruct-2506-GGUF:IQ4_XS from unsloth. I tried different quantizations, tried bartowski's quants, different prompts, but I get the same result. The generation stops when trying to write the table header. There's nothing strange in the logs. Does anyone know why? Other llms (qwen3, gemma3) succeed in writing tables.&lt;/p&gt; &lt;p&gt;I'm using llama.cpp + llama-swap + open-webui&lt;/p&gt; &lt;p&gt;edit: koboldcpp seems working fine with open-webui&lt;/p&gt; &lt;p&gt;edit 2: mistral small 3.1 doesn't work either&lt;/p&gt; &lt;p&gt;edit 3: &lt;strong&gt;solved&lt;/strong&gt;: appearently as i wrote &amp;quot;use markdown&amp;quot; (it's redundant, so removing it doesn't affect the output quality) in the prompt it broke the output&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MQuarneti"&gt; /u/MQuarneti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lndmzj/mistral_small_32_cant_generate_tables_and_stops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lndmzj/mistral_small_32_cant_generate_tables_and_stops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lndmzj/mistral_small_32_cant_generate_tables_and_stops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T12:35:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln1ij8</id>
    <title>RLHF from scratch, step-by-step, in 3 Jupyter notebooks</title>
    <updated>2025-06-29T00:23:15+00:00</updated>
    <author>
      <name>/u/ashz8888</name>
      <uri>https://old.reddit.com/user/ashz8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently implemented Reinforcement Learning from Human Feedback (RLHF) fine-tuning, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model. The three steps are implemented in the three separate notebooks on GitHub: &lt;a href="https://github.com/ash80/RLHF_in_notebooks"&gt;https://github.com/ash80/RLHF_in_notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've also recorded a detailed video walkthrough (3+ hours) of the implementation on YouTube: &lt;a href="https://youtu.be/K1UBOodkqEk"&gt;https://youtu.be/K1UBOodkqEk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope this is helpful for anyone looking to explore RLHF. Feedback is welcome 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashz8888"&gt; /u/ashz8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T00:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln93o3</id>
    <title>Is anyone here using Llama to code websites and apps? From my experience, it sucks</title>
    <updated>2025-06-29T07:48:53+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at &lt;a href="https://www.designarena.ai/models/llama-4-maverick"&gt;some examples from Llama 4&lt;/a&gt;, it seems absolutely horrific at any kind of UI/UX. Also on this &lt;a href="https://www.designarena.ai/leaderboard"&gt;benchmark for UI/UX&lt;/a&gt;, Llama 4 Maverick and Llama 4 Scout sit in the bottom 25% when compared to toher models such as GPT, Claude, Grok, etc. &lt;/p&gt; &lt;p&gt;What would you say are Llama's strengths are there if it's not coding interfaces and design? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln93o3/is_anyone_here_using_llama_to_code_websites_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmz4kf</id>
    <title>Transformer ASIC 500k tokens/s</title>
    <updated>2025-06-28T22:26:25+00:00</updated>
    <author>
      <name>/u/tvmaly</name>
      <uri>https://old.reddit.com/user/tvmaly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this company in a post where they are claiming 500k tokens/s on Llama 70B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.etched.com/blog-posts/oasis"&gt;https://www.etched.com/blog-posts/oasis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Impressive if true&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tvmaly"&gt; /u/tvmaly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmz4kf/transformer_asic_500k_tokenss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T22:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnejb6</id>
    <title>What is the best open source TTS model with multi language support?</title>
    <updated>2025-06-29T13:20:44+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently developing an addon for Anki (an open source flashcard software). One part of my plan is to integrate an option to generate audio samples based on the preexisting content of the flashcards (for language learning). The point of it is using a local TTS model that doesn't require any paid services or APIs. To my knowledge the addons that are currently available for this have no option for a free version that still generate quite good audio.&lt;/p&gt; &lt;p&gt;I've looked a lot on HF but I struggle a bit to find out which models are actually suitable and versatile enough to support enough languages. My current bet would be XTTS2 due to the broad language support and its evaluation on leaderboards, but I find it to be a little &amp;quot;glitchy&amp;quot; at times.&lt;/p&gt; &lt;p&gt;I don't know if it's a good pick because it's mostly focussed on voice cloning. Could that be an issue? Do I have to think about some sort of legal concerns when using such a model? Which voice samples am I allowed to distribute to people so they can be used for voice cloning? I guess it wouldn't be user friendly to ask them to find their own 10s voice samples for generating audio.&lt;/p&gt; &lt;p&gt;So my question to my beloved local model nerds is:&lt;br /&gt; Which models have you tested and which ones would you say are the most consistent and reliable? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnejb6/what_is_the_best_open_source_tts_model_with_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ln7rll</id>
    <title>I made a writing assistant Chrome extension. Completely free with Gemini Nano.</title>
    <updated>2025-06-29T06:20:00+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt; &lt;img alt="I made a writing assistant Chrome extension. Completely free with Gemini Nano." src="https://external-preview.redd.it/aTR3azl2YzY3dDlmMRg_TmPcBoSM13pUYzKlWo7qhuAMWmP4IKxV8h55ZV-h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=966e826f016a8a652196742e287bca65c09c3a8d" title="I made a writing assistant Chrome extension. Completely free with Gemini Nano." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2f6200d67t9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ln7rll/i_made_a_writing_assistant_chrome_extension/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T06:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnf7eo</id>
    <title>Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model</title>
    <updated>2025-06-29T13:52:29+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"&gt; &lt;img alt="Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model" src="https://preview.redd.it/cutzsrmpfv9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80c376025a4d94f4078234f25a798b979d501fb" title="Is Yann LeCun Changing Directions? - Prediction using VAEs for World Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a huge fan of Yann Lecun and follow all his work very closely, especially the world model concept which I love. And I just finished reading &lt;strong&gt;“Whole-Body Conditioned Egocentric Video Prediction” -&lt;/strong&gt; the new FAIR/Berkeley paper with Yann LeCun listed as lead author. The whole pipeline looks like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Frame codec:&lt;/strong&gt; Every past RGB frame (224 × 224) is shoved through a &lt;strong&gt;frozen Stable-Diffusion VAE&lt;/strong&gt; -&amp;gt; 32 × 32 × 4 latent grid.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamics model:&lt;/strong&gt; A &lt;strong&gt;Conditional Diffusion Transformer (CDiT)&lt;/strong&gt; autoregressively predicts the &lt;em&gt;next&lt;/em&gt; latent, conditioned on a full 3-D body-pose trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visualisation:&lt;/strong&gt; The predicted latents are pushed back through the frozen VAE decoder so we can actually &lt;em&gt;see&lt;/em&gt; the roll-outs and compute LPIPS / FID.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That’s… exactly the sort of “predict the next frame” setup Yann spends entire keynotes dunking on:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;So I’m stuck with a big &lt;strong&gt;???&lt;/strong&gt; right now.&lt;/p&gt; &lt;h1&gt;Here’s why it feels contradictory&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frozen VAE or not, you’re still using a VAE.&lt;/strong&gt; If VAEs allegedly learn lousy representations, why lean on them at all -even as a codec - when V-JEPA exists? Why not learn a proper decoder on your great JEPA models?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The model&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;is&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;autoregressive.&lt;/strong&gt; Sure, the loss is ε-prediction in latent space, but at inference time you unroll it exactly like the next-token models he calls a dead end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JEPA latents are absent.&lt;/strong&gt; If V-JEPA is so much better, why not swap it in - even without a public decoder - ignite the debate, and skip the “bad” VAE entirely?&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Or am I missing something?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Does freezing the VAE magically sidesteps the “bad representation” critique?&lt;/li&gt; &lt;li&gt;Is this just an engineering placeholder until JEPA ships with decoder?&lt;/li&gt; &lt;li&gt;Is predicting latents via diffusion fundamentally different enough from next-pixel CE that it aligns with his worldview after all?&lt;/li&gt; &lt;li&gt;Or… is Yann quietly conceding that you still need a pixel-space codec (VAE, JPEG, whatever) for any practical world-model demo?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Honestly I don’t know whether this is a change in philosophy or just pragmatic glue code to get a body-conditioned world model out the door before NeurIPS deadlines. What do you all think?&lt;/p&gt; &lt;p&gt;Has anyone from FAIR hinted at a JEPA-codec drop?&lt;br /&gt; Is there a principled reason we should stop worrying about the “no VAE, no autoregression” mantra in this context?&lt;/p&gt; &lt;p&gt;I’d love to hear takes from people who’ve played with JEPA, latent diffusion, or any large-scale world-model work. Am I missing something and totally wrong, or does this paper actually mark a shift in Yann’s stance?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cutzsrmpfv9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnf7eo/is_yann_lecun_changing_directions_prediction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T13:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnfl21</id>
    <title>KoboldCpp v1.95 with Flux Kontext support</title>
    <updated>2025-06-29T14:09:23+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt; &lt;img alt="KoboldCpp v1.95 with Flux Kontext support" src="https://b.thumbs.redditmedia.com/Acro72oIt0wqz90aniNWfVPCfmpg8vp4szhxO44ZpMU.jpg" title="KoboldCpp v1.95 with Flux Kontext support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Flux Kontext is a relatively new open weights model based on Flux that can &lt;strong&gt;edit images using natural language&lt;/strong&gt;. Easily replace backgrounds, edit text, or add extra items into your images.&lt;/p&gt; &lt;p&gt;With the release of KoboldCpp v1.95, Flux Kontext support has been added to KoboldCpp! No need for any installation or complicated workflows, just download one executable and launch with &lt;a href="https://huggingface.co/koboldcpp/kcppt/resolve/main/Flux-Kontext.kcppt"&gt;&lt;strong&gt;a ready-to-use kcppt template&lt;/strong&gt;&lt;/a&gt; (recommended at least 12gb VRAM), and you're ready to go, the necessary models will be fetched and loaded.&lt;/p&gt; &lt;p&gt;Then you can open a browser window to &lt;a href="http://localhost:5001/sdui"&gt;http://localhost:5001/sdui&lt;/a&gt;, a simple A1111 like UI.&lt;/p&gt; &lt;p&gt;Supports using up to 4 reference images. Also supports the usual inpainting, img2img, sampler settings etc. You can also load the component models individually (e.g. you can reuse the VAE or T5-XXL for Chroma, which koboldcpp also supports).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff"&gt;https://preview.redd.it/18yvthliiv9f1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b2771ec6ce97968a675d3c1facb7e19b20b5dff&lt;/a&gt;&lt;/p&gt; &lt;p&gt;KoboldCpp also emulates the A1111/Forge and ComfyUI APIs so third party tools can use it as a drop in replacement.&lt;/p&gt; &lt;p&gt;This is possible thanks to the hard work of stable-diffusion.cpp contributors leejet and stduhpf.&lt;/p&gt; &lt;p&gt;P.s. Also, gemma 3n support is included in this release too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnfl21/koboldcpp_v195_with_flux_kontext_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T14:09:23+00:00</published>
  </entry>
</feed>
