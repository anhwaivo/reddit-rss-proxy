<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-05T23:49:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lslglw</id>
    <title>Options for a lot of VRAM for local Ollama server?</title>
    <updated>2025-07-05T22:07:43+00:00</updated>
    <author>
      <name>/u/mehgcap</name>
      <uri>https://old.reddit.com/user/mehgcap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an AMD build acting as a home server. Ryzen 5600G, 32GB RAM. I want a card with all the VRAM I can get, but I don't want to spend a lot. What are my options? I'm pretty new to all this. &lt;/p&gt; &lt;p&gt;I see that MI50 cards are going for relatively cheap. Is that still a good option? 32GB is probably more than enough. I do NOT need video output at all. I have a 5600G, and this server is headless anyway. I guess my questions are: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's the best way to get at least 32GB of VRAM for not Nvidia prices? I know not to just buy a gaming card, but I'm not sure what to look for and I've never bought from somewhere like Ali Express. &lt;/li&gt; &lt;li&gt;If I find a great deal, should I get two cards to double my VRAM? Cards don't really have LSI-like crossover anymore, so I feel like this would bottleneck me. &lt;/li&gt; &lt;li&gt;How much should I expect to spend per card? Again, I don't need video out. I'm fine with a data center card with no ports. &lt;/li&gt; &lt;li&gt;Is my 5600G good enough? All the work should happen on the GPU, so I'd guess I'm fine here. I'm aware I should get more system memory. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehgcap"&gt; /u/mehgcap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lslglw/options_for_a_lot_of_vram_for_local_ollama_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lslglw/options_for_a_lot_of_vram_for_local_ollama_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lslglw/options_for_a_lot_of_vram_for_local_ollama_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T22:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrss4u</id>
    <title>THUDM/GLM-4.1V-9B-Thinking looks impressive</title>
    <updated>2025-07-04T20:37:49+00:00</updated>
    <author>
      <name>/u/ConfidentTrifle7247</name>
      <uri>https://old.reddit.com/user/ConfidentTrifle7247</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt; &lt;img alt="THUDM/GLM-4.1V-9B-Thinking looks impressive" src="https://preview.redd.it/62vkwepq4xaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded30c7e3562f37aa41502883d7aa8b656c68551" title="THUDM/GLM-4.1V-9B-Thinking looks impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking"&gt;https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentTrifle7247"&gt; /u/ConfidentTrifle7247 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/62vkwepq4xaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsm1yb</id>
    <title>GPU overclocking?</title>
    <updated>2025-07-05T22:35:51+00:00</updated>
    <author>
      <name>/u/wpg4665</name>
      <uri>https://old.reddit.com/user/wpg4665</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it beneficial for LLM inference? I have MSI Afterburner, wondering if there's any settings that would be beneficial for my 3060 ¯\_(ツ)_/¯ It's not something I've seen discussed, so I'm &lt;em&gt;assuming&lt;/em&gt; not, just figured I'd ask. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wpg4665"&gt; /u/wpg4665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsm1yb/gpu_overclocking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsm1yb/gpu_overclocking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsm1yb/gpu_overclocking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T22:35:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsf6x</id>
    <title>OCRFlux-3B</title>
    <updated>2025-07-04T20:21:20+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt; &lt;img alt="OCRFlux-3B" src="https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dcd5f194d87db7612ad57b0c05d46dc6f0cfae3" title="OCRFlux-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the HF repo:&lt;/p&gt; &lt;p&gt;&amp;quot;OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. It aims to push the current state-of-the-art to a significantly higher level.&amp;quot;&lt;/p&gt; &lt;p&gt;Claims to beat other models like olmOCR and Nanonets-OCR-s by a substantial margin. Read online that it can also merge content spanning multiple pages such as long tables. There's also a docker container with the full toolkit and a github repo. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ChatDOC/OCRFlux-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri12r</id>
    <title>Great price on a 5090</title>
    <updated>2025-07-04T12:53:29+00:00</updated>
    <author>
      <name>/u/psdwizzard</name>
      <uri>https://old.reddit.com/user/psdwizzard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt; &lt;img alt="Great price on a 5090" src="https://preview.redd.it/1en1lic1uuaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7df49758108c1feb283e4286654e01dbd232a219" title="Great price on a 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About to pull the trigger on this one I can't believe how cheap it is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psdwizzard"&gt; /u/psdwizzard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1en1lic1uuaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T12:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfpi0</id>
    <title>Help setting up an uncensored local LLM for a text-based RPG adventure / DMing</title>
    <updated>2025-07-05T17:50:29+00:00</updated>
    <author>
      <name>/u/tac7878</name>
      <uri>https://old.reddit.com/user/tac7878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I apologize if this is the Nth time something like this was posted, but I am just at my wit's end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.&lt;/p&gt; &lt;p&gt;To preface, I'm basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can &amp;quot;learn&amp;quot; if I couldn't even understand the terminologies being used. I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.&lt;/p&gt; &lt;p&gt;So far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.&lt;/p&gt; &lt;p&gt;So my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I'm 5, because with all these new emerging fields I'm pretty much a child.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tac7878"&gt; /u/tac7878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls8c2s</id>
    <title>Aveni Labs releases FinLLM technical report: a 7B domain-specific model for financial services outperforming some frontier LLMs</title>
    <updated>2025-07-05T12:04:41+00:00</updated>
    <author>
      <name>/u/Ok-Cryptographer9361</name>
      <uri>https://old.reddit.com/user/Ok-Cryptographer9361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read the &lt;a href="https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf"&gt;FinLLM technical report&lt;/a&gt; from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.&lt;/p&gt; &lt;p&gt;Key points that stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA&lt;/li&gt; &lt;li&gt;Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)&lt;/li&gt; &lt;li&gt;Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments&lt;/li&gt; &lt;li&gt;Optimized for agentic RAG setups where traceability and source-grounding are required&lt;/li&gt; &lt;li&gt;Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.&lt;/p&gt; &lt;p&gt;Anyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Cryptographer9361"&gt; /u/Ok-Cryptographer9361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsju4i</id>
    <title>Local LLM for Audio Cleanup</title>
    <updated>2025-07-05T20:53:37+00:00</updated>
    <author>
      <name>/u/AnonTheGreat12345</name>
      <uri>https://old.reddit.com/user/AnonTheGreat12345</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to clean up audio voice profiles for chatterbox ai. Would like to run an AI to clean up isolate and clean up vocals. Tried a few premium online tools and myEdit ai works the best but don’t want to use a premium tool. Extra bonus if it can do other common audio tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnonTheGreat12345"&gt; /u/AnonTheGreat12345 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsju4i/local_llm_for_audio_cleanup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsju4i/local_llm_for_audio_cleanup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsju4i/local_llm_for_audio_cleanup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T20:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsjy83</id>
    <title>(Updated) All‑in‑One Generative AI Template: Frontend, Backend, Docker, Docs &amp; CI/CD + Ollama for local LLMs</title>
    <updated>2025-07-05T20:58:40+00:00</updated>
    <author>
      <name>/u/aminedjeghri</name>
      <uri>https://old.reddit.com/user/aminedjeghri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt; &lt;p&gt;Here is a major update to my Generative AI Project Template : ⸻&lt;/p&gt; &lt;p&gt;🚀 Highlights • Frontend built with NiceGUI for a robust, clean and interactive UI&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Backend powered by FastAPI for high-performance API endpoints • Complete settings and environment management • Pre-configured Docker Compose setup for containerization • Out-of-the-box CI/CD pipeline (GitHub Actions) • Auto-generated documentation (OpenAPI/Swagger) • And much more—all wired together for a smooth dev experience! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;🔗 Check it out on GitHub&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AmineDjeghri/generative-ai-project-template"&gt;Generative AI Project Template&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aminedjeghri"&gt; /u/aminedjeghri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsjy83/updated_allinone_generative_ai_template_frontend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsjy83/updated_allinone_generative_ai_template_frontend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsjy83/updated_allinone_generative_ai_template_frontend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T20:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsh4a8</id>
    <title>AI desktop configuration recommendations for RAG and LLM training</title>
    <updated>2025-07-05T18:51:47+00:00</updated>
    <author>
      <name>/u/Square-Onion-1825</name>
      <uri>https://old.reddit.com/user/Square-Onion-1825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/"&gt; &lt;img alt="AI desktop configuration recommendations for RAG and LLM training" src="https://b.thumbs.redditmedia.com/ICNGm2ETJXIpzJmqXpJCo-cHdVJelyzwCI7CAp8OSxs.jpg" title="AI desktop configuration recommendations for RAG and LLM training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to configure a workstation that I can do some AI dev work, in particular, RAG qualitative and quantitative analysis. I also need a system that I can use to prep many unstructured documents like pdfs and powerpoints, mostly marketing material for ingestion.&lt;/p&gt; &lt;p&gt;I'm not quite sure as to how robust a system I should be spec'ing out and would like your opinion and comments. I've been using ChatGPT and Claude quite a bit for RAG but for the sake of my clients, I want to conduct all this locally on my on system.&lt;/p&gt; &lt;p&gt;Also, not sure if I should use Windows 11 with WSL2 or native Ubuntu. I would like to use this system as a business computer as well for regular biz apps, but if Windows 11 with WSL2 will significantly impact performance on my AI work, then maybe I should go with native Ubuntu.&lt;/p&gt; &lt;p&gt;What do you think? I don't really want to spend over $22k...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a"&gt;https://preview.redd.it/ik0iupjjq3bf1.jpg?width=895&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63883ed5c0546e15227c752bd22f6b3bff1939a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Square-Onion-1825"&gt; /u/Square-Onion-1825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsh4a8/ai_desktop_configuration_recommendations_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T18:51:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls95oj</id>
    <title>Apple MLX Quantizations Royal Rumble 🔥</title>
    <updated>2025-07-05T12:50:36+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt; &lt;img alt="Apple MLX Quantizations Royal Rumble 🔥" src="https://a.thumbs.redditmedia.com/-1Fa5pMoUdufyX7EbInbdiYRv8uh6lx6DYYAentaN_0.jpg" title="Apple MLX Quantizations Royal Rumble 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-8B model using Winogrande as benchmark.&lt;br /&gt; DWQ and 5bit rule! &lt;/p&gt; &lt;p&gt;🥇 dwq – 68.82%&lt;br /&gt; 🥈 5bit – 68.51%&lt;br /&gt; 🥉 6bit – 68.35%&lt;br /&gt; bf16 – 67.64%&lt;br /&gt; dynamic – 67.56%&lt;br /&gt; 8bit – 67.56%&lt;br /&gt; 4bit – 66.30%&lt;br /&gt; 3bit – 63.85%&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3"&gt;https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrz5uy</id>
    <title>Got some real numbers how llama.cpp got FASTER over last 3-months</title>
    <updated>2025-07-05T02:08:31+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I am author of Hyprnote(&lt;a href="https://github.com/fastrepl/hyprnote"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.&lt;/p&gt; &lt;p&gt;When testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5828"&gt;b5828&lt;/a&gt;(newer) .. &lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5162"&gt;b5162&lt;/a&gt;(older)&lt;/p&gt; &lt;p&gt;Thinking of writing lot longer blog post with lots of numbers &amp;amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;OS&lt;/th&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="left"&gt;RAM&lt;/th&gt; &lt;th align="left"&gt;Compute&lt;/th&gt; &lt;th align="left"&gt;Prefill Tok/s&lt;/th&gt; &lt;th align="left"&gt;Gen Tok/s&lt;/th&gt; &lt;th align="left"&gt;Median Load (ms)&lt;/th&gt; &lt;th align="left"&gt;Prefill RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Gen RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Load RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;SHA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook Pro 14-inch&lt;/td&gt; &lt;td align="left"&gt;macOS 15.3.2&lt;/td&gt; &lt;td align="left"&gt;Apple M2 Pro&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;Metal&lt;/td&gt; &lt;td align="left"&gt;615.20&lt;/td&gt; &lt;td align="left"&gt;21.69&lt;/td&gt; &lt;td align="left"&gt;362.52&lt;/td&gt; &lt;td align="left"&gt;2332.28&lt;/td&gt; &lt;td align="left"&gt;2337.67&lt;/td&gt; &lt;td align="left"&gt;2089.56&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;571.85&lt;/td&gt; &lt;td align="left"&gt;21.43&lt;/td&gt; &lt;td align="left"&gt;372.32&lt;/td&gt; &lt;td align="left"&gt;2341.77&lt;/td&gt; &lt;td align="left"&gt;2347.05&lt;/td&gt; &lt;td align="left"&gt;2102.27&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HP EliteBook 660 16-inch G11&lt;/td&gt; &lt;td align="left"&gt;Windows 11.24H2&lt;/td&gt; &lt;td align="left"&gt;Intel Core Ultra 7 155U&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;162.52&lt;/td&gt; &lt;td align="left"&gt;14.05&lt;/td&gt; &lt;td align="left"&gt;1533.99&lt;/td&gt; &lt;td align="left"&gt;3719.23&lt;/td&gt; &lt;td align="left"&gt;3641.65&lt;/td&gt; &lt;td align="left"&gt;3535.43&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;148.52&lt;/td&gt; &lt;td align="left"&gt;12.89&lt;/td&gt; &lt;td align="left"&gt;2487.26&lt;/td&gt; &lt;td align="left"&gt;3719.96&lt;/td&gt; &lt;td align="left"&gt;3642.34&lt;/td&gt; &lt;td align="left"&gt;3535.24&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T02:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls3gho</id>
    <title>Open source tool for generating training datasets from text files and pdf for fine-tuning language models.</title>
    <updated>2025-07-05T06:36:35+00:00</updated>
    <author>
      <name>/u/Idonotknow101</name>
      <uri>https://old.reddit.com/user/Idonotknow101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall I made a new open-source tool.&lt;/p&gt; &lt;p&gt;It's an app that &lt;strong&gt;creates training data for AI models from your text and PDFs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.&lt;/p&gt; &lt;p&gt;Super simple, super useful, and it's all open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Idonotknow101"&gt; /u/Idonotknow101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T06:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsflii</id>
    <title>Anyone built a home 2× A100 SXM4 node?</title>
    <updated>2025-07-05T17:45:34+00:00</updated>
    <author>
      <name>/u/Fun_Nefariousness228</name>
      <uri>https://old.reddit.com/user/Fun_Nefariousness228</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m doing self-funded AI research and recently got access to 2× NVIDIA A100 SXM4 GPUs. I want to build a quiet, stable node at home to run local models and training workloads — no cloud.&lt;/p&gt; &lt;p&gt;Has anyone here actually built a DIY system with A100 SXM4s (not PCIe)? If so: What HGX carrier board or server chassis did you use? How did you handle power + cooling safely at home? Any tips on finding used baseboards or reference systems?&lt;/p&gt; &lt;p&gt;I’m not working for any company — just serious about doing advanced AI work locally and learning by building. Happy to share progress once it’s working.&lt;/p&gt; &lt;p&gt;Thanks in advance — would love any help or photos from others doing the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Nefariousness228"&gt; /u/Fun_Nefariousness228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsflii/anyone_built_a_home_2_a100_sxm4_node/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:45:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsfmcj</id>
    <title>I created this tool I named ReddSummary.com – just paste a link and boom you got the summary</title>
    <updated>2025-07-05T17:46:40+00:00</updated>
    <author>
      <name>/u/Himanshu507</name>
      <uri>https://old.reddit.com/user/Himanshu507</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt; &lt;img alt="I created this tool I named ReddSummary.com – just paste a link and boom you got the summary" src="https://preview.redd.it/2exxosoue3bf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c1d1e13fd17ff4381b38e36072d089c11c07e48" title="I created this tool I named ReddSummary.com – just paste a link and boom you got the summary" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have developed the web app and chrome extension to summarize the long reddit threads discussion using chatgpt, it helps user to analyize thread discussions and sentiments of the discussion.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Himanshu507"&gt; /u/Himanshu507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2exxosoue3bf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsfmcj/i_created_this_tool_i_named_reddsummarycom_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T17:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls9jvu</id>
    <title>Which open source LLM has the most genuine sense of humor?</title>
    <updated>2025-07-05T13:10:47+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top_k etc). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T13:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdxc2</id>
    <title>New app for locally running AI models on Android your smartphone</title>
    <updated>2025-07-05T16:32:35+00:00</updated>
    <author>
      <name>/u/RomanKryvolapov</name>
      <uri>https://old.reddit.com/user/RomanKryvolapov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt; &lt;img alt="New app for locally running AI models on Android your smartphone" src="https://external-preview.redd.it/mzou-cvKbo89yySFKfh6cxlVrw7VRIQEkdJHPKwwKng.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba60f6210ef2ca39ebd034795d1371f991f9f7d" title="New app for locally running AI models on Android your smartphone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt; &lt;p&gt;I create Android application for locally running AI models on smartphone&lt;/p&gt; &lt;p&gt;I am interested in your opinion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher"&gt;https://play.google.com/store/apps/details?id=com.romankryvolapov.offlineailauncher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5"&gt;https://preview.redd.it/cc4u5d7h23bf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d88e77046cf08053899eda10e9fac0a9752cf5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RomanKryvolapov"&gt; /u/RomanKryvolapov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdxc2/new_app_for_locally_running_ai_models_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsx20</id>
    <title>How RAG actually works — a toy example with real math</title>
    <updated>2025-07-04T20:44:15+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt; &lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt; &lt;h1&gt;Step 1: Chunk&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot; S1: &amp;quot;Poach an egg&amp;quot; S2: &amp;quot;How to change a tire&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Embed&lt;/h1&gt; &lt;p&gt;After the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.&lt;/p&gt; &lt;p&gt;Toy demo values:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10] # “Boil an egg” V1 = [ 0.88, 0.12, 0.00, 0.09] # “Poach an egg” V2 = [-0.20, 0.40, 0.80, 0.10] # “How to change a tire” &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt; &lt;h1&gt;Step 3: Normalize&lt;/h1&gt; &lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors V0̂ = [ 0.988, 0.110, 0.000, 0.110] # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1 V1̂ = [ 0.986, 0.134, 0.000, 0.101] # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1 V2̂ = [-0.217, 0.434, 0.868, 0.108] # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 4: Index&lt;/h1&gt; &lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br /&gt; Keep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt; &lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br /&gt; “Best way to cook an egg?”&lt;/p&gt; &lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we need to find the vector that’s &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br /&gt; The most common way is cosine similarity — often written as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But since we already normalized all vectors,&lt;br /&gt; ‖A‖ = ‖B‖ = 1 → so the formula becomes just:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = A ⋅ B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br /&gt; If two vectors are exactly the same, dot product = 1.&lt;br /&gt; So we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt; &lt;p&gt;Let’s calculate the scores (example, not real)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110) ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999 Vi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101) ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999 Vi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108) ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)&lt;br /&gt; are both very close to the user input.&lt;/p&gt; &lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br /&gt; Now the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls70r2</id>
    <title>Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance</title>
    <updated>2025-07-05T10:43:05+00:00</updated>
    <author>
      <name>/u/d5dq</name>
      <uri>https://old.reddit.com/user/d5dq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt; &lt;img alt="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" src="https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a" title="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d5dq"&gt; /u/d5dq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T10:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsmtzr</id>
    <title>Is Codestral 22B still the best open LLM for local coding on 32–64 GB VRAM?</title>
    <updated>2025-07-05T23:13:57+00:00</updated>
    <author>
      <name>/u/One-Stress-6734</name>
      <uri>https://old.reddit.com/user/One-Stress-6734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt; &lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt; &lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Stress-6734"&gt; /u/One-Stress-6734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T23:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsi0gj</id>
    <title>Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control</title>
    <updated>2025-07-05T19:31:10+00:00</updated>
    <author>
      <name>/u/Old_Mathematician107</name>
      <uri>https://old.reddit.com/user/Old_Mathematician107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt; &lt;img alt="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" src="https://b.thumbs.redditmedia.com/RwkhySplb6CWm6KR-CbkXw8jpoNWLPRz3r_cADWabdw.jpg" title="Open-sourced image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.&lt;/p&gt; &lt;p&gt;deki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.&lt;/p&gt; &lt;p&gt;All the code/information is available on GitHub: &lt;a href="https://github.com/RasulOs/deki"&gt;https://github.com/RasulOs/deki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have also uploaded the model to Hugging Face:&lt;br /&gt; Space: &lt;a href="https://huggingface.co/spaces/orasul/deki"&gt;orasul/deki&lt;/a&gt;&lt;br /&gt; (Check the analyze-and-get-yolo endpoint)&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/orasul/deki-yolo"&gt;orasul/deki-yolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Mathematician107"&gt; /u/Old_Mathematician107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lsi0gj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsi0gj/opensourced_image_description_models_object/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T19:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls5b89</id>
    <title>Powerful 4B Nemotron based finetune</title>
    <updated>2025-07-05T08:43:38+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt; &lt;img alt="Powerful 4B Nemotron based finetune" src="https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637" title="Powerful 4B Nemotron based finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \ adventure finetunes at its size category.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt; &lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here"&gt;See the examples in the logs&lt;/a&gt; 🙂&lt;/li&gt; &lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels 'in the moment', kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it's a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt; &lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt; &lt;li&gt;Very decent assistant.&lt;/li&gt; &lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt; &lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \ Adventure:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"&gt;Horde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T08:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsdjnb</id>
    <title>Llama-4-Maverick 402B on a oneplus 13</title>
    <updated>2025-07-05T16:15:49+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt; &lt;img alt="Llama-4-Maverick 402B on a oneplus 13" src="https://external-preview.redd.it/aGFxcDNhNW92MmJmMekOAVV8IqMqWnkLuX31i0q6lfgmqiPYm6_ltR2U10YG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcc6e776cfcd25daabf167946f5f41fbb1c23e70" title="Llama-4-Maverick 402B on a oneplus 13" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's Llama-4-Maverick-17B-128E-Instruct on a oneplus 13, which used UFS 4.0 storage. Any phone will work, as long as the RAM size is sufficient for context and repeating layers. (8-12gb)&lt;/p&gt; &lt;p&gt;Here's the command used: &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-cli -m Llama-4-Maverick-17B-128E-Instruct-UD-IQ1_M-00001-of-00003.gguf -t 6 -p &amp;quot;hi&amp;quot; -c 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;- Why llama maverick can run on a phone at 2 T/s: The big pool of experts are only in every odd layer, and a majority of the model is loaded into RAM. Therefore, you could think of it as loading mostly a 17 billion model with an annoying piece that slows down what should have been average 17B Q4-Q2 speeds.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/QwkaFHf"&gt;https://imgur.com/a/QwkaFHf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;picture shows the model layers as seen on huggingface tensor viewer: &lt;/p&gt; &lt;p&gt;- Green: in RAM&lt;/p&gt; &lt;p&gt;- Red: read from DISC&lt;/p&gt; &lt;p&gt;Other MOEs will have less impressive results due to a difference in architecture.&lt;/p&gt; &lt;p&gt;Greater results can be obtained by increasing the quantity of Q4_0 tensors for repeating layers in place of other types IQ4_XS, Q6_K, Q4_K, Q3_K, Q2_K, etc. as many phones use a preferred backend for Increasing token generation and prompt processing. For example, this particular phone when using the special Q4_0 type will upscale activations to int8 instead of float16, which barely affects accuracy, and doubles prompt processing. You may have to run experiments for your own device. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tletuj5ov2bf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsdjnb/llama4maverick_402b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T16:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsbhzs</id>
    <title>When Should We Expect Affordable Hardware That Will Run Large LLMs With Usable Speed?</title>
    <updated>2025-07-05T14:44:47+00:00</updated>
    <author>
      <name>/u/spiritxfly</name>
      <uri>https://old.reddit.com/user/spiritxfly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been years since local models started gaining traction and hobbyist experiment at home with cheaper hardware like multi 3090s and old DDR4 servers. But none of these solutions have been good enough, with multi-GPUs not having enough ram for large models such as DeepSeek and old server not having usable speeds.&lt;/p&gt; &lt;p&gt;When can we expect hardware that will finally let us run large LLMs with decent speeds at home without spending 100k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spiritxfly"&gt; /u/spiritxfly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsbhzs/when_should_we_expect_affordable_hardware_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T14:44:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsgtvy</id>
    <title>Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)</title>
    <updated>2025-07-05T18:39:10+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt; &lt;img alt="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" src="https://b.thumbs.redditmedia.com/eWAPmUbpD-1s0XukAwtaR-aHI9krbLWkxsiWZD3Lxxk.jpg" title="Successfully Built My First PC for AI (Sourcing Parts from Alibaba - Under $1500!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a PC was always one of those &amp;quot;someday&amp;quot; projects I never got around to. As a long-time Mac user, I honestly never had a real need for it. That all changed when I stumbled into the world of local AI. Suddenly, my 16GB Mac wasn't just slow, it was a hard bottleneck.&lt;/p&gt; &lt;p&gt;So, I started mapping out what this new machine needed to be:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;32GB VRAM as the baseline.&lt;/strong&gt; I'm really bullish on the future of MoE models and think 32-64gigs of VRAM should hold quite well.&lt;br /&gt; - &lt;strong&gt;128GB of RAM as the baseline.&lt;/strong&gt; Essential for wrangling the large datasets that come with the territory.&lt;br /&gt; - &lt;strong&gt;A clean, consumer-desk look.&lt;/strong&gt; I don't want a rugged, noisy server rack.&lt;br /&gt; - &lt;strong&gt;AI inference as the main job,&lt;/strong&gt; but I didn't want a one-trick pony. It still needed to be a decent all-rounder for daily tasks and, of course, some gaming.&lt;br /&gt; - &lt;strong&gt;Room to grow.&lt;/strong&gt; I wanted a foundation I could build on later.&lt;br /&gt; - And the big one: &lt;strong&gt;Keep it under $1500.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new Mac with these specs would cost a fortune and be a dead end for upgrades. New NVIDIA cards? Forget about it, way too expensive. I looked at used 3090s, but they were still going for about $1000 where I am, and that was a definite no-no for my budget.&lt;/p&gt; &lt;p&gt;Just as I was about to give up, I discovered the AMD MI50. The price-to-performance was incredible, and I started getting excited. Sure, the raw power isn't record-breaking, but the idea of running massive models and getting such insane value for my money was a huge draw.&lt;/p&gt; &lt;p&gt;But here was the catch: these are server cards. Even though they have a display port, it doesn't actually work. That would have killed my &amp;quot;all-rounder&amp;quot; requirement.&lt;/p&gt; &lt;p&gt;I started digging deep, trying to find a workaround. That's when I hit a wall. Everywhere I looked, the consensus was the same: cross-flashing the VBIOS on these cards to enable the display port was a dead end for the 32GB version. It was largely declared impossible...&lt;/p&gt; &lt;p&gt;...until the kind-hearted &lt;a href="/u/Accurate_Ad4323"&gt;u/Accurate_Ad4323&lt;/a&gt; from China stepped in to confirm it was possible. They even told me I could get the 32GB MI50s for as cheap as $130 from China, and that some people there had even programmed custom VBIOSes specifically for these 32GB cards. With all these pieces of crucial info, I was sold.&lt;/p&gt; &lt;p&gt;I still had my doubts. Was this custom VBIOS stable? Would it mess with AI performance? There was practically no info out there about this on the 32GB cards, only the 16GB ones. Could I really trust a random stranger's advice? And with ROCm's reputation for being a bit tricky, I didn't want to make my life even harder.&lt;/p&gt; &lt;p&gt;In the end, I decided to pull the trigger. Worst-case scenario? I'd have 64GB of HBM2 memory for AI work for about $300, just with no display output. I decided to treat a working display as a bonus.&lt;/p&gt; &lt;p&gt;I found a reliable seller on Alibaba who specialized in server gear and was selling the MI50 for $137. I browsed their store and found some other lucrative deals, formulating my build list right there.&lt;/p&gt; &lt;p&gt;Here’s what I ordered from them:&lt;/p&gt; &lt;p&gt;- Supermicro X11DPI-N -&amp;gt; $320&lt;br /&gt; - Dual Xeon 6148 CPUs -&amp;gt; 27 * 2 = $54&lt;br /&gt; - 2x CPU Coolers -&amp;gt; $62&lt;br /&gt; - 2x MI50 32GB GPUs -&amp;gt; $137 * 2 = $274&lt;br /&gt; - 4x 32GB DDR4 2666hz ECC RDIMM RAM sticks -&amp;gt; $124&lt;br /&gt; - 10x 120mm RGB fans -&amp;gt; $32&lt;br /&gt; - 6x 140mm RGB fans -&amp;gt; $27&lt;br /&gt; - 2x custom cooling shrouded fans for MI50s -&amp;gt; $14&lt;br /&gt; - Shipping + Duties -&amp;gt; $187&lt;/p&gt; &lt;p&gt;I know people get skeptical about Alibaba, but in my opinion, you're safe as long as you find the right seller, use a reliable freight forwarder, and always buy through Trade Assurance.&lt;/p&gt; &lt;p&gt;When the parts arrived, one of the Xeon CPUs was DOA. It took some back-and-forth, but the seller was great and sent a replacement for free once they were convinced (I offered to cover the shipping on it, which is included in that $187 cost).&lt;/p&gt; &lt;p&gt;I also bought these peripherals brand-new:&lt;/p&gt; &lt;p&gt;- Phanteks Enthoo Pro 2 Server Edition -&amp;gt; $200&lt;br /&gt; - ProLab 1200W 80Plus Gold PSU -&amp;gt; $100&lt;br /&gt; - 2TB NVMe SSD (For Ubuntu) -&amp;gt; $100&lt;br /&gt; - 1TB 2.5 SSD (For Windows) -&amp;gt; $50&lt;/p&gt; &lt;p&gt;All in, I spent exactly &lt;strong&gt;$1544.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now for the two final hurdles:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Assembling everything without breaking it!&lt;/strong&gt; As a first-timer, it took me about three very careful days, but I'm so proud of how it turned out.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing that custom VBIOS. Did I get the &amp;quot;bonus&amp;quot;?&lt;/strong&gt; After downloading the VBIOS, finding the right version of amdvbflash to force-flash, and installing the community NimeZ drivers... it actually works!!!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Now, to answer the questions I had for myself about the VBIOS cross-flash:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Is it stable?&lt;/strong&gt; Totally. It acts just like a regular graphics card from boot-up. The only weird quirk is on Windows: if I set &amp;quot;VGA Priority&amp;quot; to the GPU in the BIOS, the NimeZ drivers get corrupted. A quick reinstall and switching the priority back to &amp;quot;Onboard&amp;quot; fixes it. This doesn't happen at all in Ubuntu with ROCm.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does the flash hurt AI performance?&lt;/strong&gt; Surprisingly, no! It performs identically. The VBIOS is based on a Radeon Pro VII, and I've seen zero difference. If anything weird pops up, I'll be sure to update.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can it game?&lt;/strong&gt; Yes! Performance is like a Radeon VII but with a ridiculous 32GB of VRAM. It comfortably handles anything I throw at it in 1080p at max settings and 60fps.&lt;/p&gt; &lt;p&gt;I ended up with 64GB of versatile VRAM for under $300, and thanks to the Supermicro board, I have a clear upgrade path to 4TB of RAM and Xeon Platinum CPUs down the line. (if needed)&lt;/p&gt; &lt;p&gt;Now, I'll end this off with a couple pictures of the build and some benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790"&gt;https://preview.redd.it/fev4jvhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=23c596ba1d922e7bf231b8dacb4add0c9b8d5790&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71"&gt;https://preview.redd.it/n6idruhbn3bf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b9a883ded03b7bb26cf5e291562beff40f68ca71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The build is still a work-in-progress with regards to cable management :facepalm)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A power limit of 150W was imposed on both GPUs for all these tests.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-128K-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-30B-A3B-128K-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | --------: | ------: | ------- | --: | ----: | ------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | pp512 | 472.40 ± 2.44 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 99 | tg128 | 49.40 ± 0.07 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Magistral-Small-2506-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Magistral-Small-2506-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | pp512 | 130.75 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 13B Q4_K - Medium | 13.50 GiB | 23.57 B | ROCm | 99 | tg128 | 20.96 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gemma-3-27b-it-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/gemma-3-27b-it-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | pp512 | 110.88 ± 3.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.40 GiB | 27.01 B | ROCm | 99 | tg128 | 17.98 ± 0.02 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_K_M:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-32B-Q4_K_M.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ----------------------- | --------: | ------: | ------- | --: | ----: | -----------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | pp512 | 91.72 ± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3 32B Q4_K - Medium | 18.40 GiB | 32.76 B | ROCm | 99 | tg128 | 16.12 ± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama-3.3-70B-Instruct-UD-Q4_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Llama-3.3-70B-Instruct-UD-Q4_K_XL.gguf -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | pp512 | 42.49 ± 0.05 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 70B Q4_K - Medium | 39.73 GiB | 70.55 B | ROCm | 99 | tg128 | 7.70 ± 0.01 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-128K-UD-Q2_K_XL:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;build/bin/llama-bench --model models/Downloads/Qwen3-235B-A22B-128K-GGUF/Qwen3-235B-A22B-128K-UD-Q2_K_XL-00001-of-00002.gguf -ot '(4-7+).ffn_._exps.=CPU' -ngl 99 --threads 40 --flash-attn --no-mmap&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | ot | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | pp512 | 29.80 ± 0.15 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 235B.A22B Q2_K - Medium | 81.96 GiB | 235.09 B | ROCm | 99 | (4-7+).ffn_._exps.=CPU | tg128 | 7.45 ± 0.09 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm aware of the severe multi-GPU performance bottleneck with llama.cpp. Just started messing with vLLM, exLlamav2 and MLC-LLM. Will update results here once I get them up and running properly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Furmark scores post VBIOS flash and NimeZ drivers on Windows:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f"&gt;https://preview.redd.it/isqld5nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c184e61a0ba5272ba7ffd63fbca1690286810f8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764"&gt;https://preview.redd.it/muxix6nwn3bf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=355644c658940db5c30be46cc702e63c795b2764&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall, this whole experience has been an adventure, but it's been overwhelmingly positive. I thought I'd share it for anyone else thinking about a similar build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T18:39:10+00:00</published>
  </entry>
</feed>
