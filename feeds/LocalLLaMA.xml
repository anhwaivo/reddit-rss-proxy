<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-01T14:50:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lox1f7</id>
    <title>Local models not following instructions</title>
    <updated>2025-07-01T09:16:05+00:00</updated>
    <author>
      <name>/u/thecookingsenpai</name>
      <uri>https://old.reddit.com/user/thecookingsenpai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some problems on applying local LLMs to structured workflows.&lt;/p&gt; &lt;p&gt;I use 8b to 24b models on my 16GB 4070 Super TI&lt;/p&gt; &lt;p&gt;I have no problems in chatting or doing web rag with my models, either using open webui or AnythingLLM or custom solutions in python or nodejs. What I am unable to do is doing some more structured work. &lt;/p&gt; &lt;p&gt;Specifically, but this is just an example, I am trying to have my models output a specific JSON format. &lt;/p&gt; &lt;p&gt;I am trying almost everything in the system prompt and even in forcing json responses from ollama, but 70% of the times the models just produce wrong outputs. &lt;/p&gt; &lt;p&gt;Now, my question is more generic than having this specific json so I am not sure about posting the prompt etc. &lt;/p&gt; &lt;p&gt;My question is: are there models that are more suited to follow instructions than others? &lt;/p&gt; &lt;p&gt;Mistral 3.2 is almost always a failure in producing a decent json, so is Gemma 12b&lt;/p&gt; &lt;p&gt;Any specific tips and tricks or models to test? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecookingsenpai"&gt; /u/thecookingsenpai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T09:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lozhqc</id>
    <title>Cannot Load any GGUF model using tools like LM Studio or Jan Ai etc</title>
    <updated>2025-07-01T11:44:42+00:00</updated>
    <author>
      <name>/u/Physical-Citron5153</name>
      <uri>https://old.reddit.com/user/Physical-Citron5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So everything was okay until I upgraded from Windows 10 to 11 and suddenly I couldn’t load any local model through these GUI interfaces. I don’t see any error; it just loads indefinitely, no VRAM will also get occupied. &lt;/p&gt; &lt;p&gt;I checked with llama cpp and it worked fine, no errors.&lt;/p&gt; &lt;p&gt;I have 2x RTX 3090 and I am just confused why this is happening. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical-Citron5153"&gt; /u/Physical-Citron5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotzy4</id>
    <title>Video Cards &amp; GPUs SPARKLE intros new Arc Pro B60 cards: one is a dual-GPU workstation card with 48GB of VRAM</title>
    <updated>2025-07-01T05:52:37+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/106121/sparkle-intros-new-arc-pro-b60-cards-one-is-dual-gpu-workstation-card-with-48gb-of-vram/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0o3i</id>
    <title>Resources to learn about samplers?</title>
    <updated>2025-07-01T12:43:05+00:00</updated>
    <author>
      <name>/u/Black-Mack</name>
      <uri>https://old.reddit.com/user/Black-Mack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could you share how to learn more about samplers?&lt;/p&gt; &lt;p&gt;Anything is fine: blogs, articles, videos, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Black-Mack"&gt; /u/Black-Mack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovqjc</id>
    <title>Best Local Model for Vision?</title>
    <updated>2025-07-01T07:46:16+00:00</updated>
    <author>
      <name>/u/xukecheng</name>
      <uri>https://old.reddit.com/user/xukecheng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe Gemma3 is the best model for vision tasks? Each image uses only 256 tokens. In my own hardware tests, it was the only model capable of processing 60 images simultaneously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xukecheng"&gt; /u/xukecheng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lomilz</id>
    <title>[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀</title>
    <updated>2025-06-30T23:22:23+00:00</updated>
    <author>
      <name>/u/Awkward-Dare-1127</name>
      <uri>https://old.reddit.com/user/Awkward-Dare-1127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt; &lt;img alt="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" src="https://external-preview.redd.it/4LxPZe7g9FzOelbXz3aOGVDeWd4mgBA3OWlLsXGi_Bc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61aa76d902ab96a1963a6d4338aa8b21a38657e" title="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Copy one portable&lt;/em&gt; &lt;code&gt;.exe&lt;/code&gt; &lt;em&gt;+ a&lt;/em&gt; &lt;code&gt;.gguf&lt;/code&gt; &lt;em&gt;model to a flash drive → double-click on any Windows PC → start chatting offline in seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;GitHub ▶︎ &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad"&gt;&lt;strong&gt;https://github.com/runzhouye/Local_LLM_Notepad&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8"&gt;https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lz6e4zmpd5af1.gif"&gt;https://i.redd.it/lz6e4zmpd5af1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;30-second Quick-Start&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Grab &lt;strong&gt;Local_LLM_Notepad-portable.exe&lt;/strong&gt; from the &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad/releases"&gt;latest release&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download a small CPU model like &lt;strong&gt;gemma-3-1b-it-Q4_K_M.gguf&lt;/strong&gt; (≈0.8 GB) from &lt;a href="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/tree/main"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Copy both files onto a USB stick.&lt;/li&gt; &lt;li&gt;Double-click the EXE on any Windows box → first run loads the model.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;✅&lt;/th&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What it means&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Plug-and-play&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Single 45 MB EXE runs without admin rights&lt;/td&gt; &lt;td align="left"&gt;Run on any computer—no install needed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Source-word highlighting&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Bold-underlines every word/number from your prompt&lt;/td&gt; &lt;td align="left"&gt;Ctrl-click to trace facts &amp;amp; tables for quick fact-checking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hotkeys&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Ctrl + SCtrl + ZCtrl + FCtrl + X&lt;/code&gt; send, stop, search, clear, etc.&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Portable chat logs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;One-click JSON export&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Dare-1127"&gt; /u/Awkward-Dare-1127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2jhr</id>
    <title>I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory</title>
    <updated>2025-07-01T14:04:51+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"&gt; &lt;img alt="I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory" src="https://external-preview.redd.it/4uXVV_gIKvEbP6L8sZKIJfYeWwmBsgdPdD9fj0WIUdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a9015fee8d0b44e9f420a04ef0902737f402d5" title="I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using `gemini` and `claude` commandline AI tools, and I wanted to have something that allowed my AI full and unrestricted access to a VM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Mounts the local directory so it can read files&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Spawns a QEMU VM with access to those files&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Runs a command&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Returns&lt;/p&gt; &lt;p&gt;node ./scratchpad-cli --verbose --vm myvm run &amp;quot;python3 --version&amp;quot; ✓ Found VM 'myvm' 🚀 Starting VM 'myvm'... Acceleration: kvm Work directory: /home/bigattichouse/workspace/Scratchpad/node SSH port: 2385 Mode: Ephemeral (changes discarded) Command: qemu-system-x86_64 -name myvm-session -machine pc -m 512M -accel kvm -cpu host -smp 2 -drive file=/home/bigattichouse/.scratchpad/vms/myvm/disk.qcow2,format=qcow2,if=virtio,snapshot=on -netdev user,id=net0,hostfwd=tcp::2385-:22 -device virtio-net-pci,netdev=net0 -virtfs local,path=/home/bigattichouse/workspace/Scratchpad/node,mount_tag=workdir,security_model=mapped-xattr,id=workdir -display none -serial null -monitor none ⏳ Connecting to VM... ✓ Connected to VM ✓ Mounted work directory&lt;/p&gt; &lt;p&gt;📝 Executing command... Command: cd /mnt/work 2&amp;gt;/dev/null || cd ~ &amp;amp;&amp;amp; python3 --version Python 3.10.12&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bigattichouse/scratchpad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokkpc</id>
    <title>A Meta-Framework for Self-Improving LLMs with Transparent Reasoning</title>
    <updated>2025-06-30T21:59:34+00:00</updated>
    <author>
      <name>/u/henryb213</name>
      <uri>https://old.reddit.com/user/henryb213</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt; &lt;img alt="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" src="https://external-preview.redd.it/GF7LOLNV1EkT3j_WQj3wN6pKRBc62ktaNGoxeqmHjug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31f4c15b33f9e40cd80aee5e1468225b045437e8" title="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Framework overview:&lt;/strong&gt; LLMs iteratively refine their own outputs—typically through a three‑phase cycle &lt;strong&gt;draft → critique → revision&lt;/strong&gt;, repeat until convergence (all phases &amp;amp; stop rules are configurable). I started coding three weeks ago after an eight‑year break and zero professional dev experience.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;The classes work as Python callables with built in observability: instances are callable -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Python,tabs=4 from recursive_companion.base import MarketingCompanion agent = MarketingCompanion() answer = agent(&amp;quot;question or problem…&amp;quot;) # final refined output print(answer) print(agent.run_log) # list[dict] of every draft, critique &amp;amp; revision &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Why it stays clean &amp;amp; modular&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Templates are plain text files (system prompts, user prompts, protocol). &lt;em&gt;Swap harsh critiques for creative ones by swapping files.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;build_templates()&lt;/code&gt; lets you compose any combination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol injection&lt;/strong&gt; cleanly separates reasoning patterns from implementation.&lt;/li&gt; &lt;li&gt;New agents in &lt;strong&gt;3 lines&lt;/strong&gt;—just inherit from &lt;code&gt;BaseCompanion&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Convergence uses &lt;strong&gt;embedding‑based cosine similarity&lt;/strong&gt; by default, but the metric is fully pluggable.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How it came together&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The design emerged from recursive dialogues with multiple LLMs—the same iterative process the framework now automates. No legacy assumptions meant every piece became independent: swap models, add phases, change convergence logic—no rewiring required.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streamlit app&lt;/strong&gt; shows the thinking live as it happens.&lt;/li&gt; &lt;li&gt;Demos cover raw orchestration &lt;em&gt;and&lt;/em&gt; LangGraph integration (agents as graph nodes).&lt;/li&gt; &lt;li&gt;Full architecture docs, comprehensive docstrings, commenting, and worked examples included.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT)&lt;/strong&gt; &lt;a href="https://github.com/hankbesser/recursive-companion"&gt;https://github.com/hankbesser/recursive-companion&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built by questioning everything. Learning by building, built for learning.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for reading and really looking for any feedback and open to contributors, no question or discussion is too big or small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryb213"&gt; /u/henryb213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hankbesser/recursive-companion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1loxw8f</id>
    <title>What is night forge?</title>
    <updated>2025-07-01T10:12:17+00:00</updated>
    <author>
      <name>/u/Professional-Ad-4376</name>
      <uri>https://old.reddit.com/user/Professional-Ad-4376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt; &lt;img alt="What is night forge?" src="https://a.thumbs.redditmedia.com/93U5v10Vycvd1XA2AyyAUDnfoGNgsP5NzRsHUeD4F_0.jpg" title="What is night forge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec"&gt;https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a webdev arena, and one was very distinct in its style but I preferred it.&lt;/p&gt; &lt;p&gt;after voting for it, it said it was nightforge? I tried googling but couldn't find anything. Am I on the moon or whats going on?&lt;/p&gt; &lt;p&gt;Does anyone know what this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Ad-4376"&gt; /u/Professional-Ad-4376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T10:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1loza95</id>
    <title>Dual RX580 2048SP (16GB) llama.cpp(vulkan)</title>
    <updated>2025-07-01T11:33:27+00:00</updated>
    <author>
      <name>/u/IVequalsW</name>
      <uri>https://old.reddit.com/user/IVequalsW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I have a server in my house with dual rx580 (16gb) in it, running llama.cpp via Vulkan. it runs the Qwen-3-32B-q5 (28GB total) at about 4.5 - 4.8 t/s. &lt;/p&gt; &lt;p&gt;does anyone want me to test any other ggufs? I could test it with 1 or both of the GPUs. &lt;/p&gt; &lt;p&gt;they work relatively well and are really cheap for a large amount of vram. Memory bus speed is about 256GB/s. &lt;/p&gt; &lt;p&gt;Give ideas in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IVequalsW"&gt; /u/IVequalsW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1loo2u3</id>
    <title>Struggling with vLLM. The instructions make it sound so simple to run, but it’s like my Kryptonite. I give up.</title>
    <updated>2025-07-01T00:35:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m normally the guy they call in to fix the IT stuff nobody else can fix. I’ll laser focus on whatever it is and figure it out probably 99% of the time. I’ve been in IT for over 28+ years. I’ve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I’ve never encountered a more difficult software package to run than trying to get vLLM working in Docker. I can run nearly anything else in Docker except for vLLM. I feel like I’m really close, but every time I think it’s going to run, BAM! some new error that i find very little information on. - I’m running Ubuntu 24.04 - I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. - Yes I have the Nvidia runtime container working - Yes I have the hugginface token generated is there an easy button somewhere that I’m missing? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T00:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp37v0</id>
    <title>LoRA training on NVIDIA Jetson AGX Orin 64GB</title>
    <updated>2025-07-01T14:32:48+00:00</updated>
    <author>
      <name>/u/ahstanin</name>
      <uri>https://old.reddit.com/user/ahstanin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt; &lt;img alt="LoRA training on NVIDIA Jetson AGX Orin 64GB" src="https://b.thumbs.redditmedia.com/oh8UHfFWv9AwkzOmulxotDC0dlTYauybGEDMEiEkogE.jpg" title="LoRA training on NVIDIA Jetson AGX Orin 64GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5"&gt;https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d"&gt;https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I successfully ran LoRA training on an NVIDIA Jetson AGX Orin 64GB. Both 8-bit and FP16 modes are working. I'm currently training the Qwen 2.5 7B model. Although the process is slow, it's sufficient for my needs since there's no urgency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahstanin"&gt; /u/ahstanin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0j7f</id>
    <title>Best open source Arabic tts</title>
    <updated>2025-07-01T12:36:32+00:00</updated>
    <author>
      <name>/u/Spiritual_Button827</name>
      <uri>https://old.reddit.com/user/Spiritual_Button827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I’ve been trying to find the best TTS options to fine tune for Arabic and I’ve kinda hit a wall with Fish audio after their release of the new S1 model, as they’ve removed the fine tuning code for older models like v1.5.&lt;/p&gt; &lt;p&gt;I tried coqui’s XTTS fork by Idap: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;https://github.com/idiap/coqui-ai-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And got good results, but I would like to try other good options.&lt;/p&gt; &lt;p&gt;I looked at &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;https://huggingface.co/spaces/TTS-AGI/TTS-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I see that not many options support Arabic.&lt;/p&gt; &lt;p&gt;My use case is: real time inference of Arabic text for an interactive chatbot&lt;/p&gt; &lt;p&gt;I’m kinda new to TTS and would appreciate any help/advice.&lt;/p&gt; &lt;p&gt;I have a good server in hand with lots of compute to test anything so any open source model with fine tuning code available and can support Arabic is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Button827"&gt; /u/Spiritual_Button827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1loswvr</id>
    <title>New to the scene. Yesterday, got 4 t/s on R1 671b q4. Today, I'm getting about 0.15 t/s... What did I break lol</title>
    <updated>2025-07-01T04:46:12+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5975wx, 512gb DDR4 3200, dual 3090s. Ollama + OpenWebUI. Running on LMDE.&lt;/p&gt; &lt;p&gt;Idk what went wrong now but I'm struggling to get it back to 4 t/s... I can work with 4 t/s, but 0.15 t/s is just terrible.&lt;/p&gt; &lt;p&gt;Any ideas? Happy to provide information upon request.&lt;/p&gt; &lt;p&gt;Total noob here, just built this a few days ago and very little terminal experience lol but have an open mind and a will to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovuxp</id>
    <title>Current state of Intel A770 16GB GPU for Inference?</title>
    <updated>2025-07-01T07:55:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I could only find old posts regarding how the Intel A770 fares with LLMs, specifically people notice the high idle power consumption and difficult setup depending on what framework you use. At least a year ago, it was supposed to be a pain to use with Ollama.&lt;/p&gt; &lt;p&gt;Here in Germany, it is by far the cheapest 16GB card, in summary:&lt;br /&gt; - Intel A770, prices starting at 280-300€&lt;br /&gt; - AMD 9060 XT starting at 370€ (+32%)&lt;br /&gt; - Nvidia RTX 5060 Ti starting at 440€ (+57%)&lt;/p&gt; &lt;p&gt;Price-wise the A770 is a no-brainer, but what is your current experience? Currently using an RTX 4060 8GB and LMStudio on Windows 11 (+32GB DDR5).&lt;/p&gt; &lt;p&gt;Thanks for any insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2h0e</id>
    <title>Training and Finetuning Sparse Embedding Models with Sentence Transformers v5</title>
    <updated>2025-07-01T14:02:02+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt; &lt;img alt="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" src="https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2eca6467642c913566d063063339907e970775c0" title="Training and Finetuning Sparse Embedding Models with Sentence Transformers v5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sentence Transformers v5.0 was just released, and it introduced sparse embedding models. These are the kind of search models that are often combined with the &amp;quot;standard&amp;quot; dense embedding models for &amp;quot;hybrid search&amp;quot;. On paper, this can help performance a lot. From the release notes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A big question is: How do sparse embedding models stack up against the “standard” dense embedding models, and what kind of performance can you expect when combining various?&lt;/p&gt; &lt;p&gt;For this, I ran a variation of our &lt;a href="https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py"&gt;hybrid_search.py&lt;/a&gt; evaluation script, with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO"&gt;NanoMSMARCO&lt;/a&gt; dataset (a subset of the MS MARCO eval split)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"&gt;Qwen/Qwen3-Embedding-0.6B&lt;/a&gt; dense embedding model&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/naver/splade-v3-doc"&gt;naver/splade-v3-doc&lt;/a&gt; sparse embedding model, inference free for queries&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base"&gt;Alibaba-NLP/gte-reranker-modernbert-base&lt;/a&gt; reranker&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Which resulted in this evaluation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dense&lt;/th&gt; &lt;th&gt;Sparse&lt;/th&gt; &lt;th&gt;Reranker&lt;/th&gt; &lt;th&gt;NDCG@10&lt;/th&gt; &lt;th&gt;MRR@10&lt;/th&gt; &lt;th&gt;MAP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;65.33&lt;/td&gt; &lt;td&gt;57.56&lt;/td&gt; &lt;td&gt;57.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67.34&lt;/td&gt; &lt;td&gt;59.59&lt;/td&gt; &lt;td&gt;59.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;72.39&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;66.99&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;67.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.37&lt;/td&gt; &lt;td&gt;62.76&lt;/td&gt; &lt;td&gt;63.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;69.02&lt;/td&gt; &lt;td&gt;63.66&lt;/td&gt; &lt;td&gt;64.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;x&lt;/td&gt; &lt;td&gt;68.28&lt;/td&gt; &lt;td&gt;62.66&lt;/td&gt; &lt;td&gt;63.44&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here, the sparse embedding model actually already outperforms the dense one, but the real magic happens when combining the two: hybrid search. In our case, we used Reciprocal Rank Fusion to merge the two rankings. &lt;/p&gt; &lt;p&gt;Rerankers also help improve the performance of the dense or sparse model here, but hurt the performance of the hybrid search, as its performance is already beyond what the reranker can achieve.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;So, on paper you can now get more freedom over the &amp;quot;lexical&amp;quot; part of your hybrid search pipelines. I'm very excited about it personally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/train-sparse-encoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotza5</id>
    <title>KrunchWrapper - a LLM compression proxy (beta)</title>
    <updated>2025-07-01T05:51:25+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt; &lt;img alt="KrunchWrapper - a LLM compression proxy (beta)" src="https://preview.redd.it/c4bjroisb7af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b39a5201c024ced3ca9aba3ebe3b3090ade2d9" title="KrunchWrapper - a LLM compression proxy (beta)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that &amp;quot;compresses&amp;quot; requests sent to models as a proof of concept. I've seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a &amp;quot;decoder&amp;quot; to the LLM via a system prompt.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Github Link&lt;/strong&gt;: &lt;a href="https://github.com/thad0ctor/KrunchWrapper"&gt;https://github.com/thad0ctor/KrunchWrapper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.&lt;/p&gt; &lt;p&gt;Between compression and (optional) comment stripping, &lt;strong&gt;I have been able to acheive &amp;gt;40% compression when passing code files to the LLM that contain lots of repetition.&lt;/strong&gt; So far I haven't had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.&lt;/p&gt; &lt;p&gt;At its core, what KrunchWrapper essentially does is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Receive:&lt;/strong&gt; Establishes a proxy server that &amp;quot;intercepts&amp;quot; prompts going to a LLM server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Analyzes those prompts for common patterns of text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assign:&lt;/strong&gt; Maps a unicode symbol (known to use fewer tokens) to that pattern of text &lt;ol&gt; &lt;li&gt;Analyzes whether savings &amp;gt; system prompt overhead&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compress:&lt;/strong&gt; Replaces all identified patterns of text with the selected symbol(s) &lt;ol&gt; &lt;li&gt; Preserves JSON, markdown, tool calls&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Passes a system prompt with the compression decoder to the LLM along with the compressed message&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct:&lt;/strong&gt; Instucts the LLM to use the compressed symbols in any response&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decompress:&lt;/strong&gt; Decodes any responses received from the LLM that contain the compressed symbols&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat:&lt;/strong&gt; Intilligently adds to and re-uses any compression dictionaries in follow-on messages&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Beyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not &amp;quot;waste&amp;quot; compression tokens on minimal impact compression opportunities.&lt;/p&gt; &lt;p&gt;Looking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; I am not a programmer by trade. I refuse to use the v-word I so often see on here but let's just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.&lt;/p&gt; &lt;p&gt;This type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4bjroisb7af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
</feed>
