<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-31T01:34:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jng9r9</id>
    <title>[2503.18908] FFN Fusion: Rethinking Sequential Computation in Large Language Models</title>
    <updated>2025-03-30T16:29:37+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.18908"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jng9r9/250318908_ffn_fusion_rethinking_sequential/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jng9r9/250318908_ffn_fusion_rethinking_sequential/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngirf</id>
    <title>Synthesize Multimodal Thinking Datasets for Spatial Reasoning</title>
    <updated>2025-03-30T16:40:54+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngirf/synthesize_multimodal_thinking_datasets_for/"&gt; &lt;img alt="Synthesize Multimodal Thinking Datasets for Spatial Reasoning" src="https://external-preview.redd.it/kj_Ee04DEpHPCSeKKwKjPg3O1y9d99_Y7wBBdZAkEqs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe96bc6532b66babfcefeff7976a82063fdfc54a" title="Synthesize Multimodal Thinking Datasets for Spatial Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spatial reasoning is a key capability for embodied AI applications like robotics.&lt;/p&gt; &lt;p&gt;After recent updates to VQASynth, you can synthesize R1-style CoT reasoning traces to train your VLM to use test-time compute for enhanced spatial reasoning. &lt;/p&gt; &lt;p&gt;Additional updates help to apply VGGT for better 3D scene reconstruction and Molmo with point prompting for SAM2.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/125lq3iqture1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=647c3c2efb8e1cef93b8d2eda62266a41313166d"&gt;https://preview.redd.it/125lq3iqture1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=647c3c2efb8e1cef93b8d2eda62266a41313166d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stay tuned for the &amp;quot;SpaceThinker&amp;quot; dataset and VLM coming soon!&lt;/p&gt; &lt;p&gt;SpaceThinker data will be formatted similar to NVIDIA's &lt;a href="https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1"&gt;https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The SpaceThinker model will use NVIDIA's &lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1"&gt;https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1&lt;/a&gt; as the LLM backbone for training a LLaVA-style VLM similar to this colab: &lt;a href="https://colab.research.google.com/drive/1R64daHgR50GnxH3yn7mcs8rnldWL1ZxF?usp=sharing"&gt;https://colab.research.google.com/drive/1R64daHgR50GnxH3yn7mcs8rnldWL1ZxF?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Make multimodal thinking data from any HF image datasets: &lt;a href="https://github.com/remyxai/VQASynth"&gt;https://github.com/remyxai/VQASynth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More discussion in HF: &lt;a href="https://huggingface.co/spaces/open-r1/README/discussions/10"&gt;https://huggingface.co/spaces/open-r1/README/discussions/10&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngirf/synthesize_multimodal_thinking_datasets_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngirf/synthesize_multimodal_thinking_datasets_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngirf/synthesize_multimodal_thinking_datasets_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnqr9j</id>
    <title>Claude making up human tags</title>
    <updated>2025-03-31T00:22:12+00:00</updated>
    <author>
      <name>/u/chespirito2</name>
      <uri>https://old.reddit.com/user/chespirito2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been extremely unimpressed with 3.7. I'm currently using it on Cursor and it's now just continuously having a fake conversation between itself and me. Text from me is being put into human tags, with the text fully made up. Anyone seen this?&lt;/p&gt; &lt;p&gt;I'm about to switch back to a local model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chespirito2"&gt; /u/chespirito2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqr9j/claude_making_up_human_tags/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqr9j/claude_making_up_human_tags/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqr9j/claude_making_up_human_tags/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:22:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnmdlk</id>
    <title>Are there ready-to-use RAG (w local llm) projects for wikis?</title>
    <updated>2025-03-30T20:54:42+00:00</updated>
    <author>
      <name>/u/la_baguette77</name>
      <uri>https://old.reddit.com/user/la_baguette77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title. Wiki pages are somewhat standardized, is there already some kind project, for throwing the content into the RAG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/la_baguette77"&gt; /u/la_baguette77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnmdlk/are_there_readytouse_rag_w_local_llm_projects_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnmdlk/are_there_readytouse_rag_w_local_llm_projects_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnmdlk/are_there_readytouse_rag_w_local_llm_projects_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T20:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmtkgo</id>
    <title>4x3090</title>
    <updated>2025-03-29T19:02:48+00:00</updated>
    <author>
      <name>/u/zetan2600</name>
      <uri>https://old.reddit.com/user/zetan2600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt; &lt;img alt="4x3090" src="https://preview.redd.it/zi8ghi2ifore1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaa2ef7723a30f4134fa44b42f76a17aa5ba357" title="4x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the only benefit of multiple GPUs concurrency of requests? I have 4x3090 but still seem limited to small models because it needs to fit in 24G vram. &lt;/p&gt; &lt;p&gt;AMD threadripper pro 5965wx 128 PCIe lanes ASUS ws pro wrx80 256G ddr4 3200 8 channels Primary PSU Corsair i1600 watt Secondary PSU 750watt 4 gigabyte 3090 turbos Phanteks Enthoo Pro II case Noctua industrial fans Artic cpu cooler&lt;/p&gt; &lt;p&gt;I am using vllm with tensor parallism of 4. I see all 4 cards loaded up and utilized evenly but doesn't seem any faster than 2 GPUs. &lt;/p&gt; &lt;p&gt;Currently using Qwen/Qwen2.5-14B-Instruct-AWQ with good success paired with Cline. &lt;/p&gt; &lt;p&gt;Will a nvlink bridge help? How can I run larger models? &lt;/p&gt; &lt;p&gt;14b seems really dumb compared to Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zetan2600"&gt; /u/zetan2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi8ghi2ifore1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmtkgo/4x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T19:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn9klk</id>
    <title>This is the Reason why I am Still Debating whether to buy RTX5090!</title>
    <updated>2025-03-30T10:27:32+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt; &lt;img alt="This is the Reason why I am Still Debating whether to buy RTX5090!" src="https://a.thumbs.redditmedia.com/fsn9OVlHRAb11iT2p_HWV3Lsw8YzibhfHmPiywKjW70.jpg" title="This is the Reason why I am Still Debating whether to buy RTX5090!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11"&gt;https://preview.redd.it/23fu4zuc0tre1.png?width=1299&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09b1c89cef3582073f35174e47b52ffef612ee11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn9klk/this_is_the_reason_why_i_am_still_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T10:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnqmsg</id>
    <title>Am I the only one using LLMs with greedy decoding for coding?</title>
    <updated>2025-03-31T00:15:43+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using greedy decoding (i.e. always choose the most probable token by setting top_k=0 or temperature=0) for coding tasks. Are there better decoding / sampling params that will give me better results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnqmsg/am_i_the_only_one_using_llms_with_greedy_decoding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T00:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnld6i</id>
    <title>We experimented with developing cross language voice cloning TTS for Indic Languages</title>
    <updated>2025-03-30T20:11:03+00:00</updated>
    <author>
      <name>/u/Aquaaa3539</name>
      <uri>https://old.reddit.com/user/Aquaaa3539</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"&gt; &lt;img alt="We experimented with developing cross language voice cloning TTS for Indic Languages" src="https://external-preview.redd.it/ZXB1a3Y5ZGl3dnJlMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=039712a29c370d05620c2cf8ca6c56b1d00c0c18" title="We experimented with developing cross language voice cloning TTS for Indic Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We at our startup FuturixAI experimented with developing cross language voice cloning TTS models for Indic Languages&lt;br /&gt; Here is the result&lt;/p&gt; &lt;p&gt;Currently developed for Hindi, Telegu and Marathi&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aquaaa3539"&gt; /u/Aquaaa3539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h9sc3kdiwvre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnld6i/we_experimented_with_developing_cross_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T20:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnewf8</id>
    <title>Has anyone tried Tarsier2 7B? Insanely impressive video language model</title>
    <updated>2025-03-30T15:28:46+00:00</updated>
    <author>
      <name>/u/dontreachyoungblud</name>
      <uri>https://old.reddit.com/user/dontreachyoungblud</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/omni-research/Tarsier2-7b"&gt;https://huggingface.co/spaces/omni-research/Tarsier2-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This one snuck under the radar on me, but from playing around with the demo and looking at the evals, it's honestly really good. I'm quite surprised at the performance for a 7B model. &lt;/p&gt; &lt;p&gt;I just wish there was an MLX or GGUF version. If anyone finds one, please share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dontreachyoungblud"&gt; /u/dontreachyoungblud &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnewf8/has_anyone_tried_tarsier2_7b_insanely_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnewf8/has_anyone_tried_tarsier2_7b_insanely_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnewf8/has_anyone_tried_tarsier2_7b_insanely_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnehr4</id>
    <title>When you prompt a non-thinking model to think, does it actually improve output?</title>
    <updated>2025-03-30T15:10:03+00:00</updated>
    <author>
      <name>/u/Kep0a</name>
      <uri>https://old.reddit.com/user/Kep0a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For instance, Mistral 3 24b is not a reasoning model. However, when prompted correctly, I can have it generate &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags, and iteratively think through the problem.&lt;/p&gt; &lt;p&gt;In practice, I can get it to answer the &amp;quot;strawberry&amp;quot; test more often correctly, but I'm not sure if it's just due to actually thinking through the problem, or just because I asked it to &lt;strong&gt;think harder&lt;/strong&gt; that it just improves the chance of being correct. &lt;/p&gt; &lt;p&gt;Is this just mimicking reasoning, or actually helpful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kep0a"&gt; /u/Kep0a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnehr4/when_you_prompt_a_nonthinking_model_to_think_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jng3cz</id>
    <title>Agent - A Local Computer-Use Operator for macOS</title>
    <updated>2025-03-30T16:21:27+00:00</updated>
    <author>
      <name>/u/sandropuppo</name>
      <uri>https://old.reddit.com/user/sandropuppo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just open-sourced Agent, our framework for running computer-use workflows across multiple apps in isolated macOS/Linux sandboxes.&lt;/p&gt; &lt;p&gt;After launching Computer a few weeks ago, we realized many of you wanted to run complex workflows that span multiple applications. Agent builds on Computer to make this possible. It works with local Ollama models (if you're privacy-minded) or cloud providers like OpenAI, Anthropic, and others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We kept hitting the same problems when building multi-app AI agents - they'd break in unpredictable ways, work inconsistently across environments, or just fail with complex workflows. So we built Agent to solve these headaches:&lt;/p&gt; &lt;p&gt;•⁠ ⁠It handles complex workflows across multiple apps without falling apart&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can use your preferred model (local or cloud) - we're not locking you into one provider&lt;/p&gt; &lt;p&gt;•⁠ ⁠You can swap between different agent loop implementations depending on what you're building&lt;/p&gt; &lt;p&gt;•⁠ ⁠You get clean, structured responses that work well with other tools&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The code is pretty straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async with Computer() as macos_computer:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;agent = ComputerAgent(&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;computer=macos_computer,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;loop=AgentLoop.OPENAI,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;model=LLM(provider=LLMProvider.OPENAI)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tasks = [&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Look for a repository named trycua/cua on GitHub.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Check the open issues, open the most recent one and read it.&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;Clone the repository if it doesn't exist yet.&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;for i, task in enumerate(tasks):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nTask {i+1}/{len(tasks)}: {task}&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;async for result in agent.run(task):&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(result)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(f&amp;quot;\nFinished task {i+1}!&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some cool things you can do with it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;•⁠ ⁠Mix and match agent loops - OpenAI for some tasks, Claude for others, or try our experimental OmniParser&lt;/p&gt; &lt;p&gt;•⁠ ⁠Run it with various models - works great with OpenAI's computer_use_preview, but also with Claude and others&lt;/p&gt; &lt;p&gt;•⁠ ⁠Get detailed logs of what your agent is thinking/doing (super helpful for debugging)&lt;/p&gt; &lt;p&gt;•⁠ ⁠All the sandboxing from Computer means your main system stays protected&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started is easy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;pip install &amp;quot;cua-agent[all]&amp;quot;&lt;/p&gt; &lt;p&gt;# Or if you only need specific providers:&lt;/p&gt; &lt;p&gt;pip install &amp;quot;cua-agent[openai]&amp;quot; # Just OpenAI&lt;/p&gt; &lt;p&gt;pip install &amp;quot;cua-agent[anthropic]&amp;quot; # Just Anthropic&lt;/p&gt; &lt;p&gt;pip install &amp;quot;cua-agent[omni]&amp;quot; # Our experimental OmniParser&lt;/p&gt; &lt;p&gt;We've been dogfooding this internally for weeks now, and it's been a game-changer for automating our workflows. &lt;strong&gt;Grab the code at&lt;/strong&gt; &lt;a href="https://github.com/trycua/cua"&gt;&lt;strong&gt;https://github.com/trycua/cua&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts LocalLLaMA community! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandropuppo"&gt; /u/sandropuppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jng3cz/agent_a_local_computeruse_operator_for_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jng3cz/agent_a_local_computeruse_operator_for_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jng3cz/agent_a_local_computeruse_operator_for_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngswb</id>
    <title>Dou (道) - Visual Knowledge Organization and Analysis Tool</title>
    <updated>2025-03-30T16:53:31+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_道_visual_knowledge_organization_and_analysis/"&gt; &lt;img alt="Dou (道) - Visual Knowledge Organization and Analysis Tool" src="https://external-preview.redd.it/GOPYVA45L4Hs0UVngiY3mnW7r51k7UY71hhV_hEB_XU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2a61ea030a8d438936d0ede1ad351db268caa9" title="Dou (道) - Visual Knowledge Organization and Analysis Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shokuninstudio/Dou"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_道_visual_knowledge_organization_and_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngswb/dou_道_visual_knowledge_organization_and_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:53:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnf28i</id>
    <title>Exploiting Large Language Models: Backdoor Injections</title>
    <updated>2025-03-30T15:36:01+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnf28i/exploiting_large_language_models_backdoor/"&gt; &lt;img alt="Exploiting Large Language Models: Backdoor Injections" src="https://external-preview.redd.it/UeXieNkboMcAP3kmecNJZk2I2AO_-fluFc5hU8_aXv8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51fe2f830643e486ffcf2effb92ea6a480e64cf2" title="Exploiting Large Language Models: Backdoor Injections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kruyt.org/llminjectbackdoor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnf28i/exploiting_large_language_models_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnf28i/exploiting_large_language_models_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnlzb6</id>
    <title>Free Search: Updates and Improvements.</title>
    <updated>2025-03-30T20:37:15+00:00</updated>
    <author>
      <name>/u/Far-Celebration-470</name>
      <uri>https://old.reddit.com/user/Far-Celebration-470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;Last week, I open sourced Free Search API. It allows sourcing results from top search engines (including google, bing) for free. It uses searxng instances for this purpose. &lt;/p&gt; &lt;p&gt;I was overwhelmed by community's response and I am glad for all the support and suggestions. Today, I have pushed several improvements that make this API more stable. These improvements include &lt;/p&gt; &lt;p&gt;1) Parallel scrapping of search results for faster response&lt;br /&gt; 2) Markdown formatting of search results&lt;br /&gt; 3) Prioritizing SearXNG instances that have faster google response time&lt;br /&gt; 4) Update/Get endpoints for searxng instances. &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/HanzlaJavaid/Free-Search/tree/main"&gt;https://github.com/HanzlaJavaid/Free-Search/tree/main&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try the deployed version: &lt;a href="https://freesearch.replit.app/docs"&gt;https://freesearch.replit.app/docs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I highly appreciate PRs, issues, stars, and any kind of feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Celebration-470"&gt; /u/Far-Celebration-470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnlzb6/free_search_updates_and_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T20:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jndsj5</id>
    <title>We built a website where you can vote on Minecraft structures generated by AI</title>
    <updated>2025-03-30T14:37:03+00:00</updated>
    <author>
      <name>/u/civilunhinged</name>
      <uri>https://old.reddit.com/user/civilunhinged</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/civilunhinged"&gt; /u/civilunhinged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://mcbench.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jndsj5/we_built_a_website_where_you_can_vote_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnbhdl</id>
    <title>I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ☺️</title>
    <updated>2025-03-30T12:36:19+00:00</updated>
    <author>
      <name>/u/Salty-Garage7777</name>
      <uri>https://old.reddit.com/user/Salty-Garage7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had similar experience with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salty-Garage7777"&gt; /u/Salty-Garage7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnhuy3</id>
    <title>Llama 3.2 going insane on Facebook</title>
    <updated>2025-03-30T17:39:40+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt; &lt;img alt="Llama 3.2 going insane on Facebook" src="https://b.thumbs.redditmedia.com/NqDMwH6Bz4GQgIvs8QbIfAzCKcWDnZaM3TyMCLpxkoc.jpg" title="Llama 3.2 going insane on Facebook" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It kept going like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnhuy3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnhuy3/llama_32_going_insane_on_facebook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T17:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jn5uto</id>
    <title>MacBook M4 Max isn't great for LLMs</title>
    <updated>2025-03-30T05:42:51+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had M1 Max and recently upgraded to M4 Max - inferance speed difference is huge improvement (~3x) but it's still much slower than 5 years old RTX 3090 you can get for 700$ USD. &lt;/p&gt; &lt;p&gt;While it's nice to be able to load large models, they're just not gonna be very usable on that machine. An example - pretty small 14b distilled Qwen 4bit quant runs pretty slow for coding (40tps, with diff frequently failing so needs to redo whole file), and quality is very low. 32b is pretty unusable via Roo Code and Cline because of low speed.&lt;/p&gt; &lt;p&gt;And this is the best a money can buy you as Apple laptop.&lt;/p&gt; &lt;p&gt;Those are very pricey machines and I don't see any mentions that they aren't practical for local AI. You likely better off getting 1-2 generations old Nvidia rig if really need it, or renting, or just paying for API, as quality/speed will be day and night without upfront cost. &lt;/p&gt; &lt;p&gt;If you're getting MBP - save yourselves thousands $ and just get minimal ram you need with a bit extra SSD, and use more specialized hardware for local AI. &lt;/p&gt; &lt;p&gt;It's an awesome machine, all I'm saying - it prob won't deliver if you have high AI expectations for it. &lt;/p&gt; &lt;p&gt;PS: to me, this is not about getting or not getting a MacBook. I've been getting them for 15 years now and think they are awesome. The top models might not be quite the AI beast you were hoping for dropping these kinda $$$$, this is all I'm saying. I've had M1 Max with 64GB for years, and after the initial euphoria of holy smokes I can run large stuff there - never did it again for the reasons mentioned above. M4 is much faster but does feel similar in that sense. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T05:42:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jngj5u</id>
    <title>I built a coding agent that allows qwen2.5-coder to use tools</title>
    <updated>2025-03-30T16:41:24+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt; &lt;img alt="I built a coding agent that allows qwen2.5-coder to use tools" src="https://preview.redd.it/1erih6euuure1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5447b0990d64ea3d82b01889605650baf3b6948d" title="I built a coding agent that allows qwen2.5-coder to use tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1erih6euuure1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnegrp</id>
    <title>3 new Llama models inside LMArena (maybe LLama 4?)</title>
    <updated>2025-03-30T15:08:49+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt; &lt;img alt="3 new Llama models inside LMArena (maybe LLama 4?)" src="https://b.thumbs.redditmedia.com/dn-wlWwvH94ULQ168bBoDHch1sjJK-d3SZVT2HvBWwc.jpg" title="3 new Llama models inside LMArena (maybe LLama 4?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnegrp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnegrp/3_new_llama_models_inside_lmarena_maybe_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T15:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnc9rd</id>
    <title>It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x</title>
    <updated>2025-03-30T13:21:39+00:00</updated>
    <author>
      <name>/u/madaerodog</name>
      <uri>https://old.reddit.com/user/madaerodog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt; &lt;img alt="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" src="https://b.thumbs.redditmedia.com/xZgN0CnCg9_dkwL0g3ohDgCJu3nIHgZs9DZKGJ0a-FQ.jpg" title="It's not much, but its honest work! 4xRTX 3060 running 70b at 4x4x4x4x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaerodog"&gt; /u/madaerodog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jnc9rd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnc9rd/its_not_much_but_its_honest_work_4xrtx_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T13:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnplb1</id>
    <title>MLX fork with speculative decoding in server</title>
    <updated>2025-03-30T23:22:58+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I forked mlx-lm and ported the speculative decoding from the generate command to the server command, so now we can launch an OpenAI compatible completions endpoint with it enabled. I’m working on tidying the tests up to submit PR to upstream but wanted to announce here in case anyone wanted this capability now. I get a 90% speed increase when using qwen coder 0.5 as draft model and 32b as main model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; mlx_lm.server --host localhost --port 8080 --model ./Qwen2.5-Coder-32B-Instruct-8bit --draft-model ./Qwen2.5-Coder-0.5B-8bit &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm"&gt;https://github.com/intelligencedev/mlx-lm/tree/add-server-draft-model-support/mlx_lm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T23:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnjrdk</id>
    <title>Benchmark: RTX 3090, 4090, and even 4080 are surprisingly strong for 1-person QwQ-32B inference. (but 5090 not yet)</title>
    <updated>2025-03-30T19:01:34+00:00</updated>
    <author>
      <name>/u/fxtentacle</name>
      <uri>https://old.reddit.com/user/fxtentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't want to send all of my code to any outside company, but I still want to use AI code completion. Accordingly, I was curious how fast various GPUs would be for hosting when there's only 1 user: me. I used vLLM and &lt;code&gt;QwQ-32B-Q4_K_M&lt;/code&gt; for benchmarking.&lt;/p&gt; &lt;p&gt;&lt;code&gt;median_ttft_ms&lt;/code&gt; measures how long it takes for the GPU to handle the context and parse my request. And then &lt;code&gt;median_otps&lt;/code&gt; is how many output tokens the GPU can generate per second. (OTPS = Output Tokens Per Second) Overall, the &lt;code&gt;median_ttft_ms&lt;/code&gt; values were all &amp;lt;1s unless the card was overloaded and I think they will rarely matter in practice. That means the race is on for the highest OTPS.&lt;/p&gt; &lt;p&gt;As expected, a H200 is fast with 334ms + 30 OTPS. The H100 NVL is still fast with 426ms + 23 OTPS. The &amp;quot;old&amp;quot; H100 with HBM3 is similar at 310ms + 22 OTPS.&lt;/p&gt; &lt;p&gt;But I did not expect 2x RTX 4080 to score 383ms + 33 OTPS, which is really close to the H200 and that's somewhat insane if you consider that I'm comparing a 34000€ datacenter product with a 1800€ home setup. An old pair of 2x RTX 3090 is also still pleasant at 564ms + 28 OTPS. And a (watercooled and gently overclocked) RTX 3090 TI rocked the ranking with 558ms + 36 OTPS. You can also clearly see that vLLM is not fully optimized for the RTX 5090 yet, because there the official docker image did not work (yet) and I had to compile from source and, still, the results were somewhat meh with 517ms + 18 TOPS, which is slightly slower than a single 4090.&lt;/p&gt; &lt;p&gt;You'll notice that the consumer GPUs are slower in the initial context and request parsing. That makes sense because that task is highly parallel, i.e. what datacenter products were optimized for. But due to higher clock speeds and more aggressive cooling, consumer GPUs outcompete both H100 and H200 at output token generation, which is the sequential part of the task.&lt;/p&gt; &lt;p&gt;Here's my raw result JSONs from &lt;code&gt;vllm/benchmarks/benchmark_serving.py&lt;/code&gt; and a table with even more hardware variations: &lt;a href="https://github.com/DeutscheKI/llm-performance-tests"&gt;https://github.com/DeutscheKI/llm-performance-tests&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, my take-aways from this would be:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;RAM clock dominates everything. OC for the win!&lt;/li&gt; &lt;li&gt;Go with 2x 4080 over a single 4090 or 5090.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fxtentacle"&gt; /u/fxtentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T19:01:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnd6px</id>
    <title>LLMs over torrent</title>
    <updated>2025-03-30T14:08:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt; &lt;img alt="LLMs over torrent" src="https://preview.redd.it/8z6t2vvu3ure1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ade8fa1e4ff10e2d71461fdb60f942583a4d442f" title="LLMs over torrent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just messing around with an idea - serving LLM models over torrent. I’ve uploaded Qwen2.5-VL-3B-Instruct to a seedbox sitting in a neutral datacenter in the Netherlands (hosted via Feralhosting).&lt;/p&gt; &lt;p&gt;If you wanna try it out, grab the torrent file here and load it up in any torrent client:&lt;/p&gt; &lt;p&gt;👉 &lt;a href="http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent"&gt;http://sbnb.astraeus.feralhosting.com/Qwen2.5-VL-3B-Instruct.torrent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is just an experiment - no promises about uptime, speed, or anything really. It might work, it might not 🤷&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Some random thoughts / open questions: 1. Only models with redistribution-friendly licenses (like Apache-2.0) can be shared this way. Qwen is cool, Mistral too. Stuff from Meta or Google gets more legally fuzzy - might need a lawyer to be sure. 2. If we actually wanted to host a big chunk of available models, we’d need a ton of seedboxes. Huggingface claims they store 45PB of data 😅 📎 &lt;a href="https://huggingface.co/docs/hub/storage-backends"&gt;https://huggingface.co/docs/hub/storage-backends&lt;/a&gt; 3. Binary deduplication would help save space. Bonus points if we can do OTA-style patch updates to avoid re-downloading full models every time. 4. Why bother? AI’s getting more important, and putting everything in one place feels a bit risky long term. Torrents could be a good backup layer or alt-distribution method.&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Anyway, curious what people think. If you’ve got ideas, feedback, or even some storage/bandwidth to spare, feel free to join the fun. Let’s see what breaks 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8z6t2vvu3ure1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T14:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnfpnr</id>
    <title>It’s been 1000 releases and 5000 commits in llama.cpp</title>
    <updated>2025-03-30T16:04:30+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt; &lt;img alt="It’s been 1000 releases and 5000 commits in llama.cpp" src="https://external-preview.redd.it/wyCM1fHzTa-IIqHgS1QTxdSYNXn668elDj0WmYMPf_k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0d58c9a49c1e9ce629e5b31dce17b727d8c6ab8" title="It’s been 1000 releases and 5000 commits in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1000th release of llama.cpp&lt;/p&gt; &lt;p&gt;Almost 5000 commits. (4998)&lt;/p&gt; &lt;p&gt;It all started with llama 1 leak.&lt;/p&gt; &lt;p&gt;Thanks you team. Someone tag ‘em if you know their handle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-30T16:04:30+00:00</published>
  </entry>
</feed>
