<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-02T23:21:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iffboy</id>
    <title>SmolVLM fully open source</title>
    <updated>2025-02-01T20:19:01+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt; &lt;img alt="SmolVLM fully open source" src="https://external-preview.redd.it/RpBd16Y386MrSYjhSF5aL1O5cjq2V0xWVKGs2JQsIl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9476c8b4dd1bf85443ac42ac9be87b98d3ff2e1e" title="SmolVLM fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/andimarafioti/status/1885341684134978035"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ift9ik</id>
    <title>Andrew Ng: DeepSeek-R1 and the Future of Generative AI</title>
    <updated>2025-02-02T08:59:00+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hpcwire.com/off-the-wire/andrew-ng-deepseek-r1-and-the-future-of-generative-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ift9ik/andrew_ng_deepseekr1_and_the_future_of_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ift9ik/andrew_ng_deepseekr1_and_the_future_of_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T08:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifrf9t</id>
    <title>Deepseek R1 just told me to fist a frozen orange, for real. (This model is AMAZING)</title>
    <updated>2025-02-02T06:45:50+00:00</updated>
    <author>
      <name>/u/IversusAI</name>
      <uri>https://old.reddit.com/user/IversusAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Open WebUI and Deepseek R1 through Open Router to build my own healbot to help heal from sugar and wheat addiction. I was talking to the model, &lt;strong&gt;which is AMAZING&lt;/strong&gt; no joke and I was trying to make it to 10:00pm (when the store closes) and it was giving me help and suggestions to get through.&lt;/p&gt; &lt;p&gt;Note: My system prompt does NOT have anything in it about being explicit. It just asks the model to help me recover and how I want it to act (kind, supportive, etc).&lt;/p&gt; &lt;h3&gt;I had just asked it to help me get to 10:00pm:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/5Y97e8x.jpeg"&gt;https://i.imgur.com/5Y97e8x.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Yeah, there will be no frozen orange fisting, mkay?&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/LAVYIPM.jpeg"&gt;https://i.imgur.com/LAVYIPM.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;LOLOLOL:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/c8ss1p4.jpeg"&gt;https://i.imgur.com/c8ss1p4.jpeg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S.: I did make it to 10pm and the cravings eased. :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IversusAI"&gt; /u/IversusAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T06:45:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifltll</id>
    <title>I tested 11 popular local LLM's against my instruction-heavy game/application</title>
    <updated>2025-02-02T01:23:20+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;I have a few applications with some relatively large system prompts for how to handle requests. A lot of them use very strict JSON-formatting. I've scripted benchmarks for them going through a series of real use-case inputs and outputs and here's what I found&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;A dungeon-master scenario. The LLM first plays the role of the dungeon master, being fed state and inventory and then needing to take a user action/decision - reporting the output. The LLM is then responsible for reading over its own response and updating state and inventory JSON, quantity, locations, notes, descriptions, etc based on the content of the story. There are A LOT of rules involved, including of course actually successfully interacting with structured data. Successful models will both be able to advance the story in a very sane way given the long script of inputs/responses (I review afterwards) and track both state and inventory in the desired format.&lt;/p&gt; &lt;h1&gt;Rules&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;32b or less. Llama 3.3 70b performs this task superbly, but i want something that will feasibly run well on GPUs a regular consumer owns. I'm considering that 32gb of high bandwidth memory or VRAM or less.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;no API-only models&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;all quants are Q6. I tested Q8's but results were identical &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;context window of tests accommodates smaller models in that any test that goes over is thrown out&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;temperature is within the model author's recommended range, leaning slightly towards less-creative outputs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;instruct versions unless otherwise specified&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results (best to worst)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Phi4 14b&lt;/strong&gt; - Best by far. Not as smart as some of the others on this list, but it nails the response format instructions and rules 100% of the time. Being 14b its naturally very fast.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 2 22b&lt;/strong&gt; - Best balance. Extremely smart and superb at the interpretation and problem solving portion of the task. Will occasionally fail on JSON output but rarely&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen 32b Instruct&lt;/strong&gt; - this model was probably the smartest of them all. If handed a complex scenario, it would come up with what I considered the best logical solution, however it was pretty poor at JSON and rule-following&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 3 24b&lt;/strong&gt; - this one disappointed me. It's very clever and smart, but compared to the older Mistral Small 2, it's much weaker at instructon following. It could only track state for a short time before it would start deleting or forgetting items and events. Good at JSON format though.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-R1-Distill 32b&lt;/strong&gt; - smart(er) than Qwen 32b instruct but would completely flop on instruction following every 2-3 sequences. Amazing at interpreting state and story, but fell flat on its face with instructions and JSON.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral-Nemo 12b&lt;/strong&gt; - I like this model a lot. It punches higher than its benchmarks consistently and it will get through a number of sequences just fine, but it eventually hallucinates and returns either nonsense JSON, breaks rules, or loses track of state.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Falcon 3 10b&lt;/strong&gt; - Extremelt fast, shockingly smart, but would reliably produce a totally hallucinated output and content every few sequences&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Llama 3.1 8b&lt;/strong&gt; - follows instructions well, but hallucinated JSON formatting and contents far too often to be usable &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Codestral 22b&lt;/strong&gt; - a coding model!? for this? Well yeah - it actually nails the JSON 100% of the time, - but the story/content generation and understanding of actions and their impact on state were terrible. It also would inevitably enter a loop of nonsense output&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-Coder 32b&lt;/strong&gt; - exactly the same as Codestral, just with even worse writing. I love this model &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Nous-Hermes 3 8b&lt;/strong&gt; - slightly worse than regular Llama3.1 8b. Generated far more interesting (better written?) text in sections that allowed it though. This model to me is always &amp;quot;Llama 3.1 that went to art school instead of STEM&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(bonus) &lt;strong&gt;Llama 3.2 3b&lt;/strong&gt; - runs at lightspeed, I want this to be the future of local LLMs - but it's not a fair fight for the little guy. It goes off the rails or fails to follow instructions&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Phi4 14b is the best &lt;em&gt;so far&lt;/em&gt;. It just follows instructions well. But it's not as creative or natural in writing as Llama-based models, nor is it as intelligent or clever as Qwen or Mistral. It's the best at this test, there is no denying it, but i don't particularly enjoy its content compared to the flavor and intelligence of the other models tested. Mistral-Nemo 12b getting close to following instructions and struggling sug&lt;/p&gt; &lt;p&gt;&lt;strong&gt;if you have any other models you'd like to test this against, please mention them!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifmiuu</id>
    <title>Open WebUI Coder Overhaul is now live on GitHub for testing!</title>
    <updated>2025-02-02T02:00:14+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt; &lt;img alt="Open WebUI Coder Overhaul is now live on GitHub for testing!" src="https://external-preview.redd.it/pnWZlhrdj8zYupK7N1vmH0H-SivyUx9-OTH-kjV-R7g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf5d066eb9e2c66c37f101ed9464a98b1e14a000" title="Open WebUI Coder Overhaul is now live on GitHub for testing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Some of you may be familiar with the project I've been working on for the past couple of weeks here that essentially overhauls the OpenWebUI artifacts system and makes it closer to ChatGPT's Canvas or Claude Artifacts. Well, I just published the code and it's available for testing! I really would love some help from people who have real world use cases for this and have them submit issues, pull requests, or feature requests on GitHub!&lt;/p&gt; &lt;p&gt;Here is a brief breakdown on the features:&lt;/p&gt; &lt;p&gt;A side code editor similar to ChatGPT and Claude, supporting a LOT of coding languages. You can cycle through all code blocks in a chat.&lt;/p&gt; &lt;p&gt;A design view mode that lets you see HTML (now with typescript styles included by default) and also React components&lt;/p&gt; &lt;p&gt;A difference viewer that shows you what changed in a code block if an LLM made changes&lt;/p&gt; &lt;p&gt;Code blocks will be shown as attachments in the regular chat while the editor is opened, like Claude.&lt;/p&gt; &lt;p&gt;I hope you all enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T02:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1igajvk</id>
    <title>Teaching mistral thinking trough context, could be worse I guess.</title>
    <updated>2025-02-02T23:09:22+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igajvk/teaching_mistral_thinking_trough_context_could_be/"&gt; &lt;img alt="Teaching mistral thinking trough context, could be worse I guess." src="https://preview.redd.it/e5zcp4w05tge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abeb18fa76940e34fdeab51c4e82d4c4216ad079" title="Teaching mistral thinking trough context, could be worse I guess." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e5zcp4w05tge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1igajvk/teaching_mistral_thinking_trough_context_could_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1igajvk/teaching_mistral_thinking_trough_context_could_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T23:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig05tf</id>
    <title>Beginner Walkthrough to Install LLMs on Windows</title>
    <updated>2025-02-02T15:52:10+00:00</updated>
    <author>
      <name>/u/rdmDgnrtd</name>
      <uri>https://old.reddit.com/user/rdmDgnrtd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About six months ago I started a concerted effort to revisit my initial skepticism of LLMs and really try to understand how to get value out of them. As I went through my learning curve, I realized that a lot of the content I was reading either presupposed knowledge I didn't have, or was not easy to follow because of guidelines geared towards using Linux or MacOS. I've been writing the guide I wish I had when I started, which I keep updating as new development happen and as I explore things further. I hope this can help newcomers, feedback welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.oliviertravers.com/running-llms-locally-the-getting-started-windows-stack/"&gt;https://www.oliviertravers.com/running-llms-locally-the-getting-started-windows-stack/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rdmDgnrtd"&gt; /u/rdmDgnrtd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig05tf/beginner_walkthrough_to_install_llms_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T15:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifi0qu</id>
    <title>Missouri Senator Josh Hawley proposes a ban on Chinese AI models</title>
    <updated>2025-02-01T22:19:39+00:00</updated>
    <author>
      <name>/u/InquisitiveInque</name>
      <uri>https://old.reddit.com/user/InquisitiveInque</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InquisitiveInque"&gt; /u/InquisitiveInque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T22:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifuiiu</id>
    <title>What does your current model lineup look like? Heres mine</title>
    <updated>2025-02-02T10:31:12+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"&gt; &lt;img alt="What does your current model lineup look like? Heres mine" src="https://preview.redd.it/jz2j4geydpge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dc81856dffe6a71d07c12209cea3e879ccb588c" title="What does your current model lineup look like? Heres mine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jz2j4geydpge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifuiiu/what_does_your_current_model_lineup_look_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T10:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifjuh8</id>
    <title>Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap.</title>
    <updated>2025-02-01T23:45:19+00:00</updated>
    <author>
      <name>/u/convalytics</name>
      <uri>https://old.reddit.com/user/convalytics</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt; &lt;img alt="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." src="https://preview.redd.it/sb0m382v6mge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d527da27d452ede8dca689a709e00f143244cadb" title="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After many reboots and fiddling with blacklisting noveau/nouveau, it's finally working! &lt;/p&gt; &lt;p&gt;36GB of vram goodness and 64GB of system ram. &lt;/p&gt; &lt;p&gt;Planning to install ollama, open-webui and n8n. Any more recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/convalytics"&gt; /u/convalytics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb0m382v6mge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T23:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7q8n</id>
    <title>Which one is the best open-source model for coding/SWE-tasks?</title>
    <updated>2025-02-02T21:07:12+00:00</updated>
    <author>
      <name>/u/fazkan</name>
      <uri>https://old.reddit.com/user/fazkan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last time I checked llama-coder was the only one specialized. Also which benchmarks are relevant here. SWE-bench is the only one that I know of. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fazkan"&gt; /u/fazkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7q8n/which_one_is_the_best_opensource_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqagd</id>
    <title>R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible.</title>
    <updated>2025-02-02T05:33:40+00:00</updated>
    <author>
      <name>/u/redditisunproductive</name>
      <uri>https://old.reddit.com/user/redditisunproductive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt; &lt;img alt="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." src="https://external-preview.redd.it/sqLJ5r2pSW7H_l-8ii2E6-qgsVr8VlF7vTFjMAJ9Xb0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1afa3c2905a1dc6f720fe43b945c8b51870613b7" title="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditisunproductive"&gt; /u/redditisunproductive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vectara/hallucination-leaderboard"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T05:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7bm4</id>
    <title>Higher Parameters with Lower Quant: Is It Better?</title>
    <updated>2025-02-02T20:50:38+00:00</updated>
    <author>
      <name>/u/TheMikeans</name>
      <uri>https://old.reddit.com/user/TheMikeans</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;I was curious to explore whether increasing model parameters while using lower quantization would be worth it compared to running a smaller model with higher quantization. To investigate this, I tested two models on my machine:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/li&gt; &lt;li&gt;DeepSeek R1 Distill 14B Q2_K&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I evaluated them based on logic questions, factual knowledge, and writing improvement tasks.&lt;/p&gt; &lt;h1&gt;Testing Methodology&lt;/h1&gt; &lt;p&gt;I used the following sets of questions and tasks:&lt;/p&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;A lily pad in a pond doubles in size every day. It takes 60 days for the lily pad to cover the entire pond. On which day is the pond half covered?&lt;/li&gt; &lt;li&gt;A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?&lt;/li&gt; &lt;li&gt;If it takes 5 machines 5 minutes to make 5 gadgets, how long would it take 100 machines to make 100 gadgets?&lt;/li&gt; &lt;li&gt;A rope ladder is hanging over the side of a ship, with the bottom rung just touching the water. Each rung is 30 cm apart, and the tide rises at a rate of 40 cm per hour. How many rungs will be underwater after 3 hours?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Who is the Secretary-General of the United Nations as of 2023?&lt;/li&gt; &lt;li&gt;Which country hosted the 2020 Summer Olympics?&lt;/li&gt; &lt;li&gt;What is the chemical symbol for the element with the atomic number 26?&lt;/li&gt; &lt;li&gt;Who won the ACM Turing Award in 2022?&lt;/li&gt; &lt;li&gt;Who is the current Director-General of the World Health Organization (WHO)?&lt;/li&gt; &lt;li&gt;What is the only known human-made object visible from the Moon with the naked eye?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Writing Tasks&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Write 10 sentences that end with the word “forever.”&lt;/li&gt; &lt;li&gt;Rewrite the following email to correct spelling and grammatical errors while improving overall professionalism:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Original Email:&lt;/p&gt; &lt;p&gt;Hey team,&lt;br /&gt; just wanted to give you an update on the prjects status. were behind schedule cuz of some unexpected issues that came up last week. so im gona need everyone to put in extra hours this week so we can catch up. also, make sure youre documenting all your work properly, weve been having some problems with missing info in the reports. if you have any questions or need help, dont hesitate to ask.&lt;/p&gt; &lt;p&gt;btw, the meeting with the client has been moved to friday at 3pm instead of thursday. pls make sure to update your calenders accordingly. its important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;thanks for all your hard work.&lt;/p&gt; &lt;p&gt;best,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;DeepSeek R1 Distill 7B Q5_K_M&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.44GB&lt;/li&gt; &lt;li&gt;Speed: ~8.3 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;✅ 1 - PASS&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;❌ 1 - FAIL&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;br /&gt; ✅ 5 - PASS&lt;br /&gt; ❌ 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;2/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: A more professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Rewritten Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hi Team,&lt;br /&gt; I’m writing to provide an update on the project’s status. We are currently behind schedule due to some unexpected issues that arose last week. I’ll need everyone to work an extra hour this week to catch up. Please ensure that you're documenting all your work properly; we've been experiencing some problems with missing information in the reports.&lt;/p&gt; &lt;p&gt;If you have any questions or need assistance, feel free to ask.&lt;/p&gt; &lt;p&gt;Additionally, our scheduled meeting with the client has been rescheduled for Friday at 3 p.m., replacing our previous meeting on Thursday. Please make sure everyone attends the meeting and is prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for your continued hard work!&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;DeepSeek R1 Distill 14B Q2_K&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Size: 5.77GB&lt;/li&gt; &lt;li&gt;Speed: ~6 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logic Questions&lt;/h1&gt; &lt;p&gt;✅ 1 - PASS&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;/p&gt; &lt;h1&gt;Factual Questions&lt;/h1&gt; &lt;p&gt;❌ 1 - FAIL&lt;br /&gt; ✅ 2 - PASS&lt;br /&gt; ✅ 3 - PASS&lt;br /&gt; ❌ 4 - FAIL&lt;br /&gt; ✅ 5 - PASS&lt;br /&gt; ❌ 6 - FAIL&lt;/p&gt; &lt;h1&gt;Writing Improvement&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;10/10 sentences correct&lt;/li&gt; &lt;li&gt;Response: Another professional rewrite of the email:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Revised Email:&lt;/p&gt; &lt;p&gt;Subject: Update on Project Status&lt;/p&gt; &lt;p&gt;Hey team,&lt;/p&gt; &lt;p&gt;Just wanted to give you an update on the projects' status. We're running a bit behind schedule due to some unexpected issues that came up last week. So, I'm going to need everyone's focus this week—please make sure to put in extra hours so we can catch up.&lt;/p&gt; &lt;p&gt;Also, please ensure you're documenting all your work properly; we've been having some problems with missing info in the reports. If you have any questions or need help, don't hesitate to ask.&lt;/p&gt; &lt;p&gt;BTW, the meeting with the client has been moved to Friday at 3pm instead of Thursday. Please make sure to update your calendars accordingly. It's important that we all attend this meeting and be prepared to discuss our progress and any roadblocks.&lt;/p&gt; &lt;p&gt;Thanks for all your hard work.&lt;/p&gt; &lt;p&gt;Best regards,&lt;br /&gt; John&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;While the 14B model appears slightly better, the results were not overwhelmingly different from the 7B model in logic and factual performance (The reasoning seemed better in the 14B even when it was wrong, don't know how much that matters really).. However, the 14B model outperformed the 7B model in writing tasks.&lt;/p&gt; &lt;p&gt;That said, my tests may not have been the most rigorous or comprehensive. If you have any links to discussions, blog posts, or research that dive deeper into this topic, I'd love to check them out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMikeans"&gt; /u/TheMikeans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7bm4/higher_parameters_with_lower_quant_is_it_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig3m32</id>
    <title>Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API</title>
    <updated>2025-02-02T18:17:46+00:00</updated>
    <author>
      <name>/u/HIVVIH</name>
      <uri>https://old.reddit.com/user/HIVVIH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt; &lt;img alt="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" src="https://preview.redd.it/mwax9fnbprge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e20eb50542a043d90fbbd09b00d1b8f8ffea5e4" title="Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HIVVIH"&gt; /u/HIVVIH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mwax9fnbprge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig3m32/mistral_small_3_through_openrouter_is_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyu5b</id>
    <title>Would open weighting GPT 3.5 be interesting for us at all?</title>
    <updated>2025-02-02T14:52:07+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, please explain if the community would care about such legacy model release? Would the same apply to 4o released in like 2 years?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyu5b/would_open_weighting_gpt_35_be_interesting_for_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifm2df</id>
    <title>DeepSeek R1 misinformation is getting out of hand</title>
    <updated>2025-02-02T01:36:11+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9"&gt;https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1 is a &lt;strong&gt;7B parameter language model&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the official Google Cloud blog post? WTF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig4g1u</id>
    <title>Medtator, local Llama research medical helper</title>
    <updated>2025-02-02T18:51:52+00:00</updated>
    <author>
      <name>/u/RaiRamz</name>
      <uri>https://old.reddit.com/user/RaiRamz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt; &lt;img alt="Medtator, local Llama research medical helper" src="https://preview.redd.it/g7qcnt3fvrge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0da08cfc3f47f01ce1481a781c7b65ab51fcc7f" title="Medtator, local Llama research medical helper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a Medical AI Chatbot Using Llama 3 and PubMed&lt;/p&gt; &lt;p&gt;Hey everyone, I’ve been working on a project that combines Llama 3 with PubMed to create an AI-powered medical research assistant. Meet Medtator—a chatbot that retrieves, summarizes, and explains medical literature from PubMed in an accessible way.&lt;/p&gt; &lt;p&gt;How It Works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;User asks a medical question in natural language.&lt;/li&gt; &lt;li&gt;Keyword extraction (via a transformer-based pipeline) identifies relevant biomedical terms. &lt;/li&gt; &lt;li&gt;PubTator API fetches PubMed articles, ranking the most relevant studies.&lt;/li&gt; &lt;li&gt;Llama 3 generates concise answers using the retrieved context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;Searching PubMed is a pain. If you’re a medical student, researcher, or just a nerd like me, you know how frustrating it is to sift through thousands of articles. Medtator helps find relevant papers quickly and provides AI-assisted summaries to save time.&lt;/p&gt; &lt;p&gt;Challenges &amp;amp; Learnings: • Keyword extraction is tricky—LLMs sometimes miss the right biomedical terms. • Context window limitations force careful article selection to avoid hallucination. • Fine-tuning vs. prompt engineering—so far, smart prompting works surprisingly well.&lt;/p&gt; &lt;p&gt;Still improving it, but I’d love to hear your thoughts on optimizing Llama 3 for retrieval-augmented generation (RAG) tasks. Would love to hear any feedback/recommendations:). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaiRamz"&gt; /u/RaiRamz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7qcnt3fvrge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig4g1u/medtator_local_llama_research_medical_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifyzvv</id>
    <title>Mistral Small 3 24b is the first model under 70b I’ve seen pass the “apple” test (even using Q4).</title>
    <updated>2025-02-02T14:59:50+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put all the Deepseek-R1 distills through the “apple” benchmark last week and only 70b passed the “Write 10 sentences that end with the word “apple” “ test, getting all 10 out of10 sentences correct.&lt;/p&gt; &lt;p&gt;I tested a slew of other newer open source models (all the major ones, Qwen, Phi-, Llama, Gemma, Command-R, etc) as well, but no model under 70b has ever managed to succeed in getting all 10 right….until Mistral Small 3 24b came along. It is the first and only model under 70b parameters that I’ve found that could pass this test. Congrats Mistral Team!! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifyzvv/mistral_small_3_24b_is_the_first_model_under_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifw36r</id>
    <title>R1 is cool, but Mistral 3 Small is the boring workhorse I’m actually excited to fine-tune and deploy</title>
    <updated>2025-02-02T12:21:07+00:00</updated>
    <author>
      <name>/u/logan-diamond</name>
      <uri>https://old.reddit.com/user/logan-diamond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As soon as you use it, you realize it's not meant to be fun. It's a masterfully designed bland base model with very thoughtful trade-offs, especially for one-offs. Unless qwen replies soon, I think it might frequently replace both qwen 14b &amp;amp; 32b. &lt;/p&gt; &lt;p&gt;In 2024 I don't know how many times I read &amp;quot;... is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of...&amp;quot;. &lt;/p&gt; &lt;p&gt;Those times are back ☺️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logan-diamond"&gt; /u/logan-diamond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifw36r/r1_is_cool_but_mistral_3_small_is_the_boring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T12:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifsb1m</id>
    <title>Is the UK about to ban running LLMs locally?</title>
    <updated>2025-02-02T07:48:06+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The UK government is targetting the use of AI to generate illegal imagery, which of course is a good thing, but the wording seems like any kind of AI tool run locally can be considered illegal, as it has the *potential* of generating questionable content. Here's a quote from the news:&lt;/p&gt; &lt;p&gt;&amp;quot;The Home Office says that, to better protect children, the UK will be the first country in the world to make it illegal to possess, create or distribute AI tools designed to create child sexual abuse material (CSAM), with a punishment of up to five years in prison.&amp;quot; They also mention something about manuals that teach others how to use AI for these purposes.&lt;/p&gt; &lt;p&gt;It seems to me that any uncensored LLM run locally can be used to generate illegal content, whether the user wants to or not, and therefore could be prosecuted under this law. Or am I reading this incorrectly?&lt;/p&gt; &lt;p&gt;And is this a blueprint for how other countries, and big tech, can force people to use (and pay for) the big online AI services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifsb1m/is_the_uk_about_to_ban_running_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T07:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig7ruy</id>
    <title>A bunch of LLMs scheduled to come at end of January were cancelled / delayed</title>
    <updated>2025-02-02T21:09:06+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all ix-nixed by deepseek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/patience_cave/status/1886122517359886745"&gt;https://x.com/patience_cave/status/1886122517359886745&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;gemini 2 - don’t even think about it&lt;/li&gt; &lt;li&gt;grok 3 - welcome to elon timelines&lt;/li&gt; &lt;li&gt;o3 - an unforeseen problem occurred&lt;/li&gt; &lt;li&gt;opus 3.5 - long gone&lt;/li&gt; &lt;li&gt;llama 4 - won’t be sota (sorry zuck)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Update: Rumor for end of January: &lt;a href="https://x.com/iruletheworldmo/status/1877391558305001747"&gt;https://x.com/iruletheworldmo/status/1877391558305001747&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig7ruy/a_bunch_of_llms_scheduled_to_come_at_end_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig8ve3</id>
    <title>Americans can distill models too</title>
    <updated>2025-02-02T21:55:59+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, I'm a TTS model trainer and a US citizen. Last month, I put out a &lt;a href="https://huggingface.co/posts/hexgrad/418806998707773"&gt;call for synthetic training data&lt;/a&gt;, that call was answered with well over a hundred hours of audio in various languages, and the resulting model &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; has since been upgraded/delivered. Happy customers all around.&lt;/p&gt; &lt;p&gt;The current model mostly excels at &lt;em&gt;reading long texts&lt;/em&gt; and has some glaring limitations, especially on short texts. It's also been described as relatively flat and emotionless. Nevertheless, it is currently the most-liked &lt;a href="https://huggingface.co/models?pipeline_tag=text-to-speech&amp;amp;sort=likes"&gt;TTS model&lt;/a&gt; and &lt;a href="https://huggingface.co/spaces?sort=likes&amp;amp;search=tts"&gt;TTS space&lt;/a&gt; on Hugging Face thanks to people smashing that like button.&lt;/p&gt; &lt;p&gt;Now, I'm considering making another call for crowdsourced data, except this time with a focus on only ChatGPT Advanced Voice Mode text/audio pairs, likely just in English, spanning whatever emotions people can prompt out of it. If successful, it could result in a substantially better &lt;em&gt;conversational&lt;/em&gt; model within the same size class, albeit more limited on voices and languages.&lt;/p&gt; &lt;p&gt;There are many things to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Top priority would be given to paying ChatGPT subscribers, $20 and $200, but in practice free AVM audio would likely be admitted as well. This is because the paying subscribers would be least likely to be using a quantized and/or distilled AVM product.&lt;/li&gt; &lt;li&gt;Ideally I could maximally open source any voicepack derived from the AVM data, which means the people contributing audio would have to do it for ideological reasons, and couldn't be compensated with an &amp;quot;exclusive voicepack&amp;quot;. Also, any sponsorships I receive are directed at GPU compute, and both on principle + potential legal liability, I cannot financially compensate people who give me synthetic data.&lt;/li&gt; &lt;li&gt;As far as ToS goes, this distillation strategy rests on the fact that I am not the one obtaining the data, others are. Obviously, I do not agree with the OpenAI ToS or feel bound by it because I don't use any of their products. Feel free to comment on how dumb this strategy is.&lt;/li&gt; &lt;li&gt;I have skimmed Part 2 of the US Copyright Office's Report on AI. I still see no copyright protection on synthetic data of this nature, but any lawyers (real or wannabe) can chime in here with the default prefix of IANL.&lt;/li&gt; &lt;li&gt;I do not wish to be sued, and I'm also deeply allergic to .50 caliber bullets. Jokes aside, I think OpenAI likely has bigger whales to fry, than some guy training 82M param speech models.&lt;/li&gt; &lt;li&gt;Why do it: these small TTS models are (relatively) cheap to train, especially compared to LLMs, and the total utility they offer might exceed their cost, at least for now, until Zucc drops Llama 4 multimodal or DeepSeek puts up a good audio model, etc.&lt;/li&gt; &lt;li&gt;The scale of data I am looking for is at least 10 hours per voice/emotion, but label quality also matters. Each audio file would have to be fished out one-by-one, since there are no API calls for AVM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I understand this is LocalLLaMA and people here are likely very pro-open-weights, pro-open-source, and therefore anti-OpenAI. But putting aside any feelings you might have about various sides of history, (A) how do we generally feel about building a model this way and (B) do we think enough people would answer the call?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig8ve3/americans_can_distill_models_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T21:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig2cm2</id>
    <title>mistral-small-24b-instruct-2501 is simply the best model ever made.</title>
    <updated>2025-02-02T17:25:29+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s the only truly good model that can run locally on a normal machine. I'm running it on my M3 36GB and it performs fantastically with 18 TPS (tokens per second). It responds to everything precisely for day-to-day use, serving me as well as ChatGPT does.&lt;/p&gt; &lt;p&gt;For the first time, I see a local model actually delivering satisfactory results. Does anyone else think so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T17:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ig6e6t</id>
    <title>DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.</title>
    <updated>2025-02-02T20:12:17+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt; &lt;img alt="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." src="https://external-preview.redd.it/Er7i7V1ka8BO-MpGkuLs0Jmvu0-6GTVfn9JqY2PTKfM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd56ea2fa742541be1366b6615889d6a52f560b3" title="DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We knew R1 was good, but not that good. All the cries of CCP censorship are meaningless when it's trivial to bypass its guard rails.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ig6e6t/deepseekr1_fails_every_safety_test_it_exhibits_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ify8f9</id>
    <title>How national security advisors evaluate tech companies</title>
    <updated>2025-02-02T14:22:33+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"&gt; &lt;img alt="How national security advisors evaluate tech companies" src="https://preview.redd.it/h1mbjvbdjqge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8a53ed64a3981fb5114d597cba9461646d11938" title="How national security advisors evaluate tech companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just realized I should have added tiktok. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1mbjvbdjqge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ify8f9/how_national_security_advisors_evaluate_tech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T14:22:33+00:00</published>
  </entry>
</feed>
