<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-17T16:39:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mrqj6y</id>
    <title>Moxie goes local</title>
    <updated>2025-08-16T09:42:55+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt; &lt;img alt="Moxie goes local" src="https://external-preview.redd.it/NjRrNWZhaTZyY2pmMSz-4GeMjZaaPuK_BtqJdauJLy8SeG31djvp2OceGUPi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f56d4e2d6d85d38d0a6fee04a3f5cd06f2d2d7df" title="Moxie goes local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a localllama version of the OpenMoxie&lt;/p&gt; &lt;p&gt;It uses faster-whisper on the local for STT or the OpenAi whisper api (when selected in setup)&lt;/p&gt; &lt;p&gt;Supports LocalLLaMA, or OpenAi for conversations.&lt;/p&gt; &lt;p&gt;I also added support for XAI (Grok3 et al ) using the XAI API.&lt;/p&gt; &lt;p&gt;allows you to select what AI model you want to run for the local service.. right now 3:2b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiwf36o6rcjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrtea</id>
    <title>Any benchmark for dual Nvidia rtx 6000 pro Blackwell?</title>
    <updated>2025-08-17T14:06:09+00:00</updated>
    <author>
      <name>/u/Reasonable_Friend_77</name>
      <uri>https://old.reddit.com/user/Reasonable_Friend_77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, can't seem to find any. I'm interested in both vllm and sglang. Anyone with this setup willing to share? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Friend_77"&gt; /u/Reasonable_Friend_77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss3us</id>
    <title>Securing and Observing MCP Servers in Production</title>
    <updated>2025-08-17T14:18:16+00:00</updated>
    <author>
      <name>/u/No-Abies7108</name>
      <uri>https://old.reddit.com/user/No-Abies7108</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"&gt; &lt;img alt="Securing and Observing MCP Servers in Production" src="https://external-preview.redd.it/MOcxvduEb_5FhLeszm41Cc6Zs_nQlMs6Kli8iZmRCP8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc5ffec244f1d6bf85300caf2e437ec2e0eb7ab4" title="Securing and Observing MCP Servers in Production" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building with &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;? Cool, now here’s the hard part: making it &lt;strong&gt;secure, reliable, and observable&lt;/strong&gt; in production. In my new article, I walk through step-by-step practices: &lt;strong&gt;structured logging&lt;/strong&gt;, Moesif &amp;amp; New Relic monitoring, &lt;strong&gt;permission models&lt;/strong&gt;, and running audits with &lt;strong&gt;MCPSafetyScanner&lt;/strong&gt;. I also cover how to prevent &lt;em&gt;tool poisoning&lt;/em&gt; and &lt;em&gt;prompt injection&lt;/em&gt;. This isn’t theory, I’ve included JSON logging examples, observability code snippets, and real-world design patterns. Devs, what’s your monitoring stack for MCP today—rolling your own dashboards or plugging into platforms? Let’s swap notes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abies7108"&gt; /u/No-Abies7108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://glama.ai/blog/2025-08-17-monitoring-and-security-for-mcp-based-ai-systems"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms96z1</id>
    <title>AI Lifecycle in a Nutshell</title>
    <updated>2025-08-16T22:01:19+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt; &lt;img alt="AI Lifecycle in a Nutshell" src="https://b.thumbs.redditmedia.com/0zwQoI10gO1GKiDgCyUSu47ZcGjND6-vGxCtl-comsg.jpg" title="AI Lifecycle in a Nutshell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d"&gt;https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI Lifecycle in a Nutshell&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pay $20 to Cursor.&lt;/li&gt; &lt;li&gt;Cursor pays $50 to Claude (with $30 from VC money).&lt;/li&gt; &lt;li&gt;Claude pays $100 to Nvidia (with $50 from VC money).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;NOTE: Just for fun, not aimed at any specific company! :D&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1msm9ip</id>
    <title>GLM 4.5 stuck in a loop. Is this bug ?</title>
    <updated>2025-08-17T09:16:10+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"&gt; &lt;img alt="GLM 4.5 stuck in a loop. Is this bug ?" src="https://preview.redd.it/tzv51i1crjjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22c7ddfc86f2bb9bc9a780dd6ba57db0633a4e76" title="GLM 4.5 stuck in a loop. Is this bug ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tzv51i1crjjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T09:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msujl3</id>
    <title>Looking for LLM recommendations for a PC build - liberal arts focus over coding</title>
    <updated>2025-08-17T15:54:41+00:00</updated>
    <author>
      <name>/u/JayoTree</name>
      <uri>https://old.reddit.com/user/JayoTree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to build a PC next year and want to choose hardware that will run a good local LLM. I'm not a programmer, so I'm looking for models that excel at liberal arts tasks rather than coding.&lt;/p&gt; &lt;p&gt;Specifically, I want an LLM that's strong at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep literary analysis&lt;/li&gt; &lt;li&gt;Close reading of complex fiction and non-fiction&lt;/li&gt; &lt;li&gt;Interpretive work with challenging texts&lt;/li&gt; &lt;li&gt;General humanities research and writing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm less interested in models heavily focused on computer science, math, or programming tasks.&lt;/p&gt; &lt;p&gt;What local LLMs would you recommend for this use case, and what kind of hardware specs should I target to run them effectively?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JayoTree"&gt; /u/JayoTree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msujl3/looking_for_llm_recommendations_for_a_pc_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msujl3/looking_for_llm_recommendations_for_a_pc_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msujl3/looking_for_llm_recommendations_for_a_pc_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T15:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1msva3w</id>
    <title>MiniPC Ryzen 7 6800H iGPU 680M LLM benchmark Vulkan backend</title>
    <updated>2025-08-17T16:23:07+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System: MiniPC AceMagic AMD Ryzen 7 &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H&lt;/a&gt; with iGPU &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;680M&lt;/a&gt; and 64GB &lt;a href="https://en.wikipedia.org/wiki/DDR5_SDRAM"&gt;DDR5&lt;/a&gt; memory on &lt;a href="https://kubuntu.org/"&gt;Kubuntu&lt;/a&gt; 25.10 and &lt;a href="https://docs.mesa3d.org/relnotes/25.1.7.html"&gt;Mesa 25.1.7&lt;/a&gt;-1ubuntu1 for AMD open drivers. &lt;/p&gt; &lt;p&gt;I'm using llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/7195"&gt;bench&lt;/a&gt; feature with &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;Vulkan&lt;/a&gt; backend. I've been using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for doing local AI stuff. I found llama.cpp is easier and faster to get LLM going compared to &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;Ollama&lt;/a&gt; with &lt;a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md#overrides-on-linux"&gt;overriding&lt;/a&gt; ROCm environment for iGPU and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;older Radeon cards&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;I download &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;llama-b6182-bin-ubuntu-vulkan-x64&lt;/a&gt; and just unzipped. Kubuntu already has AMD drivers baked into its kernel thanks to Mesa.&lt;/p&gt; &lt;p&gt;I consider 3 to 4 tokens per second (t/s) for token generation (tg128) as minimum and I like 14B models &lt;a href="https://llm-stats.com/models/compare/qwen-2.5-14b-instruct-vs-qwen2-7b-instruct"&gt;accuracy&lt;/a&gt; versus smaller models. So here we go.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF"&gt;Qwen2.5-Coder-14B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;size: 14.62 GiB&lt;/p&gt; &lt;p&gt;params: 14.77 B&lt;/p&gt; &lt;p&gt;ngl: 99&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;p&gt;Regular CPU only llama.cpp (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6187-bin-ubuntu-x64.zip"&gt;llama-b6182-bin-ubuntu-x64&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ~/build/bin/llama-bench --model /var/lib/gpustack/cache/huggingface/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q8_0.gguf load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-haswell.so | model | backend | test | t/s | | --------------- | ---------- | --------------: | -------------------: | | qwen2 14B Q8_0 | RPC | pp512 | 19.04 ± 0.05 | | qwen2 14B Q8_0 | RPC | tg128 | 3.26 ± 0.00 | build: 1fe00296 (6182) real 6m8.309s user 47m37.413s sys 0m6.497s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Vulkan CPU/iGPU llama.cpp (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6187-bin-ubuntu-vulkan-x64.zip"&gt;llama-b6187-bin-ubuntu-vulkan-x64&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ~/vulkan/build/bin/llama-bench --model /var/lib/gpustack/cache/huggingface/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q8_0.gguf load_backend: loaded RPC backend from /home/user33/vulkan/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV REMBRANDT) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/user33/vulkan/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /home/user33/vulkan/build/bin/libggml-cpu-haswell.so | model | backend | test | t/s | | -------------- | ---------- | --------------: | -------------------: | | qwen2 14B Q8_0 | RPC,Vulkan | pp512 | 79.34 ± 1.15 | | qwen2 14B Q8_0 | RPC,Vulkan | tg128 | 3.12 ± 0.75 | build: 1fe00296 (6182) real 4m21.431s user 1m1.655s sys 0m9.730s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Observation:&lt;/p&gt; &lt;p&gt;VULKAN backend total benchmark run time (real) dropped from 6m8s to 4m21s and &lt;/p&gt; &lt;p&gt;pp512 increased from 19.04 to 79.34 while &lt;/p&gt; &lt;p&gt;tg128 decreased from 3.26 to 3.12&lt;/p&gt; &lt;p&gt;Considering slight difference in token generation speed, using Vulkan backend for AMD CPU 6800H benefits from the iGPU 680M overall llama performance over CPU only. DDR5 memory bandwidth is doing the bulk of the work but we should see continuous improvements with Vulkan. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms222w</id>
    <title>"AGI" is equivalent to "BTC is going to take over the financial world"</title>
    <updated>2025-08-16T17:37:53+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;AGI&amp;quot; is really just another hypetrain. Sure AI is going to disrupt industries, displace jobs and cause mayhem in the social fabric - but the omnipotent &amp;quot;AGI&amp;quot; that governs all aspects of life and society and most importantly, ushers in &amp;quot;post labor economics&amp;quot;? Wonder how long it takes until tech bros and fanboys realize this. GPT5, Opus 4 and all others are only incremental improvements, if at all. Where's the path to &amp;quot;AGI&amp;quot; in this reality? People who believe this are going to build a bubble for themselves, detached from reality.&lt;/p&gt; &lt;p&gt;EDIT: Since this post blew up harder than BTC in the current bullrun and lots of people thought it's about denying the potential of either technology or comparing the technologies I feel it's important to point out what it's really about. All this is saying, is that both communities seem to expose a simillar psychological pattern. Excited by the undoubted potential of both technologies, some individuals and groups start to project this idea of the 'ultimate revolution', that's always just around the corner. &amp;quot;Just another 2, 5 or 10 years and we're there&amp;quot; creates this nexus of constant fear or hope that just never materializes. This is the point, that some people in both groups seem to expect this &amp;quot;day of reckoning&amp;quot; which is oddly familiar with what you'd find in religious texts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T17:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1msbfw9</id>
    <title>Qwen3 just gets me ❤️</title>
    <updated>2025-08-16T23:31:35+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt; &lt;img alt="Qwen3 just gets me ❤️" src="https://b.thumbs.redditmedia.com/0GYMAK-JtLBRlRV0DjTz-LN109QuZuO4A0Gni-WxIJI.jpg" title="Qwen3 just gets me ❤️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1msbfw9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn987</id>
    <title>Profiling Large Language Model Inference on Apple Silicon: A Quantization Perspective</title>
    <updated>2025-08-17T10:19:17+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.08531"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn987/profiling_large_language_model_inference_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn987/profiling_large_language_model_inference_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mskf61</id>
    <title>OpenEvolve Beats GEPA Benchmarks: +6.42% Overall Improvement with Evolutionary Prompt Optimization</title>
    <updated>2025-08-17T07:19:43+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Wanted to share results from &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of evolutionary prompt optimization that's achieving strong performance on benchmarks from the recent GEPA paper.&lt;/p&gt; &lt;h2&gt;Context: The GEPA Paper&lt;/h2&gt; &lt;p&gt;Researchers recently released &lt;a href="https://arxiv.org/abs/2507.19457"&gt;GEPA (Genetic-Pareto)&lt;/a&gt;, a prompt optimization technique that uses natural language reflection to improve LLM performance. GEPA reports 10-20% improvements over GRPO and 10%+ over MIPROv2, using up to 35x fewer rollouts by leveraging the interpretable nature of language as a learning medium.&lt;/p&gt; &lt;h2&gt;OpenEvolve Results (Same Benchmarks as GEPA)&lt;/h2&gt; &lt;p&gt;OpenEvolve improved prompts across 11,946 samples:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;Evolved&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;IFEval&lt;/strong&gt; (instruction following)&lt;/td&gt; &lt;td&gt;95.01%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;97.41%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+2.40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HotpotQA&lt;/strong&gt; (multi-hop reasoning)&lt;/td&gt; &lt;td&gt;77.93%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;88.62%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+10.69% 🔥&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HoVer&lt;/strong&gt; (claim verification)&lt;/td&gt; &lt;td&gt;43.83%&lt;/td&gt; &lt;td&gt;42.90%&lt;/td&gt; &lt;td&gt;-0.93%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.29%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.71%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+6.42%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That's &lt;strong&gt;767 more correct answers&lt;/strong&gt; with &lt;strong&gt;38% fewer empty responses&lt;/strong&gt;!&lt;/p&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;p&gt;OpenEvolve takes a different approach from GEPA's reflection-based optimization and DSPy's gradient-based methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MAP-Elites Algorithm&lt;/strong&gt;: Maintains diversity through multi-dimensional feature grids&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Island Evolution&lt;/strong&gt;: 4 isolated populations evolve independently with periodic migration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cascade Evaluation&lt;/strong&gt;: Quick validation (10 samples) before expensive full tests (40+ samples)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-Judge&lt;/strong&gt;: Combines quantitative accuracy with qualitative feedback on clarity/robustness&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Example Evolution (HotpotQA)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Before&lt;/strong&gt;: Basic prompt asking for answer&lt;br /&gt; &lt;strong&gt;After 50 iterations&lt;/strong&gt;: Structured multi-step reasoning with paragraph analysis, synthesis, and citation requirements&lt;/p&gt; &lt;h2&gt;Quick Start&lt;/h2&gt; &lt;p&gt;&lt;code&gt;bash git clone https://github.com/codelion/openevolve cd openevolve/examples/llm_prompt_optimization pip install -r requirements.txt python evaluate_prompts.py --dataset all --prompt-type evolved &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API (OpenRouter, vLLM, Ollama).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/openevolve"&gt;OpenEvolve Repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone's compared evolutionary vs reflection-based (GEPA) vs gradient-based (DSPy) approaches on their own tasks? What's been your experience with prompt optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T07:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss11l</id>
    <title>LL3M: Large Language 3D Modelers</title>
    <updated>2025-08-17T14:15:10+00:00</updated>
    <author>
      <name>/u/codexauthor</name>
      <uri>https://old.reddit.com/user/codexauthor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codexauthor"&gt; /u/codexauthor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threedle.github.io/ll3m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msa1n4</id>
    <title>So Steam finally got back to me</title>
    <updated>2025-08-16T22:34:49+00:00</updated>
    <author>
      <name>/u/ChrisZavadil</name>
      <uri>https://old.reddit.com/user/ChrisZavadil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 5 weeks of waiting for steam to approve my application that would allow users to input their own llms locally and communicate with them they told me that my app failed testing because it lacked the proper guardrails. They want me to block input and output for the LLM. Anybody put an unguarded LLM on Steam before?&lt;/p&gt; &lt;p&gt;I added a walledguard and re-uploaded, but for now I just made the full unrestricted version available on Itch if anyone wants to give it a try:&lt;br /&gt; &lt;a href="https://zavgaming.itch.io/megan-ai"&gt;https://zavgaming.itch.io/megan-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChrisZavadil"&gt; /u/ChrisZavadil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms9djc</id>
    <title>Wan2.2 i2v Censors Chinese-looking women in nsfw workflows</title>
    <updated>2025-08-16T22:08:13+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using wan2.2 i2v for generating over 100 nsfw videos so far. Noticed something curious. Lol When input image is chinese-looking, it never outputs nsfw videos. But when I use non-chinese input images, it outputs nsfw.&lt;/p&gt; &lt;p&gt;Anybody else experienced this? Lol really curious shiz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn7pt</id>
    <title>XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</title>
    <updated>2025-08-17T10:16:39+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; XQuant proposes caching low-bit &lt;strong&gt;layer inputs (X)&lt;/strong&gt; instead of the usual KV cache and &lt;strong&gt;rematerializing K/V on the fly&lt;/strong&gt;, trading extra compute for far less memory traffic; this gives an immediate &lt;strong&gt;~2×&lt;/strong&gt; cut vs standard KV caching and up to &lt;strong&gt;~7.7×&lt;/strong&gt; vs FP16 with &lt;strong&gt;&amp;lt;0.1&lt;/strong&gt; perplexity drop, while the cross-layer variant (&lt;strong&gt;XQuant-CL&lt;/strong&gt;) reaches &lt;strong&gt;10× (≈0.01 ppl)&lt;/strong&gt; and &lt;strong&gt;12.5× (≈0.1 ppl)&lt;/strong&gt;, with near-FP16 accuracy and better results than prior KV-quant methods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.10395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms4n55</id>
    <title>What does it feel like: Cloud LLM vs Local LLM.</title>
    <updated>2025-08-16T19:10:29+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt; &lt;img alt="What does it feel like: Cloud LLM vs Local LLM." src="https://preview.redd.it/8qtcdau4kfjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c4609d4440c5a870f624682bb7bead5dece104" title="What does it feel like: Cloud LLM vs Local LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qtcdau4kfjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjr8e</id>
    <title>Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp; 1.6B)</title>
    <updated>2025-08-17T06:39:35+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt; &lt;img alt="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" src="https://external-preview.redd.it/ODf4ePnObFjNLo_T-D3tl5IjEp3QG9wN69Zl1K75jBk.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6edbd43244d19019e3eb00f1cf461de13c681f23" title="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;2 models based on the hybrid &lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt;LFM2 architecture&lt;/a&gt;: LFM2-VL-450M and LFM2-VL-1.6B&lt;/li&gt; &lt;li&gt;Available quant: 8bit MLX, GGUF Q8 &amp;amp; Q4 (llama.cpp release &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6183"&gt;b6183&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f7cnaj82zijf1.png?width=2072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8dae64814c6f611481706d86e4a7643b7dc776"&gt;Figure 3. Processing time comparison across vision-language models.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Added GGUF availability and compatible llama.cpp release&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T06:39:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn1n0</id>
    <title>Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩</title>
    <updated>2025-08-17T10:06:04+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt; &lt;img alt="Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩" src="https://preview.redd.it/e0o9q4g90kjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96de0c19ca32f0ffe1af17db9ea73f11e103ccae" title="Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e0o9q4g90kjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msv4us</id>
    <title>GPT-OSS is not good at Brazilian Legal Framework :(</title>
    <updated>2025-08-17T16:17:35+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt; &lt;img alt="GPT-OSS is not good at Brazilian Legal Framework :(" src="https://preview.redd.it/uqksokgduljf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d0c45bd812e8cb6b20834de28e876efa3b08b4c" title="GPT-OSS is not good at Brazilian Legal Framework :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;benchmark: &lt;a href="https://huggingface.co/datasets/celsowm/legalbench.br"&gt;https://huggingface.co/datasets/celsowm/legalbench.br&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uqksokgduljf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msgl6q</id>
    <title>Added Qwen 0.6B to the small model overview in IFEval.</title>
    <updated>2025-08-17T03:41:26+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt; &lt;img alt="Added Qwen 0.6B to the small model overview in IFEval." src="https://external-preview.redd.it/9Cl7KVCIkap1D9OBhLKIL0DrKnbvINMV1azrpCVXD0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db8e296f1f9b6b888a016ade0da5771f2fa87434" title="Added Qwen 0.6B to the small model overview in IFEval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ygMzbHp.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T03:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1msb0mq</id>
    <title>For those who run large models locally.. HOW DO YOU AFFORD THOSE GPUS</title>
    <updated>2025-08-16T23:14:00+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;okay I'm just being nosy.. I mostly run models and fine tune as a hobby so I typically only run models under the 10b parameter range, is everyone that is running larger models just paying for cloud services to run them? and for those of you who do have stacks of A100/H100s is this what you do for a living, how do you afford it??&lt;/p&gt; &lt;p&gt;edit: for more context about me and my setup, I have a 3090ti and 64gb ram, I am actually a cgi generalist / 3d character artist and my industry is taking a huge hit right now, so with my extra free time and my already decent set up I've been learning to fine tune models and format data on the side, idk if ill ever do a full career 180 but I love new tech (even though these new technologies and ideas are eating my current career)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1msosdv</id>
    <title>Why does Mistral NeMo's usage keep growing even after more than a year since releasing?</title>
    <updated>2025-08-17T11:46:54+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt; &lt;img alt="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" src="https://preview.redd.it/5wd0ayxihkjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef52cf7168e409394d3d181f871e20c40bcefa5d" title="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wd0ayxihkjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T11:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn3gi</id>
    <title>Ovis2.5 9B ~ 2B - New Multi-modal LLMs from Alibaba</title>
    <updated>2025-08-17T10:09:17+00:00</updated>
    <author>
      <name>/u/Sad_External6106</name>
      <uri>https://old.reddit.com/user/Sad_External6106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing with &lt;strong&gt;Ovis2.5 (2B &amp;amp; 9B)&lt;/strong&gt; the past few days. The cool part is it now has an optional &lt;em&gt;think&lt;/em&gt; mode — the model will slow down a bit but actually self-check and refine answers, which really helps on harder reasoning tasks. Also the OCR feels way better than before, especially on messy charts and dense documents. Overall, a pretty practical upgrade if you care about reasoning + OCR.&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335"&gt;https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_External6106"&gt; /u/Sad_External6106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrnqq</id>
    <title>Wow anthropic and Google losing coding share bc of qwen 3 coder</title>
    <updated>2025-08-17T13:59:47+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt; &lt;img alt="Wow anthropic and Google losing coding share bc of qwen 3 coder" src="https://preview.redd.it/rwehyliy5ljf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67f716968425732683dc36fdd2644caa8322da3" title="Wow anthropic and Google losing coding share bc of qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwehyliy5ljf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
