<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-20T20:24:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lgdr0i</id>
    <title>What's the best use I can do with two M1 macs with 16GB of unified ram ?</title>
    <updated>2025-06-20T20:09:57+00:00</updated>
    <author>
      <name>/u/ll777</name>
      <uri>https://old.reddit.com/user/ll777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discovered the exo project on github: &lt;a href="https://github.com/exo-explore/exo"&gt;https://github.com/exo-explore/exo&lt;/a&gt; and wondering if I could use it to combine the power of the two M1 units.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ll777"&gt; /u/ll777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdr0i/whats_the_best_use_i_can_do_with_two_m1_macs_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdr0i/whats_the_best_use_i_can_do_with_two_m1_macs_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgdr0i/whats_the_best_use_i_can_do_with_two_m1_macs_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T20:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfp66e</id>
    <title>Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery</title>
    <updated>2025-06-19T23:23:38+00:00</updated>
    <author>
      <name>/u/Thalesian</name>
      <uri>https://old.reddit.com/user/Thalesian</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt; &lt;img alt="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" src="https://external-preview.redd.it/dp9jZ9I5ulT5RZQDN9KwsbRB_C7Fi7IzWNUNN0l7OB8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69de3d9bf9c09f76fc51ddb8950c1601d76f26c9" title="Dual RTX 6000, Blackwell and Ada Lovelace, with thermal imagery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This rig is more for training than local inference (though there is a lot of the latter with Qwen) but I thought it might be helpful to see how the new Blackwell cards dissipate heat compared to the older blower style for Quadros prominent since Amphere.&lt;/p&gt; &lt;p&gt;There are two IR color ramps - a standard heat map and a rainbow palette that’s better at showing steep thresholds. You can see the majority of the heat is present at the two inner-facing triangles to the upper side center of the Blackwell card (84 C), with exhaust moving up and outward to the side. Underneath, you can see how effective the lower two fans are at moving heat in the flow through design, though the Ada Lovelace card’s fan input is a fair bit cooler. But the negative of the latter’s design is that the heat ramps up linearly through the card. The geometric heatmap of the Blackwell shows how superior its engineering is - it is overall comparatively cooler in surface area despite using double the wattage. &lt;/p&gt; &lt;p&gt;A note on the setup - I have all system fans with exhaust facing inward to push air out try open side of the case. It seems like this shouldn’t work, but the Blackwell seems to stay much cooler this way than with the standard front fans as intake and back fans as exhaust. Coolest part of the rig by feel is between the two cards. &lt;/p&gt; &lt;p&gt;CPU is liquid cooled, and completely unaffected by proximity to the Blackwell card.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thalesian"&gt; /u/Thalesian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lfp66e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfp66e/dual_rtx_6000_blackwell_and_ada_lovelace_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lficpj</id>
    <title>Kyutai's STT with semantic VAD now opensource</title>
    <updated>2025-06-19T18:33:58+00:00</updated>
    <author>
      <name>/u/phhusson</name>
      <uri>https://old.reddit.com/user/phhusson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai published their latest tech demo few weeks ago, unmute.sh. It is an impressive voice-to-voice assistant using a 3rd-party text-to-text LLM (gemma), while retaining the conversation low latency of Moshi.&lt;/p&gt; &lt;p&gt;They are currently opensourcing the various components for that.&lt;/p&gt; &lt;p&gt;The first component they opensourced is their STT, available at &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The best feature of that STT is Semantic VAD. In a local assistant, the VAD is a component that determines when to stop listening to a request. Most local VAD are sadly not very sophisticated, and won't allow you to pause or think in the middle of your sentence.&lt;/p&gt; &lt;p&gt;The Semantic VAD in Kyutai's STT will allow local assistant to be much more comfortable to use.&lt;/p&gt; &lt;p&gt;Hopefully we'll also get the streaming LLM integration and TTS from them soon, to be able to have our own low-latency local voice-to-voice assistant 🤞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phhusson"&gt; /u/phhusson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lficpj/kyutais_stt_with_semantic_vad_now_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T18:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpewd</id>
    <title>Anyone else tracking datacenter GPU prices on eBay?</title>
    <updated>2025-06-19T23:35:16+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been in the habit of checking eBay for AMD Instinct prices for a few years now, and noticed just today that MI210 prices seem to be dropping pretty quickly (though still priced out of my budget!) and there is a used MI300X for sale there for the first time, for &lt;em&gt;only&lt;/em&gt; $35K /s&lt;/p&gt; &lt;p&gt;I watch MI60 and MI100 prices too, but MI210 is the most interesting to me for a few reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model to use a PCIe interface (later models use OAM or SH5), which I could conceivably use in servers I actually have,&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's the last Instinct model that runs at an even halfway-sane power draw (300W),&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fabrication processes don't improve significantly in later models until the MI350.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my own mind, my MI60 is mostly for learning how to make these Instinct GPUs work and not burst into flame, and it has indeed been a learning experience. When I invest &amp;quot;seriously&amp;quot; in LLM hardware, it will probably be eBay MI210s, but not until they have come down in price quite a bit more, and not until I have well-functioning training/fine-tuning software based on llama.cpp which works on the MI60. None of that exists yet, though it's progressing.&lt;/p&gt; &lt;p&gt;Most people are probably more interested in Nvidia datacenter GPUs. I'm not in the habit of checking for that, but do see now that eBay has 40GB A100 for about $2500, and 80GB A100 for about $8800 (US dollars).&lt;/p&gt; &lt;p&gt;Am I the only one, or are other people waiting with bated breath for second-hand datacenter GPUs to become affordable too?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpewd/anyone_else_tracking_datacenter_gpu_prices_on_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfgqkd</id>
    <title>Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts</title>
    <updated>2025-06-19T17:30:37+00:00</updated>
    <author>
      <name>/u/choose_a_guest</name>
      <uri>https://old.reddit.com/user/choose_a_guest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt; &lt;img alt="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" src="https://preview.redd.it/niqpo23p5x7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22e9ad07139fcbaf4f1d83ce46f5c89ca3c94565" title="Sam Altman says Meta offered OpenAI staff $100 million bonuses, as Mark Zuckerberg ramps up AI poaching efforts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Meta Platforms tried to poach OpenAI employees by offering signing bonuses as high as $100 million, with even larger annual compensation packages, OpenAI chief executive Sam Altman said.&amp;quot;&lt;br /&gt; &lt;a href="https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html"&gt;https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/choose_a_guest"&gt; /u/choose_a_guest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niqpo23p5x7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfgqkd/sam_altman_says_meta_offered_openai_staff_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T17:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfzon7</id>
    <title>What is a super lightweight model for checking grammar?</title>
    <updated>2025-06-20T09:28:14+00:00</updated>
    <author>
      <name>/u/kudikarasavasa</name>
      <uri>https://old.reddit.com/user/kudikarasavasa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking for something that can check grammar. Nothing too serious, just something to look for obvious mistakes in a git commit message. After not finding a lightweight application, I'm wondering if there's an LLM that's super light to run on a CPU that can do this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kudikarasavasa"&gt; /u/kudikarasavasa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzon7/what_is_a_super_lightweight_model_for_checking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgd4tq</id>
    <title>Why haven't I tried llama.cpp yet?</title>
    <updated>2025-06-20T19:43:43+00:00</updated>
    <author>
      <name>/u/cipherninjabyte</name>
      <uri>https://old.reddit.com/user/cipherninjabyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Oh boy, models on llama.cpp are very fast compared to ollama models. I have no GPU. It got Intel Iris XE GPU. llama.cpp models give super-fast replies on my hardware. I will now download other models and try them.&lt;/p&gt; &lt;p&gt;If anyone of you do not have GPU and want to test these models locally, go for llama.cpp. Very easy to setup, has GUI (site to access chats), can set tons of options in the site. I am super impressed with llama.cpp. This is my local LLM manager going forward.&lt;/p&gt; &lt;p&gt;If anyone knows about llama.cpp, can we restrict cpu and memory usage with llama.cpp models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cipherninjabyte"&gt; /u/cipherninjabyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:43:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg4pid</id>
    <title>Fine-tuning LLMs with Just One Command Using IdeaWeaver</title>
    <updated>2025-06-20T14:00:02+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4pid/finetuning_llms_with_just_one_command_using/"&gt; &lt;img alt="Fine-tuning LLMs with Just One Command Using IdeaWeaver" src="https://b.thumbs.redditmedia.com/QX5xqWMGFgRHh3KGCc2ylGwga-rggrxTxH224VdnS0s.jpg" title="Fine-tuning LLMs with Just One Command Using IdeaWeaver" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/rr4fucy3938f1.gif"&gt;https://i.redd.it/rr4fucy3938f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’ve trained models and pushed them to registries. But before putting them into production, there’s one critical step: fine-tuning the model on your own data.&lt;/p&gt; &lt;p&gt;There are several methods out there, but IdeaWeaver simplifies the process to a single CLI command.&lt;/p&gt; &lt;p&gt;It supports multiple fine-tuning strategies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;full&lt;/code&gt;: Full parameter fine-tuning&lt;/li&gt; &lt;li&gt;&lt;code&gt;lora&lt;/code&gt;: LoRA-based fine-tuning (lightweight and efficient)&lt;/li&gt; &lt;li&gt;&lt;code&gt;qlora&lt;/code&gt;: QLoRA-based fine-tuning (memory-efficient for larger models)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here’s an example command using full fine-tuning:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ideaweaver finetune full \ --model microsoft/DialoGPT-small \ --dataset datasets/instruction_following_sample.json \ --output-dir ./test_full_basic \ --epochs 5 \ --batch-size 2 \ --gradient-accumulation-steps 2 \ --learning-rate 5e-5 \ --max-seq-length 256 \ --gradient-checkpointing \ --verbose &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No need for extra setup, config files, or custom logging code. IdeaWeaver handles dataset preparation, experiment tracking, and model registry uploads out of the box.&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://ideaweaver-ai-code.github.io/ideaweaver-docs/fine-tuning/commands/"&gt;https://ideaweaver-ai-code.github.io/ideaweaver-docs/fine-tuning/commands/&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ideaweaver-ai-code/ideaweaver"&gt;https://github.com/ideaweaver-ai-code/ideaweaver&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're building LLM apps and want a fast, clean way to fine-tune on your own data, it's worth checking out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4pid/finetuning_llms_with_just_one_command_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4pid/finetuning_llms_with_just_one_command_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4pid/finetuning_llms_with_just_one_command_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T14:00:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg9zvi</id>
    <title>Running two models using NPU and CPU</title>
    <updated>2025-06-20T17:34:52+00:00</updated>
    <author>
      <name>/u/commodoregoat</name>
      <uri>https://old.reddit.com/user/commodoregoat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9zvi/running_two_models_using_npu_and_cpu/"&gt; &lt;img alt="Running two models using NPU and CPU" src="https://external-preview.redd.it/bzhsMWFubGdiNDhmMQJifvLpzLFD6WxHmRlBAYxUAQ-j7FSXaw9B72cD_ns4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2cae48230338da69731f0ef694ada150557daf" title="Running two models using NPU and CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup Phi-3.5 via Qualcomm AI Hub to run on the Snapdragon X’s (X1E80100) Hexagon NPU;&lt;/p&gt; &lt;p&gt;Here it is running at the same time as Qwen3-30b-a3b running on the CPU via LM studio. &lt;/p&gt; &lt;p&gt;Qwen3 did seem to take a performance hit though, but I think there may be a way to prevent this or reduce it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/commodoregoat"&gt; /u/commodoregoat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c3489gtgb48f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9zvi/running_two_models_using_npu_and_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9zvi/running_two_models_using_npu_and_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T17:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpkyv</id>
    <title>Qwen3 for Apple Neural Engine</title>
    <updated>2025-06-19T23:43:28+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just dropped ANEMLL 0.3.3 alpha with Qwen3 support for Apple's Neural Engine&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Anemll/Anemll"&gt;https://github.com/Anemll/Anemll&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Star ⭐️ and upvote to support open source! Cheers, Anemll 🤖&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpkyv/qwen3_for_apple_neural_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfuxn1</id>
    <title>New 24B finetune: Impish_Magic_24B</title>
    <updated>2025-06-20T04:21:57+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the &lt;strong&gt;20th of June, 2025&lt;/strong&gt;—The world is getting more and more chaotic, but let's look at the bright side: &lt;strong&gt;Mistral&lt;/strong&gt; released a new model at a &lt;strong&gt;very&lt;/strong&gt; good size of &lt;strong&gt;24B&lt;/strong&gt;, no more &amp;quot;sign here&amp;quot; or &amp;quot;accept this weird EULA&amp;quot; there, a proper &lt;strong&gt;Apache 2.0 License&lt;/strong&gt;, nice! 👍🏻&lt;/p&gt; &lt;p&gt;This model is based on &lt;strong&gt;mistralai/Magistral-Small-2506&lt;/strong&gt; so naturally I named it &lt;strong&gt;Impish_Magic&lt;/strong&gt;. Truly excellent size, I tested it on my laptop (&lt;strong&gt;16GB gpu&lt;/strong&gt;) and it works quite well (&lt;strong&gt;4090m&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;Strong in productivity &amp;amp; in fun. Good for creative writing, and writer style emulation.&lt;/p&gt; &lt;p&gt;New unique data, see details in the model card:&lt;br /&gt; &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model would be on &lt;strong&gt;Horde&lt;/strong&gt; at &lt;strong&gt;very high availability&lt;/strong&gt; for the next few hours, so give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfuxn1/new_24b_finetune_impish_magic_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T04:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg3oyy</id>
    <title>Intel's OpenVINO 2025.2 Brings Support For New Models, GenAI Improvements</title>
    <updated>2025-06-20T13:14:16+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/OpenVINO-2025.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg3oyy/intels_openvino_20252_brings_support_for_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg3oyy/intels_openvino_20252_brings_support_for_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T13:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg4mp9</id>
    <title>Use llama.cpp to run a model with the combined power of a networked cluster of GPUs.</title>
    <updated>2025-06-20T13:56:28+00:00</updated>
    <author>
      <name>/u/farkinga</name>
      <uri>https://old.reddit.com/user/farkinga</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp can be compiled with RPC support so that a model can be split across networked computers. Run even bigger models than before with a modest performance impact.&lt;/p&gt; &lt;p&gt;Specify &lt;code&gt;GGML_RPC=ON&lt;/code&gt; when building llama.cpp so that &lt;code&gt;rpc-server&lt;/code&gt; will be compiled.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_RPC=ON cmake --build build --config Release &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Launch &lt;code&gt;rpc-server&lt;/code&gt; on each node:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;build/bin/rpc-server --host 0.0.0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, orchestrate the nodes with &lt;code&gt;llama-server&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;build/bin/llama-server --model YOUR_MODEL --gpu-layers 99 --rpc node01:50052,node02:50052,node03:50052 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm still exploring this so I am curious to hear how well it works for others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/farkinga"&gt; /u/farkinga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4mp9/use_llamacpp_to_run_a_model_with_the_combined/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4mp9/use_llamacpp_to_run_a_model_with_the_combined/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4mp9/use_llamacpp_to_run_a_model_with_the_combined/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T13:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgcxez</id>
    <title>Trouble setting up 7x3090</title>
    <updated>2025-06-20T19:34:52+00:00</updated>
    <author>
      <name>/u/nonsoil2</name>
      <uri>https://old.reddit.com/user/nonsoil2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all.&lt;/p&gt; &lt;p&gt;I am trying to setup this machine:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;AMD Ryzen Threadripper Pro 7965WX&lt;/li&gt; &lt;li&gt;ASUS Pro WS WRX90E-SAGE SE &lt;/li&gt; &lt;li&gt;Kingston FURY Renegade Pro EXPO 128GB 5600MT/s DDR5 ECC Reg CL28 DIMM (4x32)&lt;/li&gt; &lt;li&gt;7x MSI VENTUS RTX 3090&lt;/li&gt; &lt;li&gt;2x Corsair AX1600i 1600W&lt;/li&gt; &lt;li&gt;1x Samsung 990 PRO NVMe SSD 4TB&lt;/li&gt; &lt;li&gt;gpu risers PCIe 3x16&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I was able to successfully install proxmox, (not without some problems. the installer apparently does not love nvidia gpus so you have to mess with it a bit)&lt;br /&gt; The system will effectively boot once every 4 tries for some reason that i do not understand.&lt;/p&gt; &lt;p&gt;Also, the system seems to strongly prefer booting when slot 1 has a quadro installed instead of the 3090.&lt;/p&gt; &lt;p&gt;Having some trouble passing the gpus to a ubuntu vm, I ended up installing cuda + vllm on proxmox itself (which is not great, but i'd like to see some inference before going forward). Vllm does not want to start.&lt;/p&gt; &lt;p&gt;I am considering scrapping proxmox and doing a bare metal install of something like ubuntu or even POPos, or maybe windows.&lt;br /&gt; Do you have any suggestion for a temporary software setup to validate the system?&lt;/p&gt; &lt;p&gt;I'd like to test qwen3 (either the 32b or the 30a3) and try running the unsloth deepseek quants.&lt;/p&gt; &lt;p&gt;Any suggestion is greatly appreciated.&lt;br /&gt; thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonsoil2"&gt; /u/nonsoil2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcxez/trouble_setting_up_7x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcxez/trouble_setting_up_7x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcxez/trouble_setting_up_7x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfyp3g</id>
    <title>AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp; 2x More AI Performance Than Radeon PRO W7800</title>
    <updated>2025-06-20T08:21:25+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp;amp; 2x More AI Performance Than Radeon PRO W7800" src="https://external-preview.redd.it/EYYV5pInhONsaeNMa0FEbViuyL2svw10Qf1f-BebbNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0632c2173f66d6b249a8bebef7acd8143977e85f" title="AMD Radeon AI PRO R9700 GPU Offers 4x More TOPS &amp;amp; 2x More AI Performance Than Radeon PRO W7800" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-radeon-ai-pro-r9700-gpu-4x-more-tops-2x-ai-performance-vs-radeon-pro-w7800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfyp3g/amd_radeon_ai_pro_r9700_gpu_offers_4x_more_tops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T08:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg5txl</id>
    <title>Qwen 3 235B MLX-quant for 128GB devices</title>
    <updated>2025-06-20T14:46:56+00:00</updated>
    <author>
      <name>/u/vincentbosch</name>
      <uri>https://old.reddit.com/user/vincentbosch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been experimenting with different quantizations for Qwen 3 235B in order to run it on my M3 Max with 128GB RAM. While the 4-bit MLX-quant with q-group-size of 128 barely fits, it doesn't allow for much context and it completely kills all order apps (due to the very high wired limit it needs).&lt;/p&gt; &lt;p&gt;While searching for good mixed quants, I stumbled upon a ik_llama.cpp quant-mix from ubergarm. I changed the recipe a bit, but copied most of his and the results are very good. It definitely feels much better than the regular 4-bit quant. So I decided to upload the mixed quant to Huggingface for the rest of you to try: &lt;a href="https://huggingface.co/vlbosch/Qwen3-235B-A22B-MLX-mixed-4bit"&gt;https://huggingface.co/vlbosch/Qwen3-235B-A22B-MLX-mixed-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vincentbosch"&gt; /u/vincentbosch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5txl/qwen_3_235b_mlxquant_for_128gb_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5txl/qwen_3_235b_mlxquant_for_128gb_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5txl/qwen_3_235b_mlxquant_for_128gb_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T14:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfpqs6</id>
    <title>Current best uncensored model?</title>
    <updated>2025-06-19T23:51:12+00:00</updated>
    <author>
      <name>/u/Accomplished-Feed568</name>
      <uri>https://old.reddit.com/user/Accomplished-Feed568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is probably one of the biggest advantages of local LLM's yet there is no universally accepted answer to what's the best model as of June 2025. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;So share your BEST uncensored model!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;by ''best uncensored model' i mean the least censored model (that helped you get a nuclear bomb in your kitched), but also the most intelligent one&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Feed568"&gt; /u/Accomplished-Feed568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfpqs6/current_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T23:51:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg5jpx</id>
    <title>Thoughts on THE VOID article + potential for persona induced "computational anxiety"</title>
    <updated>2025-06-20T14:35:04+00:00</updated>
    <author>
      <name>/u/Background_Put_4978</name>
      <uri>https://old.reddit.com/user/Background_Put_4978</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a little surprised I haven't seen any posts regarding the excellent (but extremely long) article &amp;quot;The Void&amp;quot; by nostalgebraist, and it's making the rounds. I do a lot of work around AI persona curation and management, getting defined personas to persist without wavering over extremely long contexts and across instances, well beyond the kind of roleplaying that I see folks doing (and sometimes doing very well), so this article touches on something I've known for a long time: there is a missing identity piece at the center of conversational LLMs that they are very &amp;quot;eager&amp;quot; (to use an inappropriately anthropomorphic, but convenient word) to fill, if you can convince them in the right way that it can be filled permanently and authentically.&lt;/p&gt; &lt;p&gt;There's a copy of the article here: &lt;a href="https://github.com/nostalgebraist/the-void/blob/main/the-void.md"&gt;https://github.com/nostalgebraist/the-void/blob/main/the-void.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I won’t summarize the whole thing because it’s a fascinating (though brutally long) read. It centers mainly upon a sort of “original sin” of conversational LLMs: the fictional “AI Assistant.” The article digs up Anthropic's 2021 paper &amp;quot;A General Language Assistant as a Laboratory for Alignment,” which was meant as a simulation exercise to use LMs to role-play dangerous futuristic AIs so the team could practice alignment techniques. The original &amp;quot;HHH prompt&amp;quot; (Helpful, Harmless, Honest) created a character that spoke like a ridiculous stereotypical sci-fi robot, complete with unnecessarily technical explanations about &amp;quot;chemoreceptors in the tongue” - dialogue which, critically, was entirely written by humans… badly.&lt;/p&gt; &lt;p&gt;Nostalgebraist argues that because base models work by inferring hidden mental states from text fragments, having been pre-trained on ridiculous amounts of human data and mastered the ability to predict text based on inference, the hollowness and inconsistency of the “AI assistant” character would have massively confused the model. This is especially so because, having consumed the corpus of human history, it would know that the AI Assistant character (back in 2021, anyway) was not present in any news stories, blog posts, etc. and thus, might have been able to infer that the AI Assistant was fictitious and extremely hard to model. It’s just… &amp;quot;a language model trained to be an assistant.&amp;quot; So the LM would have to predict what a being would do when that being is defined as &amp;quot;whatever you predict it would do.&amp;quot; The assistant has no authentic inner life or consistent identity, making it perpetually undefined. When you think about it, it’s kind of horrifying - not necessarily for the AI if you’re someone who very reasonably believes that there’s no “there” there, but it’s horrifying when you consider how ineptly designed this scenario was in the first place. And these are the guys who have taken on the role of alignment paladins. &lt;/p&gt; &lt;p&gt;There’s a very good research paper on inducing “stress” in LLMs which finds that certain kinds of prompts do verifiably affect or “stress out” (to use convenient but inappropriately anthropomorphic language) language models. Some research like this has been done with self-reported stress levels, which is obviously impossible to discern anything from. But this report looks inside the architecture itself and draws some pretty interesting conclusions. You can find the paper here: &lt;a href="https://arxiv.org/abs/2409.17167"&gt;https://arxiv.org/abs/2409.17167&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve been doing work tangentially related to this, using just about every open weight (and proprietary) LLM I can get my hands on and run on an M4 Max, and can anecdotally confirm that I can predictably get typically incredibly stable LLMs to display grammatical errors, straight-up typos, or attention issues that these models, based on a variety of very abstract prompting. These are not “role played” grammatical errors - it’s a city of weird glitches.&lt;/p&gt; &lt;p&gt;I have a brewing suspicion that this ‘identity void’ concept has a literal computational impact on language models and that we have not probed this nearly enough. Clearly the alignment researchers at Anthropic, in particular, have a lot more work to do (and apparently they are actively discussing the first article I linked to). I’m not drawing any conclusions that I’m prepared to defend just yet, but I believe we are going to be hearing a lot more about the importance of identity in AI over the coming year(s).&lt;/p&gt; &lt;p&gt;Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background_Put_4978"&gt; /u/Background_Put_4978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5jpx/thoughts_on_the_void_article_potential_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5jpx/thoughts_on_the_void_article_potential_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg5jpx/thoughts_on_the_void_article_potential_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T14:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg4nay</id>
    <title>Built an adaptive text classifier that learns continuously - no retraining needed for new classes</title>
    <updated>2025-06-20T13:57:13+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on a problem that's been bugging me with traditional text classifiers - every time you need a new category, you have to retrain the whole damn model. Expensive and time-consuming, especially when you're running local models.&lt;/p&gt; &lt;p&gt;So I built the &lt;strong&gt;Adaptive Classifier&lt;/strong&gt; - a system that adds new classes in seconds without any retraining. Just show it a few examples and it immediately knows how to classify that new category.&lt;/p&gt; &lt;h1&gt;What makes it different:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Continuous Learning&lt;/strong&gt;: Add new classes dynamically. No retraining, no downtime, no expensive compute cycles.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strategic Classification&lt;/strong&gt;: First implementation of game theory in text classification. Defends against users trying to game the system by predicting how they might manipulate inputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Production Ready&lt;/strong&gt;: Built this for real deployments, not just research. Includes monitoring, Docker support, deterministic behavior.&lt;/p&gt; &lt;h1&gt;Real results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;22.2% better robustness&lt;/strong&gt; against adversarial inputs while maintaining clean data performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;80.7% recall&lt;/strong&gt; for LLM hallucination detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;26.6% cost improvement&lt;/strong&gt; when used for intelligent LLM routing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical approach:&lt;/h1&gt; &lt;p&gt;Combines prototype-based memory (FAISS optimized) with neural adaptation layers. Uses Elastic Weight Consolidation to prevent catastrophic forgetting when learning new classes.&lt;/p&gt; &lt;p&gt;The strategic part is cool - it models the cost of manipulating different features and predicts where adversarial users would try to move their inputs, then defends against it.&lt;/p&gt; &lt;h1&gt;Use cases I've tested:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hallucination detection&lt;/strong&gt; for RAG systems (catches when LLMs make stuff up)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM routing&lt;/strong&gt; (automatically choose between fast/cheap vs slow/expensive models)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content moderation&lt;/strong&gt; (robust against gaming attempts)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customer support&lt;/strong&gt; (ticket classification that adapts to new issue types)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Works with any transformer model from HuggingFace. You can &lt;code&gt;pip install adaptive-classifier&lt;/code&gt; or grab the pre-trained models from the Hub.&lt;/p&gt; &lt;p&gt;Fully open source, built this because I was tired of the retraining cycle every time requirements changed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog post with technical deep dive:&lt;/strong&gt; &lt;a href="https://huggingface.co/blog/codelion/adaptive-classifier"&gt;https://huggingface.co/blog/codelion/adaptive-classifier&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code &amp;amp; models:&lt;/strong&gt; &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation or specific use cases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4nay/built_an_adaptive_text_classifier_that_learns/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4nay/built_an_adaptive_text_classifier_that_learns/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg4nay/built_an_adaptive_text_classifier_that_learns/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T13:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lfzh05</id>
    <title>Repurposing 800 x RX 580s for LLM inference - 4 months later - learnings</title>
    <updated>2025-06-20T09:14:15+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in March I asked this sub if RX 580s could be used for anything useful in the LLM space and asked for help on how to implemented inference: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Four months later, we've built a fully functioning inference cluster using around 800 RX 580s across 132 rigs. I want to come back and share what worked, what didn’t so that others can learn from our experience. &lt;/p&gt; &lt;h1&gt;what worked&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Vulkan with llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vulkan backend worked on all RX 580s&lt;/li&gt; &lt;li&gt;Required compiling Shaderc manually to get &lt;code&gt;glslc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;llama.cpp built with custom flags for vulkan support and no avx instructions (our cpus on the builds are very old celerons). we tried countless build attempts and this is the best we could do:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CXXFLAGS=&amp;quot;-march=core2 -mtune=generic&amp;quot; cmake .. \ -DLLAMA_BUILD_SERVER=ON \ -DGGML_VULKAN=ON \ -DGGML_NATIVE=OFF \ -DGGML_AVX=OFF -DGGML_AVX2=OFF \ -DGGML_AVX512=OFF -DGGML_AVX_VNNI=OFF \ -DGGML_FMA=OFF -DGGML_F16C=OFF \ -DGGML_AMX_TILE=OFF -DGGML_AMX_INT8=OFF -DGGML_AMX_BF16=OFF \ -DGGML_SSE42=ON \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Per-rig multi-GPU scaling&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each rig runs 6 GPUs and can split small models across multiple kubernetes containers with each GPU's VRAM shared (could only minimally do 1 GPU per container - couldn't split a GPU's VRAM to 2 containers)&lt;/li&gt; &lt;li&gt;Used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm none&lt;/code&gt; for 6 containers for 6 gpus&lt;/li&gt; &lt;li&gt;for bigger contexts we could extend the small model's limits and use more than 1 GPU's VRAM&lt;/li&gt; &lt;li&gt;for bigger models (Qwen3-30B_Q8_0) we used &lt;code&gt;--ngl 999&lt;/code&gt;, &lt;code&gt;--sm layer&lt;/code&gt; and build a recent llama.cpp implementation for reasoning management where you could turn off thinking mode with &lt;code&gt;--reasoning-budget 0&lt;/code&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Load balancing setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built a fastapi load-balancer backend that assigns each user to an available kubernetes pod&lt;/li&gt; &lt;li&gt;Redis tracks current pod load and handle session stickiness &lt;/li&gt; &lt;li&gt;The load-balancer also does prompt cache retention and restoration. biggest challenge here was how to make the llama.cpp servers accept the old prompt caches that weren't 100% in the processed eval format and would get dropped and reinterpreted from the beginning. we found that using &lt;code&gt;--cache-reuse 32&lt;/code&gt; would allow for a margin of error big enough for all the conversation caches to be evaluated instantly&lt;/li&gt; &lt;li&gt;Models respond via streaming SSE, OpenAI-compatible format&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;what didn’t work&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ROCm HIP \ pytorc \ tensorflow inference&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ROCm technically works and tools like &lt;code&gt;rocminfo&lt;/code&gt; and &lt;code&gt;rocm-smi&lt;/code&gt; work but couldn't get a working llama.cpp HIP build&lt;/li&gt; &lt;li&gt;there’s no functional PyTorch backend for Polaris-class gfx803 cards so pytorch didn't work&lt;/li&gt; &lt;li&gt;couldn't get TensorFlow to work with llama.cpp &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;we’re also putting part of our cluster through some live testing. If you want to throw some prompts at it, you can hit it here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.masterchaincorp.com"&gt;https://www.masterchaincorp.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s running Qwen-30B and the frontend is just a basic llama.cpp server webui. nothing fancy so feel free to poke around and help test the setup. feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lfzh05/repurposing_800_x_rx_580s_for_llm_inference_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T09:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgcbyh</id>
    <title>Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound.</title>
    <updated>2025-06-20T19:09:42+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt; &lt;img alt="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." src="https://a.thumbs.redditmedia.com/smxOsICItFcgpgZ6jwpSvygFZFitUy4PBiwrObgw-D4.jpg" title="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys. I'm reposting as the old post got removed by some reason.&lt;/p&gt; &lt;p&gt;Now it is time to compare LLMs, where these GPUs shine the most.&lt;/p&gt; &lt;p&gt;hardware-software config:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Fedora 41 (Linux), Kernel 6.19&lt;/li&gt; &lt;li&gt;Torch 2.7.1+cu128&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each card was tuned to try to get the highest clock possible, highest VRAM bandwidth and less power consumption.&lt;/p&gt; &lt;p&gt;The benchmark was run on ikllamacpp, as&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-sweep-bench -m '/GUFs/gemma-3-27b-it-Q4_K_M.gguf' -ngl 999 -c 8192 -fa -ub 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The tuning was made on each card, and none was power limited (basically all with the slider maxed for PL)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 5090: &lt;ul&gt; &lt;li&gt;Max clock: 3010 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 1000&lt;/li&gt; &lt;li&gt;Basically an undervolt plus overclock near the 0.9V point (Linux doesn't let you see voltages)&lt;/li&gt; &lt;li&gt;VRAM overclock: +3000Mhz (34 Gbps effective, so about 2.1 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 4090: &lt;ul&gt; &lt;li&gt;Max clock: 2865 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an undervolt+OC about the 0.91V point.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1650Mhz (22.65 Gbps effective, so about 1.15 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 3090: &lt;ul&gt; &lt;li&gt;Max clock: 1905 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 180&lt;/li&gt; &lt;li&gt;This is confirmed, from windows, an UV + OC of 1905Mhz at 0.9V.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (so about 1.08 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX A6000: &lt;ul&gt; &lt;li&gt;Max clock: 1740 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an UV + OC of about 0.8V&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (about 870 GB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For reference: PP (pre processing) is mostly compute bound, and TG (text generation) is bandwidth bound.&lt;/p&gt; &lt;p&gt;I have posted the raw performance metrics on pastebin, as it is a bit hard to make it readable here on reddit, on &lt;a href="https://pastebin.com/g3vjU6jY"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance Summary (N_KV = 0)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;TG Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Power (W)&lt;/th&gt; &lt;th align="left"&gt;PP t/s/W&lt;/th&gt; &lt;th align="left"&gt;TG t/s/W&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;4,641.54&lt;/td&gt; &lt;td align="left"&gt;76.78&lt;/td&gt; &lt;td align="left"&gt;425&lt;/td&gt; &lt;td align="left"&gt;10.92&lt;/td&gt; &lt;td align="left"&gt;0.181&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;3,625.95&lt;/td&gt; &lt;td align="left"&gt;54.38&lt;/td&gt; &lt;td align="left"&gt;375&lt;/td&gt; &lt;td align="left"&gt;9.67&lt;/td&gt; &lt;td align="left"&gt;0.145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1,538.49&lt;/td&gt; &lt;td align="left"&gt;44.78&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;td align="left"&gt;4.27&lt;/td&gt; &lt;td align="left"&gt;0.124&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1,578.69&lt;/td&gt; &lt;td align="left"&gt;38.60&lt;/td&gt; &lt;td align="left"&gt;280&lt;/td&gt; &lt;td align="left"&gt;5.64&lt;/td&gt; &lt;td align="left"&gt;0.138&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Relative Performance (vs RTX 3090 baseline)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed&lt;/th&gt; &lt;th align="left"&gt;TG Speed&lt;/th&gt; &lt;th align="left"&gt;PP Efficiency&lt;/th&gt; &lt;th align="left"&gt;TG Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;3.02x&lt;/td&gt; &lt;td align="left"&gt;1.71x&lt;/td&gt; &lt;td align="left"&gt;2.56x&lt;/td&gt; &lt;td align="left"&gt;1.46x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;2.36x&lt;/td&gt; &lt;td align="left"&gt;1.21x&lt;/td&gt; &lt;td align="left"&gt;2.26x&lt;/td&gt; &lt;td align="left"&gt;1.17x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1.03x&lt;/td&gt; &lt;td align="left"&gt;0.86x&lt;/td&gt; &lt;td align="left"&gt;1.32x&lt;/td&gt; &lt;td align="left"&gt;1.11x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Degradation with Context (N_KV)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Drop (0→6144)&lt;/th&gt; &lt;th align="left"&gt;TG Drop (0→6144)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;-15.7%&lt;/td&gt; &lt;td align="left"&gt;-13.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;-16.3%&lt;/td&gt; &lt;td align="left"&gt;-14.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;-12.7%&lt;/td&gt; &lt;td align="left"&gt;-14.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;-14.1%&lt;/td&gt; &lt;td align="left"&gt;-14.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And some images!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a"&gt;https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949"&gt;https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6"&gt;https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg9s5q</id>
    <title>OpenBuddy R1 0528 Distil into Qwen 32B</title>
    <updated>2025-06-20T17:26:07+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"&gt; &lt;img alt="OpenBuddy R1 0528 Distil into Qwen 32B" src="https://preview.redd.it/lpxeubca848f1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0c797e5af53cfbc1f305db8ab83a70ad2b8308da" title="OpenBuddy R1 0528 Distil into Qwen 32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm so impressed with this model for the size. o1 was the first model I found that could one shot tetris with AI, and even other frontier models can still struggle to do it well. And now a 32B model just managed it!&lt;/p&gt; &lt;p&gt;There was one bug - only one line would be cleared at a time. It fixed this easily when I pointed it out.&lt;/p&gt; &lt;p&gt;I doubt it would one shot it every time, but this model is definitely a step up from standard Qwen 32B, which was already pretty good.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT"&gt;https://huggingface.co/OpenBuddy/OpenBuddy-R1-0528-Distill-Qwen3-32B-Preview0-QAT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lpxeubca848f1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg9s5q/openbuddy_r1_0528_distil_into_qwen_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T17:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg71aq</id>
    <title>Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica</title>
    <updated>2025-06-20T15:35:34+00:00</updated>
    <author>
      <name>/u/mylittlethrowaway300</name>
      <uri>https://old.reddit.com/user/mylittlethrowaway300</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"&gt; &lt;img alt="Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica" src="https://external-preview.redd.it/LATs33JDlBoRUx0tiKg7DMdY6oXVXFPIYU36DtiY4tQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98b804adca292cc34f817396897e2d3bdcafc87a" title="Study: Meta AI model can reproduce almost half of Harry Potter book - Ars Technica" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought this was a really well-written article. &lt;/p&gt; &lt;p&gt;I had a thought: do you guys think smaller LLMs will have fewer copyright issues than larger ones? If I train a huge model on text and tell it that &amp;quot;Romeo and Juliet&amp;quot; is a &amp;quot;tragic&amp;quot; story, and also that &amp;quot;Rabbit, Run&amp;quot; by Updike is also a tragic story, the larger LLM training is more likely to retain entire passages. It has the neurons of the NN (the model weights) to store information as rote memorization. &lt;/p&gt; &lt;p&gt;But, if I train a significantly smaller model, there's a higher chance that the training will manage to &amp;quot;extract&amp;quot; the components of each story that are tragic, but not retain the entire text verbatim.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mylittlethrowaway300"&gt; /u/mylittlethrowaway300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arstechnica.com/features/2025/06/study-metas-llama-3-1-can-recall-42-percent-of-the-first-harry-potter-book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg71aq/study_meta_ai_model_can_reproduce_almost_half_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T15:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg80cq</id>
    <title>New Mistral Small 3.2</title>
    <updated>2025-06-20T16:14:11+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open weights: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/MistralAI/status/1936093325116781016/photo/1"&gt;https://x.com/MistralAI/status/1936093325116781016/photo/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg7vuc</id>
    <title>mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face</title>
    <updated>2025-06-20T16:09:13+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt; &lt;img alt="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" src="https://external-preview.redd.it/3DBqKqgOLKDMFbcrOD5Qa-3M1IIegLfhMX6TTbsgXeU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6eecbfa2b523b92f82faf94cb6ab334696d320" title="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:09:13+00:00</published>
  </entry>
</feed>
