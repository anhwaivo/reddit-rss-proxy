<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-12T09:49:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1inbt4r</id>
    <title>Local PR reviews WITHIN VSCode and Cursor</title>
    <updated>2025-02-11T22:45:07+00:00</updated>
    <author>
      <name>/u/EntelligenceAI</name>
      <uri>https://old.reddit.com/user/EntelligenceAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt; &lt;img alt="Local PR reviews WITHIN VSCode and Cursor" src="https://b.thumbs.redditmedia.com/Tukr5pK_f7zfxTUHMe9BsvOSaLkFcz7OCWYAQrOTrxo.jpg" title="Local PR reviews WITHIN VSCode and Cursor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw Cursor is charging $36(!!) for their new &amp;quot;Bug Fixes&amp;quot; feature - crazy. I just want a PR reviewer to catch my bugs before I push code so people and PR bots don't cover it with comments!&lt;/p&gt; &lt;p&gt;So I built something different: Review your code BEFORE pushing, right in your editor CURSOR or VSCode!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40"&gt;https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Super simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install the bot in VSCode or Cursor&lt;/li&gt; &lt;li&gt;Make your changes&lt;/li&gt; &lt;li&gt;Type /reviewDiff&lt;/li&gt; &lt;li&gt;Get instant line-by-line feedback&lt;/li&gt; &lt;li&gt;Fix issues before anyone sees them&lt;/li&gt; &lt;li&gt;Push clean code and get that LGTMNo more bot comments cluttering your PRs or embarrassing feedback in front of the team. Just real-time reviews while you're still coding, pulling your full file context for accurate feedback.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Check it out here: &lt;a href="https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI"&gt;https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What else would make your pre-PR workflow better? Please share how we can make this better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntelligenceAI"&gt; /u/EntelligenceAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1inm9z7</id>
    <title>Running a Local LLM on Langflow – Best Ways to Embed PDFs, CSVs, JSON, Text, Images &amp; Videos?</title>
    <updated>2025-02-12T08:17:03+00:00</updated>
    <author>
      <name>/u/HaDeSxD</name>
      <uri>https://old.reddit.com/user/HaDeSxD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I’m setting up a local LLM using Ollama and Langflow to create a personal AI assistant that can store and reason over my data. My goal is to retrieve notes, summarize documents, and get insights from various types of personal data. &lt;/p&gt; &lt;p&gt;I have a mix of: &lt;/p&gt; &lt;p&gt;Text files (.txt) &lt;/p&gt; &lt;p&gt;Documents (PDFs) &lt;/p&gt; &lt;p&gt;Structured data (CSVs, JSON) &lt;/p&gt; &lt;p&gt;Images (screenshots, scanned notes) &lt;/p&gt; &lt;p&gt;Videos (recorded explanations, tutorials) &lt;/p&gt; &lt;p&gt;I know FAISS and ChromaDB are good for vector storage, but I’m still figuring out: &lt;/p&gt; &lt;p&gt;How to properly embed each data type for retrieval? (text is easy, but what about images/videos?) &lt;/p&gt; &lt;p&gt;What’s the best way to chunk PDFs and long text files for embedding? &lt;/p&gt; &lt;p&gt;How should I handle structured data like CSVs/JSON—convert them to text first, or is there a better approach? &lt;/p&gt; &lt;p&gt;Are there good local embedding models that work well for multimodal data without cloud services? &lt;/p&gt; &lt;p&gt;Any best practices for integrating these embeddings into Langflow? &lt;/p&gt; &lt;p&gt;Would love to hear from others who have tackled similar problems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HaDeSxD"&gt; /u/HaDeSxD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1imnaj2</id>
    <title>Elon's bid for OpenAI is about making the for-profit transition as painful as possible for Altman, not about actually purchasing it (explanation in comments).</title>
    <updated>2025-02-11T01:55:14+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From @ phill__1 on twitter:&lt;/p&gt; &lt;p&gt;OpenAI Inc. (the non-profit) wants to convert to a for-profit company. But you &lt;strong&gt;cannot&lt;/strong&gt; just turn a non-profit into a for-profit – that would be an incredible tax loophole. Instead, the new for-profit OpenAI company would need to &lt;strong&gt;pay out&lt;/strong&gt; OpenAI Inc.'s technology and IP (likely in equity in the new for-profit company). &lt;/p&gt; &lt;p&gt;The valuation is tricky since OpenAI Inc. is theoretically the sole controlling shareholder of the capped-profit subsidiary, OpenAI LP. But there have been some numbers floating around. Since the rumored SoftBank investment at a $260B valuation is dependent on the for-profit move, we're using the current ~$150B valuation. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Control premiums&lt;/em&gt; in market transactions typically range between 20-30% of enterprise value; experts have predicted something around $30B-$40B. &lt;strong&gt;The key is&lt;/strong&gt;, this valuation is ultimately signed off on by the California and Delaware Attorneys General. &lt;/p&gt; &lt;p&gt;Now, if you want to &lt;strong&gt;block&lt;/strong&gt; OpenAI from the for-profit transition, but have yet to be successful in court, what do you do? &lt;em&gt;Make it as painful as possible.&lt;/em&gt; Elon Musk just gave regulators a &lt;strong&gt;perfect&lt;/strong&gt; argument for why the non-profit should get $97B for selling their technology and IP. This would instantly make the non-profit the majority stakeholder at &lt;em&gt;62%&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's a clever move that throws a major wrench into the for-profit transition, potentially even stopping it dead in its tracks. Whether OpenAI accepts the offer or not (they won't), the mere existence of this valuation benchmark will be hard for regulators to ignore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T01:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1imy7gs</id>
    <title>Android NPU prompt processing ~16k tokens using llama 8B!</title>
    <updated>2025-02-11T13:10:22+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt; &lt;img alt="Android NPU prompt processing ~16k tokens using llama 8B!" src="https://external-preview.redd.it/bHR4Mzh3eHBjaWllMcHEeWWDxXtRsAlZuyHnXPHYA8F-o65beS5TU-PcpoQ-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=271efc0a7cf13784adc6a1a4b5f128c1609db55d" title="Android NPU prompt processing ~16k tokens using llama 8B!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/83sbjwxpciie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9qsg</id>
    <title>Boosting Unsloth 1.58 Quant of Deepseek R1 671B Performance with Faster Storage – 3x Speedup!</title>
    <updated>2025-02-11T21:19:12+00:00</updated>
    <author>
      <name>/u/akumaburn</name>
      <uri>https://old.reddit.com/user/akumaburn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a test to see if I could improve the performance of &lt;strong&gt;Unsloth 1.58-bit-quantized DeepSeek R1 671B&lt;/strong&gt; by upgrading my storage setup. &lt;strong&gt;Spoiler: It worked!&lt;/strong&gt; Nearly &lt;strong&gt;tripled&lt;/strong&gt; my token generation rate, and I learned a lot along the way.&lt;/p&gt; &lt;p&gt;Hardware Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5900X (4.5GHz, 12 cores)&lt;/li&gt; &lt;li&gt;GPU: XFX AMD Radeon 7900 XTX Black (24GB GDDR6)&lt;/li&gt; &lt;li&gt;RAM: 96GB DDR4 3600MHz (mismatched 4 sticks, not ideal)&lt;/li&gt; &lt;li&gt;Motherboard: MSI X570 Tomahawk MAX WIFI&lt;/li&gt; &lt;li&gt;OS: EndeavourOS (Arch Linux)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Storage:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single NVMe (BTRFS, on motherboard): XPG 4TB GAMMIX S70 Blade PCIe Gen4&lt;/li&gt; &lt;li&gt;Quad NVMe RAID 0 (XFS, via ASUS Hyper M.2 x16 Gen5 card): 4× 2TB Silicon Power US75&lt;/li&gt; &lt;li&gt;Key Optimisations: &lt;ul&gt; &lt;li&gt;Scheduler: Set to kyber&lt;/li&gt; &lt;li&gt;read_ahead_kb: Set to 128 for better random read performance&lt;/li&gt; &lt;li&gt;File System Tests: Tried F2FS, BTRFS, and XFS – XFS performed the best on the RAID array&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Findings &amp;amp; Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;This result is only valid for low context sizes&lt;/strong&gt; (~2048). Higher contexts dramatically increase memory &amp;amp; VRAM usage. (I'm planning on running some more tests for higher context sizes, but suspect I will run out of RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Couldn’t fully utilise the RAID 0 speeds&lt;/strong&gt; – capped at 16GB/s on Linux, likely due to PCIe lane limitations (both on-board NVMe slots are filled + the 7900 XTX eats up bandwidth).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Biggest impact? read_ahead_kb&lt;/strong&gt; had the most noticeable effect. mmap relies heavily on random read throughput, which is greatly affected by this setting. (lower seems better to a degree)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If I did it again?&lt;/strong&gt; (or if was doing it from scratch and not just upgrading my main PC) I'd go Threadripper for more PCIe lanes and I'd try to get faster memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4TB NVME Single Drive:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /home/akumaburn/Desktop/Projects/LLaMA/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 5.11 ± 0.01 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 1.29 ± 0.09 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;4x2TB NVME Raid-0:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /mnt/xfs_raid0/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 6.01 ± 0.05 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 3.30 ± 0.15 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akumaburn"&gt; /u/akumaburn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmu01</id>
    <title>Best LLM router: comparison</title>
    <updated>2025-02-12T09:00:36+00:00</updated>
    <author>
      <name>/u/GrandMoo1</name>
      <uri>https://old.reddit.com/user/GrandMoo1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was recently tasked to look into LLM routers as the company I'm working for wants to start working more with AI orchestration and LLM routing. With the growing AI infrastructure solutions, I started looking more in depth into these platforms.&lt;/p&gt; &lt;p&gt;The task is definitely not easy and I was looking into different services with the main key capabilities that impact ease of use, cost and performance. However, I created this cheat sheet where I was trying to compare a range of different features that make the platforms effective when it comes to managing and deploying large language models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1Xx7vE2rV1UoknzDnYcwxm1Hsof3ZPDtjt4z_E2AQGN4/edit?gid=0#gid=0"&gt;https://docs.google.com/spreadsheets/d/1Xx7vE2rV1UoknzDnYcwxm1Hsof3ZPDtjt4z_E2AQGN4/edit?gid=0#gid=0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My main considerations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM routing&lt;/strong&gt;. It ensures the requests are directed efficiently and the most suitable model for the request is picked.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unified API for multiple models&lt;/strong&gt;. Reduces the complexity of working with different providers and also simplifies the integration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal AI support&lt;/strong&gt;. A crucial aspect when it comes to enabling text, audio and image processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI deployment&lt;/strong&gt;. How easy or difficult it is when it comes to integrating AI models into operational environments. Even better if the platform has real time deployment capability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM optimization&lt;/strong&gt;. Optimizing models and model selection. Also, optimizing the execution of the models as well as the cost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ease of integration&lt;/strong&gt;. It's great if you need minimal changes to the code or can determine how quickly a solution fits into an existing workflow. Moreover, customization play another key factor in the case of how easily and flexible are the AI applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalability and efficiency&lt;/strong&gt;. How well can you scale without losing efficiency with the current models and being able to balance the cost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM observability&lt;/strong&gt;. Rather obvious one but extremely important to monitor LLMs for their behavior, reliability and performance.&lt;/li&gt; &lt;li&gt;Security. Security remains a top priority, making data privacy and security features critical.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All the current tools in this table are for sure different and have different features as well as capabilities but I wanted to gather everything in one place and make them somewhat comparable, as you can summarize certain aspects of said features.&lt;/p&gt; &lt;p&gt;It has really made it easier for me and while it's not perfect and some things are difficult to compare due to different criteria, I hope it will be useful to at least some of you, as this is the best I've got.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Currently, I've reviewed these LLM routers:&lt;/strong&gt; Portkey, TrueFoundry, Martian, Pruna AI and Unify, but I will constantly be adding new ones.&lt;/p&gt; &lt;p&gt;Any kind of suggestions or feedback from you are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrandMoo1"&gt; /u/GrandMoo1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmu01/best_llm_router_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmu01/best_llm_router_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmu01/best_llm_router_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T09:00:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1imxthq</id>
    <title>I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)</title>
    <updated>2025-02-11T12:49:40+00:00</updated>
    <author>
      <name>/u/JakeAndAI</name>
      <uri>https://old.reddit.com/user/JakeAndAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt; &lt;img alt="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" src="https://external-preview.redd.it/Y2NpeDU4eXVhaWllMXfHHmD6JzwUZ6Bm0WYsf1XL3J4yUspRtDfHF7Qk2Xoa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4df278042d4308f102ad830d111c5117d2eea27" title="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JakeAndAI"&gt; /u/JakeAndAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9howo9yuaiie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T12:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1indxb7</id>
    <title>Cool new TTS</title>
    <updated>2025-02-12T00:20:57+00:00</updated>
    <author>
      <name>/u/throwawayacc201711</name>
      <uri>https://old.reddit.com/user/throwawayacc201711</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"&gt; &lt;img alt="Cool new TTS" src="https://external-preview.redd.it/ckDm43nxsYs8mLGkY2iPQxL86a_8AnLyS95I0lJP1I8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264273fc9f005840ca55e0bad8426aadecc23da2" title="Cool new TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across this new TTS model (open source and open weights) call zonos. &lt;/p&gt; &lt;p&gt;Saw this post with a demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/zyphraai/status/1888996367923888341?s=46&amp;amp;t=EKmVivcLrgePIH2fyc-IfA"&gt;https://x.com/zyphraai/status/1888996367923888341?s=46&amp;amp;t=EKmVivcLrgePIH2fyc-IfA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gonna try and get some time to test it out. Has anyone used it before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawayacc201711"&gt; /u/throwawayacc201711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zyphra/Zonos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T00:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1injegi</id>
    <title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title>
    <updated>2025-02-12T05:05:27+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.07374"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T05:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9zth</id>
    <title>Thomson Reuters Wins First Major AI Copyright Case in the US</title>
    <updated>2025-02-11T21:29:31+00:00</updated>
    <author>
      <name>/u/tofous</name>
      <uri>https://old.reddit.com/user/tofous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt; &lt;img alt="Thomson Reuters Wins First Major AI Copyright Case in the US" src="https://external-preview.redd.it/-0DUUc5f5aqTMh8-ZeH6fmczh-Ih5wc4UWlpTHp3exU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f55fcc6771896d54eec682ba109deef9ce2630d" title="Thomson Reuters Wins First Major AI Copyright Case in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tofous"&gt; /u/tofous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1inn034</id>
    <title>Phi-4, but pruned and unsafe</title>
    <updated>2025-02-12T09:13:06+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some things just start on a &lt;strong&gt;whim&lt;/strong&gt;. This is the story of &lt;strong&gt;Phi-Lthy4&lt;/strong&gt;, pretty much:&lt;/p&gt; &lt;p&gt;&amp;gt; yo sicarius can you make phi-4 smarter?&lt;br /&gt; nope. but i can still make it better.&lt;br /&gt; &amp;gt; wdym??&lt;br /&gt; well, i can yeet a couple of layers out of its math brain, and teach it about the wonders of love and intimate relations. maybe. idk if its worth it.&lt;br /&gt; &amp;gt; lol its all synth data in the pretrain. many before you tried.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;fine. ill do it.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;But... why?&lt;/h1&gt; &lt;p&gt;The trend it seems, is to make AI models more &lt;strong&gt;assistant-oriented&lt;/strong&gt;, use as much &lt;strong&gt;synthetic data&lt;/strong&gt; as possible, be more &lt;strong&gt;'safe'&lt;/strong&gt;, and be more &lt;strong&gt;benchmaxxed&lt;/strong&gt; (hi qwen). Sure, this makes great assistants, but &lt;strong&gt;sanitized&lt;/strong&gt; data (like in the &lt;strong&gt;Phi&lt;/strong&gt; model series case) butchers &lt;strong&gt;creativity&lt;/strong&gt;. Not to mention that the previous &lt;strong&gt;Phi 3.5&lt;/strong&gt; wouldn't even tell you how to &lt;strong&gt;kill a process&lt;/strong&gt; and so on and so forth...&lt;/p&gt; &lt;p&gt;This little side project took about &lt;strong&gt;two weeks&lt;/strong&gt; of on-and-off fine-tuning. After about &lt;strong&gt;1B tokens&lt;/strong&gt; or so, I lost track of how much I trained it. The idea? A &lt;strong&gt;proof of concept&lt;/strong&gt; of sorts to see if sheer will (and 2xA6000) will be enough to shape a model to &lt;strong&gt;any&lt;/strong&gt; parameter size, behavior or form.&lt;/p&gt; &lt;p&gt;So I used mergekit to perform a crude &lt;strong&gt;LLM brain surgery&lt;/strong&gt;— and yeeted some &lt;strong&gt;useless&lt;/strong&gt; neurons that dealt with math. How do I know that these exact neurons dealt with math? Because &lt;strong&gt;ALL&lt;/strong&gt; of Phi's neurons dealt with math. Success was guaranteed.&lt;/p&gt; &lt;p&gt;Is this the best Phi-4 &lt;strong&gt;11.9B&lt;/strong&gt; RP model in the &lt;strong&gt;world&lt;/strong&gt;? It's quite possible, simply because tuning &lt;strong&gt;Phi-4&lt;/strong&gt; for RP is a completely stupid idea, both due to its pretraining data, &amp;quot;limited&amp;quot; context size of &lt;strong&gt;16k&lt;/strong&gt;, and the model's MIT license.&lt;/p&gt; &lt;p&gt;Surprisingly, it's &lt;strong&gt;quite good at RP&lt;/strong&gt;, turns out it didn't need those 8 layers after all. It could probably still solve a basic math question, but I would strongly recommend using a calculator for such tasks. Why do we want LLMs to do basic math anyway?&lt;/p&gt; &lt;p&gt;Oh, regarding &lt;strong&gt;censorship&lt;/strong&gt;... Let's just say it's... &lt;strong&gt;Phi-lthy&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The BEST Phi-4 Roleplay&lt;/strong&gt; finetune in the &lt;strong&gt;world&lt;/strong&gt; (Not that much of an achievement here, Phi roleplay finetunes can probably be counted on a &lt;strong&gt;single hand&lt;/strong&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compact size &amp;amp; fully healed from the brain surgery&lt;/strong&gt; Only &lt;strong&gt;11.9B&lt;/strong&gt; parameters. &lt;strong&gt;Phi-4&lt;/strong&gt; wasn't that hard to run even at &lt;strong&gt;14B&lt;/strong&gt;, now with even fewer brain cells, your new phone could probably run it easily. (&lt;strong&gt;SD8Gen3&lt;/strong&gt; and above recommended).&lt;/li&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities. This really surprised me. &lt;strong&gt;Actually good&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Writes and roleplays &lt;strong&gt;quite uniquely&lt;/strong&gt;, probably because of lack of RP\writing slop in the &lt;strong&gt;pretrain&lt;/strong&gt;. Who would have thought?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt; - It kept some of the smarts, and our little Phi-Lthy here will be quite eager to answer your naughty questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quite good&lt;/strong&gt; at following the &lt;strong&gt;character card&lt;/strong&gt;. Finally, it puts its math brain to some productive tasks. Gooner technology is becoming more popular by the day.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T09:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1inf39f</id>
    <title>Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit</title>
    <updated>2025-02-12T01:16:26+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt; &lt;img alt="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" src="https://external-preview.redd.it/6TXevgYpufzYpfTjezpGg52UERA8ZIyKmvoa8ynszgs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1ee4f4645a585d0b344f74ef45c2a3af79f5ac" title="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.sky.com/story/politics-latest-immigration-labour-starmer-badenoch-farage-live-news-12593360?postid=9087865#liveblog-body"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T01:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1inahmj</id>
    <title>AI-RP GUI, thoughts?</title>
    <updated>2025-02-11T21:50:23+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt; &lt;img alt="AI-RP GUI, thoughts?" src="https://external-preview.redd.it/anUxbWh5MXF3a2llMZDcyaEte4yY5OTnn_MraO3a1mLbSyQuEc8JUukRHPMy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffabf7e7e829026793cb0c9fe2931fd6dab7c685" title="AI-RP GUI, thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9ej41y1qwkie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1in4b81</id>
    <title>Why AMD or Intel doesn't sell card with huge amount of Vram ?</title>
    <updated>2025-02-11T17:39:14+00:00</updated>
    <author>
      <name>/u/Euphoric_Tutor_5054</name>
      <uri>https://old.reddit.com/user/Euphoric_Tutor_5054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, we saw that even with an epyc processor and 512 gb of ram you can run deepseek pretty fast, but compared to a graphic card it's pretty slow. But the problem is that you need a lot of vram on your graphic card so why AMD and intel doesn't sell such card with enormous amount of vram ? especially since 8gb of gddr6 is super cheap now ! like 3$ I believe, look here : &lt;a href="https://www.dramexchange.com/"&gt;https://www.dramexchange.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would be a killer for inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Tutor_5054"&gt; /u/Euphoric_Tutor_5054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T17:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbili</id>
    <title>UK and US refuse to sign international AI declaration</title>
    <updated>2025-02-11T22:32:41+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt; &lt;img alt="UK and US refuse to sign international AI declaration" src="https://external-preview.redd.it/4F7TAGCOz8Rfg3WqXnrQP8MWV7RA9T-cLoTH3Je-EA8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa17295cbd542f368737b59db21a9f78ea0823" title="UK and US refuse to sign international AI declaration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bbc.com/news/articles/c8edn0n58gwo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1inieoe</id>
    <title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title>
    <updated>2025-02-12T04:07:22+00:00</updated>
    <author>
      <name>/u/ekaesmem</name>
      <uri>https://old.reddit.com/user/ekaesmem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt; &lt;img alt="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" src="https://b.thumbs.redditmedia.com/xPf7D_ti2_9HmqPFloVt-vKCywazKffYdOe7zEWXDAg.jpg" title="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca"&gt;https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.06703"&gt;[2502.06703] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekaesmem"&gt; /u/ekaesmem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T04:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1in0938</id>
    <title>I made Iris: A fully-local realtime voice chatbot!</title>
    <updated>2025-02-11T14:49:16+00:00</updated>
    <author>
      <name>/u/Born_Search2534</name>
      <uri>https://old.reddit.com/user/Born_Search2534</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt; &lt;img alt="I made Iris: A fully-local realtime voice chatbot!" src="https://external-preview.redd.it/stLjrcu85AgTrwW4zDhH8loF7eayJD3_hD2XyXmIgUw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65bd986bcf0e0f5ab0f244100b8cb83d8e1266a9" title="I made Iris: A fully-local realtime voice chatbot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Search2534"&gt; /u/Born_Search2534 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=XK-37m-p11k&amp;amp;feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1in83vw</id>
    <title>Chonky Boi has arrived</title>
    <updated>2025-02-11T20:12:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt; &lt;img alt="Chonky Boi has arrived" src="https://external-preview.redd.it/YKjiYoZG5DJKHF8InRyOIM7iTEJQimQj1eCDwySpqqE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69fcf80dcc8f1be1b6b900fa7ba68bf7b62ee469" title="Chonky Boi has arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/kh64WJy.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1in69s3</id>
    <title>4x3090 in a 4U case, don't recommend it</title>
    <updated>2025-02-11T18:58:29+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt; &lt;img alt="4x3090 in a 4U case, don't recommend it" src="https://a.thumbs.redditmedia.com/78a1YQ1sn0cHJ3q5f1VyWRySQ3DMLohcp9folBrA-j0.jpg" title="4x3090 in a 4U case, don't recommend it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1in69s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9vsb</id>
    <title>NYT: Vance speech at EU AI summit</title>
    <updated>2025-02-11T21:24:49+00:00</updated>
    <author>
      <name>/u/Mediocre_Tree_5690</name>
      <uri>https://old.reddit.com/user/Mediocre_Tree_5690</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt; &lt;img alt="NYT: Vance speech at EU AI summit" src="https://preview.redd.it/vjltv4twukie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ab17775dd480a87f8fde7e76f52035c4a8b6cc" title="NYT: Vance speech at EU AI summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archive.is/eWNry"&gt;https://archive.is/eWNry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's an archive link in case anyone wants to read the article. Macron spoke about lighter regulation at the AI summit as well. Are we thinking safetyism is finally on its way out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Tree_5690"&gt; /u/Mediocre_Tree_5690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjltv4twukie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1imyp19</id>
    <title>If you want my IT department to block HF, just say so.</title>
    <updated>2025-02-11T13:35:20+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt; &lt;img alt="If you want my IT department to block HF, just say so." src="https://preview.redd.it/h1dbbwhxiiie1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1bafacbc3514f10c5b939ade16c607722c1d9b0" title="If you want my IT department to block HF, just say so." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1dbbwhxiiie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1in7nka</id>
    <title>ChatGPT 4o feels straight up stupid after using o1 and DeepSeek for awhile</title>
    <updated>2025-02-11T19:54:09+00:00</updated>
    <author>
      <name>/u/Getabock_</name>
      <uri>https://old.reddit.com/user/Getabock_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And to think I used to be really impressed with 4o. Crazy. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Getabock_"&gt; /u/Getabock_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1in8vya</id>
    <title>EU mobilizes $200 billion in AI race against US and China</title>
    <updated>2025-02-11T20:44:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt; &lt;img alt="EU mobilizes $200 billion in AI race against US and China" src="https://external-preview.redd.it/X_db72AfOkvUPacuwPMCLwkrfkqSycSFXdfcaogRMnw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6dff9e983d7903f62f20823b94ae4fd34bac0f8" title="EU mobilizes $200 billion in AI race against US and China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
</feed>
