<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-14T03:55:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jxv644</id>
    <title>I chopped the screen off my MacBook Air to be a full time LLM server</title>
    <updated>2025-04-13T00:12:36+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"&gt; &lt;img alt="I chopped the screen off my MacBook Air to be a full time LLM server" src="https://preview.redd.it/qrnzf9tguhue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16bb48263ccc8f44559ef992fd4e2e9901fdac0f" title="I chopped the screen off my MacBook Air to be a full time LLM server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got the thing for ¬£250 used with a broken screen; finally just got around to removing it permanently lol&lt;/p&gt; &lt;p&gt;Runs Qwen-7b at 14 tokens-per-second, which isn‚Äôt amazing, but honestly is actually a lot better than I expected for an M1 8gb chip!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrnzf9tguhue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxv644/i_chopped_the_screen_off_my_macbook_air_to_be_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T00:12:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy1x1b</id>
    <title>Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)</title>
    <updated>2025-04-13T07:07:25+00:00</updated>
    <author>
      <name>/u/townofsalemfangay</name>
      <uri>https://old.reddit.com/user/townofsalemfangay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"&gt; &lt;img alt="Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)" src="https://external-preview.redd.it/mDmX7u3VLHRX-kGFVwmgM9o7wWa38WRF25RrGfaO_ys.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd2b04faff1651442b1a681ea604e94423b30bd0" title="Vocalis: Local Conversational AI Assistant (Speech ‚ÜîÔ∏è Speech in Real Time with Vision Capabilities)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;Been a long project, but I have Just released &lt;strong&gt;Vocalis&lt;/strong&gt;, a real-time local assistant that goes full speech-to-speech‚ÄîCustom VAD, Faster Whisper ASR, LLM in the middle, TTS out. Built for speed, fluidity, and actual usability in voice-first workflows. Latency will depend on your setup, ASR preference and LLM/TTS model size (all configurable via the .env in backend).&lt;/p&gt; &lt;p&gt;üí¨ &lt;strong&gt;Talk to it like a person&lt;/strong&gt;.&lt;br /&gt; üéß &lt;strong&gt;Interrupt mid-response&lt;/strong&gt; (barge-in).&lt;br /&gt; üß† &lt;strong&gt;Silence detection for follow-ups&lt;/strong&gt; (the assistant will speak without you following up based on the context of the conversation).&lt;br /&gt; üñºÔ∏è &lt;strong&gt;Image analysis support to provide multi-modal context to non-vision capable endpoints&lt;/strong&gt; (&lt;a href="https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct"&gt;SmolVLM-256M&lt;/a&gt;).&lt;br /&gt; üßæ &lt;strong&gt;Session save/load support&lt;/strong&gt; with full context.&lt;/p&gt; &lt;p&gt;It uses your local LLM via OpenAI-style endpoint (LM Studio, llama.cpp, GPUStack, etc), and any TTS server (like my &lt;a href="https://github.com/Lex-au/Orpheus-FastAPI"&gt;Orpheus-FastAPI&lt;/a&gt; or for super low latency, &lt;a href="https://github.com/remsky/Kokoro-FastAPI"&gt;Kokoro-FastAPI&lt;/a&gt;). Frontend is React, backend is FastAPI‚ÄîWebSocket-native with real-time audio streaming and UI states like &lt;em&gt;Listening&lt;/em&gt;, &lt;em&gt;Processing&lt;/em&gt;, and &lt;em&gt;Speaking&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speech Recognition Performance (using Vocalis-Q4_K_M + Koroko-FASTAPI TTS)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system uses Faster-Whisper with the &lt;code&gt;base.en&lt;/code&gt; model and a beam size of 2, striking an optimal balance between accuracy and speed. This configuration achieves:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ASR Processing&lt;/strong&gt;: ~0.43 seconds for typical utterances&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Response Generation&lt;/strong&gt;: ~0.18 seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Round-Trip Latency&lt;/strong&gt;: ~0.61 seconds&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real-world example from system logs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;INFO:faster_whisper:Processing audio with duration 00:02.229 INFO:backend.services.transcription:Transcription completed in 0.51s: Hi, how are you doing today?... INFO:backend.services.tts:Sending TTS request with 147 characters of text INFO:backend.services.tts:Received TTS response after 0.16s, size: 390102 bytes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's a full breakdown of the architecture and latency information on my readme.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lex-au/VocalisConversational"&gt;https://github.com/Lex-au/VocalisConversational&lt;/a&gt;&lt;br /&gt; model (optional): &lt;a href="https://huggingface.co/lex-au/Vocalis-Q4_K_M.gguf"&gt;https://huggingface.co/lex-au/Vocalis-Q4_K_M.gguf&lt;/a&gt;&lt;br /&gt; Some demo videos during project progress here: &lt;a href="https://www.youtube.com/@AJ-sj5ik"&gt;https://www.youtube.com/@AJ-sj5ik&lt;/a&gt;&lt;br /&gt; License: Apache 2.0&lt;/p&gt; &lt;p&gt;Let me know what you think or if you have questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/townofsalemfangay"&gt; /u/townofsalemfangay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Lex-au/Vocalis"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy1x1b/vocalis_local_conversational_ai_assistant_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T07:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyfygt</id>
    <title>Collaborative A2A Knowledge Graphs</title>
    <updated>2025-04-13T19:34:23+00:00</updated>
    <author>
      <name>/u/Ragecommie</name>
      <uri>https://old.reddit.com/user/Ragecommie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyfygt/collaborative_a2a_knowledge_graphs/"&gt; &lt;img alt="Collaborative A2A Knowledge Graphs" src="https://external-preview.redd.it/MEOkVWy74xZRscfX6UD5UydEACEn5gx4IWcOuQ_pYrU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a4710a0c0fa286d1e8ccd9222c72667602f1973" title="Collaborative A2A Knowledge Graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;Just drafted a PR for Google's A2A protocol adding some distributed knowledge graph management features: &lt;a href="https://github.com/google/A2A/pull/141"&gt;https://github.com/google/A2A/pull/141&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The final version will support a number of transactional languages, starting with GraphQL, as well as loading custom EBNF grammars.&lt;/p&gt; &lt;p&gt;The Python implementation is mostly done, with the JS sample and UI demo coming shortly.&lt;/p&gt; &lt;p&gt;We're working on a hierarchical planning agent based on this updates A2A spec, hope someone else finds it useful too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ragecommie"&gt; /u/Ragecommie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google/A2A/pull/141"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyfygt/collaborative_a2a_knowledge_graphs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyfygt/collaborative_a2a_knowledge_graphs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T19:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6627</id>
    <title>I benchmarked the top models used for translation on openrouter V2!</title>
    <updated>2025-04-13T12:07:56+00:00</updated>
    <author>
      <name>/u/AdventurousFly4909</name>
      <uri>https://old.reddit.com/user/AdventurousFly4909</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"&gt; &lt;img alt="I benchmarked the top models used for translation on openrouter V2!" src="https://preview.redd.it/87czci55flue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20e1c9e41710c1bf16869ecf9ef366993c47d550" title="I benchmarked the top models used for translation on openrouter V2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I benchmarked the top models listed on openrouter(that are used for translation) on 1000 Chinese-English pairs. I asked each model to translate a Chinese passage to English. I then ranked the translation with &lt;a href="https://github.com/Unbabel/COMET"&gt;comet&lt;/a&gt;. The origin of the test data are Chinese web novels translated into english you can find the test data in the repo. The results are really similar to the results of my last post(The standings of a model compared to others rather than the precise score). This suggest that the ranking is pretty trustworthy especially after a increase of 5x of the test data.&lt;/p&gt; &lt;p&gt;A lot of people had concerns about the scores being too similar I think this is partly because of human nature of how it perceives 0.7815 and 78.15 differently while they are essentially the same. And secondly of really close &lt;strong&gt;some&lt;/strong&gt; of these results are to each other but fret not because can still make trustworthy judgements based on the results.&lt;/p&gt; &lt;p&gt;How to comprehend these results: If the first decimal place differs then the quality difference will be very noticeable. If the second decimal place differs it means that there is a noticeable quality difference. If the third decimal place differs then there will be a minimal quality difference noticeable. If only the fourth place differs then the models can be considered the same&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ProgrammedInsanity/llm_eval_on_test_data"&gt;Repo with all the code and data&lt;/a&gt;. Btw the comet score is from 0 to 1. You could also scale the score with 100 to get for example for deepseek-v3 a score of 78.15.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousFly4909"&gt; /u/AdventurousFly4909 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/87czci55flue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6627/i_benchmarked_the_top_models_used_for_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:07:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxu0f7</id>
    <title>We should have a monthly ‚Äúwhich models are you using‚Äù discussion</title>
    <updated>2025-04-12T23:12:01+00:00</updated>
    <author>
      <name>/u/Arkhos-Winter</name>
      <uri>https://old.reddit.com/user/Arkhos-Winter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since a lot of people keep coming on here and asking which models they should use (either through API or on their GPU), I propose that we have a formalized discussion on what we think are the best models (both proprietary and open-weights) for different purposes (coding, writing, etc.) on the 1st of every month.&lt;/p&gt; &lt;p&gt;It‚Äôll go something like this: ‚ÄúI‚Äôm currently using Deepseek v3.1, 4o (March 2025 version), and Gemini 2.5 Pro for writing, and I‚Äôm using R1, Qwen 2.5 Max, and Sonnet 3.7 (thinking) for coding.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arkhos-Winter"&gt; /u/Arkhos-Winter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-12T23:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyl37p</id>
    <title>Combating code smells that arise from LLM generated code in Python</title>
    <updated>2025-04-13T23:30:14+00:00</updated>
    <author>
      <name>/u/m1tm0</name>
      <uri>https://old.reddit.com/user/m1tm0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR - vibelint&lt;/p&gt; &lt;p&gt;Namespace Management: - Visualize your global namespace to identify and resolve naming collisions&lt;/p&gt; &lt;p&gt;Python Documentation Enhancement: - Validate docstrings include relative filepath references to help LLMs &amp;quot;remember&amp;quot; the location of methods within your project structure&lt;/p&gt; &lt;p&gt;Codebase Snapshots: - Generate full codebase snapshots optimized for ultra-long context LLMs (Gemini 2.5 Pro, Llama4 Scout) - Customize snapshots with include/exclude glob patterns&lt;/p&gt; &lt;p&gt;Anecdotally, this approach has helped me improve my LLM python programming performance.&lt;/p&gt; &lt;hr /&gt; &lt;h1&gt;The &amp;quot;Vibe Coding&amp;quot; Phenomenon&lt;/h1&gt; &lt;p&gt;While this approach enables rapid development, it often leads to structural problems in the codebase:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inconsistent naming patterns across files&lt;/li&gt; &lt;li&gt;Redundant implementations of similar functionality&lt;/li&gt; &lt;li&gt;Confusing namespace collisions that create ambiguity&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Specific Problem vibelint Addresses&lt;/h1&gt; &lt;p&gt;I witnessed this firsthand when asking an LLM to help me modify a &lt;code&gt;query()&lt;/code&gt; function in my project. The LLM got confused because I had inadvertently created three different &lt;code&gt;query()&lt;/code&gt; functions scattered across the codebase:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One for database operations&lt;/li&gt; &lt;li&gt;Another for API requests&lt;/li&gt; &lt;li&gt;A third for search functionality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Though these files weren't importing each other (so traditional linters didn't flag anything), this duplication created chaos when using AI tools to help modify the code.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Now that i've gotten that intro out of the way (thanks claude), I wanted to add one more disclaimer, I definitely fall into the class of &amp;quot;Vibe Coder&amp;quot; by most people's standards.&lt;/p&gt; &lt;p&gt;After a painstaking weekend of trial and error, I came up with something that works on my macbook and theoretically should work on windows. Notice the lack of unit and integration tests (I hate writing tests). Vibelint definitely has some code smells (and no unit testing). This will be to vibelint's detriment, but I really think a tool like this is needed even if it isn't perfect.&lt;/p&gt; &lt;p&gt;If anyone in the open source community is interested in integrating vibelint's features into their linter/formatter/analyzer, please do, as it is released under the MIT license. I would appreciate credit, but getting these features into the hands of the public is more important.&lt;/p&gt; &lt;p&gt;If you want to collaborate, my socials are linked to my Github. Feel free to reach out.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mithranm/vibelint"&gt;https://github.com/mithranm/vibelint&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m1tm0"&gt; /u/m1tm0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyl37p/combating_code_smells_that_arise_from_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyl37p/combating_code_smells_that_arise_from_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyl37p/combating_code_smells_that_arise_from_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T23:30:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jybxfb</id>
    <title>Vision and voice enabled real-time AI assistant using livekit</title>
    <updated>2025-04-13T16:42:38+00:00</updated>
    <author>
      <name>/u/Traditional_Tap1708</name>
      <uri>https://old.reddit.com/user/Traditional_Tap1708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I've been playing a little with Livekit for making voice assistants having very low response time, and wanted to share what I've put together so far.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/taresh18/conversify-speech"&gt;https://github.com/taresh18/conversify-speech&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My goal was to build something responsive that runs mostly on local AI models (Whisper STT, local LLM via API, KokoroTTS). It's still a learning project (definitely WIP!), but it can already:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hold a voice conversation.&lt;/li&gt; &lt;li&gt;Use basic vision (takes snapshots from video).&lt;/li&gt; &lt;li&gt;Remember past chats between sessions using memoripy.&lt;/li&gt; &lt;li&gt;Focuses on low latency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For STT, I used whisper-large-v3-turbo with inference using &lt;a href="https://github.com/SYSTRAN/faster-whisper"&gt;faster-whisper&lt;/a&gt;. For LLM, I used qwen-2.5VL-7B served via sglang and for TTS, I used the &lt;a href="https://github.com/remsky/Kokoro-FastAPI"&gt;kokoro fast api&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd love any feedback or suggestions you have! Especially interested in ideas for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Making the vision/memory smarter?&lt;/li&gt; &lt;li&gt;Squeezing out more performance?&lt;/li&gt; &lt;li&gt;Cool features to add?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional_Tap1708"&gt; /u/Traditional_Tap1708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jybxfb/vision_and_voice_enabled_realtime_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jybxfb/vision_and_voice_enabled_realtime_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jybxfb/vision_and_voice_enabled_realtime_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T16:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy5p12</id>
    <title>Another budget build. 160gb of VRAM for $1000, maybe?</title>
    <updated>2025-04-13T11:38:27+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt; &lt;img alt="Another budget build. 160gb of VRAM for $1000, maybe?" src="https://b.thumbs.redditmedia.com/hXo0FDMBRfQWGUqiymAq8zmPyIP8zsjaaUWaeo6d10Q.jpg" title="Another budget build. 160gb of VRAM for $1000, maybe?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just grabbed 10 AMD MI50 gpus from eBay, $90 each. $900. I bought an Octominer Ultra x12 case (CPU, MB, 12 pcie slots, fan, ram, ethernet all included) for $100. Ideally, I should be able to just wire them up with no extra expense. Unfortunately the Octominer I got has weak PSU, 3 750w for a total of 2250W. The MI50 consumes 300w. For a peak total of 3000W, the rest of the system itself perhaps bout 350w. I'm team llama.cpp so it won't put much load, and only the active GPU will be used, so it might be possible to stuff 10 GPUs in there (with power limited and using an 8pin to dual 8pin splitter, I won't recommend) I plan on doing 6 first and seeing how it performs. Then either I put the rest in the same case or I split it 5/5 for now across another Octominer case. Specs wise, the MI50 looks about the same as the P40s, it's no longer unofficial supported by AMD, but who cares? :-)&lt;/p&gt; &lt;p&gt;If you plan to do a GPU only build, get this case. The octominer system is a weak system, it's designed for crypto mining, so weak celeron CPUs, weak memory. Don't try to offload, they usually come with about 4-8gb of ram. Mine came with 4gb. Will have hiveOS installed, you can install Ubuntu in it. No NVME, it's a few years ago, but it does take SSDs, it has 4 USB ports, it has a built in ethernet that's suppose to be a gigabit port, but mine is only 100M, I probably have a much older model. It has inbuilt VGA &amp;amp; HDMI port. So no need to be 100% headless. It has 140x38 fans that can uses static pressure to move air through the case. Sounds like a jet, however, you can control it. beats my fan rig for the P40s. My guess is the PCIe slot is x1 electrical lanes. So don't get this if you plan on doing training, unless if you are training a smol model maybe.&lt;/p&gt; &lt;p&gt;Putting a motherboard, CPU, ram, fan, PSU, risers, case/air frame, etc adds up. You will not match this system for $200. Yet you can pick up one with for $200.&lt;/p&gt; &lt;p&gt;There, go get you an Octominer case if you're team GPU.&lt;/p&gt; &lt;p&gt;With that said, I can't say much on the MI50s yet. I'm currently hiking the AMD/Vulkan path of hell, Linux already has vulkan by default. I built llama.cpp, but inference output is garbage, still trying to sort it out. I did a partial RPC offload to one of the cards and output was reasonable so cards are not garbage. With the 100Mbps network traffic, file transfer is slow, so in a few hours, I'm going to go to the store and pick up a 1Gbps network card or ethernet USB stick. More updates to come.&lt;/p&gt; &lt;p&gt;The goal is to add this to my build so I can run even better quant of DeepSeek R1/V3. Unsloth team cooked the hell out of their UD quants.&lt;/p&gt; &lt;p&gt;If you have experience with these AMD instinct MI cards, please let me know how the heck to get them to behave with llama.cpp if you have the experience.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9oq5fzei9lue1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e138eed8ee146e284d2b109c84ea8af5b3259b03"&gt;https://preview.redd.it/9oq5fzei9lue1.jpg?width=3072&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e138eed8ee146e284d2b109c84ea8af5b3259b03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Go ye forth my friends and be resourceful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T11:38:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyn934</id>
    <title>Token generation Performance as Context Increases MLX vs Llama.cpp</title>
    <updated>2025-04-14T01:24:27+00:00</updated>
    <author>
      <name>/u/davewolfs</name>
      <uri>https://old.reddit.com/user/davewolfs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that if the context fills up to about 50% when using Llama.cpp with LMStudio things slow down dramatically e.g. on Scout token speed drops from say 35 t/s to 15 t/s nearly a 60% decrease. With MLX you are going from say 47 to 35 about a 25% decrease. Why is the drop in speed so much more dramatic with Llama.cpp?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davewolfs"&gt; /u/davewolfs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyn934/token_generation_performance_as_context_increases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyn934/token_generation_performance_as_context_increases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyn934/token_generation_performance_as_context_increases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T01:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy16yi</id>
    <title>LMArena ruined language models</title>
    <updated>2025-04-13T06:16:18+00:00</updated>
    <author>
      <name>/u/Dogeboja</name>
      <uri>https://old.reddit.com/user/Dogeboja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LMArena is way too easy to game, you just optimize for whatever their front-end is capable of rendering and especially focus on bulleted lists since those seem to get the most clicks. Maybe sprinkle in some emojis and that's it, no need to actually produce excellent answers.&lt;/p&gt; &lt;p&gt;Markdown especially is starting to become very tightly ingrained into all model answers, it's not like it's the be-all and end-all of human communication. You can somewhat combat this with system instructions but I am worried it could cause unexpected performance degradation.&lt;/p&gt; &lt;p&gt;The recent LLaMA 4 fiasco and the fact that Claude Sonnet 3.7 is at rank 22 below models like Gemma 3 27B tells the whole story.&lt;/p&gt; &lt;p&gt;How could this be fixed at this point? My solution would be to simply disable Markdown in the front-end, I really think language generation and formatting should be separate capabilities.&lt;/p&gt; &lt;p&gt;By the way, if you are struggling with this, try this system prompt: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Prefer natural language, avoid formulaic responses.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This works quite well most of the time but it can sometimes lead to worse answers if the formulaic answer was truly the best style for that prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogeboja"&gt; /u/Dogeboja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy16yi/lmarena_ruined_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T06:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyid0v</id>
    <title>Chapter summaries using Llama 3.1 8B UltraLong 1M</title>
    <updated>2025-04-13T21:19:48+00:00</updated>
    <author>
      <name>/u/autonoma_2042</name>
      <uri>https://old.reddit.com/user/autonoma_2042</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my novel, early chapters have two different scenes, each on its own timeline, clearly and consistently marked in the prose. Using ollama, the following models failed to acknowledge (remember?) the first of the two scenes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;deepseek-r1:32b: ignores first half&lt;/li&gt; &lt;li&gt;mistral-small:24b: hallucinates&lt;/li&gt; &lt;li&gt;granite3.1-dense: ignores first half&lt;/li&gt; &lt;li&gt;qwen2.5:32b: didn't work&lt;/li&gt; &lt;li&gt;gemma3:27b: doesn't continue summarizing&lt;/li&gt; &lt;li&gt;technobyte/c4ai-command-r7b-12-2024:Q5_K_M: fails remarkably&lt;/li&gt; &lt;li&gt;granite3.2: didn't work&lt;/li&gt; &lt;li&gt;qwq:32b: doesn't understand the task&lt;/li&gt; &lt;li&gt;command-r: doesn't understand, continues writing story&lt;/li&gt; &lt;li&gt;llama3:70b-instruct-q4_K_M: didn't work&lt;/li&gt; &lt;li&gt;ingu627/llama4-scout-q4: didn't work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are a skilled literary analyst. Write a concise, insightful summary of the following chapter. Focus on key plot developments, character insights, and thematic elements. Note that ### indicates separate different scenes, so be sure to capture both scenes in the summary, if present. Write in clear, engaging language that captures the essence of the chapter. Provide the summary without introductory phrases.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I suspected a longer context window was required to summarize the chapters. I asked DeepSeek to translate my bash script for running ollama into a Python program that runs Llama 3.1 8B, instead.&lt;/p&gt; &lt;p&gt;Here's a summary of chapter two produced by the Python code:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In the first scene, Xander Angelos is haunted by a memory of his half-brother Tyf√≥s, who terrorized him as a child. This event is juxtaposed with the discovery of letters from their father, which Xander cherishes. The contrast between the two experiences reflects the complex relationship between family dynamics and the pursuit of wisdom. The father's letters serve as a source of comfort and guidance, while Tyf√≥s's actions embody the darker aspects of sibling rivalry and abuse. &lt;/p&gt; &lt;p&gt;Fast-forward to Xander's date with Cassandra, where he meticulously prepares for a night of fun at a juke joint re√´nactment. The scene is rich with sensory details, capturing the atmosphere of the venue and the chemistry between the two characters. Cassandra's presence ignites Xander's passion, and their playful banter showcases their growing connection. The use of puns and playful jabs highlights their compatibility and ease with each other.&lt;/p&gt; &lt;p&gt;As the chapter progresses, Xander and Cassandra engage in a virtual reality game called Psyn√¶ris, which serves as a metaphor for their relationship and the complexities of communication. The contrast between the vibrant world of the game and the real-life stressors that Cassandra faces illustrates the challenges of navigating emotions and expectations in a relationship. Xander's desire to support her while also grappling with his own feelings reflects the tension between intimacy and independence.&lt;/p&gt; &lt;p&gt;The introduction of Y≈´na Futaba in the second scene shifts the narrative to a more serious tone. Y≈´na is tasked with interacting with a metal brain named Pr√¥tos, which is experiencing anxiety and fear. The conversation reveals Pr√¥tos's struggles with its own identity and the looming presence of a &amp;quot;mean man,&amp;quot; hinting at the dangers of manipulation and control. Y≈´na's role as an observer and communicator highlights the importance of understanding and empathy in technological advancements. The tension between safety and the unknown is palpable, as Pr√¥tos's fears resonate with Y≈´na's own concerns about the implications of artificial intelligence. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'm floored. If there's interest, I'll post the Python code, instructions, and prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autonoma_2042"&gt; /u/autonoma_2042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyid0v/chapter_summaries_using_llama_31_8b_ultralong_1m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T21:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyjwjl</id>
    <title>Best multimodal for 4gb card?</title>
    <updated>2025-04-13T22:31:22+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;wanting to script some photo classification, but haven't messed with local multimodals. I have 32 gb of ram also.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyjwjl/best_multimodal_for_4gb_card/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyo2ds</id>
    <title>Word Synth - Llama 3.2 tiny LLM with sampling parameters exposed</title>
    <updated>2025-04-14T02:09:01+00:00</updated>
    <author>
      <name>/u/Brave_Variety6275</name>
      <uri>https://old.reddit.com/user/Brave_Variety6275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built this as an intuition builder around LLM sampling--it's a bit rough around the edges but sharing in case its useful to anyone else trying to get it straight which sampling parameters do what. &lt;/p&gt; &lt;p&gt;&lt;a href="http://wordsynth.latenthomer.com/"&gt;http://wordsynth.latenthomer.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your browser will yell at you because I didn't use https. Sorry. &lt;/p&gt; &lt;p&gt;Also apologies if it breaks or is really slow, this was also an experiment to deploy. &lt;/p&gt; &lt;p&gt;Thanks for reading :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave_Variety6275"&gt; /u/Brave_Variety6275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T02:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jydrnr</id>
    <title>AgenticSeek, one month later</title>
    <updated>2025-04-13T18:01:15+00:00</updated>
    <author>
      <name>/u/fawendeshuo</name>
      <uri>https://old.reddit.com/user/fawendeshuo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, I shared a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jbwk65/made_a_manusai_alternative_that_run_locally/"&gt;post&lt;/a&gt; on a local-first alternative to ManusAI that I was working on with a friend: &lt;a href="http://github.com/Fosowl/agenticSeek"&gt;AgenticSeek&lt;/a&gt;. Back then I didn‚Äôt expect such interest! I saw blogs and even a video pop up about our tool, which was awesome but overwhelming since the project wasn‚Äôt quite ready for such success. &lt;/p&gt; &lt;p&gt;Thanks to some community feedback and some helpful contributions, we‚Äôve made big strides in just a few weeks. So I thought it would be nice to share our advancements!&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick rundown of the main improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smoother web navigation and note-taking.&lt;/li&gt; &lt;li&gt;Smarter task routing with task complexity estimation.&lt;/li&gt; &lt;li&gt;Added a planner agent to handle complex tasks.&lt;/li&gt; &lt;li&gt;Support for more providers, like LM-Studio and local APIs.&lt;/li&gt; &lt;li&gt;Integrated searxng for free web search.&lt;/li&gt; &lt;li&gt;Ability to use web input forms.&lt;/li&gt; &lt;li&gt;Improved captcha solving and stealthier browser automation.&lt;/li&gt; &lt;li&gt;Agent router now supports multiple languages (previously a prompt in Japanese or French would assign a random agent).&lt;/li&gt; &lt;li&gt;Squashed tons of bugs.&lt;/li&gt; &lt;li&gt;Set up a community server and updates on my X account (see readme).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next?&lt;/strong&gt; I‚Äôm focusing on improving the planner agent, handling more type of web inputs, and adding support for MCP, and possibly a finetune of deepseek üëÄ &lt;/p&gt; &lt;p&gt;There‚Äôs still a lot to do, but it‚Äôs delivering solid results compared to a month ago. Can't wait to get more feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fawendeshuo"&gt; /u/fawendeshuo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T18:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyhd6i</id>
    <title>[2503.23817] MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration</title>
    <updated>2025-04-13T20:35:30+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;https://arxiv.org/abs/2503.23817&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads before and after in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities. This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29√ó speedup and 30.5√ó energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18√ó and 1.31√ó throughput improvements, along with 3.04√ó and 2.35√ó energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.23817"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyhd6i/250323817_mvdram_enabling_gemv_execution_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy9g9y</id>
    <title>Waifu GPU for AI GF?</title>
    <updated>2025-04-13T14:53:23+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt; &lt;img alt="Waifu GPU for AI GF?" src="https://external-preview.redd.it/BnNrd2081VNoP-o3smbLrBMM4J_6XEXvavemnoJv_qM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea747f555a0c92c80b42a36a37e0db735083b1b2" title="Waifu GPU for AI GF?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lpqhvyq68mue1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad31ac4af8529144b8d1be6323d09048cbf4d8b4"&gt;https://videocardz.com/newz/asus-officially-reveals-first-geforce-rtx-5060-ti-ahead-of-launch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I dont know these characters, but is this the future of mankind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy9g9y/waifu_gpu_for_ai_gf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:53:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jycfvf</id>
    <title>You can preview quantizations of Llama 4 Maverick 17Bx128E at acceptable speeds even without the necessary memory</title>
    <updated>2025-04-13T17:04:14+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably many already know this, but with llama.cpp it's possible to perform inference off models larger than the available total physical memory; this is thanks to the magic of &lt;code&gt;mmap&lt;/code&gt;. Inference speed might be surprisingly faster than you'd think.&lt;/p&gt; &lt;p&gt;I tested this with &lt;a href="https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ2_M"&gt;Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M&lt;/a&gt;, which is about 143 GB in total and shouldn't fit within my 64GB of DDR4 memory + one RTX3090 (24GB).&lt;/p&gt; &lt;p&gt;It takes a while for prompt processing to occur (admittedly at a fairly slow rate compared to normal), during which NVMe reads appear to be intense (5-6 GiB/s), which can be tracked on Linux with &lt;code&gt;iostat -s 1&lt;/code&gt;, but once that is done, inference speed is fairly decent.&lt;/p&gt; &lt;p&gt;Here's a benchmark with &lt;code&gt;llama-bench&lt;/code&gt; (I couldn't load more than 3 model layers on the GPU):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ./build/bin/llama-bench -m ~/models/Llama-4-Maverick-17B-128E-Instruct-UD-IQ2_M.gguf -ngl 3 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | test | t/s | | ------------------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | pp512 | 16.43 ¬± 0.25 | | llama4 17Bx128E (Maverick) IQ2_M - 2.7 bpw | 143.06 GiB | 400.71 B | CUDA | 3 | tg128 | 3.45 ¬± 0.26 | build: 06bb53ad (5115) # free total used free shared buff/cache available Mem: 65523176 8262924 600336 184900 57572992 57260252 Swap: 65523172 14129384 51393788 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details for the flag that would prevent this behavior (disabling &lt;code&gt;mmap&lt;/code&gt;): &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/1876"&gt;https://github.com/ggml-org/llama.cpp/discussions/1876&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;--no-mmap&lt;/code&gt;: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you're not using &lt;code&gt;--mlock&lt;/code&gt;. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: from a suggestion in the comments below by PhoenixModBot, starting Llama.cpp with &lt;code&gt;-ngl 999 -ot \\d+.ffn_.*_exps.=CPU&lt;/code&gt; can increase inference speed to &lt;strong&gt;8~18 tokens/s&lt;/strong&gt; (depending on which experts get cached on RAM). What this does is loading the shared model parameters on the GPU, while keeping the FFN layers (the routed experts) on the CPU (RAM). This is documented here: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11397"&gt;https://github.com/ggml-org/llama.cpp/pull/11397&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Additionally, in my own tests I've observed better prompt processing speeds by configuring both the physical and logical batch size to the same value of 2048. This can increase memory usage, though. &lt;code&gt;-b 2048 -ub 2048&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jycfvf/you_can_preview_quantizations_of_llama_4_maverick/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T17:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jxy26m</id>
    <title>Sam Altman: "We're going to do a very powerful open source model... better than any current open source model out there."</title>
    <updated>2025-04-13T02:55:45+00:00</updated>
    <author>
      <name>/u/mw11n19</name>
      <uri>https://old.reddit.com/user/mw11n19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt; &lt;img alt="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" src="https://external-preview.redd.it/eDJobnVwZ3luaXVlMdXj0QNvtvvTvdLhyylbR9Y6PzQjPjUyfN1eoWAw2jEe.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a5f48835aebe28a468ef3c09a1d306d926d0876" title="Sam Altman: &amp;quot;We're going to do a very powerful open source model... better than any current open source model out there.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mw11n19"&gt; /u/mw11n19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wzjs6qgyniue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jxy26m/sam_altman_were_going_to_do_a_very_powerful_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T02:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk399</id>
    <title>Dual 5090 va single 5090</title>
    <updated>2025-04-13T22:40:38+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt; &lt;img alt="Dual 5090 va single 5090" src="https://preview.redd.it/z1xl2ob1koue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a09792ff8b0785b5b36ea4cb15fed716f6a7feaf" title="Dual 5090 va single 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man these dual 5090s are awesome. Went from 4t/s on 29b Gemma 3 to 28t/s when going from 1 to 2. I love these things! Easily runs 70b fast! I only wish they were a little cheaper but can‚Äôt wait till the RTX 6000 pro comes out with 96gb because I am totally eyeballing the crap out of it‚Ä¶. Who needs money when u got vram!!!&lt;/p&gt; &lt;p&gt;Btw I got 2 fans right under earn, 5 fans in front, 3 on top and one mac daddy on the back, and bout to put the one that came with the gigabyte 5090 on it too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xl2ob1koue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy8h2i</id>
    <title>Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data</title>
    <updated>2025-04-13T14:08:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt; &lt;img alt="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" src="https://b.thumbs.redditmedia.com/Xv5xeh-T-_FI_nUFqM0bvDA9nB-t2tXuTjAVsmlXjdE.jpg" title="Skywork-OR1: new SOTA 32B thinking model with open weight, training code, and training data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;github repo: &lt;a href="https://github.com/SkyworkAI/Skywork-OR1"&gt;https://github.com/SkyworkAI/Skywork-OR1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680"&gt;https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680&lt;/a&gt;&lt;/p&gt; &lt;p&gt;huggingface: &lt;a href="https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9"&gt;https://huggingface.co/collections/Skywork/skywork-or1-67fa1bcb41b436ef2def76b9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e"&gt;https://preview.redd.it/uuodxdre0mue1.png?width=1532&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0567ead14bd49bdc33066bf3bca19e1ad566676e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy8h2i/skyworkor1_new_sota_32b_thinking_model_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T14:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy813d</id>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
    <updated>2025-04-13T13:47:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T13:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyp2no</id>
    <title>If we had models like QwQ-32B and Gemma-3-27B two years ago, people would have gone crazy.</title>
    <updated>2025-04-14T03:04:47+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine if we had QwQ-32B or Gemma-3-27B or some of the smaller models, 18-24 months ago. It would have been the craziest thing.&lt;/p&gt; &lt;p&gt;24 months ago, GPT-4 was released. GPT-4o was released 11 months ago. Sometimes we not only forgot how quick things have been moving, but we also forget how good these small models actually are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T03:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6ns6</id>
    <title>Coming soon‚Ä¶..</title>
    <updated>2025-04-13T12:36:19+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt; &lt;img alt="Coming soon‚Ä¶.." src="https://preview.redd.it/1cwv3wz7klue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abbae222e535c2c110583987226650f6391ac918" title="Coming soon‚Ä¶.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1cwv3wz7klue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jygxmu</id>
    <title>Open-Weights Model next week?</title>
    <updated>2025-04-13T20:16:32+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt; &lt;img alt="Open-Weights Model next week?" src="https://preview.redd.it/iph04cputnue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f58d7addbdbe94c34055c810ba04a1042cb757a3" title="Open-Weights Model next week?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iph04cputnue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk213</id>
    <title>Still true 3 months later</title>
    <updated>2025-04-13T22:38:59+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt; &lt;img alt="Still true 3 months later" src="https://preview.redd.it/7644n1vqjoue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0b79a5e35c4e594b33dc646534a2248d3db9159" title="Still true 3 months later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They rushed the release so hard it's been full of implementation bugs. And let's not get started on the custom model to hill climb lmarena alop&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7644n1vqjoue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:38:59+00:00</published>
  </entry>
</feed>
