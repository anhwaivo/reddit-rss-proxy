<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-28T11:36:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1majfwi</id>
    <title>Are ~70B Models Going Out of Fashion?</title>
    <updated>2025-07-27T10:57:30+00:00</updated>
    <author>
      <name>/u/HvskyAI</name>
      <uri>https://old.reddit.com/user/HvskyAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt; &lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt; &lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt; &lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt; &lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt; &lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...&lt;/p&gt; &lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt; &lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt; &lt;p&gt;I suppose I'm partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt; &lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HvskyAI"&gt; /u/HvskyAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbc9d3</id>
    <title>Understanding Local Language Models: A Beginner’s Guide</title>
    <updated>2025-07-28T10:09:36+00:00</updated>
    <author>
      <name>/u/120-dev</name>
      <uri>https://old.reddit.com/user/120-dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR A local language model is like a mini-brain for your computer. It’s trained to understand and generate text, like answering questions or writing essays. Unlike online AI (like ChatGPT), local LLMs don’t need a cloud server—you run them directly on your machine. But to do this, you need to know about &lt;strong&gt;model size&lt;/strong&gt;, &lt;strong&gt;context&lt;/strong&gt;, and &lt;strong&gt;hardware&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;1. Model Size: How Big Is the Brain?&lt;/h1&gt; &lt;p&gt;The “size” of an LLM is measured in &lt;strong&gt;parameters&lt;/strong&gt;, which are like the brain cells of the model. More parameters mean a smarter model, but it also needs a more powerful computer. Let’s look at the three main size categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Small Models (1–3 billion parameters):&lt;/strong&gt;These are like tiny, efficient brains. They don’t need much power and can run on most laptops.&lt;strong&gt;Example:&lt;/strong&gt; Imagine a small model as a basic calculator—it’s great for simple tasks like answering short questions or summarizing a paragraph. A model like LLaMA 3B (3 billion parameters) needs only about &lt;strong&gt;4 GB of GPU memory&lt;/strong&gt; (VRAM) and &lt;strong&gt;8 GB of regular computer memory&lt;/strong&gt; (RAM). If your laptop has 8–16 GB of RAM, you can run this model. This is how llama 3.2 running on my MacBook Air M1 8GB RAM:[video]&lt;strong&gt;Real-world use:&lt;/strong&gt; Writing short emails, summarizing or answering basic questions like, “What’s the capital of France?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium Models (7–13 billion parameters):&lt;/strong&gt;These are like a high-school student’s brain—smarter, but they need a better computer.&lt;strong&gt;Example:&lt;/strong&gt; A medium model like LLaMA 8B (8 billion parameters) needs about &lt;strong&gt;12 GB of VRAM&lt;/strong&gt; and &lt;strong&gt;16 GB of RAM&lt;/strong&gt;. This is like needing a gaming PC with a good graphics card (like an NVIDIA RTX 3090). It can handle more complex tasks, like writing a short story or analyzing a document.&lt;strong&gt;Real-world use:&lt;/strong&gt; Creating a blog post or helping with homework.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large Models (30+ billion parameters):&lt;/strong&gt;These are like genius-level brains, but they need super-powerful computers.&lt;strong&gt;Example:&lt;/strong&gt; A huge model like LLaMA 70B (70 billion parameters) might need &lt;strong&gt;48 GB of VRAM&lt;/strong&gt; (like two high-end GPUs) and &lt;strong&gt;64 GB of RAM&lt;/strong&gt;. This is like needing a fancy workstation, not a regular PC. These models are great for advanced tasks, but most people can’t run them at home.&lt;strong&gt;Real-world use:&lt;/strong&gt; Writing a detailed research paper or analyzing massive datasets.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; The bigger the model, the more “thinking power” it has, but it needs a stronger computer. A small model is fine for basic tasks, while larger models are for heavy-duty work.&lt;/p&gt; &lt;h1&gt;2. Context Window: How Much Can the Model “Remember”?&lt;/h1&gt; &lt;p&gt;The &lt;strong&gt;context window&lt;/strong&gt; is how much text the model can “think about” at once. Think of it like the model’s short-term memory. It’s measured in &lt;strong&gt;tokens&lt;/strong&gt; (a token is roughly a word or part of a word). A bigger context window lets the model remember more, but it uses a lot more memory.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; If you’re chatting with an AI and it can only “remember” 2,048 tokens (about 1,500 words), it might forget the start of a long conversation. But if it has a 16,384-token context (about 12,000 words), it can keep track of a much longer discussion. &lt;ul&gt; &lt;li&gt;A 2,048-token context might use &lt;strong&gt;0.7 GB of GPU memory&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;A 16,384-token context could jump to &lt;strong&gt;46 GB of GPU memory&lt;/strong&gt;—way more!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; If you only need short answers (like a quick fact), use a small context to save memory. But if you’re summarizing a long article, you’ll need a bigger context, which requires a stronger computer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Keep the context window small unless you need the model to remember a lot of text. Bigger context = more memory needed.&lt;/p&gt; &lt;h1&gt;3. Hardware: What Kind of Computer Do You Need?&lt;/h1&gt; &lt;p&gt;To run a local LLM, your computer needs two key things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU VRAM&lt;/strong&gt; (video memory on your graphics card, if you have one).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System RAM&lt;/strong&gt; (regular computer memory).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here’s a simple guide to match your hardware to the right model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Basic Laptop (8 GB VRAM, 16 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;small models&lt;/strong&gt; (1–3 billion parameters).&lt;strong&gt;Example:&lt;/strong&gt; A typical laptop with a mid-range GPU (4–6 GB VRAM) can handle a 3B model for simple tasks like answering questions or writing short texts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gaming PC (12–16 GB VRAM, 32 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;medium models&lt;/strong&gt; (7–13 billion parameters).&lt;strong&gt;Example:&lt;/strong&gt; A PC with a high-performance GPU (12 GB VRAM) can run an 8B model to write stories or assist with coding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High-End Setup (24–48 GB VRAM, 64 GB RAM):&lt;/strong&gt;You can run &lt;strong&gt;large models&lt;/strong&gt; (30+ billion parameters), but optimization techniques may be required (I will explain further in the next part).&lt;strong&gt;Example:&lt;/strong&gt; A workstation with two high-end GPUs (24 GB VRAM each) can handle a 70B model for advanced tasks like research or complex analysis.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Check your computer’s VRAM and RAM to pick the right model. If you don’t have a powerful GPU, stick to smaller models.&lt;/p&gt; &lt;h1&gt;4. Tricks to Run Bigger Models on Smaller Computers&lt;/h1&gt; &lt;p&gt;Even if your computer isn’t super powerful, you can use some clever tricks to run bigger models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; This is like compressing a big file to make it smaller. It reduces the model’s memory needs by using less precise math.&lt;strong&gt;Example:&lt;/strong&gt; A 70B model normally needs &lt;strong&gt;140 GB of VRAM&lt;/strong&gt;, but with 4-bit quantization, it might only need &lt;strong&gt;35 GB&lt;/strong&gt;. That’s still a lot, but it’s much more doable on a good gaming PC.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free Up Memory:&lt;/strong&gt; Close other programs (like games or browsers) to give your GPU more room to work.&lt;strong&gt;Example:&lt;/strong&gt; If your GPU has 12 GB of VRAM, make sure at least 10–11 GB is free for the model to run smoothly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smaller Context and Batch Size:&lt;/strong&gt; Use a smaller context window or fewer tasks at once to save memory.&lt;strong&gt;Example:&lt;/strong&gt; If you’re just asking for a quick answer, set the context to 2,048 tokens instead of 16,384 to save VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Simple Rule:&lt;/strong&gt; Quantization is like magic—it lets you run bigger models on smaller computers! For a step-by-step guide on how to do this, I found this tutorial super helpful from Hugging Face: &lt;a href="https://huggingface.co/docs/transformers/v4.53.3/quantization/overview"&gt;https://huggingface.co/docs/transformers/v4.53.3/quantization/overview&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;5. How to Choose the Right Model for You&lt;/h1&gt; &lt;p&gt;Here’s a quick guide to pick the best model for your computer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Basic Laptop (8 GB VRAM, 16 GB RAM):&lt;/strong&gt; Choose a &lt;strong&gt;1–3B model&lt;/strong&gt;. It’s perfect for simple tasks like answering questions or writing short texts.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Write a 100-word story about a cat.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gaming PC (12–16 GB VRAM, 32 GB RAM):&lt;/strong&gt; Go for a &lt;strong&gt;7–13B model&lt;/strong&gt;. These are great for more complex tasks like writing essays or coding.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Write a Python program to calculate my monthly budget.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High-End PC (24–48 GB VRAM, 64 GB RAM):&lt;/strong&gt; Try a &lt;strong&gt;30B+ model&lt;/strong&gt; with quantization. These are for heavy tasks like research or big projects.&lt;strong&gt;Example Task:&lt;/strong&gt; Ask the model, “Analyze this 10-page report and summarize it in 500 words.”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your computer isn’t strong enough for a big model, you can also use &lt;strong&gt;cloud services&lt;/strong&gt; (ChatGPT, Claude, Grok, Google Gemini, etc.) for large models.&lt;/p&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;Running a local language model is like having your own personal AI assistant on your computer. By understanding model size, context window, and your computer’s hardware, you can pick the right model for your needs. Start small if you’re new, and use tricks like quantization to get more out of your setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; Always leave a bit of extra VRAM and RAM free, as models can slow down if your computer is stretched to its limit. Happy AI experimenting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/120-dev"&gt; /u/120-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc9d3/understanding_local_language_models_a_beginners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc9d3/understanding_local_language_models_a_beginners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc9d3/understanding_local_language_models_a_beginners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T10:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbcwek</id>
    <title>Opensource: The AI Model Router - Automating AI Model Selection</title>
    <updated>2025-07-28T10:48:12+00:00</updated>
    <author>
      <name>/u/Idonotknow101</name>
      <uri>https://old.reddit.com/user/Idonotknow101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbcwek/opensource_the_ai_model_router_automating_ai/"&gt; &lt;img alt="Opensource: The AI Model Router - Automating AI Model Selection" src="https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=142b9f59812a7af5da3822cb118e31ad38a1664b" title="Opensource: The AI Model Router - Automating AI Model Selection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall, I built an opensource AI Model Router that automatically picks the best AI provider (OpenAI, Anthropic, Google, local), model, and settings for your prompts. No more guessing between openai Claude, or Gemini!&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Idonotknow101"&gt; /u/Idonotknow101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MonkWarrior08/Model_Router"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbcwek/opensource_the_ai_model_router_automating_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbcwek/opensource_the_ai_model_router_automating_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T10:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1maxfeb</id>
    <title>What happened to the Yi models?</title>
    <updated>2025-07-27T20:59:49+00:00</updated>
    <author>
      <name>/u/GabryIta</name>
      <uri>https://old.reddit.com/user/GabryIta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember some of them were really solid, but it's been over a year since we've seen a new release.&lt;br /&gt; Is the team still active, or has the project quietly died?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabryIta"&gt; /u/GabryIta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maxfeb/what_happened_to_the_yi_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T20:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1maywaw</id>
    <title>Devstral &amp; Magistral as adapters of Mistral</title>
    <updated>2025-07-27T22:01:31+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt; &lt;img alt="Devstral &amp;amp; Magistral as adapters of Mistral" src="https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ed0b082947f94ee079c2a6004328efe6c66fc9" title="Devstral &amp;amp; Magistral as adapters of Mistral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7"&gt;The initials of Devstral, Mistral, and Magistral as connected puzzle pieces&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tl;dr: title. Here are the weights: &lt;a href="https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision"&gt;Devstral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision"&gt;Magistral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA"&gt;Devstral-Small-2507-Rebased-Vision-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been using Mistral-Small-3.2 for the past few weeks. It's pretty solid, and the combination of vision and speed make it a really good pick for me, but...&lt;/p&gt; &lt;p&gt;I'm using sglang and it's really memory hungry which means it's hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I've tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at ~Q6-Q8, but meh.&lt;/p&gt; &lt;p&gt;Then I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for ~1.1x the VRAM? Yes, please! I tried &lt;a href="https://github.com/arcee-ai/mergekit#lora-extraction"&gt;mergekit&lt;/a&gt; but it requires the same architecture, so I'd either have to drop vision (which I also tried, and it seemed to work, but I don't like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it's actually pretty easy, you just have to copy the &lt;code&gt;model&lt;/code&gt; weights over the &lt;code&gt;language_model&lt;/code&gt; weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &lt;span class="md-spoiler-text"&gt;Yes, I know 2+ other attempts were made (&lt;em&gt;one by unsloth, from whom I stole the weights, lol&lt;/em&gt;) for the &lt;em&gt;exact&lt;/em&gt; same thing, and could've saved me a whole day of pain, but I only remembered about it ~5 mins ago, but this wasn't the core of what I wanted to do anyway so we'll conveniently call it a draw D:&lt;/span&gt;&lt;/p&gt; &lt;p&gt;With the &amp;quot;new&amp;quot; models in place, the next step was to try creating LoRAs again. Well, mergekit didn't work. I almost quit, but decided to search the web for another method and I ended up finding &lt;a href="https://github.com/thomasgauthier/LoRD"&gt;LoRD&lt;/a&gt;, the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn't even tell me why, I just get a generic error, but it's probably the vision parts, or 1+ of the modules (linear_1 / linear_2 / merging_layer / lm_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in &lt;a href="https://github.com/vllm-project/vllm/issues/18574"&gt;vLLM&lt;/a&gt;). In either case, it meant I couldn't run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.&lt;/p&gt; &lt;p&gt;If I'm not too lazy (which I'll likely be), I'll give this another go sometime, but now I'll just start my 761435 Karl Franz campaign.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T22:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1maipzo</id>
    <title>A new 21B-A3B model that can run 30 token/s on i9 CPU</title>
    <updated>2025-07-27T10:11:52+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt; &lt;img alt="A new 21B-A3B model that can run 30 token/s on i9 CPU" src="https://external-preview.redd.it/oKW2EqBWyvLdyTeoAGbQ_-d8-23kNb7Q9kBmGRYJM1E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb9a721495b0f0942a0fc51a0050cda33d2ef637" title="A new 21B-A3B model that can run 30 token/s on i9 CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f"&gt;https://preview.redd.it/e54liysd6eff1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=123b8a4dcb375d14ed980880bb55304b8133c96f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3"&gt;https://preview.redd.it/5t6qakxf6eff1.png?width=2004&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c934cd0bc6fbd428a6f1c46e1214db800d83c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker"&gt;https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maipzo/a_new_21ba3b_model_that_can_run_30_tokens_on_i9/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:11:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbako7</id>
    <title>Fine Tuning; Attribution at Inference Time</title>
    <updated>2025-07-28T08:20:42+00:00</updated>
    <author>
      <name>/u/Iam_Alastair</name>
      <uri>https://old.reddit.com/user/Iam_Alastair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a new model that allows for attribution of trained on data to be identified at the time of inference. One of my hypothesis being that if the the data being used at inference can be attributed then the next round of fine tuning can, &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Trim data that wasn't used at inference&lt;br /&gt;&lt;/li&gt; &lt;li&gt;More data could be added that is contextual to the outcome&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'd love to get some initial feedback on this thinking, would it be helpful when fine tuning your own models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iam_Alastair"&gt; /u/Iam_Alastair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbako7/fine_tuning_attribution_at_inference_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbako7/fine_tuning_attribution_at_inference_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbako7/fine_tuning_attribution_at_inference_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T08:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbce7b</id>
    <title>Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</title>
    <updated>2025-07-28T10:17:33+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://jerryliang24.github.io/DnD/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbce7b/draganddrop_llms_zeroshot_prompttoweights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbce7b/draganddrop_llms_zeroshot_prompttoweights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T10:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1maq0hg</id>
    <title>Why hasn't LoRA gained more popularity?</title>
    <updated>2025-07-27T16:03:21+00:00</updated>
    <author>
      <name>/u/dabomb007</name>
      <uri>https://old.reddit.com/user/dabomb007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabomb007"&gt; /u/dabomb007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T16:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mao95d</id>
    <title>Running LLMs exclusively on AMD Ryzen AI NPU</title>
    <updated>2025-07-27T14:52:33+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM&lt;/strong&gt; — a fast, runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and other models &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;. No CPU or iGPU fallback — just lean, efficient, &lt;strong&gt;NPU-native inference&lt;/strong&gt;. Think &lt;strong&gt;Ollama&lt;/strong&gt;, but purpose-built and deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;server mode (REST API)&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;LLaMA, Qwen, DeepSeek&lt;/strong&gt;, and more&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deeply hardware-optimized&lt;/strong&gt;, NPU-only inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; support (e.g., 128K for LLaMA)&lt;/li&gt; &lt;li&gt;Over &lt;strong&gt;11× power efficiency&lt;/strong&gt; compared to iGPU/CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating quickly and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (on remote machine):&lt;/strong&gt; Don’t have a Ryzen AI PC? Instantly try FastFlowLM on a &lt;strong&gt;remote AMD Ryzen AI 5 340 NPU system with 32 GB RAM&lt;/strong&gt; — no installation needed. &lt;a href="https://open-webui.testdrive-fastflowlm.com/"&gt;Launch Demo&lt;/a&gt; &lt;strong&gt;Login:&lt;/strong&gt; &lt;code&gt;guest@flm.npu&lt;/code&gt; &lt;strong&gt;Password:&lt;/strong&gt; &lt;code&gt;0000&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT"&gt;youtube.com/@FastFlowLM-YT&lt;/a&gt; &lt;em&gt;→ Quick start guide, performance benchmarks, and comparisons vs Ollama / LM Studio / Lemonade&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Community:&lt;/strong&gt; &lt;a href="https://discord.gg/Sze3Qsv5"&gt;discord.gg/Sze3Qsv5&lt;/a&gt; &lt;em&gt;→ Join us to ask questions, report issues, or contribute ideas&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know what works, what breaks, and what you’d love to see next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mao95d/running_llms_exclusively_on_amd_ryzen_ai_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T14:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb3xi3</id>
    <title>UI/UX Benchmark Update 7/27: 50 Models, Humanity, Voice, and new models from an AI lab on the horizon?</title>
    <updated>2025-07-28T01:58:00+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb3xi3/uiux_benchmark_update_727_50_models_humanity/"&gt; &lt;img alt="UI/UX Benchmark Update 7/27: 50 Models, Humanity, Voice, and new models from an AI lab on the horizon?" src="https://b.thumbs.redditmedia.com/ZEny5wZEwSnVbrdPI5YELoeX21mbA0nHIa9_2IoeNNo.jpg" title="UI/UX Benchmark Update 7/27: 50 Models, Humanity, Voice, and new models from an AI lab on the horizon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my last post as &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6ztb2/uiux_benchmark_update_722_newest_qwen_models/"&gt;context&lt;/a&gt;. Otherwise let's get to the exciting updates about &lt;a href="https://www.designarena.ai/"&gt;the benchmark&lt;/a&gt;. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;50 Models:&lt;/strong&gt; I've lost track of the count, but since the benchmark began a little over a month ago, we've added over &lt;a href="https://www.designarena.ai/changelog"&gt;50 models&lt;/a&gt; so far. In the past few days, we've added Imagen 4 Ultra from Google, Qwen3-235B-A22B-Thinking-2507, Ideogram 3.0, and UIGen X 32B. We're trying to add new models everyday, so let us know what you would like to see here or on our &lt;a href="https://discord.com/channels/1390777934218006580/1396581263305084998"&gt;Discord&lt;/a&gt;. I think we've gotten most of people's requests (expect some of the GLM models which I WILL add, sorry I just keep forgetting). &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;UIGEN:&lt;/strong&gt; Our friends developing the &lt;a href="https://huggingface.co/Tesslate"&gt;UIGen&lt;/a&gt; are developing some killer open-source models for frontend dev, and we've added a couple of their models to the benchmark, though inference is quite slow. It would be great if anyone knows of any good inference providers or could request provider support on HuggingFace. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Humanity:&lt;/strong&gt; This feature is still experimental and in beta, but we want to add a &lt;a href="https://www.designarena.ai/humanity"&gt;human baseline&lt;/a&gt; to the benchmark (similar to ARC-AGI) where models are compared to designs and work from people. Users submit an image of a design or code (keep it to HTML/CSS/JS to be consistent with models), and then those designs (after a short review process to ensure there's not spam) and code are compared (anonymously) to model generations. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Voice&lt;/strong&gt;. Well UI/UX is our primary focus, our goal is to generally evaluate how models perform on all kinds of qualitative aspects that are hard to measure deterministically (e.g. such as how well models might hold or resemble a human conversation, debate, etc.). As a beta feature, we've added a &lt;a href="https://www.reddit.com/r/LocalLLaMA/submit/?type=IMAGE"&gt;voice category&lt;/a&gt; where 2 voice models will have a conversation about a prompt you provide, and then you can choose which model you liked better. There are still some bugs to sort out with this feature, but would appreciate any feedback on this. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;New Models on the Horizon?&lt;/strong&gt; After the Qwen releases last week, there's some buzz that we might see some model drops over the next week. We'll be keeping a watchful eye and attempting to get those models (whenever they come out) on Design Arena as fast as possible. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let us know if you have any feedback or questions! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mb3xi3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb3xi3/uiux_benchmark_update_727_50_models_humanity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb3xi3/uiux_benchmark_update_727_50_models_humanity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T01:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb77c7</id>
    <title>2x RTX 3090 24GB or 8x 3060 12GB</title>
    <updated>2025-07-28T04:49:32+00:00</updated>
    <author>
      <name>/u/twotemp</name>
      <uri>https://old.reddit.com/user/twotemp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, apologies if this question has been posted before i haven’t been able to find any concrete info on it. &lt;/p&gt; &lt;p&gt;In my area i can get 8 3060 12GBs for the exact same price as two 3090s, I’m looking to run LLMs, Heavy ComfyUI workflows, training models, LoRas and just about any other AI development haha.&lt;/p&gt; &lt;p&gt;I’ve never ran anything on a 2x+-gpu set up, is doubling the VRAM even worth the effort and time setting up? (big home labber, i can figure it out)&lt;/p&gt; &lt;p&gt;and are 3060s even fast enough to use those 96GB of vram effectively? what’s the better bang for the buck? prices are the EXACT same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/twotemp"&gt; /u/twotemp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T04:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb2dcp</id>
    <title>Byte-Vision is a privacy-first (Llama.cpp) document intelligence platform that transforms static documents into an interactive, searchable knowledge base. Built on Elasticsearch with RAG (Retrieval-Augmented Generation) capabilities, it offers document parsing, OCR processing, and modern UI.</title>
    <updated>2025-07-28T00:41:00+00:00</updated>
    <author>
      <name>/u/Important_Half_8277</name>
      <uri>https://old.reddit.com/user/Important_Half_8277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2dcp/bytevision_is_a_privacyfirst_llamacpp_document/"&gt; &lt;img alt="Byte-Vision is a privacy-first (Llama.cpp) document intelligence platform that transforms static documents into an interactive, searchable knowledge base. Built on Elasticsearch with RAG (Retrieval-Augmented Generation) capabilities, it offers document parsing, OCR processing, and modern UI." src="https://external-preview.redd.it/ywpzKzmrsuXJqmShKQ64gatOoAFIfbPYe9pFc1NIqDQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d7c803f441c5cf105e320d67c7290e56955a330" title="Byte-Vision is a privacy-first (Llama.cpp) document intelligence platform that transforms static documents into an interactive, searchable knowledge base. Built on Elasticsearch with RAG (Retrieval-Augmented Generation) capabilities, it offers document parsing, OCR processing, and modern UI." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important_Half_8277"&gt; /u/Important_Half_8277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kbrisso/byte-vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2dcp/bytevision_is_a_privacyfirst_llamacpp_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2dcp/bytevision_is_a_privacyfirst_llamacpp_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T00:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb6jzz</id>
    <title>Why I'm Betting Against AI Agents in 2025 (Despite Building Them)</title>
    <updated>2025-07-28T04:12:40+00:00</updated>
    <author>
      <name>/u/Ilovekittens345</name>
      <uri>https://old.reddit.com/user/Ilovekittens345</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ilovekittens345"&gt; /u/Ilovekittens345 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://utkarshkanwat.com/writing/betting-against-agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6jzz/why_im_betting_against_ai_agents_in_2025_despite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6jzz/why_im_betting_against_ai_agents_in_2025_despite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T04:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbc8tb</id>
    <title>Are there any examples of 14B+ reputable models that outperform models twice their size or more?</title>
    <updated>2025-07-28T10:08:35+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for examples where smaller reputable models (Llama, Qwen, DeepSeek, …) are widely recognized as better - not just in benchmarks, but in broader evaluations for general tasks.&lt;/p&gt; &lt;p&gt;I sometimes see claims that 70B-range models beat 300B+ ones, often based on benchmark results. But in practice or broader testing, the opposite often turns out to be true.&lt;/p&gt; &lt;p&gt;I’m wondering if LLMs have reached a level of maturity where it’s now extremely unlikely for a smaller model to genuinely outperform one that’s twice its size or more.&lt;/p&gt; &lt;p&gt;Edit: in terms of quality of the model answers (Response accuracy only), speed and VRAM requirements excluded.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc8tb/are_there_any_examples_of_14b_reputable_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc8tb/are_there_any_examples_of_14b_reputable_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbc8tb/are_there_any_examples_of_14b_reputable_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T10:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mammv5</id>
    <title>Qwen3-235B-A22B 2507 is so good</title>
    <updated>2025-07-27T13:43:37+00:00</updated>
    <author>
      <name>/u/z_3454_pfk</name>
      <uri>https://old.reddit.com/user/z_3454_pfk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The non-reasoning model is about as good as 2.5 flash with 4k reasoning tokens. The latency of no reasoning vs reasoning makes it so much better than 2.5 flash. I also prefer the shorter outputs than the verbose asf gemini. &lt;/p&gt; &lt;p&gt;The markdown formatting is so much better and the outputs are just so much nicer to read than flash. Knowledge wise, it's a bit worse than 2.5 flash but that's probably because it's smaller model. better at coding than flash too. &lt;/p&gt; &lt;p&gt;running unsloth Q8. I haven't tried the thinking one yet. what do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z_3454_pfk"&gt; /u/z_3454_pfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mammv5/qwen3235ba22b_2507_is_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T13:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbavi1</id>
    <title>My first finetune: Gemma 3 4B unslop via GRPO</title>
    <updated>2025-07-28T08:41:18+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Training code is included, so maybe someone with more hardware than me can do cooler stuff.&lt;/p&gt; &lt;p&gt;I also uploaded a Q4_K_M GGUF made with unsloth's imatrix.&lt;/p&gt; &lt;p&gt;It's released as a LoRA adapter because my internet sucks and I can't successfully upload the whole thing. If you want full quality you'll need to merge it with &lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The method is based on my own statistical analysis of lots of gemma 3 4b text, plus some patterns i don't like. i also reinforce the correct number of words asked for in the prompt, and i reward lexical diversity &amp;gt; 100.&lt;/p&gt; &lt;p&gt;dataset not included, but i did include an example of what my dataset looks like for anyone trying to recreate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO"&gt;https://huggingface.co/electroglyph/gemma-3-4b-it-unslop-GRPO&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbavi1/my_first_finetune_gemma_3_4b_unslop_via_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T08:41:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbdm6t</id>
    <title>GLM 4.5 possibly releasing today according to Bloomberg</title>
    <updated>2025-07-28T11:26:56+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"&gt; &lt;img alt="GLM 4.5 possibly releasing today according to Bloomberg" src="https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11470efb2209e35e2be0d434f089cd6d797726ba" title="GLM 4.5 possibly releasing today according to Bloomberg" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bloomberg writes:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/organizations/zai-org/activity/collections"&gt;https://huggingface.co/organizations/zai-org/activity/collections&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T11:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1majemr</id>
    <title>Suprise suprise!!</title>
    <updated>2025-07-27T10:55:19+00:00</updated>
    <author>
      <name>/u/GoodGuyLafarge</name>
      <uri>https://old.reddit.com/user/GoodGuyLafarge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"&gt; &lt;img alt="Suprise suprise!!" src="https://preview.redd.it/k64e9lwtdeff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d09af7edf96adcd3793cd8970c2cab58d53352b" title="Suprise suprise!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodGuyLafarge"&gt; /u/GoodGuyLafarge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k64e9lwtdeff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1majemr/suprise_suprise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T10:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb7tb7</id>
    <title>Watch Alibaba Cloud Founder on China’s AI Future</title>
    <updated>2025-07-28T05:25:13+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"&gt; &lt;img alt="Watch Alibaba Cloud Founder on China’s AI Future" src="https://external-preview.redd.it/grOevYCkkhDi2lNOhhkTLldJ3vjPBtyjZzAD6KyhuGI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe991643a370499bccf6b3299fa7518b3c1e355e" title="Watch Alibaba Cloud Founder on China’s AI Future" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/videos/2025-07-28/alibaba-cloud-founder-on-china-s-ai-future-video"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb7tb7/watch_alibaba_cloud_founder_on_chinas_ai_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T05:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb2y1z</id>
    <title>The Untold Revolution in iOS 26: WebGPU Is Coming</title>
    <updated>2025-07-28T01:08:57+00:00</updated>
    <author>
      <name>/u/WooFL</name>
      <uri>https://old.reddit.com/user/WooFL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"&gt; &lt;img alt="The Untold Revolution in iOS 26: WebGPU Is Coming" src="https://external-preview.redd.it/LyD_1wQqYUDOfxUDg_36aYU0Ld7GP8TKYS-wDVU6gWY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bac2a180967dbb6dd0c4544eaf16660950fa7c43" title="The Untold Revolution in iOS 26: WebGPU Is Coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WooFL"&gt; /u/WooFL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brandlens.io/blog/the-untold-revolution-beneath-ios-26-webgpu-is-coming-everywhere-and-it-changes-everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb2y1z/the_untold_revolution_in_ios_26_webgpu_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T01:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb98cm</id>
    <title>Granite 4 small and medium might be 30B6A/120B30A?</title>
    <updated>2025-07-28T06:53:38+00:00</updated>
    <author>
      <name>/u/Kryesh</name>
      <uri>https://old.reddit.com/user/Kryesh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"&gt; &lt;img alt="Granite 4 small and medium might be 30B6A/120B30A?" src="https://external-preview.redd.it/HsLxSV9iQYiHn_HZBXHd4eTY-jpAHtvg9nNDBZ3sa94.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71914d4ace28f4302812fa4b60aacea3654d064d" title="Granite 4 small and medium might be 30B6A/120B30A?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kryesh"&gt; /u/Kryesh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=UxUD88TRlBY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb98cm/granite_4_small_and_medium_might_be_30b6a120b30a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T06:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb6uhm</id>
    <title>Pi AI studio</title>
    <updated>2025-07-28T04:28:59+00:00</updated>
    <author>
      <name>/u/koumoua01</name>
      <uri>https://old.reddit.com/user/koumoua01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"&gt; &lt;img alt="Pi AI studio" src="https://b.thumbs.redditmedia.com/CP9KFtIHMzNxz_IXwevaJpIQ_DH-LieoKpFIOifsV_Q.jpg" title="Pi AI studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This 96GB device cost around $1000. Has anyone tried it before? Can it host small LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koumoua01"&gt; /u/koumoua01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mb6uhm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb6uhm/pi_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T04:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb15g2</id>
    <title>UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.</title>
    <updated>2025-07-27T23:42:37+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"&gt; &lt;img alt="UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." src="https://b.thumbs.redditmedia.com/AX8rU0Ar21fTE1Um6Zy39yTbpXN7nfUgowthOtgI49Y.jpg" title="UIGEN-X-0727 Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-32B-0727"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt; Releasing 4B in 24 hours and 32B now. &lt;/p&gt; &lt;p&gt;Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mb15g2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb15g2/uigenx0727_runs_locally_and_crushes_it_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T23:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mb9uy8</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-28T07:33:42+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No model card as of yet&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mb9uy8/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T07:33:42+00:00</published>
  </entry>
</feed>
