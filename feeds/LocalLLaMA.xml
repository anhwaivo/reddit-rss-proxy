<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-06T02:20:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jsc27r</id>
    <title>Llama 4 Maverick 2nd on lmarena</title>
    <updated>2025-04-05T19:54:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsc27r/llama_4_maverick_2nd_on_lmarena/"&gt; &lt;img alt="Llama 4 Maverick 2nd on lmarena" src="https://preview.redd.it/wujmprm4n2te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67d8c4d2f5e24a06d18d1eecedd5cf7c9c07d6f" title="Llama 4 Maverick 2nd on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wujmprm4n2te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsc27r/llama_4_maverick_2nd_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsc27r/llama_4_maverick_2nd_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsagyr</id>
    <title>With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one.</title>
    <updated>2025-04-05T18:45:09+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"&gt; &lt;img alt="With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one." src="https://preview.redd.it/9tclfgid92te1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277ab988a5a396735a74b80fcdd3d4b467c43f25" title="With no update in 4 months, livebench was getting saturated and benchmaxxed, so I'm really looking forward to this one." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to tweet: &lt;a href="https://x.com/bindureddy/status/1908296208025870392"&gt;https://x.com/bindureddy/status/1908296208025870392&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9tclfgid92te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsagyr/with_no_update_in_4_months_livebench_was_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jseb4r</id>
    <title>Llama 4 was a giant disappointment, let's wait for Qwen 3.</title>
    <updated>2025-04-05T21:34:59+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Youre telling me that a 109B parameter model performs the same as a 24B model? Lol. You cant make this stuff up, how could people possibly be happy with a model that takes 4x more computer to run that performs similarly to a 24B LLM. Im guessing that either Meta needed to release something to keep their investors, or mabye they have just fallen behind in the LLM scene. I still cant believe that they didn't release a normal 8b model and that they decided to go in the MoE direction instead. Even Gemini 2.5 beats Llama 4 behemoth in the benchmarks. It really is disappointing to see that there is no non MoE (dense) LLMs that were released by Meta but mabye when Qwen 3 is released in 2 weeks, we will have a model that will finally meet our expectations of what Llama 4 should have been.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseb4r/llama_4_was_a_giant_disappointment_lets_wait_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseb4r/llama_4_was_a_giant_disappointment_lets_wait_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jseb4r/llama_4_was_a_giant_disappointment_lets_wait_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1js335l</id>
    <title>Karamaru - An "Edo period" LLM trained on 17th-19th century japanese literature.</title>
    <updated>2025-04-05T13:09:39+00:00</updated>
    <author>
      <name>/u/nomad_lw</name>
      <uri>https://old.reddit.com/user/nomad_lw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt; &lt;img alt="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Karamaru - An &amp;quot;Edo period&amp;quot; LLM trained on 17th-19th century japanese literature." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this a few days ago where a researcher from Sakana AI continually pretrained a Llama-3 Elyza 8B model on classical japanese literature. &lt;/p&gt; &lt;p&gt;What's cool about is that it builds towards an idea that's been brewing on my mind and evidently a lot of other people here,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A model that's able to be a Time-travelling subject matter expert. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;Researcher's tweet: &lt;a href="https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19"&gt;https://x.com/tkasasagi/status/1907998360713441571?t=PGhYyaVJQtf0k37l-9zXiA&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1"&gt;https://huggingface.co/spaces/SakanaAI/Llama-3-Karamaru-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomad_lw"&gt; /u/nomad_lw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/karamaru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js335l/karamaru_an_edo_period_llm_trained_on_17th19th/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T13:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jscjex</id>
    <title>Llama 4 is the first major model hosted on Hugging Face using Xet</title>
    <updated>2025-04-05T20:15:51+00:00</updated>
    <author>
      <name>/u/jsulz</name>
      <uri>https://old.reddit.com/user/jsulz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt; &lt;img alt="Llama 4 is the first major model hosted on Hugging Face using Xet" src="https://external-preview.redd.it/JGJA9bp-fjQd-DW0Up7N-YoOwvacHf3g0ERwmbk7qZg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84f2e4f629da6082e9745bb113ccba95ee01d469" title="Llama 4 is the first major model hosted on Hugging Face using Xet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just dropped Llama 4, and the Xet team has been working behind the scenes to make sure it’s fast and accessible for the entire HF community.&lt;/p&gt; &lt;p&gt;Here’s what’s new:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All Llama 4 models on Hugging Face use the Xet backend&lt;/strong&gt; — a chunk-based storage system built for large AI models.&lt;/li&gt; &lt;li&gt;This enabled us to upload &lt;strong&gt;terabyte-scale model weights in record time&lt;/strong&gt;, and it’s already making downloads faster too.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deduplication hits ~25%&lt;/strong&gt; on base models, and we expect to see at least &lt;strong&gt;40%&lt;/strong&gt; for fine-tuned or quantized variants. That means less bandwidth, faster sharing, and smoother collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We built Xet for this moment, to give model builders and users a better way to version, share, and iterate on large models without the Git LFS pain.&lt;/p&gt; &lt;p&gt;Here’s a quick snapshot of the impact on a few select repositories 👇&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oldvpapxu2te1.png?width=1025&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae6c5cc8c3dbbd339e8aba12a634bfa84b8564eb"&gt;https://preview.redd.it/oldvpapxu2te1.png?width=1025&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae6c5cc8c3dbbd339e8aba12a634bfa84b8564eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what models you’re fine-tuning or quantizing from Llama 4. We’re continuing to optimize the storage layer so you can go from “I’ve got weights” to “it’s live on the Hub” faster than ever.&lt;/p&gt; &lt;p&gt;Related blog post: &lt;a href="https://huggingface.co/blog/llama4-release"&gt;https://huggingface.co/blog/llama4-release&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsulz"&gt; /u/jsulz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jscjex/llama_4_is_the_first_major_model_hosted_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T20:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1js0g38</id>
    <title>Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order</title>
    <updated>2025-04-05T10:27:38+00:00</updated>
    <author>
      <name>/u/Marcuss2</name>
      <uri>https://old.reddit.com/user/Marcuss2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt; &lt;img alt="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" src="https://external-preview.redd.it/bKa6_zcgR56sbwG-Cqtu_jN8tcBni2YVCOOrskJ4IzI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d47a097527e5e5e57498d3ac6192eb2f6741fe55" title="Tenstorrent Blackhole PCI-e cards with 32 GB of GDDR6 available for order" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marcuss2"&gt; /u/Marcuss2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tenstorrent.com/hardware/blackhole"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js0g38/tenstorrent_blackhole_pcie_cards_with_32_gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T10:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbjr3</id>
    <title>Turn local and private repos into prompts in one click with the gitingest VS Code Extension!</title>
    <updated>2025-04-05T19:31:44+00:00</updated>
    <author>
      <name>/u/Sanjuwa</name>
      <uri>https://old.reddit.com/user/Sanjuwa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"&gt; &lt;img alt="Turn local and private repos into prompts in one click with the gitingest VS Code Extension!" src="https://external-preview.redd.it/Zml5d2NtOGdpMnRlMVdzmEgr56-P0X2MdMr8l29_GfTj5L1NSLkdzC5Bz-eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08af159dd0608d37536754708bb572a540be16a2" title="Turn local and private repos into prompts in one click with the gitingest VS Code Extension!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;First of thanks to &lt;a href="/u/MrCyclopede"&gt;u/MrCyclopede&lt;/a&gt; for amazing work !!&lt;/p&gt; &lt;p&gt;Initially, I converted the his original Python code to TypeScript and then built the extension.&lt;/p&gt; &lt;p&gt;It's simple to use.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open the Command Palette (&lt;code&gt;Ctrl+Shift+P&lt;/code&gt; or &lt;code&gt;Cmd+Shift+P&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Type &amp;quot;Gitingest&amp;quot; to see available commands: &lt;ul&gt; &lt;li&gt;&lt;code&gt;Gitingest: Ingest Local Directory&lt;/code&gt;: Analyze a local directory&lt;/li&gt; &lt;li&gt;&lt;code&gt;Gitingest: Ingest Git Repository&lt;/code&gt;: Analyze a remote Git repository&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the prompts to select a directory or enter a repository URL&lt;/li&gt; &lt;li&gt;View the results in a new text document&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d love for you to check it out and share your feedback:&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lakpahana/export-to-llm-gitingest"&gt;https://github.com/lakpahana/export-to-llm-gitingest&lt;/a&gt; ( please give me a 🌟)&lt;br /&gt; Marketplace: &lt;a href="https://marketplace.visualstudio.com/items?itemName=lakpahana.export-to-llm-gitingest"&gt;https://marketplace.visualstudio.com/items?itemName=lakpahana.export-to-llm-gitingest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know your thoughts—any feedback or suggestions would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sanjuwa"&gt; /u/Sanjuwa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6s9t5n5gi2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjr3/turn_local_and_private_repos_into_prompts_in_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbjmm</id>
    <title>Llama reasoning soon and llama 4 behemoth</title>
    <updated>2025-04-05T19:31:35+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"&gt; &lt;img alt="Llama reasoning soon and llama 4 behemoth" src="https://preview.redd.it/m1tookk0j2te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=346d6c2a99dd5be293d57eccf5ee3cc161e4faea" title="Llama reasoning soon and llama 4 behemoth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1tookk0j2te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbjmm/llama_reasoning_soon_and_llama_4_behemoth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsalxn</id>
    <title>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</title>
    <updated>2025-04-05T18:51:11+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt; &lt;img alt="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" src="https://external-preview.redd.it/5GYklgQz-p1iWSTGvDsKHeD_QUDxP-9vHZQeXTsgRz4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65f85ee3e9068eb521d7e3ef4dce3cee7c471c03" title="The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsalxn/the_llama_4_herd_the_beginning_of_a_new_era_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jseqbs</id>
    <title>Llama 4 scout is not doing well in "write a raytracer" code creativity benchmark</title>
    <updated>2025-04-05T21:54:44+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt; &lt;img alt="Llama 4 scout is not doing well in &amp;quot;write a raytracer&amp;quot; code creativity benchmark" src="https://external-preview.redd.it/c292Ets96l9SIyQ2uIjvrw2seHBUtAYlhc01uwhFHEU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14a26df29ae2a87e5ac7be16d882a5cac76e40c4" title="Llama 4 scout is not doing well in &amp;quot;write a raytracer&amp;quot; code creativity benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jisuq4/deepseek_v30324_has_caught_up_to_sonnet_37_in_my/"&gt;previously experimented&lt;/a&gt; with a code creativity benchmark where I asked LLMs to write a small python program to create a raytraced image.&lt;/p&gt; &lt;p&gt;&amp;gt; &lt;code&gt;Write a raytracer that renders an interesting scene with many colourful lightsources in python. Output a 800x600 image as a png&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I only allowed one shot, no iterative prompting to solve broken code. I think execute the program and evaluate the imagine. It turns out this is a proxy for code creativity.&lt;/p&gt; &lt;p&gt;In the mean time I tested some new models: LLama 4 scout, Gemini 2.5 exp and Quasar Alpha&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ruh9dufe83te1.png?width=1367&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08bd5968b9ecdc3568380e3c3d1a67a30ce3a005"&gt;https://preview.redd.it/ruh9dufe83te1.png?width=1367&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08bd5968b9ecdc3568380e3c3d1a67a30ce3a005&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLama4 scout underwhelms in quality of generated images compared to the others. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/egq5ugj883te1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5132f98a77b707d8353c4478047dc48b9f4c06c"&gt;https://preview.redd.it/egq5ugj883te1.png?width=588&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5132f98a77b707d8353c4478047dc48b9f4c06c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, there is some magic sauce in the fine-tuning of DeepSeek V3-0324, Sonnet 3.7 and Gemini 2.5 Pro that makes them create longer and more varied programs. I assume it is a RL step. Really fascinating, as it seems not all labs have caught up on this yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cpldcpu/llmbenchmark"&gt;Repository here.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jseqbs/llama_4_scout_is_not_doing_well_in_write_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsadt3</id>
    <title>Llama4 Released</title>
    <updated>2025-04-05T18:41:26+00:00</updated>
    <author>
      <name>/u/latestagecapitalist</name>
      <uri>https://old.reddit.com/user/latestagecapitalist</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/latestagecapitalist"&gt; /u/latestagecapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsadt3/llama4_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbseu</id>
    <title>Llama4 Scout downloading</title>
    <updated>2025-04-05T19:42:45+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"&gt; &lt;img alt="Llama4 Scout downloading" src="https://preview.redd.it/5nx0y06wk2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=895ab95c094f3843276b8881066b3a8eb61a7d34" title="Llama4 Scout downloading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama4 Scout downloading 😁👍&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nx0y06wk2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbseu/llama4_scout_downloading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsfm5j</id>
    <title>Potential Llama 4.2 - 7b</title>
    <updated>2025-04-05T22:36:53+00:00</updated>
    <author>
      <name>/u/medcanned</name>
      <uri>https://old.reddit.com/user/medcanned</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release, I got curious and looked around the implementation code of the Llama4 models in transformers and found something interesting:&lt;/p&gt; &lt;p&gt;&lt;code&gt;model = Llama4ForCausalLM.from_pretrained(&amp;quot;meta-llama4/Llama4-2-7b-hf&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Given the type of model, it will be text-only. So, we just have to be patient :)&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/huggingface/transformers/blob/9bfae2486a7b91dc6d4380b7936e0b2b8c1ed708/src/transformers/models/llama4/modeling_llama4.py#L997"&gt;https://github.com/huggingface/transformers/blob/9bfae2486a7b91dc6d4380b7936e0b2b8c1ed708/src/transformers/models/llama4/modeling_llama4.py#L997&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medcanned"&gt; /u/medcanned &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsfm5j/potential_llama_42_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T22:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsafqw</id>
    <title>Llama 4 announced</title>
    <updated>2025-04-05T18:43:42+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://www.llama.com/llama4/"&gt;https://www.llama.com/llama4/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsafqw/llama_4_announced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsgliv</id>
    <title>it looks like Meta's new model's key innovation of "interleaved no-RoPE attention" for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago.</title>
    <updated>2025-04-05T23:24:45+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt; &lt;img alt="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." src="https://preview.redd.it/7dyflct7o3te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9183b8c88d6a952ada033ccc2507a72f82046e45" title="it looks like Meta's new model's key innovation of &amp;quot;interleaved no-RoPE attention&amp;quot; for infinite context is actually the same thing as Cohere's Command-A model introduced a few days ago." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7dyflct7o3te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsgliv/it_looks_like_metas_new_models_key_innovation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T23:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jshwxe</id>
    <title>First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:</title>
    <updated>2025-04-06T00:32:51+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt; &lt;img alt="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:" src="https://preview.redd.it/1zt2gzrq04te1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3abfffb312e36148337fcbbdd96100c2f53bd88c" title="First results are in. Llama 4 Maverick 17B active / 400B total is blazing fast with MLX on an M3 Ultra — 4-bit model generating 1100 tokens at 50 tok/sec:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zt2gzrq04te1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jshwxe/first_results_are_in_llama_4_maverick_17b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-06T00:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdtew</id>
    <title>Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models</title>
    <updated>2025-04-05T21:12:16+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt; &lt;img alt="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" src="https://external-preview.redd.it/cW9oa2FtZXAwM3RlMZqijIi1GCa_F1Pp7Yxzhw_7Ni36eaah2O36NNbIKvPq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80afd573b70e01bfb67d4f3998b5e0b518af08d" title="Initial UI tests: Llama 4 Maverick and Scout, very disappointing compared to other similar models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/j7p6nqep03te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdtew/initial_ui_tests_llama_4_maverick_and_scout_very/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsbdm8</id>
    <title>Llama 4 benchmarks</title>
    <updated>2025-04-05T19:24:22+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 benchmarks" src="https://preview.redd.it/cl35fq7qh2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff22b91338fb54450168b9339d67ee62bd7a48ee" title="Llama 4 benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cl35fq7qh2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsbdm8/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsdq4p</id>
    <title>Llama 4 Maverick - Python hexagon test failed</title>
    <updated>2025-04-05T21:08:02+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt; &lt;img alt="Llama 4 Maverick - Python hexagon test failed" src="https://b.thumbs.redditmedia.com/32CwgGLDK_Ju5fOPFA4pivpATQq39V8dluXhV3-Prqw.jpg" title="Llama 4 Maverick - Python hexagon test failed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254"&gt;https://preview.redd.it/ea46ym5303te1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0375fbf11fa3a54613a2c3aa567f2fe05c3cd254&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Prompt:&lt;/p&gt; &lt;p&gt;Write a Python program that shows 20 balls bouncing inside a spinning heptagon:&lt;br /&gt; - All balls have the same radius.&lt;br /&gt; - All balls have a number on it from 1 to 20.&lt;br /&gt; - All balls drop from the heptagon center when starting.&lt;br /&gt; - Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d, #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e32, #e17b34, #dd7a56, #db8449, #d66a35&lt;br /&gt; - The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.&lt;br /&gt; - The material of all the balls determines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.&lt;br /&gt; - All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.&lt;br /&gt; - The heptagon is spinning around its center, and the speed of spinning is 360 degrees per 5 seconds.&lt;br /&gt; - The heptagon size should be large enough to contain all the balls.&lt;br /&gt; - Do not use the pygame library; implement collision detection algorithms and collision response etc. by yourself. The following Python libraries are allowed: tkinter, math, numpy, dataclasses, typing, sys.&lt;br /&gt; - All codes should be put in a single Python file.&lt;/p&gt; &lt;p&gt;DeepSeek R1 and Gemini 2.5 Pro do this in one request. Maverick failed in 8 requests&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsdq4p/llama_4_maverick_python_hexagon_test_failed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T21:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jscww3</id>
    <title>Gemini 2.5 Pro is better than Llama 4 behemoth on benchmarks</title>
    <updated>2025-04-05T20:32:01+00:00</updated>
    <author>
      <name>/u/Glittering-Bag-4662</name>
      <uri>https://old.reddit.com/user/Glittering-Bag-4662</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Specifically GPQA Diamond and MMLU Pro. Zuck lying out here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Bag-4662"&gt; /u/Glittering-Bag-4662 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jscww3/gemini_25_pro_is_better_than_llama_4_behemoth_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T20:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1js4iy0</id>
    <title>I think I overdid it.</title>
    <updated>2025-04-05T14:21:22+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt; &lt;img alt="I think I overdid it." src="https://preview.redd.it/i5f8b0knz0te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1448cae5bed745aa96ac7b2801a7bf32c07afd26" title="I think I overdid it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i5f8b0knz0te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T14:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsahy4</id>
    <title>Llama 4 is here</title>
    <updated>2025-04-05T18:46:20+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsahy4/llama_4_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsax3p</id>
    <title>Llama 4 Benchmarks</title>
    <updated>2025-04-05T19:04:21+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt; &lt;img alt="Llama 4 Benchmarks" src="https://preview.redd.it/o2cd1y15e2te1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01928d53f0ef81a88115f299ef15628aacc38783" title="Llama 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o2cd1y15e2te1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsax3p/llama_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T19:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsabgd</id>
    <title>Meta: Llama4</title>
    <updated>2025-04-05T18:38:40+00:00</updated>
    <author>
      <name>/u/pahadi_keeda</name>
      <uri>https://old.reddit.com/user/pahadi_keeda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt; &lt;img alt="Meta: Llama4" src="https://external-preview.redd.it/cwgFslgMUPL6p26FpnXYan8AI9J3Uz-yA2DZbRx4puk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3c2d0eac2996298f7e242609a095f7deafa5ac1" title="Meta: Llama4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pahadi_keeda"&gt; /u/pahadi_keeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.llama.com/llama-downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsabgd/meta_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:38:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jsampe</id>
    <title>Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!</title>
    <updated>2025-04-05T18:52:08+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt; &lt;img alt="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" src="https://external-preview.redd.it/Z3p2aHZudXhiMnRlMYW4H8xHgtzR3pjuficV95KktJ2KVETiew0YUMQL020k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b332bfe887b8dc264280ed80e4cedb70e9cd787" title="Mark presenting four Llama 4 models, even a 2 trillion parameters model!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source from his instagram page&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7bgnzhtxb2te1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-05T18:52:08+00:00</published>
  </entry>
</feed>
