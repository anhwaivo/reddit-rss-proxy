<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T17:06:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1miebln</id>
    <title>OPENAI OPENSOURCE MODEL LEAKED BEFORE RELEASE</title>
    <updated>2025-08-05T16:45:32+00:00</updated>
    <author>
      <name>/u/x8ko_dev</name>
      <uri>https://old.reddit.com/user/x8ko_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model set to release today by openai is &amp;quot;gpt-oss-120b&amp;quot;.&lt;/p&gt; &lt;p&gt;It is currently unreleased but for those of you using other coding tools you can access the model through an openai compatible endpoint on &lt;a href="https://cloud.cerebras.ai/"&gt;https://cloud.cerebras.ai/&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The model is currently unlisted and hidden, but it is still accessible through the API, simply set the custom model id as &amp;quot;gpt-oss-120b&amp;quot; And yes, you can use it for free currently.&lt;br /&gt; Guess thats why you dont host a model before release even if you dont document it...&lt;/p&gt; &lt;p&gt;Base URL is: &amp;quot;&lt;a href="https://api.cerebras.ai/v1"&gt;https://api.cerebras.ai/v1&lt;/a&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Post Powered by LogiQ CLI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x8ko_dev"&gt; /u/x8ko_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miebln/openai_opensource_model_leaked_before_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miebln/openai_opensource_model_leaked_before_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miebln/openai_opensource_model_leaked_before_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhctd</id>
    <title>üöÄ Meet Qwen-Image</title>
    <updated>2025-08-04T15:58:11+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt; &lt;img alt="üöÄ Meet Qwen-Image" src="https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476" title="üöÄ Meet Qwen-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Meet Qwen-Image ‚Äî a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;üîç Key Highlights:&lt;/p&gt; &lt;p&gt;üîπ SOTA text rendering ‚Äî rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;üîπ In-pixel text generation ‚Äî no overlays, fully integrated&lt;/p&gt; &lt;p&gt;üîπ Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;üé® Also excels at general image generation ‚Äî from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7a463it8z0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1midrb7</id>
    <title>new open weight model from OpenAI will have computer use</title>
    <updated>2025-08-05T16:24:34+00:00</updated>
    <author>
      <name>/u/mvp525</name>
      <uri>https://old.reddit.com/user/mvp525</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mvp525"&gt; /u/mvp525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/_aidan_clark_/status/1952760702122557684"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midrb7/new_open_weight_model_from_openai_will_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midrb7/new_open_weight_model_from_openai_will_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhgu6t</id>
    <title>Sam Altman watching Qwen drop model after model</title>
    <updated>2025-08-04T15:38:39+00:00</updated>
    <author>
      <name>/u/TheRealSerdra</name>
      <uri>https://old.reddit.com/user/TheRealSerdra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt; &lt;img alt="Sam Altman watching Qwen drop model after model" src="https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=460e01fe2091b6bc2347e41a918d258022a40353" title="Sam Altman watching Qwen drop model after model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealSerdra"&gt; /u/TheRealSerdra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7t8cmgrv0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhiqqn</id>
    <title>Qwen-Image is out</title>
    <updated>2025-08-04T16:49:14+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt; &lt;img alt="Qwen-Image is out" src="https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2392cbc2ee2f68a83914b65971bd3a688fc24739" title="Qwen-Image is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1952398250121756992"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better than Flux Kontext, gpt-image level&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4077mfg081hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhdig</id>
    <title>QWEN-IMAGE is released!</title>
    <updated>2025-08-04T15:58:55+00:00</updated>
    <author>
      <name>/u/TheIncredibleHem</name>
      <uri>https://old.reddit.com/user/TheIncredibleHem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt; &lt;img alt="QWEN-IMAGE is released!" src="https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a65aaca919e956009709dd069f70c0c907403912" title="QWEN-IMAGE is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIncredibleHem"&gt; /u/TheIncredibleHem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1vov</id>
    <title>Qwen-image now supported in ComfyUI</title>
    <updated>2025-08-05T06:37:16+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At last after wait of few hours, ComfyUI now has support for Qwen-Image. Its from their &lt;a href="https://github.com/comfyanonymous/ComfyUI/pull/9179"&gt;git repo&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1miagcn</id>
    <title>Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments.</title>
    <updated>2025-08-05T14:18:17+00:00</updated>
    <author>
      <name>/u/Loud_Possibility_148</name>
      <uri>https://old.reddit.com/user/Loud_Possibility_148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"&gt; &lt;img alt="Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments." src="https://preview.redd.it/ew3ww50cm7hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4453fb44558bfa1a595869b6a3ef9f9cdc4bc1de" title="Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Possibility_148"&gt; /u/Loud_Possibility_148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew3ww50cm7hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi45h1</id>
    <title>Kitten TTS Web Demo</title>
    <updated>2025-08-05T09:04:22+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick web demo of the new &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt;. Loads the model up using transformers.js in the browser, running fully locally client-side: &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;https://clowerweb.github.io/kitten-tts-web-demo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/clowerweb/kitten-tts-web-demo"&gt;https://github.com/clowerweb/kitten-tts-web-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Only uses CPU for now, but I'm going to add WebGPU support for it later today, plus maybe a Whisper implementation also in transformers.js for a nice little local STS pipeline, if anyone is interested in something like that.&lt;/p&gt; &lt;p&gt;I also have a little open-source chat interface in progress that I might plop the STS pipeline into here: &lt;a href="https://github.com/clowerweb/Simple-AI"&gt;https://github.com/clowerweb/Simple-AI&lt;/a&gt; (built with Nuxt 3 &amp;amp; Tailwind 4) -- supports chat tabs &amp;amp; history, markdown, code highlighting, and LaTeX, and also lets you run Qwen3 4B via transformers.js or add your own custom API endpoints, with settings for temperature, top_p, top_k, etc. Only supports OpenAI-compatible endpoints currently. You can add custom API providers (including your own llama.cpp servers and whatnot), custom models with their own settings, custom system prompts, etc. If you're interested in seeing an STS pipeline added to that though with Kitten &amp;amp; Whisper, lemme know what the interest levels are for something like that. I'll probably toss this project into Electron when it's ready and make it into a desktop app for Mac, Windows, and Linux as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T09:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mibd4n</id>
    <title>Flown under the Radar: Eloisa-Qwen3-8B</title>
    <updated>2025-08-05T14:52:45+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not my finetune, but thought Eloisa-Qwen3-8B by nbeerbower was a surprisingly competent model. It's one of the only finetunes trained on Qwen3-R1-SLERP-Q3T-8B, which is a slerp merge between the original Qwen3 instruct, and the Deepseek R1 0528 Qwen3 distill, using the superior qwen3 tokenizer, getting the best benefits and advantages of both models. This finetune trains on gutenberg data which is one of the best datasets that improves writing quality, naturalness, reduces AI slop, and usually even improves most benchmark scores despite it not overfitting on any sort of benchmark related data. Historically, alot of the best finetunes were trained on gutenberg data. It's also trained on a decensoring dataset designed specifically to get rid of chinese censorship. This is a light finetune, of only 1 epoch so it should not have any strong biases or strange quirks, and was also my experience in testing. Found it to be very well rounded and is currently my favorite 8B model. I've tested quite a few of these models in this size range and I very rarely find stuff I like more than the original qwen instruct so this was a pleasant surprise. Thought I'd share since this model is a little under the radar. Would be happy to hear from others what they think of it in their own testing or benchmarks. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nbeerbower/Eloisa-Qwen3-8B"&gt;https://huggingface.co/nbeerbower/Eloisa-Qwen3-8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibd4n/flown_under_the_radar_eloisaqwen38b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibd4n/flown_under_the_radar_eloisaqwen38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mibd4n/flown_under_the_radar_eloisaqwen38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieqcb</id>
    <title>openai/gpt-oss-120b ¬∑ Hugging Face</title>
    <updated>2025-08-05T17:00:37+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt; &lt;img alt="openai/gpt-oss-120b ¬∑ Hugging Face" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="openai/gpt-oss-120b ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1midu35</id>
    <title>GPT-OSS today!</title>
    <updated>2025-08-05T16:27:22+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Keep an eye on these links! &lt;a href="https://github.com/openai/harmony"&gt;https://github.com/openai/harmony&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models"&gt;https://openai.com/open-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gpt-oss.com"&gt;https://gpt-oss.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: also this &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mibuho</id>
    <title>Kitten-TTS : Smallest ever TTS model (25MB, 15M params), runs on CPU</title>
    <updated>2025-08-05T15:11:00+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just checked out Kitten-TTS, an open-sourced TTS model 1/5th the size of Kokoro 82M, and giving out decent enough results. The model is optimized for CPU and looks great given its size. Also, the inference is quite fast and is able to generate samples within seconds on a CPU as well. &lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://youtu.be/oyu58Aei6U4"&gt;https://youtu.be/oyu58Aei6U4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T15:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1fdc</id>
    <title>DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!</title>
    <updated>2025-08-05T06:09:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt; &lt;img alt="DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!" src="https://preview.redd.it/sv779zmy65hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=935b56fc5932d42d92b2f9647262c1937a3e7446" title="DFLoat11 Quantization for Qwen-Image Drops ‚Äì Run It on 17GB VRAM with CPU Offloading!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sv779zmy65hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0luy</id>
    <title>generated using Qwen</title>
    <updated>2025-08-05T05:20:23+00:00</updated>
    <author>
      <name>/u/Vision--SuperAI</name>
      <uri>https://old.reddit.com/user/Vision--SuperAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt; &lt;img alt="generated using Qwen" src="https://b.thumbs.redditmedia.com/xCXpPKDOF28RU5xW4gyR6Ubwr1ZXuMCJDS0Yp62MVtI.jpg" title="generated using Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vision--SuperAI"&gt; /u/Vision--SuperAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mi0luy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1miec9u</id>
    <title>Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers</title>
    <updated>2025-08-05T16:46:13+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt; &lt;img alt="Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers" src="https://external-preview.redd.it/o3wB6ioZOZhpNvqAVXe5Ffp8Gi7bbUI44EsDB2_JzvY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b72a9d22224f12c017f5e0c04a52db64d195c9b" title="Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/releases/tag/v4.55.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:46:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6bkf</id>
    <title>The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!</title>
    <updated>2025-08-05T11:13:33+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt; &lt;img alt="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" src="https://preview.redd.it/h2p8ceo4p6hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ddb7dfeb9a7a66e33196efe1d60a4acb157f2a" title="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h2p8ceo4p6hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1miaugk</id>
    <title>II-Search-4B: model tuned for reasoning with search tools</title>
    <updated>2025-08-05T14:33:02+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt; &lt;img alt="II-Search-4B: model tuned for reasoning with search tools" src="https://preview.redd.it/w6vtnupyo7hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=592d2a50ce20e6ddff73d1324a8bdda6dc148b14" title="II-Search-4B: model tuned for reasoning with search tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most search models need the cloud.&lt;/p&gt; &lt;p&gt;II-Search-4B doesn‚Äôt.&lt;/p&gt; &lt;p&gt;4B model tuned for reasoning with search tools, built for local use.&lt;/p&gt; &lt;p&gt;Performance of models 10x its size.&lt;/p&gt; &lt;p&gt;Search that is small, smart, and open.&lt;/p&gt; &lt;p&gt;II-Search-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;II-Search-CIR-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://ii.inc/web/blog/post/ii-search"&gt;https://ii.inc/web/blog/post/ii-search&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w6vtnupyo7hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0co2</id>
    <title>Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!</title>
    <updated>2025-08-05T05:06:08+00:00</updated>
    <author>
      <name>/u/MrJiks</name>
      <uri>https://old.reddit.com/user/MrJiks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt; &lt;img alt="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" src="https://preview.redd.it/9z1vbpnsu4hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb282a6194808b25822f60262b2b9d1dd1570e" title="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Dario Amodei's recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/jikkujose/status/1952588432280051930"&gt;https://x.com/jikkujose/status/1952588432280051930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJiks"&gt; /u/MrJiks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z1vbpnsu4hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:06:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6brm</id>
    <title>Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices.</title>
    <updated>2025-08-05T11:13:52+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt; &lt;img alt="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." src="https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02f020e4a288dbe74ee37d8070bfab6a8b6d543" title="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.&lt;/p&gt; &lt;p&gt;Github Link: &lt;a href="https://github.com/OHF-Voice/piper1-gpl"&gt;https://github.com/OHF-Voice/piper1-gpl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4f9mf37ap6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi8lbl</id>
    <title>Qwen3 Coder vs. Kimi K2 vs. Sonnet 4 Coding Comparison (Tested on Qwen CLI)</title>
    <updated>2025-08-05T13:02:08+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba released Qwen3‚ÄëCoder (480B ‚Üí 35B active) alongside Qwen Code CLI, a complete fork of Gemini CLI for agentic coding workflows specifically adapted for Qwen3 Coder. I tested it head-to-head with Kimi K2 and Claude Sonnet 4 in practical coding tasks using the same CLI via &lt;strong&gt;OpenRouter&lt;/strong&gt; to keep things consistent for all models. The results surprised me.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;Note:&lt;/strong&gt; All test timings are based on the OpenRouter providers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've done some real-world coding tests for all three, not just regular prompts. Here are the three questions I asked all three models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLI Chat MCP Client in Python:&lt;/strong&gt; Build a CLI chat MCP client in Python. More like a chat room. Integrate Composio integration for tool calls (Gmail, Slack, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geometry Dash WebApp Simulation:&lt;/strong&gt; Build a web version of Geometry Dash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Typing Test WebApp:&lt;/strong&gt; Build a monkeytype-like typing test app with a theme switcher (Catppuccin theme) and animations (typing trail).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4 was the most reliable across all tasks, with complete, production-ready outputs. It was also the fastest, usually taking 5‚Äì7 minutes.&lt;/li&gt; &lt;li&gt;Qwen3-Coder surprised me with solid results, much faster than Kimi, though not quite on Claude‚Äôs level.&lt;/li&gt; &lt;li&gt;Kimi K2 writes good UI and follows standards well, but it is slow (20+ minutes on some tasks) and sometimes non-functional.&lt;/li&gt; &lt;li&gt;On tool-heavy prompts like MCP + Composio, Claude was the only one to get it right in one try.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Verdict&lt;/h1&gt; &lt;p&gt;Honestly, Qwen3-Coder feels like the best middle ground if you want budget-friendly coding without massive compromises. But for real coding speed, Claude still dominates all these recent models.&lt;/p&gt; &lt;p&gt;I can't see much hype around Kimi K2, to be honest. It's just painfully slow and not really as great as they say it is in coding. It's mid! (Keep in mind, timings are noted based on the OpenRouter providers.)&lt;/p&gt; &lt;p&gt;Here's a complete blog post with timings for all the tasks for each model and a nice demo here: &lt;a href="https://composio.dev/blog/qwen-3-coder-vs-kimi-k2-vs-claude-4-sonnet-coding-comparison"&gt;Qwen 3 Coder vs. Kimi K2 vs. Claude 4 Sonnet: Coding comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has benchmarked these models with real coding projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T13:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1midi67</id>
    <title>GPT-OSS today?</title>
    <updated>2025-08-05T16:14:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt; &lt;img alt="GPT-OSS today?" src="https://preview.redd.it/2br9oi8178hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20cd517e2220fa7745b9e909a9f4bfcf589d5f03" title="GPT-OSS today?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;because this is almost merged &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2br9oi8178hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7bem</id>
    <title>New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`</title>
    <updated>2025-08-05T12:04:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt; &lt;img alt="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" src="https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b" title="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15077"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mic8kf</id>
    <title>Llama.cpp: Add GPT-OSS</title>
    <updated>2025-08-05T15:25:57+00:00</updated>
    <author>
      <name>/u/atgctg</name>
      <uri>https://old.reddit.com/user/atgctg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt; &lt;img alt="Llama.cpp: Add GPT-OSS" src="https://external-preview.redd.it/SMmA2lbsQDUuflgGCV0_YBw5k-KcfZS-9iAMN58tb_s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a9e3e605eab496c9ba148173f1d7e68a1d7e9f" title="Llama.cpp: Add GPT-OSS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atgctg"&gt; /u/atgctg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T15:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyzp7</id>
    <title>Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2025-08-05T03:52:26+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt; &lt;img alt="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc51726f166f8072e9a0767410a3b105af874a6d" title="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kitten ML has released open source code and weights of their new TTS model's preview.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt; &lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt; &lt;li&gt;Can literally run anywhere lol : Forget ‚ÄúNo gpu required.‚Äù - this thing can even run on raspberry pi‚Äôs and phones. Great news for gpu-poor folks like me.&lt;/li&gt; &lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdfv5uihi4hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T03:52:26+00:00</published>
  </entry>
</feed>
