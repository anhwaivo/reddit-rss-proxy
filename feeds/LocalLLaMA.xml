<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-02T07:48:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ifjtln</id>
    <title>Using o3 to build a open source researching system that can reason what it knows &amp; it does not know</title>
    <updated>2025-02-01T23:44:04+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjtln/using_o3_to_build_a_open_source_researching/"&gt; &lt;img alt="Using o3 to build a open source researching system that can reason what it knows &amp;amp; it does not know" src="https://preview.redd.it/lkfl0n1d6mge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26f70b040757a29f5f0b9f681ccdc87c58c33775" title="Using o3 to build a open source researching system that can reason what it knows &amp;amp; it does not know" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lkfl0n1d6mge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjtln/using_o3_to_build_a_open_source_researching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjtln/using_o3_to_build_a_open_source_researching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T23:44:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifpylh</id>
    <title>When/how should you rephrase the last user message to improve retrieval accuracy in RAG? It so happens you don’t need to hit that wall every time…</title>
    <updated>2025-02-02T05:13:40+00:00</updated>
    <author>
      <name>/u/Terrible_Attention83</name>
      <uri>https://old.reddit.com/user/Terrible_Attention83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpylh/whenhow_should_you_rephrase_the_last_user_message/"&gt; &lt;img alt="When/how should you rephrase the last user message to improve retrieval accuracy in RAG? It so happens you don’t need to hit that wall every time…" src="https://preview.redd.it/2xsbu80gtnge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a1a4e9ebabf5de210c2e366cc66ca6e15e22993" title="When/how should you rephrase the last user message to improve retrieval accuracy in RAG? It so happens you don’t need to hit that wall every time…" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Long story short, when you work on a chatbot that uses rag, the user question is sent to the rag instead of being directly fed to the LLM.&lt;/p&gt; &lt;p&gt;You use this question to match data in a vector database, embeddings, reranker, whatever you want.&lt;/p&gt; &lt;p&gt;Issue is that for example :&lt;/p&gt; &lt;p&gt;Q : What is Sony ? A : It's a company working in tech. Q : How much money did they make last year ?&lt;/p&gt; &lt;p&gt;Here for your embeddings model, How much money did they make last year ? it's missing Sony all we got is they.&lt;/p&gt; &lt;p&gt;The common approach is to try to feed the conversation history to the LLM and ask it to rephrase the last prompt by adding more context. Because you don’t know if the last user message was a related question you must rephrase every message. That’s excessive, slow and error prone &lt;/p&gt; &lt;p&gt;Now, all you need to do is write a simple intent-based handler and the gateway routes prompts to that handler with structured parameters across a multi-turn scenario. Guide: &lt;a href="https://docs.archgw.com/build_with_arch/multi_turn.html"&gt;https://docs.archgw.com/build_with_arch/multi_turn.html&lt;/a&gt; - &lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Attention83"&gt; /u/Terrible_Attention83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2xsbu80gtnge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpylh/whenhow_should_you_rephrase_the_last_user_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpylh/whenhow_should_you_rephrase_the_last_user_message/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T05:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifgy1a</id>
    <title>What happened to Differential Transformer ?</title>
    <updated>2025-02-01T21:31:16+00:00</updated>
    <author>
      <name>/u/LelouchZer12</name>
      <uri>https://old.reddit.com/user/LelouchZer12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt; &lt;img alt="What happened to Differential Transformer ?" src="https://b.thumbs.redditmedia.com/GitzFNEYZ_6WcAIz0fTMaK7vF4Cm93GvyPMzEMSp2cs.jpg" title="What happened to Differential Transformer ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nxrbdaktilge1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee964c9c99fc8cdbb282f76d8f08ad9e9dec24b7"&gt;https://preview.redd.it/nxrbdaktilge1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee964c9c99fc8cdbb282f76d8f08ad9e9dec24b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2410.05258"&gt;https://arxiv.org/pdf/2410.05258&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What happened to this idea ? It seemed to increase performance with almost no drawbacks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LelouchZer12"&gt; /u/LelouchZer12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifgy1a/what_happened_to_differential_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T21:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifpaje</id>
    <title>Are there any uncensored versions of 4o, o1 or DeepSeek?</title>
    <updated>2025-02-02T04:33:40+00:00</updated>
    <author>
      <name>/u/PangurBanTheCat</name>
      <uri>https://old.reddit.com/user/PangurBanTheCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Listen, I'm gonna be honest with you, I just want it's help making NSFW chatbots and I'm tired of trying to convince AI that it is in fact not aiding me in that quest. &lt;/p&gt; &lt;p&gt;lol. ¯\_(ツ)_/¯&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangurBanTheCat"&gt; /u/PangurBanTheCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpaje/are_there_any_uncensored_versions_of_4o_o1_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpaje/are_there_any_uncensored_versions_of_4o_o1_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifpaje/are_there_any_uncensored_versions_of_4o_o1_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T04:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieurv8</id>
    <title>My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:</title>
    <updated>2025-02-01T01:11:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt; &lt;img alt="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" src="https://preview.redd.it/jixqkaabhfge1.gif?width=216&amp;amp;crop=smart&amp;amp;s=c67a878b6f732544b4693cf47d6dc14a8220e551" title="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jixqkaabhfge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T01:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifomzq</id>
    <title>If you have a MacBook with only 8G and you want to run Deepseek R1 Distill - use the latest GPT4ALL.</title>
    <updated>2025-02-02T03:57:01+00:00</updated>
    <author>
      <name>/u/Internet--Traveller</name>
      <uri>https://old.reddit.com/user/Internet--Traveller</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifomzq/if_you_have_a_macbook_with_only_8g_and_you_want/"&gt; &lt;img alt="If you have a MacBook with only 8G and you want to run Deepseek R1 Distill - use the latest GPT4ALL." src="https://b.thumbs.redditmedia.com/u6M7BOE3WHtRiP6uUV3W0i71NISy7-Hnk3zi0jZvuYY.jpg" title="If you have a MacBook with only 8G and you want to run Deepseek R1 Distill - use the latest GPT4ALL." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest GPT4ALL 3.8 can run Deepseek R1 Distill 7B smoothly. My MacBook Pro M1 8G is running it at over 10 tokens/sec.&lt;/p&gt; &lt;p&gt;Use this model: &lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf"&gt;https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DO NOT use the llama-distill version, currently it doesn't work - it loads but outputs gibberish.&lt;/p&gt; &lt;p&gt;RAG works great with this LLM, here's a test example (using an obscure book to make sure it haven't been trained on the content):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c9waiwe8fnge1.jpg?width=2304&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=26134091d704fabd0e51a81225d61d9a8555bb80"&gt;https://preview.redd.it/c9waiwe8fnge1.jpg?width=2304&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=26134091d704fabd0e51a81225d61d9a8555bb80&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internet--Traveller"&gt; /u/Internet--Traveller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifomzq/if_you_have_a_macbook_with_only_8g_and_you_want/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifomzq/if_you_have_a_macbook_with_only_8g_and_you_want/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifomzq/if_you_have_a_macbook_with_only_8g_and_you_want/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T03:57:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1if7hm3</id>
    <title>How To Run Deepseek R1 671b Fully Locally On a $2000 EPYC Server</title>
    <updated>2025-02-01T14:30:39+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://digitalspaceport.com/how-to-run-deepseek-r1-671b-fully-locally-on-2000-epyc-rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if7hm3/how_to_run_deepseek_r1_671b_fully_locally_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1rls</id>
    <title>We've been incredibly fortunate with how things have developed over the past year</title>
    <updated>2025-02-01T08:11:39+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I still remember how in late 2023, people were speculating that Mixtral-8x7b was the best open-weights model that the community would get &amp;quot;for a long time&amp;quot;, and possibly ever. Shortly afterwards, Mistral published a controversial blog post that appeared to indicate that they were moving away from open weights – an ominous sign at a time when there were very few open-weights models available, and Anthropic and OpenAI seemed as far out of reach as the stars.&lt;/p&gt; &lt;p&gt;But since then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta released the excellent Llama 3 series as open weights (though not entirely free software).&lt;/li&gt; &lt;li&gt;Contrary to what many had feared, Mistral continued to publish open-weights models, even releasing the weights for Mistral Large, which was previously API-only, and now publishing their latest Mistral Small under the Apache License, when the previous version was still under their proprietary MRL.&lt;/li&gt; &lt;li&gt;Yi-34b transitioned from a proprietary license to Apache.&lt;/li&gt; &lt;li&gt;Microsoft has been publishing a number of excellent small models under permissive licenses.&lt;/li&gt; &lt;li&gt;Qwen came out of nowhere, and released the best models that can be run on consumer hardware, almost all of them under permissive licenses.&lt;/li&gt; &lt;li&gt;DeepSeek upended the entire industry, and &lt;strong&gt;an MIT-licensed model is now ranked joint #1 on style-controlled LMSYS,&lt;/strong&gt; on par with cutting-edge, proprietary, API-only models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was completely unforeseeable a year ago. Reality has outpaced the wildest dreams of the most naive optimists. Some doomsayers even predicted that open-weights models would soon be outlawed. The exact opposite has happened, and continues to happen.&lt;/p&gt; &lt;p&gt;To get an idea for what could easily have been, just look at the world of image generation models. In 15 months, there have only been two significant open-weights releases: SD3, and Flux.1D. SD3 was mired in controversy due to Stability's behavior and has been all but ignored by the community, and Flux is crippled by distillation. Both models are censored to a degree that has become the stuff of memes, and their licenses essentially make them unusable for anything except horsing around.&lt;/p&gt; &lt;p&gt;That is how the LLM world could have turned out. Instead, we have a world where I don't even download every new model anymore, because there are multiple exciting releases every week and I simply lack the time to take all of them for a spin. I now regularly delete models from my hard drive that I would have given my right hand for not too long ago. It's just incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T08:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifeu07</id>
    <title>New benchmark about multi-turn conversation that challenge frontier LLMs and capture Sonet 3.5 advantage: all LLMs perform below 50% accuracy</title>
    <updated>2025-02-01T19:57:31+00:00</updated>
    <author>
      <name>/u/TheIdealHominidae</name>
      <uri>https://old.reddit.com/user/TheIdealHominidae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn"&gt;https://paperswithcode.com/paper/multichallenge-a-realistic-multi-turn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIdealHominidae"&gt; /u/TheIdealHominidae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifeu07/new_benchmark_about_multiturn_conversation_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T19:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifrf9t</id>
    <title>Deepseek R1 just told me to fist a frozen orange, for real. (This model is AMAZING)</title>
    <updated>2025-02-02T06:45:50+00:00</updated>
    <author>
      <name>/u/IversusAI</name>
      <uri>https://old.reddit.com/user/IversusAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using Open WebUI and Deepseek R1 through Open Router to build my own healbot to help heal from sugar and wheat addiction. I was talking to the model, &lt;strong&gt;which is AMAZING&lt;/strong&gt; no joke and I was trying to make it to 10:00pm (when the store closes) and it was giving me help and suggestions to get through.&lt;/p&gt; &lt;p&gt;Note: My system prompt does NOT have anything in it about being explicit. It just asks the model to help me recover and how I want it to act (kind, supportive, etc).&lt;/p&gt; &lt;h3&gt;I had just asked it to help me get to 10:00pm:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/5Y97e8x.jpeg"&gt;https://i.imgur.com/5Y97e8x.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Yeah, there will be no frozen orange fisting, mkay?&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/LAVYIPM.jpeg"&gt;https://i.imgur.com/LAVYIPM.jpeg&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;LOLOLOL:&lt;/h3&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/c8ss1p4.jpeg"&gt;https://i.imgur.com/c8ss1p4.jpeg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S.: I did make it to 10pm and the cravings eased. :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IversusAI"&gt; /u/IversusAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifrf9t/deepseek_r1_just_told_me_to_fist_a_frozen_orange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T06:45:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifh4ec</id>
    <title>Sam Altman says OpenAI will embrace two new AI approaches, one from DeepSeek and another from Meta https://www.businessinsider.com/sam-altman-openai-ai-approaches-deepseek-meta-open-source-2025-1</title>
    <updated>2025-02-01T21:39:08+00:00</updated>
    <author>
      <name>/u/Then_Knowledge_719</name>
      <uri>https://old.reddit.com/user/Then_Knowledge_719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Shouldn't he just open source that thing? 🙄🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Then_Knowledge_719"&gt; /u/Then_Knowledge_719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifh4ec/sam_altman_says_openai_will_embrace_two_new_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifh4ec/sam_altman_says_openai_will_embrace_two_new_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifh4ec/sam_altman_says_openai_will_embrace_two_new_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T21:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iflhsg</id>
    <title>DeepSeek-R1 on Quad P40 + Dual Xeon E5-2699v5</title>
    <updated>2025-02-02T01:06:21+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iflhsg/deepseekr1_on_quad_p40_dual_xeon_e52699v5/"&gt; &lt;img alt="DeepSeek-R1 on Quad P40 + Dual Xeon E5-2699v5" src="https://external-preview.redd.it/N2sxNDh4ZTNsbWdlMZySiIMoI84ShDgPN3YZg7NYJsbyWo4KcrMX5n7IC3K2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36c1a0291eec8a69fd27b49e1433c47162a5aefc" title="DeepSeek-R1 on Quad P40 + Dual Xeon E5-2699v5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g186w0f3lmge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iflhsg/deepseekr1_on_quad_p40_dual_xeon_e52699v5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iflhsg/deepseekr1_on_quad_p40_dual_xeon_e52699v5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1if8x64</id>
    <title>llama.cpp now supports tool calling (OpenAI-compatible)</title>
    <updated>2025-02-01T15:39:17+00:00</updated>
    <author>
      <name>/u/Federal_Discipline_4</name>
      <uri>https://old.reddit.com/user/Federal_Discipline_4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/9639"&gt;https://github.com/ggerganov/llama.cpp/pull/9639&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On top of generic support for &lt;em&gt;all&lt;/em&gt; models, it supports 8+ models’ native formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.x&lt;/li&gt; &lt;li&gt;Functionary 3&lt;/li&gt; &lt;li&gt;Hermes 2/3&lt;/li&gt; &lt;li&gt;Qwen 2.5&lt;/li&gt; &lt;li&gt;Mistral Nemo&lt;/li&gt; &lt;li&gt;Firefunction 2&lt;/li&gt; &lt;li&gt;DeepSeek R1 (WIP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Runs locally anywhere (incl. Raspberry Pi 5), e.g. on a Mac:&lt;/p&gt; &lt;p&gt;&lt;code&gt; brew install llama.cpp llama-server --jinja -fa -hf bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Still fresh / lots of bugs to discover: feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Discipline_4"&gt; /u/Federal_Discipline_4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if8x64/llamacpp_now_supports_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T15:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifltll</id>
    <title>I tested 11 popular local LLM's against my instruction-heavy game/application</title>
    <updated>2025-02-02T01:23:20+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;I have a few applications with some relatively large system prompts for how to handle requests. A lot of them use very strict JSON-formatting. I've scripted benchmarks for them going through a series of real use-case inputs and outputs and here's what I found&lt;/p&gt; &lt;h1&gt;The Test&lt;/h1&gt; &lt;p&gt;A dungeon-master scenario. The LLM first plays the role of the dungeon master, being fed state and inventory and then needing to take a user action/decision - reporting the output. The LLM is then responsible for reading over its own response and updating state and inventory JSON, quantity, locations, notes, descriptions, etc based on the content of the story. There are A LOT of rules involved, including of course actually successfully interacting with structured data. Successful models will both be able to advance the story in a very sane way given the long script of inputs/responses (I review afterwards) and track both state and inventory in the desired format.&lt;/p&gt; &lt;h1&gt;Rules&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;32b or less. Llama 3.3 70b performs this task superbly, but i want something that will feasibly run well on GPUs a regular consumer owns. I'm considering that 32gb of high bandwidth memory or VRAM or less.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;no API-only models&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;all quants are Q6. I tested Q8's but results were identical &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;context window of tests accommodates smaller models in that any test that goes over is thrown out&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;temperature is within the model author's recommended range, leaning slightly towards less-creative outputs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;instruct versions unless otherwise specified&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results (best to worst)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Phi4 14b&lt;/strong&gt; - Best by far. Not as smart as some of the others on this list, but it nails the response format instructions and rules 100% of the time. Being 14b its naturally very fast.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 2 22b&lt;/strong&gt; - Best balance. Extremely smart and superb at the interpretation and problem solving portion of the task. Will occasionally fail on JSON output but rarely&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen 32b Instruct&lt;/strong&gt; - this model was probably the smartest of them all. If handed a complex scenario, it would come up with what I considered the best logical solution, however it was pretty poor at JSON and rule-following&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral Small 3 24b&lt;/strong&gt; - this one disappointed me. It's very clever and smart, but compared to the older Mistral Small 2, it's much weaker at instructon following. It could only track state for a short time before it would start deleting or forgetting items and events. Good at JSON format though.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-R1-Distill 32b&lt;/strong&gt; - smart(er) than Qwen 32b instruct but would completely flop on instruction following every 2-3 sequences. Amazing at interpreting state and story, but fell flat on its face with instructions and JSON.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Mistral-Nemo 12b&lt;/strong&gt; - I like this model a lot. It punches higher than its benchmarks consistently and it will get through a number of sequences just fine, but it eventually hallucinates and returns either nonsense JSON, breaks rules, or loses track of state.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Falcon 3 10b&lt;/strong&gt; - Extremelt fast, shockingly smart, but would reliably produce a totally hallucinated output and content every few sequences&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Llama 3.1 8b&lt;/strong&gt; - follows instructions well, but hallucinated JSON formatting and contents far too often to be usable &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Codestral 22b&lt;/strong&gt; - a coding model!? for this? Well yeah - it actually nails the JSON 100% of the time, - but the story/content generation and understanding of actions and their impact on state were terrible. It also would inevitably enter a loop of nonsense output&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Qwen-Coder 32b&lt;/strong&gt; - exactly the same as Codestral, just with even worse writing. I love this model &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Nous-Hermes 3 8b&lt;/strong&gt; - slightly worse than regular Llama3.1 8b. Generated far more interesting (better written?) text in sections that allowed it though. This model to me is always &amp;quot;Llama 3.1 that went to art school instead of STEM&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(bonus) &lt;strong&gt;Llama 3.2 3b&lt;/strong&gt; - runs at lightspeed, I want this to be the future of local LLMs - but it's not a fair fight for the little guy. It goes off the rails or fails to follow instructions&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Phi4 14b is the best &lt;em&gt;so far&lt;/em&gt;. It just follows instructions well. But it's not as creative or natural in writing as Llama-based models, nor is it as intelligent or clever as Qwen or Mistral. It's the best at this test, there is no denying it, but i don't particularly enjoy its content compared to the flavor and intelligence of the other models tested. Mistral-Nemo 12b getting close to following instructions and struggling sug&lt;/p&gt; &lt;p&gt;&lt;strong&gt;if you have any other models you'd like to test this against, please mention them!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifltll/i_tested_11_popular_local_llms_against_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:23:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffgj4</id>
    <title>DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)</title>
    <updated>2025-02-01T20:24:46+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt; &lt;img alt="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" src="https://external-preview.redd.it/gMJZu1czNWIsX2vol0q37qYGLTI_zKgwHfEyO-m9Uqw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48df94f11b5f4243cdde43be4517d1e3d09e3712" title="DeepSeek R1 671B MoE LLM running on Epyc 9374F and 384GB of RAM (llama.cpp + PR #11446, Q4_K_S, real time)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wKZHoGlllu4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffgj4/deepseek_r1_671b_moe_llm_running_on_epyc_9374f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:24:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1if71w7</id>
    <title>o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot.</title>
    <updated>2025-02-01T14:08:08+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt; &lt;img alt="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." src="https://external-preview.redd.it/aTBxM2VyeG5iamdlMUWHkN0UG3UwPNFPGIT0TYE7p36ybavsfv5qTlMpE8Gi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c32bf16da98a479fb59fe074cec8f9ff9f2587" title="o3-mini is now the SOTA coding model. It is truly something to behold. Procedural clouds in one-shot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x607arxnbjge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if71w7/o3mini_is_now_the_sota_coding_model_it_is_truly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T14:08:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5q97</id>
    <title>Just canceled my ChatGPT Plus subscription</title>
    <updated>2025-02-01T12:56:16+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I initially subscribed when they introduced uploading documents when it was limited to the plus plan. I kept holding onto it for o1 since it really was a game changer for me. But since R1 is free right now (when it’s available at least lol) and the quantized distilled models finally fit onto a GPU I can afford, I cancelled my plan and am going to get a GPU with more VRAM instead. I love the direction that open source machine learning is taking right now. It’s crazy to me that distillation of a reasoning model to something like Llama 8B can boost the performance by this much. I hope we soon will get more advancements in more efficient large context windows and projects like Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1if43uf</id>
    <title>Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source</title>
    <updated>2025-02-01T11:08:19+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt; &lt;img alt="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" src="https://preview.redd.it/iewy2sxsfige1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad808e1a916c32f2181d2dc562c2065ba8cb4c99" title="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iewy2sxsfige1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:08:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifmiuu</id>
    <title>Open WebUI Coder Overhaul is now live on GitHub for testing!</title>
    <updated>2025-02-02T02:00:14+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt; &lt;img alt="Open WebUI Coder Overhaul is now live on GitHub for testing!" src="https://external-preview.redd.it/pnWZlhrdj8zYupK7N1vmH0H-SivyUx9-OTH-kjV-R7g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf5d066eb9e2c66c37f101ed9464a98b1e14a000" title="Open WebUI Coder Overhaul is now live on GitHub for testing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Some of you may be familiar with the project I've been working on for the past couple of weeks here that essentially overhauls the OpenWebUI artifacts system and makes it closer to ChatGPT's Canvas or Claude Artifacts. Well, I just published the code and it's available for testing! I really would love some help from people who have real world use cases for this and have them submit issues, pull requests, or feature requests on GitHub!&lt;/p&gt; &lt;p&gt;Here is a brief breakdown on the features:&lt;/p&gt; &lt;p&gt;A side code editor similar to ChatGPT and Claude, supporting a LOT of coding languages. You can cycle through all code blocks in a chat.&lt;/p&gt; &lt;p&gt;A design view mode that lets you see HTML (now with typescript styles included by default) and also React components&lt;/p&gt; &lt;p&gt;A difference viewer that shows you what changed in a code block if an LLM made changes&lt;/p&gt; &lt;p&gt;Code blocks will be shown as attachments in the regular chat while the editor is opened, like Claude.&lt;/p&gt; &lt;p&gt;I hope you all enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifmiuu/open_webui_coder_overhaul_is_now_live_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T02:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifqagd</id>
    <title>R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible.</title>
    <updated>2025-02-02T05:33:40+00:00</updated>
    <author>
      <name>/u/redditisunproductive</name>
      <uri>https://old.reddit.com/user/redditisunproductive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt; &lt;img alt="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." src="https://external-preview.redd.it/sqLJ5r2pSW7H_l-8ii2E6-qgsVr8VlF7vTFjMAJ9Xb0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1afa3c2905a1dc6f720fe43b945c8b51870613b7" title="R1 has a 14% (!) hallucination rate in this evaluation. R1 is too loose and untamed in my experience, with poor instruction following to boot. Hopefully someone tunes it without sacrificing its raw brilliance, if that's possible." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditisunproductive"&gt; /u/redditisunproductive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vectara/hallucination-leaderboard"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifqagd/r1_has_a_14_hallucination_rate_in_this_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T05:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3lq1</id>
    <title>Sam Altman acknowledges R1</title>
    <updated>2025-02-01T10:31:35+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt; &lt;img alt="Sam Altman acknowledges R1" src="https://preview.redd.it/ot5nsk399ige1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca17a8d86fa20881ff4876577c465ae2c733d9" title="Sam Altman acknowledges R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Straight from the horses mouth. Without R1, or bigger picture open source competitive models, we wouldn’t be seeing this level of acknowledgement from OpenAI. &lt;/p&gt; &lt;p&gt;This highlights the importance of having open models, not only that, but open models that actively compete and put pressure on closed models. &lt;/p&gt; &lt;p&gt;R1 for me feels like a real &lt;em&gt;hard takeoff&lt;/em&gt; moment. &lt;/p&gt; &lt;p&gt;No longer can OpenAI or other closed companies dictate the rate of release. &lt;/p&gt; &lt;p&gt;No longer do we have to get the scraps of what they decide to give us. &lt;/p&gt; &lt;p&gt;Now they have to actively compete in an open market.&lt;/p&gt; &lt;p&gt;No moat. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Source: &lt;a href="https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC"&gt;https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ot5nsk399ige1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iffboy</id>
    <title>SmolVLM fully open source</title>
    <updated>2025-02-01T20:19:01+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt; &lt;img alt="SmolVLM fully open source" src="https://external-preview.redd.it/RpBd16Y386MrSYjhSF5aL1O5cjq2V0xWVKGs2JQsIl0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9476c8b4dd1bf85443ac42ac9be87b98d3ff2e1e" title="SmolVLM fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/andimarafioti/status/1885341684134978035"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iffboy/smolvlm_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T20:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifjuh8</id>
    <title>Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap.</title>
    <updated>2025-02-01T23:45:19+00:00</updated>
    <author>
      <name>/u/convalytics</name>
      <uri>https://old.reddit.com/user/convalytics</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt; &lt;img alt="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." src="https://preview.redd.it/sb0m382v6mge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d527da27d452ede8dca689a709e00f143244cadb" title="Got my 3090 and 3060 working on a fresh Ubuntu installation. Please clap." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After many reboots and fiddling with blacklisting noveau/nouveau, it's finally working! &lt;/p&gt; &lt;p&gt;36GB of vram goodness and 64GB of system ram. &lt;/p&gt; &lt;p&gt;Planning to install ollama, open-webui and n8n. Any more recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/convalytics"&gt; /u/convalytics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb0m382v6mge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifjuh8/got_my_3090_and_3060_working_on_a_fresh_ubuntu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T23:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifi0qu</id>
    <title>Missouri Senator Josh Hawley proposes a ban on Chinese AI models</title>
    <updated>2025-02-01T22:19:39+00:00</updated>
    <author>
      <name>/u/InquisitiveInque</name>
      <uri>https://old.reddit.com/user/InquisitiveInque</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InquisitiveInque"&gt; /u/InquisitiveInque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifi0qu/missouri_senator_josh_hawley_proposes_a_ban_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T22:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ifm2df</id>
    <title>DeepSeek R1 misinformation is getting out of hand</title>
    <updated>2025-02-02T01:36:11+00:00</updated>
    <author>
      <name>/u/serialx_net</name>
      <uri>https://old.reddit.com/user/serialx_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9"&gt;https://medium.com/google-cloud/running-deepseek-from-open-source-model-to-production-ready-api-on-google-cloud-vertexai-8d3f57e488b9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-R1 is a &lt;strong&gt;7B parameter language model&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In the official Google Cloud blog post? WTF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serialx_net"&gt; /u/serialx_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ifm2df/deepseek_r1_misinformation_is_getting_out_of_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-02T01:36:11+00:00</published>
  </entry>
</feed>
