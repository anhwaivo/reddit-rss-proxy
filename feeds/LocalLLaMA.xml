<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-14T12:52:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lyvsqv</id>
    <title>Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)</title>
    <updated>2025-07-13T15:37:33+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm releasing a v1.0 of my &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt; &lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how it works and also checkout my &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzjsu3</id>
    <title>Are there any local LLMS that support Browser use MCP?</title>
    <updated>2025-07-14T11:23:06+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried Cline/Roo Code/OpenHands Used Devstral, GLM4 32b, codellama etc When trying to navigate website using MCP server, the LLM gets stuck and cannot press on actual buttons and escape the captcha page / allow cookies pop up.&lt;/p&gt; &lt;p&gt;Is there a better model to try? Or its only API claude model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T11:23:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzk041</id>
    <title>A mid range PC build for Dual GPU Local LLMs and SLMs.</title>
    <updated>2025-07-14T11:33:54+00:00</updated>
    <author>
      <name>/u/iammhk</name>
      <uri>https://old.reddit.com/user/iammhk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a mid range Desktop to fine-tuning and host Small Language Models and LLMs. I am thinking about using 2 AMD Radeon 9060XT 16GB to reach 32 GB VRAM on budget. Will it help? Since 32GB Cards like Nvidia RTX5090 are absurdly expensive. What are your suggestions about the Motherboard and CPU for my build? Should I go for a Mac Mini M4 cluster, or other Single Board Chip cluster to achieve high VRAM? I am in India, btw.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iammhk"&gt; /u/iammhk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T11:33:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzkrwg</id>
    <title>Suggestions/Alternatives for Image captions with efficient system requirements</title>
    <updated>2025-07-14T12:13:52+00:00</updated>
    <author>
      <name>/u/palaniappan_05</name>
      <uri>https://old.reddit.com/user/palaniappan_05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL. &lt;/p&gt; &lt;p&gt;I was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. &lt;/p&gt; &lt;p&gt;Qwen 2.5 VL 3B&lt;br /&gt; Caption generation - average time taken for max pixel 768*768 - 1.62s&lt;br /&gt; Caption generation - average time taken for max pixel 1024*1024 - 2.02s&lt;br /&gt; Caption generation - average time taken for max pixel 1280*1280 - 2.79s&lt;/p&gt; &lt;p&gt;Qwen 2.5 VL 7B&lt;br /&gt; Caption generation - average time taken for max pixel 768*768 - 2.21s&lt;br /&gt; Caption generation - average time taken for max pixel 1024*1024 - 2.73s&lt;br /&gt; Caption generation - average time taken for max pixel 1280*1280 - 3.64s &lt;/p&gt; &lt;p&gt;Qwen 2.5 VL 7B AWQ&lt;br /&gt; Caption generation - average time taken for max pixel 768*768 - 2.84s&lt;br /&gt; Caption generation - average time taken for max pixel 1024*1024 - 2.94s&lt;br /&gt; Caption generation - average time taken for max pixel 1280*1280 - 3.85s &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why 7B AWQ is slower than 7B?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/palaniappan_05"&gt; /u/palaniappan_05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T12:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvah4</id>
    <title>Tried Kimi K2 for writing and reasoning, and was not impressed.</title>
    <updated>2025-07-13T15:16:28+00:00</updated>
    <author>
      <name>/u/GlompSpark</name>
      <uri>https://old.reddit.com/user/GlompSpark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here's a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character's thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It's very prudish.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit : Im using Kimi k2 on &lt;a href="http://www.kimi.com"&gt;www.kimi.com&lt;/a&gt; btw.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlompSpark"&gt; /u/GlompSpark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv750</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:12:40+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae684eb96341704e9ce19cc7a34eb9eaea57f63" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.&lt;/p&gt; &lt;p&gt;The app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt; &lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt; &lt;p&gt;Here's a brief guide to make this work with ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download the app from GitHub: &lt;a href="https://github.com/coreply/coreply"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt; &lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt; &lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href="http://192.168.xxx.xxx"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt; &lt;p&gt;Let me know how's your experience with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyxf1f</id>
    <title>Benchmarking Qwen3 30B and 235B on dual RTX PRO 6000 Blackwell Workstation Edition</title>
    <updated>2025-07-13T16:44:00+00:00</updated>
    <author>
      <name>/u/blackwell_tart</name>
      <uri>https://old.reddit.com/user/blackwell_tart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised in the banana thread. OP delivers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following benchmarks were taken using official Qwen3 models from Huggingface's Qwen repo for consistency:&lt;/p&gt; &lt;p&gt;MoE:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B BF16 in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B BF16 on a single GPU&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant on a single GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dense:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3 32B BF16 on a single GPU&lt;/li&gt; &lt;li&gt;Qwen3 32B BF16 in Tensor Parallel&lt;/li&gt; &lt;li&gt;Qwen3 14B BF16 on a single GPU&lt;/li&gt; &lt;li&gt;Qwen3 14B BF16 in Tensor Parallel&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All benchmarking was done with &lt;code&gt;vllm bench throughput ...&lt;/code&gt; using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, &lt;del&gt;512GB&lt;/del&gt; 768GB DDR5 5200 MT/s, PCIe 5.0 x16.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 24.04.2&lt;/li&gt; &lt;li&gt;NVidia drivers 575.57.08&lt;/li&gt; &lt;li&gt;CUDA 12.9&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was the magic Torch incantation that got everything working: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Otherwise these instructions worked well despite being for WSL: &lt;a href="https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3"&gt;https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MoE Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192 Throughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384 Throughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s Total num prompt tokens: 16383966 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024 Throughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096 Throughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024 Throughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096 Throughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024 Throughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096 Throughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Dense Model Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 Throughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 Throughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 Throughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 Throughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 Throughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 Throughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 Throughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 Throughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 Throughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 Throughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s Total num prompt tokens: 1021646 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 Throughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s Total num prompt tokens: 4091212 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 Throughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s Total num prompt tokens: 8189599 Total num output tokens: 128000 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackwell_tart"&gt; /u/blackwell_tart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T16:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyyhwz</id>
    <title>Never seen fastllm mentioned here, anyone using it? (kimi k2 local)</title>
    <updated>2025-07-13T17:27:42+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of waiting for k2 ggufs and found this guy:&lt;br /&gt; &lt;a href="https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main"&gt;https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a typo in the commands but it seems to work great, and really easy to get going:&lt;br /&gt; pip install ftllm&lt;br /&gt; ftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40&lt;/p&gt; &lt;p&gt;and just like that I'm getting 7-10T/s on my 5090 + DDR5 Xeon machine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzkcg3</id>
    <title>Multiple 5060 Ti's</title>
    <updated>2025-07-14T11:52:05+00:00</updated>
    <author>
      <name>/u/snorixx</name>
      <uri>https://old.reddit.com/user/snorixx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:&lt;/p&gt; &lt;p&gt;Would it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.&lt;/p&gt; &lt;p&gt;The card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?&lt;/p&gt; &lt;p&gt;Because utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don't know how much that matters...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snorixx"&gt; /u/snorixx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T11:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lze20x</id>
    <title>Can VRAM be combined of 2 brands</title>
    <updated>2025-07-14T05:20:33+00:00</updated>
    <author>
      <name>/u/tonyleungnl</name>
      <uri>https://old.reddit.com/user/tonyleungnl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.&lt;/p&gt; &lt;p&gt;Q: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyleungnl"&gt; /u/tonyleungnl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T05:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz81ea</id>
    <title>Which LLM should I use to generate high quality Q&amp;A from physics textbook chapters?</title>
    <updated>2025-07-14T00:10:40+00:00</updated>
    <author>
      <name>/u/WhiteTentacle</name>
      <uri>https://old.reddit.com/user/WhiteTentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for LLMs to generate questions and answers from physics textbook chapters. The chapters I’ll provide can be up to 10 pages long and may include images. I’ve tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didn’t work either as it rejects the input file, saying it’s too large. Which LLM model would you recommend me to try next? It doesn’t have to be free. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhiteTentacle"&gt; /u/WhiteTentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T00:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzdu0l</id>
    <title>Practice Pytorch like Leetcode? (Also with cool LLM questions)</title>
    <updated>2025-07-14T05:07:20+00:00</updated>
    <author>
      <name>/u/exorust_fire</name>
      <uri>https://old.reddit.com/user/exorust_fire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created &lt;a href="https://github.com/Exorust/TorchLeet"&gt;&lt;strong&gt;TorchLeet&lt;/strong&gt;&lt;/a&gt;! It's a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.&lt;/p&gt; &lt;p&gt;It’s split into:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PyTorch Problems&lt;/strong&gt; (Basic → Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Problems&lt;/strong&gt;: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love feedback from the community and help taking this forward!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exorust_fire"&gt; /u/exorust_fire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T05:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyw5u2</id>
    <title>Audiobook Creator - v1.4 - Added support for Orpheus along with Kokoro</title>
    <updated>2025-07-13T15:52:39+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm releasing a new version of my &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;audiobook creator app&lt;/a&gt; which now supports Kokoro and Orpheus. This release adds support for &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS&lt;/a&gt; which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS FastAPI Server&lt;/a&gt; repository.&lt;/p&gt; &lt;p&gt;Listen to a sample audiobook generated using this app: &lt;a href="https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus"&gt;https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;App Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Advanced TTS Engine Support&lt;/strong&gt;: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gradio UI App&lt;/strong&gt;: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker Support&lt;/strong&gt;: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emotion Tags Addition&lt;/strong&gt;: Emotion tags which are supported in Orpheus TTS can be added to the book's text intelligently using an LLM to enhance character voice expression.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Identification&lt;/strong&gt;: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Audiobook Narration&lt;/strong&gt;: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Includes progress bars and execution time measurements for efficient monitoring.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Licensed under GPL v3.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Checkout the Audiobook Creator Repo here: &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;https://github.com/prakharsr/audiobook-creator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how the audiobooks sound and if you like the app :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lylo75</id>
    <title>Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing</title>
    <updated>2025-07-13T06:09:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt; &lt;img alt="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" src="https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg" title="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench responses:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lylo75"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T06:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lz1s8x</id>
    <title>Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones.</title>
    <updated>2025-07-13T19:40:41+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt; &lt;img alt="Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones." src="https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34bb099dc9d99a1928035eecc7f6474b2da46ba7" title="Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI there guys, hoping you're doing fine.&lt;/p&gt; &lt;p&gt;As always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.&lt;/p&gt; &lt;p&gt;As it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.&lt;/p&gt; &lt;p&gt;Also many thanks to ubergarm (&lt;a href="/u/VoidAlchemy"&gt;u/VoidAlchemy&lt;/a&gt;) for his data on his quants and Q8_0/FP8 baseline!&lt;/p&gt; &lt;p&gt;For the quants that aren't from him, I did run them with the same command he did, with wiki.text.raw:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-perplexity -m 'model_name.gguf' \ -c 512 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -mla 3 -amb 256 -fmoe \ -f wiki.test.raw &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--------------------------&lt;/p&gt; &lt;p&gt;For baselines, we have this data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 0528 Q8: 3.2119&lt;/li&gt; &lt;li&gt;DeepSeek V3 0324 Q8 and q8_cache (important*): 3.2454&lt;/li&gt; &lt;li&gt;DeepSeek V3 0324 Q8 and F16 cache extrapolated*: 3.2443&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;*Based on &lt;a href="https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241&lt;/a&gt;, on R1 0528 at Q8_0, the difference between F16 and Q8_0 cache is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;-ctk fp16&lt;/code&gt; &lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;-ctk q8_0&lt;/code&gt; &lt;code&gt;3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So then, F16 cache is 0.03% better than Q8_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.&lt;/p&gt; &lt;p&gt;Quants tested for R1 0528:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;IQ1_S_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;UD-TQ1_0&lt;/li&gt; &lt;li&gt;IQ2_KT (ubergarm)&lt;/li&gt; &lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q2_K_XL&lt;/li&gt; &lt;li&gt;IQ3_XXS&lt;/li&gt; &lt;li&gt;IQ3_KS (ubergarm, my bad here as I named it IQ3_KT)&lt;/li&gt; &lt;li&gt;Q3_K_XL&lt;/li&gt; &lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;q4_0 (pure)&lt;/li&gt; &lt;li&gt;IQ4_KS_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quants tested for V3 0324:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q1_S_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q2_K_XL&lt;/li&gt; &lt;li&gt;IQ3_XXS&lt;/li&gt; &lt;li&gt;Q3_K_XL&lt;/li&gt; &lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;IQ3_K_R4_Pure (ubergarm)&lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;IQ4_K_R4 (ubergarm)&lt;/li&gt; &lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So here we go:&lt;/p&gt; &lt;h1&gt;DeepSeek R1 0528&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb"&gt;R1 0528 comparison (IQ3_KT is IQ3_KS, my bad)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2_K_XL, Q3_K_XL, IQ4_XS and Q8_0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542"&gt;R1 0528 Q2_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0"&gt;R1 0528 Q3_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1723d977f7c034496eb7a95bed576b6b53572542"&gt;R1 0528 IQ4_XS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a"&gt;R1 0528 Q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So with a table format, it looks like this (ordered by best to worse PPL)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size (GB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;665.3&lt;/td&gt; &lt;td align="left"&gt;8.000&lt;/td&gt; &lt;td align="left"&gt;3.2119&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_KS_R4&lt;/td&gt; &lt;td align="left"&gt;367.8&lt;/td&gt; &lt;td align="left"&gt;4.701&lt;/td&gt; &lt;td align="left"&gt;3.2286&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;333.1&lt;/td&gt; &lt;td align="left"&gt;4.260&lt;/td&gt; &lt;td align="left"&gt;3.2598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;q4_0&lt;/td&gt; &lt;td align="left"&gt;352.6&lt;/td&gt; &lt;td align="left"&gt;4.508&lt;/td&gt; &lt;td align="left"&gt;3.2895&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4&lt;/td&gt; &lt;td align="left"&gt;300.9&lt;/td&gt; &lt;td align="left"&gt;3.847&lt;/td&gt; &lt;td align="left"&gt;3.2730&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_KT&lt;/td&gt; &lt;td align="left"&gt;272.5&lt;/td&gt; &lt;td align="left"&gt;3.483&lt;/td&gt; &lt;td align="left"&gt;3.3056&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;275.6&lt;/td&gt; &lt;td align="left"&gt;3.520&lt;/td&gt; &lt;td align="left"&gt;3.3324&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;254.2&lt;/td&gt; &lt;td align="left"&gt;3.250&lt;/td&gt; &lt;td align="left"&gt;3.3805&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_K_R4&lt;/td&gt; &lt;td align="left"&gt;220.0&lt;/td&gt; &lt;td align="left"&gt;2.799&lt;/td&gt; &lt;td align="left"&gt;3.5069&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;233.9&lt;/td&gt; &lt;td align="left"&gt;2.990&lt;/td&gt; &lt;td align="left"&gt;3.6062&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_KT&lt;/td&gt; &lt;td align="left"&gt;196.7&lt;/td&gt; &lt;td align="left"&gt;2.514&lt;/td&gt; &lt;td align="left"&gt;3.6378&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-TQ1_0&lt;/td&gt; &lt;td align="left"&gt;150.8&lt;/td&gt; &lt;td align="left"&gt;1.927&lt;/td&gt; &lt;td align="left"&gt;4.7567&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S_R4&lt;/td&gt; &lt;td align="left"&gt;130.2&lt;/td&gt; &lt;td align="left"&gt;1.664&lt;/td&gt; &lt;td align="left"&gt;4.8805&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;DeepSeek V3 0324&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81"&gt;V3 0324 Comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here Q2_K_XL performs really good, even better than R1 Q2_K_XL. Reason is unkown for now. ALso, IQ3_XXS is not here as it failed the test with nan, also unkown.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357"&gt;V3 0324 Q2_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5389b135a13c86ff471d38540909a7586e2282ff"&gt;V3 0324 Q3_K_XL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9"&gt;V3 0324 IQ4_XS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea"&gt;V3 0324 Q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So with a table format, from best to lower PPL:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size (GB)&lt;/th&gt; &lt;th align="left"&gt;BPW&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;665.3&lt;/td&gt; &lt;td align="left"&gt;8.000&lt;/td&gt; &lt;td align="left"&gt;3.2454&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_K_R4&lt;/td&gt; &lt;td align="left"&gt;386.2&lt;/td&gt; &lt;td align="left"&gt;4.936&lt;/td&gt; &lt;td align="left"&gt;3.2596&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;333.1&lt;/td&gt; &lt;td align="left"&gt;4.260&lt;/td&gt; &lt;td align="left"&gt;3.2598&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4_Pure&lt;/td&gt; &lt;td align="left"&gt;352.5&lt;/td&gt; &lt;td align="left"&gt;4.505&lt;/td&gt; &lt;td align="left"&gt;3.2942&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_K_R4&lt;/td&gt; &lt;td align="left"&gt;324.0&lt;/td&gt; &lt;td align="left"&gt;4.141&lt;/td&gt; &lt;td align="left"&gt;3.3193&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;281.5&lt;/td&gt; &lt;td align="left"&gt;3.600&lt;/td&gt; &lt;td align="left"&gt;3.3690&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;233.9&lt;/td&gt; &lt;td align="left"&gt;2.990&lt;/td&gt; &lt;td align="left"&gt;3.5264&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ2_K_R4&lt;/td&gt; &lt;td align="left"&gt;226.0&lt;/td&gt; &lt;td align="left"&gt;2.889&lt;/td&gt; &lt;td align="left"&gt;3.5614&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ1_S_R4&lt;/td&gt; &lt;td align="left"&gt;130.2&lt;/td&gt; &lt;td align="left"&gt;1.664&lt;/td&gt; &lt;td align="left"&gt;5.1292&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IQ3_XXS&lt;/td&gt; &lt;td align="left"&gt;254.2&lt;/td&gt; &lt;td align="left"&gt;3.250&lt;/td&gt; &lt;td align="left"&gt;NaN (failed)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;-----------------------------------------&lt;/p&gt; &lt;p&gt;Finally, a small comparison between R1 0528 and V3 0324&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779"&gt;https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-------------------------------------&lt;/p&gt; &lt;p&gt;So that's all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T19:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzjaf5</id>
    <title>Foundations of Large Language Models (LLMs) | NLP Lab Research</title>
    <updated>2025-07-14T10:54:29+00:00</updated>
    <author>
      <name>/u/LeveredRecap</name>
      <uri>https://old.reddit.com/user/LeveredRecap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458"&gt;&lt;strong&gt;Foundations of Large Language Models (LLMs)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)&lt;/li&gt; &lt;li&gt;Original Source: &lt;a href="https://arxiv.org/abs/2501.09223"&gt;https://arxiv.org/abs/2501.09223&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: Claude 4.0 Sonnet&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: The research paper is v2, originally submitted on Jan 16, 2025 and revised on Jun 15, 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeveredRecap"&gt; /u/LeveredRecap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T10:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzikqt</id>
    <title>Annoyed with LibreChat</title>
    <updated>2025-07-14T10:11:33+00:00</updated>
    <author>
      <name>/u/Charming_Support726</name>
      <uri>https://old.reddit.com/user/Charming_Support726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let's me say ... dont know .. clumsy?&lt;/p&gt; &lt;p&gt;So I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....&lt;/p&gt; &lt;p&gt;I went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything look promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward. &lt;/p&gt; &lt;p&gt;Integration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.&lt;/p&gt; &lt;p&gt;Through. Done. Annoyed. Frustrated. Nuts. Rant over. &lt;/p&gt; &lt;p&gt;Back to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Support726"&gt; /u/Charming_Support726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T10:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzhqz8</id>
    <title>Responses keep dissolving into word salad - how to stop it?</title>
    <updated>2025-07-14T09:18:05+00:00</updated>
    <author>
      <name>/u/Gilgameshcomputing</name>
      <uri>https://old.reddit.com/user/Gilgameshcomputing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"&gt; &lt;img alt="Responses keep dissolving into word salad - how to stop it?" src="https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4" title="Responses keep dissolving into word salad - how to stop it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt; &lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt; &lt;p&gt;I've tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt; &lt;p&gt;I've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt; &lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gilgameshcomputing"&gt; /u/Gilgameshcomputing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lr7kq1452tcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T09:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyy39n</id>
    <title>IndexTTS2, the most realistic and expressive text-to-speech model so far, has leaked their demos ahead of the official launch! And... wow!</title>
    <updated>2025-07-13T17:11:10+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech&lt;/h1&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully local with open weights.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.&lt;/li&gt; &lt;li&gt;Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.&lt;/li&gt; &lt;li&gt;Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.&lt;/li&gt; &lt;li&gt;Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard &amp;quot;free length&amp;quot; mode where it automatically lets the audio become as long as necessary.&lt;/li&gt; &lt;li&gt;Supported text to speech languages that it can output: English and Chinese. Like most models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's a few real-world use cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that's the perfect length for dubbing.&lt;/li&gt; &lt;li&gt;Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.&lt;/li&gt; &lt;li&gt;Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;So how did it leak?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;They have been preparing a website at &lt;a href="https://index-tts2.github.io/"&gt;https://index-tts2.github.io/&lt;/a&gt; which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they've been preparing, along with demo files.&lt;/li&gt; &lt;li&gt;Here's an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: &lt;a href="https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4"&gt;https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The entire presentation page is here: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;To download all demos and watch the HTML presentation locally, you can also &amp;quot;git clone &lt;a href="https://github.com/index-tts/index-tts2.github.io.git"&gt;https://github.com/index-tts/index-tts2.github.io.git&lt;/a&gt;&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual &lt;em&gt;acting!&lt;/em&gt; Bravo, Bilibili, the company behind this research!&lt;/p&gt; &lt;p&gt;They are planning to release it &amp;quot;soon&amp;quot;, and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I'd say it's coming this month or the next.&lt;/p&gt; &lt;p&gt;Their previous model was Apache 2 license, both for the source code and the weights. Let's hope the next model is the same awesome license.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T17:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzl5zk</id>
    <title>UTCP: A safer, scalable tool-calling alternative to MCP</title>
    <updated>2025-07-14T12:33:01+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt; &lt;img alt="UTCP: A safer, scalable tool-calling alternative to MCP" src="https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95" title="UTCP: A safer, scalable tool-calling alternative to MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wv84vx7h3ucf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T12:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lze1r3</id>
    <title>Diffusion model support in llama.cpp.</title>
    <updated>2025-07-14T05:20:04+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"&gt; &lt;img alt="Diffusion model support in llama.cpp." src="https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c75c6786f093153f6a5dc5065d5f9e2b741b5086" title="Diffusion model support in llama.cpp." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It's very cool to watch it do it's thing. Make sure to use the --diffusion-visual flag. It's still a PR but has been approved so it should be merged soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T05:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzcuom</id>
    <title>Kimi-K2 is a DeepSeek V3 with more experts</title>
    <updated>2025-07-14T04:12:33+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;dense layer#&lt;/th&gt; &lt;th align="left"&gt;MoE layer#&lt;/th&gt; &lt;th align="left"&gt;shared&lt;/th&gt; &lt;th align="left"&gt;active/routed&lt;/th&gt; &lt;th align="left"&gt;Shared&lt;/th&gt; &lt;th align="left"&gt;Active&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Active%&lt;/th&gt; &lt;th align="left"&gt;fp16 kv@128k&lt;/th&gt; &lt;th align="left"&gt;kv%&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-MoE-16B&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;1.42B&lt;/td&gt; &lt;td align="left"&gt;2.83B&lt;/td&gt; &lt;td align="left"&gt;16.38B&lt;/td&gt; &lt;td align="left"&gt;17.28%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;85.47%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2-Lite&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/64&lt;/td&gt; &lt;td align="left"&gt;1.31B&lt;/td&gt; &lt;td align="left"&gt;2.66B&lt;/td&gt; &lt;td align="left"&gt;15.71B&lt;/td&gt; &lt;td align="left"&gt;16.93%&lt;/td&gt; &lt;td align="left"&gt;3.8GB&lt;/td&gt; &lt;td align="left"&gt;12.09%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;6/160&lt;/td&gt; &lt;td align="left"&gt;12.98B&lt;/td&gt; &lt;td align="left"&gt;21.33B&lt;/td&gt; &lt;td align="left"&gt;235.74B&lt;/td&gt; &lt;td align="left"&gt;8.41%&lt;/td&gt; &lt;td align="left"&gt;8.44GB&lt;/td&gt; &lt;td align="left"&gt;1.78%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;58&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/256&lt;/td&gt; &lt;td align="left"&gt;17.01B&lt;/td&gt; &lt;td align="left"&gt;37.45B&lt;/td&gt; &lt;td align="left"&gt;671.03B&lt;/td&gt; &lt;td align="left"&gt;5.58%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.64%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi-K2&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;8/384&lt;/td&gt; &lt;td align="left"&gt;11.56B&lt;/td&gt; &lt;td align="left"&gt;32.70B&lt;/td&gt; &lt;td align="left"&gt;1026.41B&lt;/td&gt; &lt;td align="left"&gt;3.19%&lt;/td&gt; &lt;td align="left"&gt;8.578GB&lt;/td&gt; &lt;td align="left"&gt;0.42%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;1.53B&lt;/td&gt; &lt;td align="left"&gt;3.34B&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;10.94%&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;19.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;94&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;8/128&lt;/td&gt; &lt;td align="left"&gt;7.95B&lt;/td&gt; &lt;td align="left"&gt;22.14B&lt;/td&gt; &lt;td align="left"&gt;235.09B&lt;/td&gt; &lt;td align="left"&gt;9.42%&lt;/td&gt; &lt;td align="left"&gt;23.5GB&lt;/td&gt; &lt;td align="left"&gt;4.998%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Scout-17B-16E&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;48&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/16&lt;/td&gt; &lt;td align="left"&gt;11.13B&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;107.77B&lt;/td&gt; &lt;td align="left"&gt;15.93%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;11.13%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1/128&lt;/td&gt; &lt;td align="left"&gt;14.15B&lt;/td&gt; &lt;td align="left"&gt;17.17B&lt;/td&gt; &lt;td align="left"&gt;400.71B&lt;/td&gt; &lt;td align="left"&gt;4.28%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;2.99%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x7B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;1.60B&lt;/td&gt; &lt;td align="left"&gt;12.88B&lt;/td&gt; &lt;td align="left"&gt;46.70B&lt;/td&gt; &lt;td align="left"&gt;27.58%&lt;/td&gt; &lt;td align="left"&gt;24GB&lt;/td&gt; &lt;td align="left"&gt;25.696%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral-8x22B&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;56&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;2/8&lt;/td&gt; &lt;td align="left"&gt;5.33B&lt;/td&gt; &lt;td align="left"&gt;39.15B&lt;/td&gt; &lt;td align="left"&gt;140.62B&lt;/td&gt; &lt;td align="left"&gt;27.84%&lt;/td&gt; &lt;td align="left"&gt;28GB&lt;/td&gt; &lt;td align="left"&gt;9.956%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt; &lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt; &lt;p&gt;Edited: Per &lt;a href="/u/Aaaaaaaaaeeeee"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; 's request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T04:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzhns3</id>
    <title>Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)</title>
    <updated>2025-07-14T09:12:20+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"&gt; &lt;img alt="Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)" src="https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4" title="Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt; &lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt; &lt;li&gt;Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt; &lt;li&gt;Note that quantizations are not same. It's just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mark the solution green if it's accepted.&lt;/li&gt; &lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt; &lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt; &lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn’t treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt; &lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt; &lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt; &lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt; &lt;p&gt;All questions are recent, with no data leakage involved. So don’t come back saying “LeetCode problems are easy for models, this test isn’t meaningful”. It's just your test questions have been seen by the model before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nyu5vpzx2tcf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T09:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzampg</id>
    <title>Training an LLM only on books from the 1800's - no modern bias</title>
    <updated>2025-07-14T02:16:53+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"&gt; &lt;img alt="Training an LLM only on books from the 1800's - no modern bias" src="https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e60398c5e2e84881134a46e0acf601c56ba81942" title="Training an LLM only on books from the 1800's - no modern bias" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that's why I didn't go the fine tune route. It will have no modern bias and will only be able to reason within the time period it's trained on. It's super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T02:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lzfhhq</id>
    <title>Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman</title>
    <updated>2025-07-14T06:48:39+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt; &lt;img alt="Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman" src="https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8" title="Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt; &lt;p&gt;I don't know how the French and European authorities could accept this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syyfccpldscf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-14T06:48:39+00:00</published>
  </entry>
</feed>
