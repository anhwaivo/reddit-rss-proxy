<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-12T08:38:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j9d6o4</id>
    <title>GPU situation a year from now</title>
    <updated>2025-03-12T06:18:58+00:00</updated>
    <author>
      <name>/u/Ok-Anxiety8313</name>
      <uri>https://old.reddit.com/user/Ok-Anxiety8313</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to hear your predictions on the state of GPU and GPU market a year from now, in particular VRAM-high GPUs for home AI rigs.&lt;/p&gt; &lt;p&gt;Is it going to remain as bad? &lt;/p&gt; &lt;p&gt;Are we going to have 5090 at MSRP / cheaper than MSRP in seconhand market? Is this going to make secondhand 4090 affordable again?&lt;/p&gt; &lt;p&gt;My opinion is: right now we are in the awkward spot where 4090 are not made anymore and 5090 are not quite shipped yet. so basically it does not get worse than that and will improve a lot. Am I being too optimistic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Anxiety8313"&gt; /u/Ok-Anxiety8313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9d6o4/gpu_situation_a_year_from_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9d6o4/gpu_situation_a_year_from_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9d6o4/gpu_situation_a_year_from_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9df59</id>
    <title>AMD new open source Vision Language model: Instella-VL-1B</title>
    <updated>2025-03-12T06:31:32+00:00</updated>
    <author>
      <name>/u/v1an1</name>
      <uri>https://old.reddit.com/user/v1an1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v1an1"&gt; /u/v1an1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/Instella-BL-1B-VLM/README.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9df59/amd_new_open_source_vision_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9df59/amd_new_open_source_vision_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:31:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8v0wk</id>
    <title>Kokoro Voice Composer (generate new voices + TTS)</title>
    <updated>2025-03-11T16:35:28+00:00</updated>
    <author>
      <name>/u/al4sdair</name>
      <uri>https://old.reddit.com/user/al4sdair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"&gt; &lt;img alt="Kokoro Voice Composer (generate new voices + TTS)" src="https://external-preview.redd.it/uM9Sn9FDN3MK3bSThOTPvi96U67JUdhcE60zJX8XsG0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cdda1f9e601d1b7049f6d7d58a497aa9d625d0c3" title="Kokoro Voice Composer (generate new voices + TTS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/al4sdair"&gt; /u/al4sdair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/alasdairforsythe/kokoro-voice-composer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8v0wk/kokoro_voice_composer_generate_new_voices_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8mrju</id>
    <title>Alibaba just dropped R1-Omni!</title>
    <updated>2025-03-11T09:24:29+00:00</updated>
    <author>
      <name>/u/Optifnolinalgebdirec</name>
      <uri>https://old.reddit.com/user/Optifnolinalgebdirec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba just dropped R1-Omni! Redefining emotional intelligence with Omni-Multimodal Emotion Recognition and Reinforcement Learning!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optifnolinalgebdirec"&gt; /u/Optifnolinalgebdirec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8mrju/alibaba_just_dropped_r1omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T09:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9cvyd</id>
    <title>After changing to 9800x3D DDR5 6000, the performance improvement is very noticeable</title>
    <updated>2025-03-12T06:03:24+00:00</updated>
    <author>
      <name>/u/q8019222</name>
      <uri>https://old.reddit.com/user/q8019222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally my computer was 3500x ddr4 3600 graphics card 3060ti 8G&lt;/p&gt; &lt;p&gt;The CPU was changed to 9800x3D ddr5 6000 and the graphics card remained unchanged&lt;/p&gt; &lt;p&gt;Running 70B increased from 0.4t/s to 1.18t/s, almost 3 times&lt;/p&gt; &lt;p&gt;When the GPU is bad, upgrading the CPU and RAM is still very effective&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/q8019222"&gt; /u/q8019222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9cvyd/after_changing_to_9800x3d_ddr5_6000_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9cvyd/after_changing_to_9800x3d_ddr5_6000_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9cvyd/after_changing_to_9800x3d_ddr5_6000_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j95nt7</id>
    <title>Realized I should use API's for LLMs and do photos locally with my 3090</title>
    <updated>2025-03-12T00:00:17+00:00</updated>
    <author>
      <name>/u/Comfortable-Mine3904</name>
      <uri>https://old.reddit.com/user/Comfortable-Mine3904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been pushing my 3090 to its limits lately, running both large language models (LLMs) and various photo and video generation models. Today, I had a bit of a revelation: when it comes to raw throughput and efficiency, I’m probably better off dedicating my local hardware to photo generation and relying on APIs for the LLMs. Here’s why.&lt;/p&gt; &lt;p&gt;On the LLM side, I’ve been running models ranging from 14 billion to 32 billion parameters, depending on the task. With my setup, I’m getting around 18 to 20 tokens per second (tkps) on average. If I were to fully utilize my GPU for 24 hours straight, that would theoretically amount to about 1.7 million tokens generated in a day. To be conservative and account for some overhead like preprocessing or other inefficiencies, let’s round that down to 1.5 million tokens per day.&lt;/p&gt; &lt;p&gt;On the other hand, when it comes to photo generation, my rig can produce about 3 images per minute. If I were to run it non-stop for 24 hours, that would come out to approximately 4,000 images in a day. &lt;/p&gt; &lt;p&gt;Now, here’s the kicker: if I were to use an API like QwQ 32 through Open Router for generating that same volume of tokens, it would cost me roughly $1 per day. &lt;/p&gt; &lt;p&gt;Photo generation APIs typically charge around $0.04 per image. At that rate, generating 4,000 images would cost me $160 per day. That’s a massive difference, and it makes a strong case for using my local hardware for photo generation while offloading LLM tasks to APIs.&lt;/p&gt; &lt;p&gt;If anyone knows of a cheaper photo generation API than $0.04 per image, I’d love to hear about it! But for now, this breakdown has convinced me to rethink how I allocate my resources. By focusing my GPU on photo generation and APIs for LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Mine3904"&gt; /u/Comfortable-Mine3904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95nt7/realized_i_should_use_apis_for_llms_and_do_photos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95nt7/realized_i_should_use_apis_for_llms_and_do_photos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j95nt7/realized_i_should_use_apis_for_llms_and_do_photos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T00:00:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8wfsk</id>
    <title>Reka Flash 3 and the infamous spinning hexagon prompt</title>
    <updated>2025-03-11T17:33:00+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt; &lt;img alt="Reka Flash 3 and the infamous spinning hexagon prompt" src="https://external-preview.redd.it/1wLtzmIJNY8IXPRc2HGItEr1OuV-7ei5csHuGx1DeYc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0494b48062ab05674239b1ffd75a741fd0ea172" title="Reka Flash 3 and the infamous spinning hexagon prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran the following prompt with the 3bit MLX version of the new &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;Reka Flash 3&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;Create a pygame script with a spinning hexagon and a bouncing ball confined within. Handle collision detection, gravity and ball physics as good as you possibly can.&lt;/p&gt; &lt;p&gt;I DID NOT expect the result to be as clean as it turned out to be. Of all the models under 10GB that I've tested with the same prompt, this(3bit quant!) one's clearly the winner!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j8wfsk/video/ved8j31vi3oe1/player"&gt;https://reddit.com/link/1j8wfsk/video/ved8j31vi3oe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8wfsk/reka_flash_3_and_the_infamous_spinning_hexagon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T17:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9erfw</id>
    <title>Try Gemma 3 with our new Gemma Python library!</title>
    <updated>2025-03-12T07:43:09+00:00</updated>
    <author>
      <name>/u/ResponsibleSolid8404</name>
      <uri>https://old.reddit.com/user/ResponsibleSolid8404</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleSolid8404"&gt; /u/ResponsibleSolid8404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gemma-llm.readthedocs.io/en/latest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9erfw/try_gemma_3_with_our_new_gemma_python_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T07:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j91e71</id>
    <title>Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!</title>
    <updated>2025-03-11T20:55:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"&gt; &lt;img alt="Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!" src="https://external-preview.redd.it/ddQTqHwEP1NhKvcKA2BhPTcEYYfofNwH1aagxf91uVw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39197fb9b8aef7cafa4c8c1c0bddc77b3d5e3f62" title="Drummer's Gemmasutra Small 4B v1 - The best portable RP model is back with a heftier punch!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemmasutra-Small-4B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j91e71/drummers_gemmasutra_small_4b_v1_the_best_portable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9egmi</id>
    <title>Gemma 3 vs Qwen 2.5 benchmark comparison (Instructed)</title>
    <updated>2025-03-12T07:26:33+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9egmi/gemma_3_vs_qwen_25_benchmark_comparison_instructed/"&gt; &lt;img alt="Gemma 3 vs Qwen 2.5 benchmark comparison (Instructed)" src="https://b.thumbs.redditmedia.com/meWW-5qqGrqbHMPDHcW8q1ZnJROF-DayPdgrMe509bY.jpg" title="Gemma 3 vs Qwen 2.5 benchmark comparison (Instructed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ikccbcnkn7oe1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a0257d47de9e565402464cb25f267e80e390a0f"&gt;https://preview.redd.it/ikccbcnkn7oe1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a0257d47de9e565402464cb25f267e80e390a0f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0pkhm6ln7oe1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ed6593d176a31bb30e6e1de9ea762e61a74e154"&gt;https://preview.redd.it/c0pkhm6ln7oe1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ed6593d176a31bb30e6e1de9ea762e61a74e154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instruction fine-tuned (IT) versions&lt;/p&gt; &lt;p&gt;source:&lt;/p&gt; &lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen2.5-llm/"&gt;https://qwenlm.github.io/blog/qwen2.5-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"&gt;https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9egmi/gemma_3_vs_qwen_25_benchmark_comparison_instructed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9egmi/gemma_3_vs_qwen_25_benchmark_comparison_instructed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9egmi/gemma_3_vs_qwen_25_benchmark_comparison_instructed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T07:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j91zx4</id>
    <title>7B reasoning model outperforming Claude-3.7 Sonnet on IOI</title>
    <updated>2025-03-11T21:20:30+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"&gt; &lt;img alt="7B reasoning model outperforming Claude-3.7 Sonnet on IOI" src="https://preview.redd.it/rzu5zsd0n4oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=733bdf42006187dcf40def927d8ea4bf5171e755" title="7B reasoning model outperforming Claude-3.7 Sonnet on IOI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzu5zsd0n4oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j91zx4/7b_reasoning_model_outperforming_claude37_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T21:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8uvg0</id>
    <title>New Reasoning model (Reka Flash 3 - 21B)</title>
    <updated>2025-03-11T16:29:11+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt; &lt;img alt="New Reasoning model (Reka Flash 3 - 21B)" src="https://preview.redd.it/fgldu1ml73oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f085204b9fc6819966a9114f4e794afbed28a54f" title="New Reasoning model (Reka Flash 3 - 21B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fgldu1ml73oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8uvg0/new_reasoning_model_reka_flash_3_21b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:29:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8tfh5</id>
    <title>Reka Flash 3, New Open Source 21B Model</title>
    <updated>2025-03-11T15:29:02+00:00</updated>
    <author>
      <name>/u/DreamGenAI</name>
      <uri>https://old.reddit.com/user/DreamGenAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tweet: &lt;a href="https://x.com/RekaAILabs/status/1899481289495031825"&gt;https://x.com/RekaAILabs/status/1899481289495031825&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3"&gt;https://huggingface.co/RekaAI/reka-flash-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://www.reka.ai/news/introducing-reka-flash"&gt;https://www.reka.ai/news/introducing-reka-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DreamGenAI"&gt; /u/DreamGenAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8tfh5/reka_flash_3_new_open_source_21b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T15:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9drfk</id>
    <title>Gemma 3: Technical Report</title>
    <updated>2025-03-12T06:49:36+00:00</updated>
    <author>
      <name>/u/David-Kunz</name>
      <uri>https://old.reddit.com/user/David-Kunz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/David-Kunz"&gt; /u/David-Kunz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9drfk/gemma_3_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8r2nr</id>
    <title>M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)</title>
    <updated>2025-03-11T13:44:15+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt; &lt;img alt="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" src="https://external-preview.redd.it/Z3KKrFryWMuFPZGHYHDmgzf48KaEB5A-Ze6pFibC3lk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992e2d56bcc2473a9ea6913ceadc30c7eb46bb1f" title="M3 Ultra 512GB does 18T/s with Deepseek R1 671B Q4 (DAVE2D REVIEW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=J4qwuCXyAcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8r2nr/m3_ultra_512gb_does_18ts_with_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T13:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9ev94</id>
    <title>Gemma 3 is fast and good!</title>
    <updated>2025-03-12T07:48:56+00:00</updated>
    <author>
      <name>/u/codemaker1</name>
      <uri>https://old.reddit.com/user/codemaker1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ev94/gemma_3_is_fast_and_good/"&gt; &lt;img alt="Gemma 3 is fast and good!" src="https://preview.redd.it/ge05mubqr7oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6677bdcf6e063e9b89c903ac9a4b49362ecb1535" title="Gemma 3 is fast and good!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codemaker1"&gt; /u/codemaker1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ge05mubqr7oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ev94/gemma_3_is_fast_and_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9ev94/gemma_3_is_fast_and_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T07:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j95fjo</id>
    <title>Gemma 3 is confirmed to be coming soon</title>
    <updated>2025-03-11T23:49:44+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt; &lt;img alt="Gemma 3 is confirmed to be coming soon" src="https://preview.redd.it/0iudkfrrd5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5436382ec43a49f4f586d49c5ecdf024a9a21612" title="Gemma 3 is confirmed to be coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0iudkfrrd5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j95fjo/gemma_3_is_confirmed_to_be_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T23:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j8u90g</id>
    <title>New Gemma models on 12th of March</title>
    <updated>2025-03-11T16:03:39+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt; &lt;img alt="New Gemma models on 12th of March" src="https://preview.redd.it/8qfnwj7433oe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed4ac1bb57e9292b5685c7637a5bd9e4ac889d7c" title="New Gemma models on 12th of March" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X pos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qfnwj7433oe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j8u90g/new_gemma_models_on_12th_of_march/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dt8l</id>
    <title>Gemma 3 on Huggingface</title>
    <updated>2025-03-12T06:52:16+00:00</updated>
    <author>
      <name>/u/DataCraftsman</name>
      <uri>https://old.reddit.com/user/DataCraftsman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Gemma 3! Comes in 1B, 4B, 12B, 27B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-1b-it"&gt;https://huggingface.co/google/gemma-3-1b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;https://huggingface.co/google/gemma-3-4b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it"&gt;https://huggingface.co/google/gemma-3-12b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-27b-it"&gt;https://huggingface.co/google/gemma-3-27b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text string, such as a question, a prompt, or a document to be summarized&lt;/li&gt; &lt;li&gt;Images, normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context of 8192 tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update: They have added it to Ollama already!&lt;/p&gt; &lt;p&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apparently it has an ELO of 1338 on Chatbot Arena, better than DeepSeek V3 671B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataCraftsman"&gt; /u/DataCraftsman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dt8l/gemma_3_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dmny</id>
    <title>Gemma 3 27B</title>
    <updated>2025-03-12T06:42:38+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt; &lt;img alt="Gemma 3 27B" src="https://preview.redd.it/foonq7ewf7oe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30b8fa38fe5d18d26cf0aca72f47b346cd9ad56" title="Gemma 3 27B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/foonq7ewf7oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dmny/gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:42:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j90u4u</id>
    <title>What happened to the promised open source o3-mini ?</title>
    <updated>2025-03-11T20:32:37+00:00</updated>
    <author>
      <name>/u/i-have-the-stash</name>
      <uri>https://old.reddit.com/user/i-have-the-stash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does everybody forget that this was once promised ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i-have-the-stash"&gt; /u/i-have-the-stash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j90u4u/what_happened_to_the_promised_open_source_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-11T20:32:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j981ci</id>
    <title>This is the first response from an LLM that has made me cry laughing</title>
    <updated>2025-03-12T01:50:28+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt; &lt;img alt="This is the first response from an LLM that has made me cry laughing" src="https://preview.redd.it/kw96telpz5oe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7223e09ee41672180f06db34a031ef87fae195a" title="This is the first response from an LLM that has made me cry laughing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kw96telpz5oe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j981ci/this_is_the_first_response_from_an_llm_that_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T01:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9bvll</id>
    <title>Gemma 3 27b now available on Google AI Studio</title>
    <updated>2025-03-12T05:13:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt; &lt;img alt="Gemma 3 27b now available on Google AI Studio" src="https://external-preview.redd.it/4sjcMoBy8c8hywZZD7DFEQHtY85E3eDlhYRBqIdn2eQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f55bb78cef85467f757df883df24bca99ee8925" title="Gemma 3 27b now available on Google AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://aistudio.google.com/"&gt;https://aistudio.google.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context length 128k&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output length 8k&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/2WvMTPS"&gt;&lt;strong&gt;https://imgur.com/a/2WvMTPS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd"&gt;https://preview.redd.it/1pbvvqtwz6oe1.png?width=1259&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0da97a547c24c616b8c3c1cc1ccd43e659245dd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9bvll/gemma_3_27b_now_available_on_google_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T05:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j96j3g</id>
    <title>I hacked Unsloth's GRPO code to support agentic tool use. In 1 hour of training on my RTX 4090, Llama-8B taught itself to take baby steps towards deep research! (23%→53% accuracy)</title>
    <updated>2025-03-12T00:40:21+00:00</updated>
    <author>
      <name>/u/diegocaples</name>
      <uri>https://old.reddit.com/user/diegocaples</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I've been experimenting with getting &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;Llama-8B to bootstrap its own research skills through self-play.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified Unsloth's GRPO implementation (❤️ Unsloth!) to support function calling and agentic feedback loops.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Llama generates its own questions about documents (you can have it learn from any documents, but I chose the Apollo 13 mission report)&lt;/li&gt; &lt;li&gt;It learns to search for answers in the corpus using a search tool&lt;/li&gt; &lt;li&gt;It evaluates its own success/failure using llama-as-a-judge&lt;/li&gt; &lt;li&gt;Finally, it trains itself through RL to get better at research&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The model starts out hallucinating and making all kinds of mistakes, but after an hour of training on my 4090, it quickly improves. It goes from getting 23% of answers correct to 53%!&lt;/p&gt; &lt;p&gt;Here is the full &lt;a href="https://github.com/dCaples/AutoDidact/"&gt;code and instructions&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diegocaples"&gt; /u/diegocaples &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j96j3g/i_hacked_unsloths_grpo_code_to_support_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T00:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9dkvh</id>
    <title>Gemma 3 Release - a google Collection</title>
    <updated>2025-03-12T06:39:59+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt; &lt;img alt="Gemma 3 Release - a google Collection" src="https://external-preview.redd.it/XbF6RBBvzvCU6XDYyRoYk_HGSNjj77rcnuXfCRK9sgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d72653de324cc030e9dad7f7ea4df6ef94e0688" title="Gemma 3 Release - a google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9dkvh/gemma_3_release_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T06:39:59+00:00</published>
  </entry>
</feed>
