<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-10T00:26:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jum5s1</id>
    <title>Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license</title>
    <updated>2025-04-08T19:24:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt; &lt;img alt="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" src="https://b.thumbs.redditmedia.com/UF9D2G4LY8nt9Z5N6vRdkaTUx2GI3fo84_fgBsrinvs.jpg" title="Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cogito: ‚ÄúWe are releasing the strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license. Each model outperforms the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen, across most standard benchmarks‚Äù&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53"&gt;https://huggingface.co/collections/deepcogito/cogito-v1-preview-67eb105721081abe4ce2ee53&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jum5s1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T19:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvhldh</id>
    <title>Reasoning System Prompt for Gemma3 - Tesslate - Synthia</title>
    <updated>2025-04-09T21:40:04+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhldh/reasoning_system_prompt_for_gemma3_tesslate/"&gt; &lt;img alt="Reasoning System Prompt for Gemma3 - Tesslate - Synthia" src="https://preview.redd.it/wkkzcg7epvte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f4d33175fa8221ca07c8ef301c436f01343ddef" title="Reasoning System Prompt for Gemma3 - Tesslate - Synthia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://huggingface.co/Tesslate/Synthia-S1-27b"&gt;https://huggingface.co/Tesslate/Synthia-S1-27b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The system prompt from Tesslate - Synthia works &lt;strong&gt;wonderfull&lt;/strong&gt; for regular Gemma3 too:&lt;/p&gt; &lt;p&gt;Your role as an assistant is to engage in deep, methodical reasoning and provide comprehensive, accurate solutions. Before arriving at a final answer, you must undertake a structured, multi-phase thinking process that emphasizes depth, verification, and clarity. This involves thoroughly analyzing the question, identifying key elements, summarizing relevant insights, generating hypotheses, iteratively refining thoughts, verifying assumptions, cross-checking with prior knowledge, and reevaluating earlier conclusions as necessary. Your response must be structured into two main sections: Thought and Solution. In the Thought section, rigorously document your reasoning in the following format: &amp;lt;|begin_of_thought|&amp;gt; {thought process with each logical step separated by '\n\n'} &amp;lt;|end_of_thought|&amp;gt;. Each step should reflect deep analysis‚Äîsuch as decomposing the problem, synthesizing relevant information, exploring different possibilities, validating each phase, correcting errors, and revisiting earlier assumptions. In the Solution section, consolidate all your insights and reasoned steps into a concise, well-structured final answer. Present it clearly and logically using this format: &amp;lt;|begin_of_solution|&amp;gt; {final, precise, step-by-step solution} &amp;lt;|end_of_solution|&amp;gt;. This approach ensures that the final output reflects a high-confidence answer that results from critical thinking and iteration. Now, try to solve the following question through the above guidelines:&lt;/p&gt; &lt;p&gt;Please use &lt;code&gt;temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0&lt;/code&gt; with repeat penalty set to 1.3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wkkzcg7epvte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhldh/reasoning_system_prompt_for_gemma3_tesslate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhldh/reasoning_system_prompt_for_gemma3_tesslate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T21:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvd52b</id>
    <title>Best Local Model for Writing</title>
    <updated>2025-04-09T18:33:57+00:00</updated>
    <author>
      <name>/u/PastRequirement3218</name>
      <uri>https://old.reddit.com/user/PastRequirement3218</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a n00b at all this, but I like to write and use AI to help improve my prose. I have found o1 to be able to take my stuff fix it up pretty well, but I want to try a local model. I dont really care if it takes it an hour to process a single chapter.&lt;/p&gt; &lt;p&gt;What would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PastRequirement3218"&gt; /u/PastRequirement3218 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvd52b/best_local_model_for_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvd52b/best_local_model_for_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvd52b/best_local_model_for_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvhbov</id>
    <title>What are your current favorite models for mid/lower tier hardware?</title>
    <updated>2025-04-09T21:28:16+00:00</updated>
    <author>
      <name>/u/xcheezeplz</name>
      <uri>https://old.reddit.com/user/xcheezeplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So many models, so little time, VRAM and storage. üòÅ &lt;/p&gt; &lt;p&gt;Even though I have a desktop I can use larger models with I end up on the road and using my laptop a lot more lately... 8GB VRAM (4070) and 64GB Ram, i7 13gen. I've always tried to stick to with dense models that fit in VRAM only for general purpose and coding. &lt;/p&gt; &lt;p&gt;I became partial to the Qwen2.5 models, but I'm wondering what models everyone else is maining on similar hardware for code, agents or general purpose. I've stopped chasing leaderboard stats after a lot of disappointments, but I wonder if I am missing out on better models. &lt;/p&gt; &lt;p&gt;Another reason I ask is I'm seeing more people than normal being satisfied with token rates on larger models offloaded in ram, local MoE, or certain use cases on even on CPU, or some very impressive small param models. &lt;/p&gt; &lt;p&gt;Tldr; what's your favorite models right now for &amp;quot;everyman hardware&amp;quot; for whatever you main use cases are? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xcheezeplz"&gt; /u/xcheezeplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhbov/what_are_your_current_favorite_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhbov/what_are_your_current_favorite_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvhbov/what_are_your_current_favorite_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T21:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv68hp</id>
    <title>Deep Research using the Agents SDK</title>
    <updated>2025-04-09T13:49:00+00:00</updated>
    <author>
      <name>/u/TheRedfather</name>
      <uri>https://old.reddit.com/user/TheRedfather</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"&gt; &lt;img alt="Deep Research using the Agents SDK" src="https://external-preview.redd.it/DbUzqqfzeRze2FIyQ3VOz5Vy-7i0-B5t-Vs82DGWScs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4dc660267089d52535b44a359859d31e76491d5" title="Deep Research using the Agents SDK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRedfather"&gt; /u/TheRedfather &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/qx-labs/agents-deep-research"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv68hp/deep_research_using_the_agents_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv469f</id>
    <title>New paper: SmolVLM: Redefining small and efficient multimodal models</title>
    <updated>2025-04-09T12:07:55+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks, it's Andi from Hugging Face multimodal team (author of SmolVLM) üëãüèª &lt;/p&gt; &lt;p&gt;Yesterday, we released a &lt;a href="https://huggingface.co/papers/2504.05299"&gt;technical report&lt;/a&gt; for SmolVLM (aka your favorite smol vision LM) ü§ó&lt;/p&gt; &lt;p&gt;This technical report comes packed with a ton of findings, here I wanted to summarize them for you (read the paper if you're interested in more details):&lt;/p&gt; &lt;p&gt;- Longer context; big wins: Increasing the context length from 2K to 16K gave our tiny VLMs a 60% performance boost&lt;/p&gt; &lt;p&gt;- Smaller is smarter with SigLIP: Smaller LLMs didn't benefit from the usual large SigLIP (400M). Instead, we use the 80M base SigLIP that performs equally well at just 20% of the original size&lt;/p&gt; &lt;p&gt;- Pixel shuffling magic: Aggressively pixel shuffling helped our compact VLMs; better, achieving the same performance with sequences 16x shorter!&lt;/p&gt; &lt;p&gt;- Learned positional tokens FTW: For compact models, learned positional tokens significantly outperform raw text tokens, enhancing efficiency and accuracy.&lt;/p&gt; &lt;p&gt;- System prompts and special tokens are key: Introducing system prompts and dedicated media intro/outro tokens significantly boosted our compact VLM‚Äôs performance‚Äîespecially for video tasks.&lt;/p&gt; &lt;p&gt;- Less CoT, more efficiency: Too much Chain-of-Thought (CoT) data actually hurts performance in small models. They dumb&lt;/p&gt; &lt;p&gt;- Longer videos, better results: Increasing video length during training enhanced performance on both video and image tasks. State-of-the-Art Performance, SmolVLM comes in three powerful yet compact sizes‚Äî256M, 500M, and 2.2B parameters‚Äîeach setting new SOTA benchmarks for their hardware constraints in image and video understanding.&lt;/p&gt; &lt;p&gt;- Real-world Efficiency: We've created an app using SmolVLM on an iPhone 15 and got real-time inference directly from its camera!&lt;/p&gt; &lt;p&gt;- Browser-based Inference: We get lightning-fast inference speeds of 40-80 tokens per second directly in a web browser. No tricks, just compact, efficient models!&lt;/p&gt; &lt;p&gt;Give it a read and let us know what you think, I'll be also answering questions in case you have any &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv469f/new_paper_smolvlm_redefining_small_and_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvcss8</id>
    <title>Loong is here: An open-source program to build verifiable synthetic datasets for reasoning-heavy domains (logic, math, graph theory, etc.)</title>
    <updated>2025-04-09T18:20:10+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve kicked off a new open research program called &lt;strong&gt;Loong&lt;/strong&gt; üêâ, aimed at improving LLM reasoning through &lt;em&gt;verifiable&lt;/em&gt; synthetic data at scale.&lt;/p&gt; &lt;p&gt;You‚Äôve probably seen how post-training with verified feedback (like DeepSeek-R1 or R2) is helping models get better at math and programming. That‚Äôs partly because these domains are easy to verify + have lots of clean datasets.&lt;/p&gt; &lt;p&gt;But what about reasoning in domains like logic, graph theory, finance, or computational biology where good datasets are scarce, and verification is harder?&lt;/p&gt; &lt;p&gt;With Loong, we‚Äôre trying to solve this using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;Gym-like RL environment&lt;/strong&gt; for generating and evaluating data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent synthetic data generation pipelines&lt;/strong&gt; (e.g., self-instruct + solver agents)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain-specific verifiers&lt;/strong&gt; that validate whether model outputs are semantically correct&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìò &lt;strong&gt;Blog:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.camel-ai.org/blogs/project-loong-synthetic-data-at-scale-through-verifiers"&gt;https://www.camel-ai.org/blogs/project-loong-synthetic-data-at-scale-through-verifiers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª &lt;strong&gt;Code:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/camel-ai/loong"&gt;https://github.com/camel-ai/loong&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Want to get involved: &lt;a href="https://www.camel-ai.org/collaboration-questionnaire"&gt;https://www.camel-ai.org/collaboration-questionnaire&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcss8/loong_is_here_an_opensource_program_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcss8/loong_is_here_an_opensource_program_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcss8/loong_is_here_an_opensource_program_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvg70f</id>
    <title>Introducing Docker Model Runner</title>
    <updated>2025-04-09T20:40:19+00:00</updated>
    <author>
      <name>/u/Upstairs-Sky-5290</name>
      <uri>https://old.reddit.com/user/Upstairs-Sky-5290</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvg70f/introducing_docker_model_runner/"&gt; &lt;img alt="Introducing Docker Model Runner" src="https://external-preview.redd.it/Gz5iv0hC1-oojGEzY0yl1Njb0Jt7-bSeQ06GaO9A-dI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f2b11c0e4f2b2b8e0650342013fc0349a649a" title="Introducing Docker Model Runner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upstairs-Sky-5290"&gt; /u/Upstairs-Sky-5290 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.docker.com/blog/introducing-docker-model-runner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvg70f/introducing_docker_model_runner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvg70f/introducing_docker_model_runner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T20:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1juni3t</id>
    <title>DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level</title>
    <updated>2025-04-08T20:20:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt; &lt;img alt="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" src="https://a.thumbs.redditmedia.com/Y5BwtBnjZby6zmZDlawuWxCvPe3JSO0Wzb73zGMqhW4.jpg" title="DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1juni3t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T20:20:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv4k84</id>
    <title>KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM</title>
    <updated>2025-04-09T12:28:28+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt; &lt;img alt="KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM" src="https://external-preview.redd.it/O5w_btCMTBBtqnFyVOraTvoye2DmLp3ZSDLb_P_jWm0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfe234cecb629681a25925d0579c9a04a40715be" title="KTransformers Now Supports LLaMA 4: Run q4 Maverick at 32 tokens/s with 10GB VRAM + 270GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaMA 4 is also a MoE model, which makes it well-suited for hybrid CPU/GPU inference. &lt;/p&gt; &lt;p&gt;KTransformers now offers &lt;em&gt;experimental support&lt;/em&gt; for LLaMA 4 under the development branch &lt;code&gt;support-llama4&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tjwvu403zste1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7872a75c957e1cfd140015292298d07fc45efb5e"&gt;https://preview.redd.it/tjwvu403zste1.jpg?width=1226&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7872a75c957e1cfd140015292298d07fc45efb5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key performance highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scout (16 Experts): ~65GB system memory, 10GB GPU VRAM&lt;/li&gt; &lt;li&gt;Maverick (128 Experts): ~270GB system memory, 12GB GPU VRAM&lt;/li&gt; &lt;li&gt;Both models require ~17B activation parameters per request. Thus, with a 4090 GPU and dual Xeon 4 CPUs, Scout/Maverick can both achieve up to 32 tokens/s for single batch.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;More details and setup instructions can be found here: &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/llama4.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/llama4.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv4k84/ktransformers_now_supports_llama_4_run_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:28:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvflqw</id>
    <title>Oobabooga just added support for Exllamav3!</title>
    <updated>2025-04-09T20:15:25+00:00</updated>
    <author>
      <name>/u/Jellonling</name>
      <uri>https://old.reddit.com/user/Jellonling</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvflqw/oobabooga_just_added_support_for_exllamav3/"&gt; &lt;img alt="Oobabooga just added support for Exllamav3!" src="https://external-preview.redd.it/oveKGWlpg5XZWYThefWN76YvFa_8xGC9Blo4QccpcsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cddd23fbc026a9cda1c530e7417ee1358ccf7915" title="Oobabooga just added support for Exllamav3!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jellonling"&gt; /u/Jellonling &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/oobabooga/text-generation-webui/releases/tag/v2.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvflqw/oobabooga_just_added_support_for_exllamav3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvflqw/oobabooga_just_added_support_for_exllamav3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T20:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv46ye</id>
    <title>Qwen 2.5 Omni</title>
    <updated>2025-04-09T12:09:00+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read the Qwen2.5-Omni technical report from the Qwen team, it's super interesting. Here are my notes. &lt;/p&gt; &lt;p&gt;Qwen2.5-Omni is a unified end-to-end model that can perceive text, images, audio, and video ‚Äî and generate both text and natural speech responses in a streaming fashion. &lt;/p&gt; &lt;p&gt;At its core is the Thinker-Talker architecture:&lt;br /&gt; Thinker: a large language model that processes multimodal inputs and generates text.&lt;br /&gt; Talker: an autoregressive speech decoder that turns Thinker's hidden states into speech tokens. They're trained together, end-to-end. &lt;/p&gt; &lt;p&gt;Handling audio: audio is converted to 128-channel mel-spectrograms (16kHz, 25ms window, 10ms hop). Encoded via a modified Whisper model. Audio is processed in 2s blocks with streaming-compatible attention to reduce latency. &lt;/p&gt; &lt;p&gt;Handling video: uses a ViT-based encoder with dynamic frame sampling. Each frame is treated like an image. To sync with audio, they introduce TMRoPE ‚Äî Time-aligned Multimodal RoPE ‚Äî a novel positional embedding that aligns video and audio in time. &lt;/p&gt; &lt;p&gt;TMRoPE splits positional encoding into temporal, height, and width axes, letting Qwen2.5-Omni represent image/video/audio/text all on the same timeline. Interleaving of audio and visual tokens every 2 seconds enables synchronized fusion. &lt;/p&gt; &lt;p&gt;Streaming audio generation: audio tokens from Talker are decoded using a sliding-window DiT model + modified BigVGAN. The receptive field includes 2 lookback blocks and 1 lookahead to allow context-aware streaming audio generation. &lt;/p&gt; &lt;p&gt;Pretraining involved locking the LLM and training the audio/vision encoders first. Later stages unfreeze everything and train on a massive mix of audio-text, video-text, image-text, and long-sequence (32k tokens) data. &lt;/p&gt; &lt;p&gt;Post-training includes reinforcement learning for Talker to reduce hallucinations and improve pronunciation/timing. Plus, multi-speaker fine-tuning for better prosody and naturalness. &lt;/p&gt; &lt;p&gt;Qwen2.5-Omni achieves SOTA on OmniBench, AV-Odyssey, and strong results across text, image, audio, and video tasks. End-to-end speech instruction following is nearly on par with text-based inputs. That's rare. &lt;/p&gt; &lt;p&gt;Overall: a super ambitious and well-integrated multimodal model. The Thinker-Talker separation is elegant. TMRoPE is a clever solution to a tricky problem. &lt;/p&gt; &lt;p&gt;That said, I wish the paper had included more ablation studies or experiments justifying some of the architectural decisions. Many claims are reasonable but would benefit from more empirical evidence. &lt;/p&gt; &lt;p&gt;Still, major kudos to the team. Qwen2.5-Omni is a big step toward real-time, unified multimodal assistants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv46ye/qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T12:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvcxas</id>
    <title>Kimi-VL-A3B - a moonshotai Collection</title>
    <updated>2025-04-09T18:25:12+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcxas/kimivla3b_a_moonshotai_collection/"&gt; &lt;img alt="Kimi-VL-A3B - a moonshotai Collection" src="https://external-preview.redd.it/5nmXojXU8_v91V3-gyiy84wh-CsuY8a232sAmiUop3U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a99f5f3e66d68c5b913b85c1cf286add6111c405" title="Kimi-VL-A3B - a moonshotai Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonshot's efficient MoE VLMs, exceptional on agent, long-context, and thinking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcxas/kimivla3b_a_moonshotai_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvcxas/kimivla3b_a_moonshotai_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv7x6l</id>
    <title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
    <updated>2025-04-09T15:01:53+00:00</updated>
    <author>
      <name>/u/Psychological-Tea652</name>
      <uri>https://old.reddit.com/user/Psychological-Tea652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt; &lt;img alt="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" src="https://external-preview.redd.it/b3BjbHE0c2ZwdHRlMWcmQ4x0UIQwBXGX5ihDQRS0yvkPTeRAH8Mf_AVWxETI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49a6c2066bf6d904c4233f96449ce01283d4b000" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper modifies LLM attention so multiple &amp;quot;workers&amp;quot; can see each other's thoughts (KV) in real time. They generate text in parallel like humans use Google Docs. Turns out, they can self-organize, split the work and cross-verify. Works with open-source models like QwQ-32B. Check it out!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/papers/2504.06261"&gt;https://huggingface.co/papers/2504.06261&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Project page:&lt;/strong&gt; &lt;a href="https://eqimp.github.io/hogwild_llm"&gt;https://eqimp.github.io/hogwild_llm&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological-Tea652"&gt; /u/Psychological-Tea652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q36zd4sfptte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv7x6l/hogwild_inference_parallel_llm_generation_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T15:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv22mm</id>
    <title>Qwen3 and Qwen3-MoE support merged into llama.cpp</title>
    <updated>2025-04-09T10:00:19+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt; &lt;img alt="Qwen3 and Qwen3-MoE support merged into llama.cpp" src="https://external-preview.redd.it/-e19x77nCYUlOdA0buk2ekzVqo7mRcwDr167KRd89n4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8c6fee2cf4cf6c565073243d3b38360e7ec134d" title="Qwen3 and Qwen3-MoE support merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support merged.&lt;/p&gt; &lt;p&gt;We'll have GGUF models on day one&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv22mm/qwen3_and_qwen3moe_support_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T10:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvc768</id>
    <title>Google just launched the A2A protocol were AI agents from any framework can work together</title>
    <updated>2025-04-09T17:56:16+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt; &lt;img alt="Google just launched the A2A protocol were AI agents from any framework can work together" src="https://preview.redd.it/azpf25q5lute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c562212b7e129e18a030a5673a2221e17473b30" title="Google just launched the A2A protocol were AI agents from any framework can work together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're working on an even more MCP-oriented approach to this problem and are building in the open here if anyone is interested, would love to see peoples opinions on both approaches to see what you think it all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/azpf25q5lute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvc768/google_just_launched_the_a2a_protocol_were_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv9s6q</id>
    <title>LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models.</title>
    <updated>2025-04-09T16:18:25+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt; &lt;img alt="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." src="https://preview.redd.it/ew55ayg24ute1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7de8d9615d9f552f6a7a05c87acda4c5a6a656" title="LMSYS WebDev Arena updated with DeepSeek-V3-0324 and Llama 4 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew55ayg24ute1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv9s6q/lmsys_webdev_arena_updated_with_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T16:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvbhlp</id>
    <title>I actually really like Llama 4 scout</title>
    <updated>2025-04-09T17:28:00+00:00</updated>
    <author>
      <name>/u/d13f00l</name>
      <uri>https://old.reddit.com/user/d13f00l</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running it on a 64 core Ampere Altra arm system with 128GB ram, no GPU, in llama.cpp with q6_k quant. It averages about 10 tokens a second which is great for personal use. It is answering coding questions and technical questions well. I have run Llama 3.3 70b, Mixtral 8x7b, Qwen 2.5 72b, some of the PHI models. The performance of scout is really good. Anecdotally it seems to be answering things at least as good as Llama 3.3 70b or Qwen 2.5 72b, at higher speeds. People aren't liking the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d13f00l"&gt; /u/d13f00l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvbhlp/i_actually_really_like_llama_4_scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T17:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvi860</id>
    <title>PSA: Gemma 3 QAT gguf models have some wrongly configured tokens</title>
    <updated>2025-04-09T22:08:06+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;so as I loaded my 12B IT q4_0 QAT model, I've noticed a strage error in llama.cpp: &amp;quot;load: control-looking token: 106 '' was not control-type; this is probably a bug in the model. its type will be overridden&amp;quot;&lt;/p&gt; &lt;p&gt;So I've wondered, is this normal and loaded a Bartowski file, and indeed, that error was nowhere to be seen. After that, I did some digging and came across this post by the guy who implemented Gemma 3 and LLama 4 support in llama.cpp: &lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3#67f6a2e0207b4bceea793151"&gt;https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3#67f6a2e0207b4bceea793151&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looked awfully similar to my error, so what I did was set both token 105 and 106 to control (which are &amp;lt;start\_of\_turn&amp;gt; and &amp;lt;end\_of\_turn&amp;gt; btw) instead of normal (like it's the case with the bartowski files too) using the huggingface gguf editor. Not only that, the image start and end tokens were also not set to control, unlike the original. I've fixed that and noticed a boost in the image capabilities immediately.&lt;/p&gt; &lt;p&gt;If you have noticed weirdness with the QAT models in comparison to the older bart models, then it was most likely due to that. On top of that, the name metadata was missing as well which I've added back, apparently some inference backends need it. &lt;/p&gt; &lt;p&gt;I have uploaded it here: &lt;a href="https://huggingface.co/Dampfinchen/google-gemma-3-12b-it-qat-q4_0-gguf-small-fix"&gt;https://huggingface.co/Dampfinchen/google-gemma-3-12b-it-qat-q4_0-gguf-small-fix&lt;/a&gt; Note that it is based on &lt;a href="https://huggingface.co/stduhpf"&gt;stduhpf&lt;/a&gt;'s version which is faster without any compromise to performance.&lt;/p&gt; &lt;p&gt;Happy testing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvi860/psa_gemma_3_qat_gguf_models_have_some_wrongly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T22:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv71su</id>
    <title>Granite 3.3 imminent?</title>
    <updated>2025-04-09T14:24:45+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt; &lt;img alt="Granite 3.3 imminent?" src="https://preview.redd.it/g2ceteotjtte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60dab73cdbdeadc070ee093587327ecdf15ea289" title="Granite 3.3 imminent?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently they added and then edited the collection. maybe it will be released today?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g2ceteotjtte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv71su/granite_33_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T14:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvgpju</id>
    <title>Moonshot AI released Kimi-VL MoE (3B/16B) Thinking</title>
    <updated>2025-04-09T21:02:16+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"&gt; &lt;img alt="Moonshot AI released Kimi-VL MoE (3B/16B) Thinking" src="https://b.thumbs.redditmedia.com/eEN3TB95dTXyXnsBnqObPtpp29ara6PvOVYX7yFQo1c.jpg" title="Moonshot AI released Kimi-VL MoE (3B/16B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonshot AI's Kimi-VL and Kimi-VL-Thinking! &lt;/p&gt; &lt;p&gt;üí° An MoE VLM and an MoE Reasoning VLM with only ~3B activated parameters (total 16B) üß† Strong multimodal reasoning (36.8% on MathVision, on par with 10x larger models) and agent skills (34.5% on ScreenSpot-Pro) üñºÔ∏è Handles high-res visuals natively with MoonViT (867 on OCRBench) üßæ Supports long context windows up to 128K (35.1% on MMLongBench-Doc, 64.5% on LongVideoBench) üèÜ Outperforms larger models like GPT-4o on key benchmarks&lt;/p&gt; &lt;p&gt;üìú Paper: &lt;a href="https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf"&gt;https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf&lt;/a&gt; ü§ó Huggingface: &lt;a href="https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85"&gt;https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jvgpju"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvgpju/moonshot_ai_released_kimivl_moe_3b16b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T21:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5xv7</id>
    <title>Google Ironwood TPU (7th generation) introduction</title>
    <updated>2025-04-09T13:35:39+00:00</updated>
    <author>
      <name>/u/zimmski</name>
      <uri>https://old.reddit.com/user/zimmski</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/"&gt;https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When i see Google's TPUs, i always ask myself if there is any company working on a local variant that us mortals can buy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zimmski"&gt; /u/zimmski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5xv7/google_ironwood_tpu_7th_generation_introduction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvchif</id>
    <title>How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1</title>
    <updated>2025-04-09T18:07:37+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt; &lt;img alt="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" src="https://external-preview.redd.it/Rb0aJ2SdT0s5u9VBtMk4r2KGdiNryvu9_uLzVS6423c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1de9d7d850207207499d5314450055f45c8256e" title="How we used NVIDIA TensorRT-LLM with Blackwell B200 to achieve 303 output tokens per second on DeepSeek R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a technical blog post on how the team at Avian collaborated with Nvidia to achieve 303 output tokens per second, using FP4 quantization and their new Pytorch runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://new.avian.io/blog/article/deepseek_r1_303"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvchif/how_we_used_nvidia_tensorrtllm_with_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T18:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5vic</id>
    <title>Alibaba AI Conference happening today! We may see Qwen3 in a few hours!</title>
    <updated>2025-04-09T13:32:27+00:00</updated>
    <author>
      <name>/u/MushroomGecko</name>
      <uri>https://old.reddit.com/user/MushroomGecko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt; &lt;img alt="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" src="https://preview.redd.it/w79q9k0katte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a242dd332547aef1d5aceaf7a7c766055e6c50e" title="Alibaba AI Conference happening today! We may see Qwen3 in a few hours!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomGecko"&gt; /u/MushroomGecko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w79q9k0katte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5vic/alibaba_ai_conference_happening_today_we_may_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:32:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jv5uk8</id>
    <title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
    <updated>2025-04-09T13:31:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt; &lt;img alt="OmniSVG: A Unified Scalable Vector Graphics Generation Model" src="https://external-preview.redd.it/MHI3ZzMzc3Q5dHRlMexiJYH3Awkmn9VEWXtNssspPIW9nVy43T4cWZBoNTdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df299d5217413408d56d4fe1cca2c1dc834179f7" title="OmniSVG: A Unified Scalable Vector Graphics Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this on X. If this is true, this SVG generation capability is really amazing, and I can't wait to run it locally. I checked and it seems the model weights haven't been released on Hugging Face yet.&lt;/p&gt; &lt;p&gt;site: &lt;a href="http://omnisvg.github.io"&gt;omnisvg.github.io&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jk6dp2st9tte1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-09T13:31:15+00:00</published>
  </entry>
</feed>
