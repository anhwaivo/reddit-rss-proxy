<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-24T06:41:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k6fy0j</id>
    <title>SurveyGOÔºöOpen DeepResearch. Automated AI-generated surveys</title>
    <updated>2025-04-24T01:20:06+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By TsinghuaNLP team, great job guys !&lt;/p&gt; &lt;p&gt;SurveyGO can turn massive paper piles into high-quality, concise, citation-rich surveys. &lt;/p&gt; &lt;p&gt;üëç Under the hood lies &lt;strong&gt;LLM√óMapReduce‚ÄëV2&lt;/strong&gt;, a novel test-time scaling strategy designed to enhance LLMs' ability to process extremely long inputs.&lt;/p&gt; &lt;p&gt;üåê Demo: &lt;a href="https://surveygo.thunlp.org/"&gt;https://surveygo.thunlp.org/&lt;/a&gt;&lt;br /&gt; üìÑ Paper: &lt;a href="https://arxiv.org/abs/2504.05732"&gt;https://arxiv.org/abs/2504.05732&lt;/a&gt;&lt;br /&gt; üíª Code: &lt;a href="https://github.com/thunlp/LLMxMapReduce/"&gt;GitHub - thunlp/LLMxMapReduce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://surveygo.thunlp.org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fy0j/surveygoopen_deepresearch_automated_aigenerated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fy0j/surveygoopen_deepresearch_automated_aigenerated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T01:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5qqst</id>
    <title>Llama 4 Maverick Locally at 45 tk/s on a Single RTX 4090 - I finally got it working!</title>
    <updated>2025-04-23T04:38:26+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;I just wrapped up a follow-up demo where I got 45+ tokens per second out of Meta‚Äôs massive 400 billion-parameter, 128-expert Llama 4 Maverick, and I wanted to share the full setup in case it helps anyone else pushing these models locally. Here‚Äôs what made it possible: CPU: Intel Engineering Sample QYFS (similar to Xeon Platinum 8480+ with 56 cores / 112 threads) with AMX acceleration&lt;/p&gt; &lt;p&gt;GPU: Single NVIDIA RTX 4090 (no dual-GPU hack needed!) RAM: 512 GB DDR5 ECC OS: Ubuntu 22.04 LTS&lt;/p&gt; &lt;p&gt;Environment: K-Transformers support-llama4 branch&lt;/p&gt; &lt;p&gt;Below is the link to video : &lt;a href="https://youtu.be/YZqUfGQzOtk"&gt;https://youtu.be/YZqUfGQzOtk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're interested in the hardware build: &lt;a href="https://youtu.be/r7gVGIwkZDc"&gt;https://youtu.be/r7gVGIwkZDc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5qqst/llama_4_maverick_locally_at_45_tks_on_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T04:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6kjmo</id>
    <title>Time to get into LLM's in a big way this next Monday</title>
    <updated>2025-04-24T05:27:13+00:00</updated>
    <author>
      <name>/u/Guilty-History-9249</name>
      <uri>https://old.reddit.com/user/Guilty-History-9249</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My new system if finally being built and should be ready by Monday.&lt;/p&gt; &lt;p&gt;285K + 96GB's of DDR5-6600 + 5090 + uber fast SSD all on Ubuntu.&lt;/p&gt; &lt;p&gt;If the build shop could gotten me to 6600MHz on the AMD I would have went with the better(for gamers) 9950x3d.&lt;/p&gt; &lt;p&gt;While I certainly wouldn't want to run a large LLM totally in system ram as the dual channel nature of consumer CPU's is a bottleneck. But I do see running something like a 40B at Q8 model with 28GB's on the 5090 and 12gb's in system RAM. Squeezing a little more perhaps allows running a 70B class of models becomes workable.&lt;/p&gt; &lt;p&gt;So, I'm looking for suggestions as to what possibilities this'll open up in terms of &amp;quot;local quality&amp;quot; and training possibilities. I do python programming to make Stable Diffusion super fast(294 images per second at 512x512 on my 4090) so I can get into the low level stuff quite readily. I like to experiment and wonder what interesting things I could try on the new box.&lt;/p&gt; &lt;p&gt;NOTE: The more I think about it, instead of refurbishing my current system and selling it I'll likely have my 4090 moved to my new system as a little brother. Today I did tell the guy building it to upgrade the PS from 1200 watts to 1600 just in case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guilty-History-9249"&gt; /u/Guilty-History-9249 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6kjmo/time_to_get_into_llms_in_a_big_way_this_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6kjmo/time_to_get_into_llms_in_a_big_way_this_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6kjmo/time_to_get_into_llms_in_a_big_way_this_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T05:27:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5zum2</id>
    <title>Running 32b LLM with low VRAM (12Gb or less)</title>
    <updated>2025-04-23T13:58:24+00:00</updated>
    <author>
      <name>/u/Low-Woodpecker-4522</name>
      <uri>https://old.reddit.com/user/Low-Woodpecker-4522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that there is a huge performance penalty when the model doesn't fit on the VRAM, but considering the new low bit quantizations, and that you can find some 32b models that could fit in VRAM, I wonder if it's practical to run those models with low VRAM.&lt;/p&gt; &lt;p&gt;What are the speed results of running low bit imatrix quants of 32b models with 12Gb VRAM?&lt;br /&gt; What is your experience ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Woodpecker-4522"&gt; /u/Low-Woodpecker-4522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5zum2/running_32b_llm_with_low_vram_12gb_or_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:58:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5t2cq</id>
    <title>Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today</title>
    <updated>2025-04-23T07:12:28+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt; &lt;img alt="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" src="https://external-preview.redd.it/BKRijIKtfRZRNLNOU5KghR-oMM4YnWGWd_YjBkqgBfE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7741f3556461371cbf440be2d26db7ce7f09a007" title="Pytorch 2.7.0 with support for Blackwell (5090, B200) to come out today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This stable release of pytorch 2.7.0 should allow most projects to work with 5090 series out of the box without having to use nightly releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pytorch/pytorch.github.io/pull/1989/files"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5t2cq/pytorch_270_with_support_for_blackwell_5090_b200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T07:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5yw16</id>
    <title>Pattern-Aware Vector Database and ANN Algorithm</title>
    <updated>2025-04-23T13:15:32+00:00</updated>
    <author>
      <name>/u/yumojibaba</name>
      <uri>https://old.reddit.com/user/yumojibaba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt; &lt;img alt="Pattern-Aware Vector Database and ANN Algorithm" src="https://preview.redd.it/cwgw5y593lwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10ef81375a693303e9267eadf91b3c5c3a52d00f" title="Pattern-Aware Vector Database and ANN Algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are releasing the beta version of PatANN, a vector search framework we've been working on that takes a different approach to ANN search by leveraging pattern recognition within vectors before distance calculations.&lt;/p&gt; &lt;p&gt;Our benchmarks on standard datasets show that PatANN achieved 4- 10x higher QPS than existing solutions (HNSW, ScaNN, FAISS) while maintaining &amp;gt;99.9% recall.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fully asynchronous execution: Decomposes queries for parallel execution across threads&lt;/li&gt; &lt;li&gt;True hybrid memory management: Works efficiently both in-memory and on-disk&lt;/li&gt; &lt;li&gt;Pattern-aware search algorithm that addresses hubness effects in high-dimensional spaces&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We have posted technical documentation and initial benchmarks at &lt;a href="https://patann.dev"&gt;https://patann.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a beta release, and work is in progress, so we are particularly interested in feedback on stability, integration experiences, and performance in different workloads, especially those working with large-scale vector search applications.&lt;/p&gt; &lt;p&gt;We invite you to download code samples from the GitHub repo (Python, Android (Java/Kotlin), iOS (Swift/Obj-C)) and try them out. We look forward to feedback. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yumojibaba"&gt; /u/yumojibaba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwgw5y593lwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5yw16/patternaware_vector_database_and_ann_algorithm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T13:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6fj84</id>
    <title>Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</title>
    <updated>2025-04-24T00:59:16+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively learn difficult subgoals that elude autoregressive approaches. We propose Multi-Granularity Diffusion Modeling (MGDM), which prioritizes subgoals based on difficulty during learning. On complex tasks like Countdown, Sudoku, and Boolean Satisfiability Problems, MGDM significantly outperforms autoregressive models without using search techniques. For instance, MGDM achieves 91.5\% and 100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and 20.7\% for autoregressive models. Our work highlights the potential of diffusion-based approaches in advancing AI capabilities for sophisticated language understanding and problem-solving tasks. All associated codes are available at &lt;a href="https://github.com/HKUNLP/diffusion-vs-ar"&gt;https://github.com/HKUNLP/diffusion-vs-ar&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2410.14157v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fj84/beyond_autoregression_discrete_diffusion_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6fj84/beyond_autoregression_discrete_diffusion_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T00:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k63o9h</id>
    <title>Aider appreciation post</title>
    <updated>2025-04-23T16:34:33+00:00</updated>
    <author>
      <name>/u/myoddity</name>
      <uri>https://old.reddit.com/user/myoddity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aider-chat just hits too right for me.&lt;/p&gt; &lt;p&gt;It is powerful, yet light and clean. &lt;/p&gt; &lt;p&gt;It lives in terminal, yet is simply approachable. &lt;/p&gt; &lt;p&gt;It can do all the work, yet encourages to bring-your-own-context.&lt;/p&gt; &lt;p&gt;It's free, yet it just works. &lt;/p&gt; &lt;p&gt;What more is needed, for one who can code, yet cannot code.&lt;/p&gt; &lt;p&gt;(Disclaimer: No chatgpt was used to write this. Only heart.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/myoddity"&gt; /u/myoddity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k63o9h/aider_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6h0au</id>
    <title>Just upgraded from an M1 MacBook Pro to an m4 MacBook Pro... Anyone else get load coil whine with LLMs?</title>
    <updated>2025-04-24T02:13:28+00:00</updated>
    <author>
      <name>/u/cmndr_spanky</name>
      <uri>https://old.reddit.com/user/cmndr_spanky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(load = loud .. but honestly its not loud relatively speaking :) )&lt;/p&gt; &lt;p&gt;My M1 was dead silent, my new M4 MacBook Pro running a model in Ollama makes a very noticeable fast chirping sound (It's very faint, but noticeable and not something the M1 Pro had). Anyone else experience this or is there something wrong with this thing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmndr_spanky"&gt; /u/cmndr_spanky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6h0au/just_upgraded_from_an_m1_macbook_pro_to_an_m4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T02:13:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6jxpe</id>
    <title>How good is QwQ 32B's OCR?</title>
    <updated>2025-04-24T04:49:28+00:00</updated>
    <author>
      <name>/u/Impressive_Chicken_</name>
      <uri>https://old.reddit.com/user/Impressive_Chicken_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it the same as Qwen2.5 VL? I need a model to analyse Mathematics and Physics textbooks, and QwQ seems to be the best in reasoning at its size, but i don't know if it could handle the complex images in them. The Kaggle page for QwQ doesn't mention images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Chicken_"&gt; /u/Impressive_Chicken_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jxpe/how_good_is_qwq_32bs_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jxpe/how_good_is_qwq_32bs_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jxpe/how_good_is_qwq_32bs_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6k1df</id>
    <title>How much vram do you have?</title>
    <updated>2025-04-24T04:55:40+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm doing some research for my local inference engine project. I‚Äôll follow up with more polls. Thanks for participating!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1k6k1df"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1df/how_much_vram_do_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1df/how_much_vram_do_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1df/how_much_vram_do_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k60mlw</id>
    <title>LaSearch: Fully local semantic search app (with CUSTOM "embeddings" model)</title>
    <updated>2025-04-23T14:31:25+00:00</updated>
    <author>
      <name>/u/joelkunst</name>
      <uri>https://old.reddit.com/user/joelkunst</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt; &lt;img alt="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" src="https://external-preview.redd.it/aDV1d2g4MTRobHdlMcf6y9HMrkeunVjc93oLf19y0pwTcXwF2-pbO3PezUgo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb00308374c95d7c2665d585962792eeb747f97c" title="LaSearch: Fully local semantic search app (with CUSTOM &amp;quot;embeddings&amp;quot; model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have build my own &amp;quot;embeddings&amp;quot; model that's ultra small and lightweight. It does not function in the same way as usual ones and is not as powerful as they are, but it's orders of magnitude smaller and faster.&lt;/p&gt; &lt;p&gt;It powers my fully local semantic search app.&lt;/p&gt; &lt;p&gt;No data goes outside of your machine, and it uses very little resources to function.&lt;/p&gt; &lt;p&gt;MCP server is coming so you can use it to get relevant docs for RAG.&lt;/p&gt; &lt;p&gt;I've been testing with a small group but want to expand for more diverse feedback. If you're interested in trying it out or have any questions about the technology, let me know in the comments or sign up on the website.&lt;/p&gt; &lt;p&gt;Would love your thoughts on the concept and implementation!&lt;br /&gt; &lt;a href="https://lasearch.app"&gt;https://lasearch.app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joelkunst"&gt; /u/joelkunst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/31aodc14hlwe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k60mlw/lasearch_fully_local_semantic_search_app_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T14:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6k0jr</id>
    <title>What GPU do you use?</title>
    <updated>2025-04-24T04:54:14+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm doing some research for my local inference engine project. I‚Äôll follow up with more polls. Thanks for participating!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1k6k0jr"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k0jr/what_gpu_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k0jr/what_gpu_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k0jr/what_gpu_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:54:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k65cmy</id>
    <title>Unpopular Opinion: I'm Actually Loving Llama-4-Scout</title>
    <updated>2025-04-23T17:41:40+00:00</updated>
    <author>
      <name>/u/Far_Buyer_7281</name>
      <uri>https://old.reddit.com/user/Far_Buyer_7281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of negativity surrounding the new Llama-4-Scout, and I wanted to share my experience is completely different. I love especially the natural tone and large context understanding &lt;/p&gt; &lt;p&gt;I'm curious to hear if anyone else is having a positive experience with Llama-4-Scout, or if there are specific use cases where it shines. What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far_Buyer_7281"&gt; /u/Far_Buyer_7281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k65cmy/unpopular_opinion_im_actually_loving_llama4scout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k665cg</id>
    <title>Anyone try UI-TARS-1.5-7B new model from ByteDance</title>
    <updated>2025-04-23T18:13:14+00:00</updated>
    <author>
      <name>/u/Muted-Celebration-47</name>
      <uri>https://old.reddit.com/user/Muted-Celebration-47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt; &lt;img alt="Anyone try UI-TARS-1.5-7B new model from ByteDance" src="https://external-preview.redd.it/MEb00d1gLWxp1-4OYIxzng3fr7CjMC7BtYqeV0pZ5Zc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79e173fc5596ae79fab0463a41a31cb51d11923" title="Anyone try UI-TARS-1.5-7B new model from ByteDance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summary, It allows AI to use your computer or web browser.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**Edit**&lt;br /&gt; I managed to make it works with gemma3:27b. But it still failed to find the correct coordinate in &amp;quot;Computer use&amp;quot; mode.&lt;/p&gt; &lt;p&gt;Here the steps:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Dowload gemma3:27b with ollama =&amp;gt; ollama run gemma3:27b 2. Increase context length at least 16k (16384) 3. Download UI-TARS Desktop 4. Click setting =&amp;gt; select provider: Huggingface for UI-TARS-1.5; base url: http://localhost:11434/v1; API key: test; model name: gemma3:27b; save; 5. Select &amp;quot;Browser use&amp;quot; and try &amp;quot;Go to google and type reddit in the search box and hit Enter (DO NOT ctrl+c)&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried to use it with Ollama and connected it to UI-TARS Desktop, but it failed to follow the prompt. It just took multiple screenshots. What's your experience with it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8sfb6fc8lmwe1.png?width=1737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=38228461bcca820366ff63549025975f0070f5ec"&gt;UI TARS Desktop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Muted-Celebration-47"&gt; /u/Muted-Celebration-47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k665cg/anyone_try_uitars157b_new_model_from_bytedance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T18:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k650xj</id>
    <title>The best translator is a hybrid translator - combining a corpus of LLMs</title>
    <updated>2025-04-23T17:28:49+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"&gt; &lt;img alt="The best translator is a hybrid translator - combining a corpus of LLMs" src="https://external-preview.redd.it/DtSOjZDQhCIQrR9MXzfYsDwli-PvO8iAuPXRBhYivls.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33bb3dd09e1348f194cfb304ced2dd662da82a0f" title="The best translator is a hybrid translator - combining a corpus of LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/the_best_translator_is_a_hybrid_translator"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k650xj/the_best_translator_is_a_hybrid_translator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:28:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6jslj</id>
    <title>LLM content on YT becoming repetitive</title>
    <updated>2025-04-24T04:40:57+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been following the discussion and content around LLMs very closely from the beginning of the AI craze on youtube and am subscribed to most LLM related channels. While in the beginning and well throughout most of the last one or two years there was a ton of new content every day, covering all aspects. Content felt very diverse. From RAG to inference, to evals and frameworks like Dspy, chunking strategies and ingestion pipelines, fine tuning libraries like unsloth and agentic frameworks like crewAI and autogen. Or of course the AI IDEs like cursor and windsurf and things like liteLLM need to be mentioned as well, and there's many more which don't come to mind right now.&lt;/p&gt; &lt;p&gt;Fast forward to today and the channels are still around, but they seem to cover only specific topics like MCP and then all at once. Clearly, once something new has been talked about you can't keep bringing it up. But at the same time I have a hard time believing that even in those established projects there's nothing new to talk about.&lt;/p&gt; &lt;p&gt;There would be so much room to speak about the awesome stuff you could do with all these tools, but to me it seems content creators have fallen into a routine. Do you share the same impression? What are channels you are watching that keep bringing innovative and inspiring content still at this stage of where the space has gotten to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k655wa</id>
    <title>LlamaCon is in 6 days</title>
    <updated>2025-04-23T17:34:15+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt; &lt;img alt="LlamaCon is in 6 days" src="https://external-preview.redd.it/__nAvfZl_lg7YJNwP-IInXWe8ebatQ8ExlHyPqG5yUM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=953181b2ca7b3814274602f6fa358f0bc7519113" title="LlamaCon is in 6 days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kcvsj160emwe1.png?width=597&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f3a091dd458f203a46e49bc23ef13ce69aeeda"&gt;Zuck, Ghodsi, Nadella&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü¶ô &lt;strong&gt;LlamaCon ‚Äì April 29, 2025&lt;/strong&gt;&lt;br /&gt; Meta's first-ever developer conference dedicated to their open-source AI, held &lt;strong&gt;in person&lt;/strong&gt; at Meta HQ in Menlo Park, CA ‚Äî with &lt;strong&gt;select sessions live-streamed online&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Agenda:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10:00 AM PST ‚Äì LlamaCon Keynote&lt;/strong&gt;&lt;br /&gt; Celebrating the open-source community and showcasing the latest in the Llama model ecosystem.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Chris Cox ‚Äì Chief Product Officer, Meta&lt;br /&gt; ‚Ä¢ Manohar Paluri ‚Äì VP of AI, Meta&lt;br /&gt; ‚Ä¢ Angela Fan ‚Äì Research Scientist in Generative AI, Meta&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10:45 AM PST ‚Äì A Conversation with Mark Zuckerberg &amp;amp; Ali Ghodsi&lt;/strong&gt;&lt;br /&gt; Open source AI, building with LLMs, and advice for founders.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Mark Zuckerberg ‚Äì Founder &amp;amp; CEO, Meta&lt;br /&gt; ‚Ä¢ Ali Ghodsi ‚Äì Co-founder &amp;amp; CEO, Databricks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4:00 PM PST ‚Äì A Conversation with Mark Zuckerberg &amp;amp; Satya Nadella&lt;/strong&gt;&lt;br /&gt; AI trends, real-world applications, and future outlooks.&lt;br /&gt; &lt;strong&gt;Speakers:&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Mark Zuckerberg ‚Äì Founder &amp;amp; CEO, Meta&lt;br /&gt; ‚Ä¢ Satya Nadella ‚Äì Chairman &amp;amp; CEO, Microsoft&lt;/p&gt; &lt;p&gt;üîó &lt;a href="https://www.llama.com/events/llamacon/2025/?utm_source=llama-home&amp;amp;utm_medium=llama-referral&amp;amp;utm_campaign=llama-utm&amp;amp;utm_offering=llamacon-learnmore&amp;amp;utm_product=llama"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k655wa/llamacon_is_in_6_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T17:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k63kpq</id>
    <title>A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis</title>
    <updated>2025-04-23T16:30:32+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"&gt; &lt;img alt="A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis" src="https://external-preview.redd.it/mWvSuKRH-R24cuYFUnmlYRdyhyET4x6NAvj4TSYw978.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a3b94039246ff5688098f9a39ede8e0e6a75a64" title="A summary of the progress AMD has made to improve it's AI capabilities in the past 4 months from SemiAnalysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this report, we will discuss the many positive changes AMD has made. They are on the right track but need to increase the R&amp;amp;D budget for GPU hours and make further investments in AI talent. We will provide additional recommendations and elaborate on AMD management‚Äôs blind spot: how they are uncompetitive in the race for AI Software Engineers due to compensation structure benchmarking to the wrong set of companies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjUvMDQvMjMvYW1kLTItMC1uZXctc2Vuc2Utb2YtdXJnZW5jeS1taTQ1MHgtY2hhbmNlLXRvLWJlYXQtbnZpZGlhLW52aWRpYXMtbmV3LW1vYXQvIl19LCJleHAiOjE3NDgwMDM1MTgsImlhdCI6MTc0NTQxMTUxOCwiaXNzIjoiaHR0cHM6Ly9zZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIyaUFXTUs0U0F2RFU3WkpaTGdzR2NYIiwidXNlIjoiYWNjZXNzIn0.K4tPYV6TgV6HszD-hFW0Vql1f9IXKrEx9ZjL2SxfSXAqHYkdk4uCxhwq_Iu4oWCjSyXPCveZLaNDQ19GD3ua9Q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k63kpq/a_summary_of_the_progress_amd_has_made_to_improve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T16:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6k1pq</id>
    <title>What OS do you use?</title>
    <updated>2025-04-24T04:56:13+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm doing some research for my local inference engine project. I‚Äôll follow up with more polls. Thanks for participating!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1k6k1pq"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5x7a2</id>
    <title>Created a calculator for modelling GPT token-generation throughput</title>
    <updated>2025-04-23T11:52:09+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt; &lt;img alt="Created a calculator for modelling GPT token-generation throughput" src="https://external-preview.redd.it/blv2LZ-IrTm3FyQojwoj082So0qC55XGIytRyhb8H3w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff38a981027aac6178b194fa35693fd435150d30" title="Created a calculator for modelling GPT token-generation throughput" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.desmos.com/calculator/qtkabsqhxt"&gt;https://www.desmos.com/calculator/qtkabsqhxt&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k5x7a2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5x7a2/created_a_calculator_for_modelling_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6hah2</id>
    <title>SmolBoi: watercooled 3x RTX 3090 FE &amp; EPYC 7642 in O11D (with build pics)</title>
    <updated>2025-04-24T02:25:43+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt; &lt;img alt="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" src="https://a.thumbs.redditmedia.com/wDcGZwo_b01L-EApmvhJ6AR3vHGWlV4tE6C_nRqL234.jpg" title="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;The initial idea for build started with a single RTX 3090 FE I bought about a year and a half ago, right after the crypto crash. Over the next few months, I bought two more 3090 FEs. &lt;/p&gt; &lt;p&gt;From the beginning, my criteria for this build were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Buy components based on good deals I find in local classifieds, ebay, or tech forums.&lt;/li&gt; &lt;li&gt;Everything that can be bought 2nd hand, shall be bought 2nd hand.&lt;/li&gt; &lt;li&gt;I already had a Lian Li O11D case (not XL, not Evo), so everything shall fit there.&lt;/li&gt; &lt;li&gt;Watercooled to keep noise and temps low despite the size.&lt;/li&gt; &lt;li&gt;ATX motherboard to give myself a bit more space inside the case.&lt;/li&gt; &lt;li&gt;Xeon Scalable or Epyc: I want plenty PCIe lanes, U.2 for storage, lots of RAM, plenty of bandwidth, and I want it cheap.&lt;/li&gt; &lt;li&gt;U.2 SSDs because they're cheaper and more reliable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took a couple more months to source all components, but in the end, here is what ended in this rig, along with purchase price:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro H12SSL-i: 300‚Ç¨.&lt;/li&gt; &lt;li&gt;AMD EPYC 7642: 220‚Ç¨ (bought a few of those together)&lt;/li&gt; &lt;li&gt;512GB 8x64GB Samsung DDR4-2666 ECCRDIMM: 350‚Ç¨&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE: 1550‚Ç¨&lt;/li&gt; &lt;li&gt;2x Samsung PM1735 1.6TB U.2 Gen 4 SSD: 125‚Ç¨&lt;/li&gt; &lt;li&gt;256GB M.2 Gen 3 NVME: 15‚Ç¨&lt;/li&gt; &lt;li&gt;4x Bykski waterblocks: 60‚Ç¨/block&lt;/li&gt; &lt;li&gt;Bykski waterblock GPU bridge: 24‚Ç¨&lt;/li&gt; &lt;li&gt;Alphacool Eisblock XPX Pro 1U: 65‚Ç¨&lt;/li&gt; &lt;li&gt;EVGA 1600W PSU: 100‚Ç¨&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE 21-pin power adapter cable: 45‚Ç¨&lt;/li&gt; &lt;li&gt;3x PCIe Gen 4 x16 risers: 70‚Ç¨&lt;/li&gt; &lt;li&gt;EK 360mm 45mm + 2x alphacool 360mm 30mm: 100‚Ç¨&lt;/li&gt; &lt;li&gt;EK Quantum Kinetic 120mm reservoir: 35‚Ç¨&lt;/li&gt; &lt;li&gt;Xylem D5 pump: 35‚Ç¨&lt;/li&gt; &lt;li&gt;10x Arctic P12 Max: 70‚Ç¨ (9 used)&lt;/li&gt; &lt;li&gt;Arctic P8 Max: 5‚Ç¨&lt;/li&gt; &lt;li&gt;tons of fittings from Aliexpress: 50-70‚Ç¨&lt;/li&gt; &lt;li&gt;Lian Li X11 upright GPU mount: 15‚Ç¨&lt;/li&gt; &lt;li&gt;Anti-sagging GPU brace: 8‚Ç¨&lt;/li&gt; &lt;li&gt;5M fishtank 10x13mm PVC tube: 10‚Ç¨&lt;/li&gt; &lt;li&gt;Custom Aluminum plate for upright GPU mount: 45‚Ç¨&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total: ~3400‚Ç¨&lt;/p&gt; &lt;p&gt;I'm excluding the Mellanox ConnextX-3 56gb infiniband. It's not technically needed, and it was like 13‚Ç¨.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, it's a pretty tight fit. Took a lot of planning and redesign to make everything fit in.&lt;/p&gt; &lt;p&gt;My initial plan was to just plug the watercooled cards into the motherboard witha triple bridge (Bykski sells those, and they'll even make you a custom bridge if you ask nicely, which is why I went for their blocks). Unbeknown to me, the FE cards I went with because they're shorter (I thought easier fit) are also quite a bit taller than reference cards. This made it impossible to fit the cards in the case, as even low profile fitting adapter (the piece that converts the ports on the block to G1/4 fittings) was too high to fit in my case. I explored other case options that could fit three 360mm radiators but couldn't find any that would also have enough height for the blocks.&lt;/p&gt; &lt;p&gt;This height issue necessitated a radical rethinking of how I'd fit the GPUs. I started playing with one GPU with the block attached inside the case to see how I could fit them, and the idea of dangling two from the top of the case was born. I knew Lian Li sold the upright GPU mount, but that was for the EVO. I didn't want to buy the EVO because that would mean reducing the top radiator to 240mm, and I wanted that to be 45mm to do the heavy lifting of removing most heat. &lt;/p&gt; &lt;p&gt;I used my rudimentary OpenSCAD skills to design a plate that would screw to a 120mm fan and provide mounting holes for the upright GPU bracket. With that, I could hang two GPUs. I used JLCPCB to make 2 of them. With two out of the way, finding a place for the 3rd GPU was much easier. The 2nd plate ended having the perfect hole spacing for mounting the PCIe riser connector, providing a base for the 3rd GPU. An anti-sagging GPU brace provided the last bit of support needed to keep the 3rd GPU safe.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, the aluminum (2mm 7075) plate is bent. This was because the case was left on it's side with the two GPUs dangling for well over a month. It was supposed to a few hours, but health issues stopped the build abruptly. The motherboard also died on me (common issue with H12SSL, cost 50‚Ç¨ to fix at Supermicro, including shipping. Motherboard price includes repair cost), which delayed things further. The pictures are from reassembling after I got it back.&lt;/p&gt; &lt;p&gt;The loop (from coldest side) out of the bottom radiator, into the two GPUs, on to the the 3rd GPU, then pump, into the CPU, onwards to the top radiator, leading to the side radiator, and back to the bottom radiator. Temps on the GPUs peak ~51C so far. Though the board's BMC monitors GPU temps directly (I didn't know it could), having the warmest water go to the CPU means the fans will ramp up even if there's no CPU load. The pump PWM is not connected, keeping it at max rpm on purpose for high circulation. Cooling is provided by distilled water with a few drops of Iodine. Been running that on my quad P40 rig for months now without issue.&lt;/p&gt; &lt;p&gt;At idle, the rig is very quiet. Fans idle at 1-1.1k rpm. Haven't checked RPM under load.&lt;/p&gt; &lt;p&gt;Model storage is provided by the two Gen4 PM1735s in RAID0 configuration. Haven't benchmarked them yet, but I saw 13GB/s on nvtop while loading Qwen 32B and Nemotron 49B. The GPUs report Gen4 X16 in nvtop, but I haven't checked for errors. I am blowen by the speed with which models load from disk, even when I tested with --no-mmap.&lt;/p&gt; &lt;p&gt;DeepSeek V3 is still downloading...&lt;/p&gt; &lt;p&gt;And now, for some LLM inference numbers using llama.cpp (b5172). I filled the loop yesterday and got Ubuntu installed today, so I haven't gotten to try vLLM yet. GPU power is the default 350W. Apart from Gemma 3 QAT, all models are Q8.&lt;/p&gt; &lt;h3&gt;Mistral-Small-3.1-24B-Instruct-2503 with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -md /models/Mistral-Small-3.1-DRAFT-0.5B.Q8_0.gguf -fa -sm row --no-mmap -ngl 99 -ngld 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --device-draft CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.35&lt;/td&gt; &lt;td&gt;1044&lt;/td&gt; &lt;td&gt;30.92&lt;/td&gt; &lt;td&gt;34347.16&lt;/td&gt; &lt;td&gt;1154&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.29055 ( 446 accepted / 1535 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Mistral-Small-3.1-24B no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -fa -sm row --no-mmap -ngl 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.06&lt;/td&gt; &lt;td&gt;992&lt;/td&gt; &lt;td&gt;30.41&lt;/td&gt; &lt;td&gt;33205.86&lt;/td&gt; &lt;td&gt;1102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -md /models/gemma-3-1b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 -ngld 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA0,CUDA1 --device-draft CUDA0 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;151.36&lt;/td&gt; &lt;td&gt;1806&lt;/td&gt; &lt;td&gt;14.87&lt;/td&gt; &lt;td&gt;122161.81&lt;/td&gt; &lt;td&gt;1913&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.23570 ( 787 accepted / 3339 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27b no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;152.85&lt;/td&gt; &lt;td&gt;1957&lt;/td&gt; &lt;td&gt;20.96&lt;/td&gt; &lt;td&gt;94078.01&lt;/td&gt; &lt;td&gt;2064&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;QwQ-32B.Q8&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/QwQ-32B.Q8_0.gguf --temp 0.6 --top-k 40 --repeat-penalty 1.1 --min-p 0.0 --dry-multiplier 0.5 -fa -sm row --no-mmap -ngl 99 --port 9008 -c 80000 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;132.51&lt;/td&gt; &lt;td&gt;2313&lt;/td&gt; &lt;td&gt;19.50&lt;/td&gt; &lt;td&gt;119326.49&lt;/td&gt; &lt;td&gt;2406&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B QAT Q4&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-q4_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row -ngl 99 -c 65536 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0 --tensor-split 1,0,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9004 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1042.04&lt;/td&gt; &lt;td&gt;2411&lt;/td&gt; &lt;td&gt;36.13&lt;/td&gt; &lt;td&gt;2673.49&lt;/td&gt; &lt;td&gt;2424&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;634.28&lt;/td&gt; &lt;td&gt;14505&lt;/td&gt; &lt;td&gt;24.58&lt;/td&gt; &lt;td&gt;385537.97&lt;/td&gt; &lt;td&gt;23418&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Qwen2.5-Coder-32B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf --top-k 20 -fa --top-p 0.9 --min-p 0.1 --temp 0.7 --repeat-penalty 1.05 -sm row -ngl 99 -c 65535 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9005 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.50&lt;/td&gt; &lt;td&gt;11709&lt;/td&gt; &lt;td&gt;15.48&lt;/td&gt; &lt;td&gt;558661.10&lt;/td&gt; &lt;td&gt;19390&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Llama-3_3-Nemotron-Super-49B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Llama-3_3-Nemotron-Super-49B/nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q8_0-00001-of-00002.gguf -fa -sm row -ngl 99 -c 32768 --device CUDA0,CUDA1,CUDA2 --tensor-split 1,1,1 --slots --metrics --numa distribute -t 40 --no-mmap --port 9001 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;120.56&lt;/td&gt; &lt;td&gt;1164&lt;/td&gt; &lt;td&gt;17.21&lt;/td&gt; &lt;td&gt;68414.89&lt;/td&gt; &lt;td&gt;1259&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;70.11&lt;/td&gt; &lt;td&gt;11644&lt;/td&gt; &lt;td&gt;14.58&lt;/td&gt; &lt;td&gt;274099.28&lt;/td&gt; &lt;td&gt;13219&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k6hah2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T02:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5wdw0</id>
    <title>HP wants to put a local LLM in your printers</title>
    <updated>2025-04-23T11:05:18+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt; &lt;img alt="HP wants to put a local LLM in your printers" src="https://preview.redd.it/9wawej40hkwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f75beba5aa65b4f7a42767d2301f3c23268219c3" title="HP wants to put a local LLM in your printers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wawej40hkwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6ably</id>
    <title>Bartowski just updated his glm-4-32B quants. working in lmstudio soon?</title>
    <updated>2025-04-23T21:02:39+00:00</updated>
    <author>
      <name>/u/ieatrox</name>
      <uri>https://old.reddit.com/user/ieatrox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt; &lt;img alt="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ieatrox"&gt; /u/ieatrox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T21:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6je2v</id>
    <title>Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model</title>
    <updated>2025-04-24T04:16:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt; &lt;img alt="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" src="https://external-preview.redd.it/RTiZ46sO11nfXyNlVM8vyr9cqgUVM4y93u2zm8v-5Bg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df7083b743c70efd512caf939d946bd65171d252" title="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V2-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:16:54+00:00</published>
  </entry>
</feed>
