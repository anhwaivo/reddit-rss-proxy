<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-13T04:46:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ly983h</id>
    <title>Local Llama with Home Assistant Integration and Multilingual-Fuzzy naming</title>
    <updated>2025-07-12T19:43:22+00:00</updated>
    <author>
      <name>/u/NicolaZanarini533</name>
      <uri>https://old.reddit.com/user/NicolaZanarini533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! First time poster - thought I'd share a project I've been working on - it's local LLama integration with HA and custom functions outside of HA; my main goal was to have a system that could understand descriptions of items instead of hard-names (like &amp;quot;turn on the light above the desk&amp;quot; instead of &amp;quot;turn on the desk light&amp;quot; and which could do so in multiple languages, without having to use English words in Spanish (for example).&lt;/p&gt; &lt;p&gt;Project is still in the early stages but I do have ideas for it an intend to develop it further - feedback and thoughts are appreciated!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Nemesis533/Local_LLHAMA/"&gt;https://github.com/Nemesis533/Local_LLHAMA/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S - had to re-do the post as the other one was done with the wrong account.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NicolaZanarini533"&gt; /u/NicolaZanarini533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnwtg</id>
    <title>Does this mean it’s likely not gonna be open source?</title>
    <updated>2025-07-12T01:15:34+00:00</updated>
    <author>
      <name>/u/I_will_delete_myself</name>
      <uri>https://old.reddit.com/user/I_will_delete_myself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt; &lt;img alt="Does this mean it’s likely not gonna be open source?" src="https://preview.redd.it/awwe19btgccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60378b44c9da263732f5cf2435d56a487edcf966" title="Does this mean it’s likely not gonna be open source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_will_delete_myself"&gt; /u/I_will_delete_myself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awwe19btgccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyg6z</id>
    <title>Have you tried that new devstral?! Myyy! The next 8x7b?</title>
    <updated>2025-07-12T11:43:56+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been here since llama1 area.. what a crazy ride!&lt;br /&gt; Now we have that little devstral 2507.&lt;br /&gt; To me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).&lt;br /&gt; Do you feel the same thing? Ho my.. open weights models won't be as fun without Mistral 🇨🇵&lt;/p&gt; &lt;p&gt;(To me it feels like 8x7b again but better 😆 )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmr2h</id>
    <title>Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt; &lt;img alt="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6bbfe217f90907d855e12d2f6b2845d320a54e6" title="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;✅ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ah6imcae6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyitq9</id>
    <title>Any suggestions for generating academic-style/advanced plots?</title>
    <updated>2025-07-13T03:25:25+00:00</updated>
    <author>
      <name>/u/plsendfast</name>
      <uri>https://old.reddit.com/user/plsendfast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA community,&lt;/p&gt; &lt;p&gt;I am a researcher, and recently I have noticed that LLMs such as OpenAI's and Google's are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn’t work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.&lt;/p&gt; &lt;p&gt;Has anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!&lt;/p&gt; &lt;p&gt;TL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/plsendfast"&gt; /u/plsendfast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxr5s3</id>
    <title>Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps</title>
    <updated>2025-07-12T04:06:23+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt; &lt;img alt="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" src="https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cc494cf2008a19ce100d156817257c3630b664e" title="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md"&gt;ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt; &lt;p&gt;Be careful of the DRAM OOM.&lt;/p&gt; &lt;p&gt;It is a Big Beautiful Model.&lt;br /&gt; Enjoy it&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T04:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyen05</id>
    <title>Laptop GPU for Agentic Coding -- Worth it?</title>
    <updated>2025-07-12T23:48:50+00:00</updated>
    <author>
      <name>/u/randomqhacker</name>
      <uri>https://old.reddit.com/user/randomqhacker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone who actually codes with local LLM on their laptops, what's your setup and are you happy with the quality and speed? Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomqhacker"&gt; /u/randomqhacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T23:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxw3zz</id>
    <title>We built an open-source medical triage benchmark</title>
    <updated>2025-07-12T09:12:26+00:00</updated>
    <author>
      <name>/u/Significant-Pair-275</name>
      <uri>https://old.reddit.com/user/Significant-Pair-275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the &amp;quot;digital front door&amp;quot; for health concerns—replacing the instinct to just Google it.&lt;/p&gt; &lt;p&gt;Getting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;strong&gt;TriageBench&lt;/strong&gt;, a reproducible framework for evaluating LLM triage accuracy. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standard clinical dataset (Semigran vignettes)&lt;/li&gt; &lt;li&gt;Paired McNemar's test to detect model performance differences on small datasets&lt;/li&gt; &lt;li&gt;Full methodology and evaluation code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/medaks/medask-benchmark"&gt;https://github.com/medaks/medask-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MedAsk: &lt;strong&gt;87.6% accuracy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;o3: &lt;strong&gt;75.6%&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;GPT‑4.5: &lt;strong&gt;68.9%&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main limitation is dataset size (45 vignettes). We're looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.&lt;/p&gt; &lt;p&gt;Blog post with full results: &lt;a href="https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/"&gt;https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Pair-275"&gt; /u/Significant-Pair-275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T09:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly5g2t</id>
    <title>What's the most natural sounding TTS model for local right now?</title>
    <updated>2025-07-12T17:04:18+00:00</updated>
    <author>
      <name>/u/Siigari</name>
      <uri>https://old.reddit.com/user/Siigari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I'm working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?&lt;/p&gt; &lt;p&gt;I saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that's local that sounds pretty good?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Siigari"&gt; /u/Siigari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly7sb0</id>
    <title>[Rust] qwen3-rs: Educational Qwen3 Architecture Inference (No Python, Minimal Deps)</title>
    <updated>2025-07-12T18:41:55+00:00</updated>
    <author>
      <name>/u/eis_kalt</name>
      <uri>https://old.reddit.com/user/eis_kalt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all!&lt;br /&gt; I've just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Educational:&lt;/strong&gt; Core algorithms are reimplemented from scratch for learning and transparency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CLI tools:&lt;/strong&gt; Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular:&lt;/strong&gt; Clean separation between export, inference, and CLI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Future plans:&lt;/strong&gt; I would be curious to see how to extend it to support: &lt;ul&gt; &lt;li&gt;fine-tuning of a small models&lt;/li&gt; &lt;li&gt;optimize inference performance (e.g. matmul operations)&lt;/li&gt; &lt;li&gt;WASM build to run inference in a browser&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, I used &lt;a href="https://github.com/adriancable/qwen3.c"&gt;qwen3.c&lt;/a&gt; as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.&lt;/p&gt; &lt;p&gt;GitHub: [&lt;a href="https://github.com/reinterpretcat/qwen3-rs%5D(vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)"&gt;https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eis_kalt"&gt; /u/eis_kalt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T18:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxpidc</id>
    <title>Where that Unsloth Q0.01_K_M GGUF at?</title>
    <updated>2025-07-12T02:37:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt; &lt;img alt="Where that Unsloth Q0.01_K_M GGUF at?" src="https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4380544f532ff369f435679247aa08f3c9afdb66" title="Where that Unsloth Q0.01_K_M GGUF at?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2em6rucvccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T02:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyb8tz</id>
    <title>Banana for scale</title>
    <updated>2025-07-12T21:11:10+00:00</updated>
    <author>
      <name>/u/blackwell_tart</name>
      <uri>https://old.reddit.com/user/blackwell_tart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"&gt; &lt;img alt="Banana for scale" src="https://preview.redd.it/3gsbxg74eicf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f149ec7379ca441b40ea5b4c75ffb6609f7405" title="Banana for scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackwell_tart"&gt; /u/blackwell_tart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gsbxg74eicf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T21:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyj81f</id>
    <title>Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)</title>
    <updated>2025-07-13T03:47:18+00:00</updated>
    <author>
      <name>/u/mathsTeacher82</name>
      <uri>https://old.reddit.com/user/mathsTeacher82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia. Google Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024. Any open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/vJjgtOcXq8A"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mathsTeacher82"&gt; /u/mathsTeacher82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyjgwv</id>
    <title>[Help] Fastest model for real-time UI automation? (Browser-Use too slow)</title>
    <updated>2025-07-13T04:00:54+00:00</updated>
    <author>
      <name>/u/BulkyAd7044</name>
      <uri>https://old.reddit.com/user/BulkyAd7044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I’ve been using &lt;strong&gt;Browser-Use&lt;/strong&gt;, which is solid for tracking state/actions, but execution is too slow — especially when an LLM is in the loop at each step.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example flow (on Google settings):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://myaccount.google.com"&gt;myaccount.google.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click “Data &amp;amp; privacy”&lt;/li&gt; &lt;li&gt;Scroll down&lt;/li&gt; &lt;li&gt;Click “Delete a service or your account”&lt;/li&gt; &lt;li&gt;Click “Delete your Google Account”&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Looking for suggestions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest models for small structured decision tasks &lt;/li&gt; &lt;li&gt;Ways to be under 1s per step (ideally &amp;lt;500ms)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don’t need full chat reasoning — just high-confidence decisions from small JSON lists.&lt;/p&gt; &lt;p&gt;Would love to hear what setups/models have worked for you in similar low-latency UI agent tasks 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BulkyAd7044"&gt; /u/BulkyAd7044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly894z</id>
    <title>mlx-community/Kimi-Dev-72B-4bit-DWQ</title>
    <updated>2025-07-12T19:01:40+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt; &lt;img alt="mlx-community/Kimi-Dev-72B-4bit-DWQ" src="https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d6276c1fe0578c79ffa2210b8dfa820b87e4242" title="mlx-community/Kimi-Dev-72B-4bit-DWQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnsh1</id>
    <title>OpenAI delays its open weight model again for "safety tests"</title>
    <updated>2025-07-12T01:09:38+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt; &lt;img alt="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" src="https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe88bccce70567bd39edea238607127c143134db" title="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5xvjxzefccf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxycdh</id>
    <title>Safety first, or whatever🙄</title>
    <updated>2025-07-12T11:37:36+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt; &lt;img alt="Safety first, or whatever🙄" src="https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326cdedce8274c918a8336924d8741c3576c2f5a" title="Safety first, or whatever🙄" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/idk5uvesjfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyfngg</id>
    <title>How do you keep up with all these things?</title>
    <updated>2025-07-13T00:39:07+00:00</updated>
    <author>
      <name>/u/ontologicalmemes</name>
      <uri>https://old.reddit.com/user/ontologicalmemes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontologicalmemes"&gt; /u/ontologicalmemes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T00:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly6cg6</id>
    <title>Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!</title>
    <updated>2025-07-12T17:41:49+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.&lt;/p&gt; &lt;p&gt;It's &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/issues/64"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly4zh8</id>
    <title>Okay kimi-k2 is an INSANE model WTF those one-shot animations</title>
    <updated>2025-07-12T16:44:50+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt; &lt;img alt="Okay kimi-k2 is an INSANE model WTF those one-shot animations" src="https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ce744efba81890d05b5b715ce402a332366d7a" title="Okay kimi-k2 is an INSANE model WTF those one-shot animations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/74d8efoh2hcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8fyj</id>
    <title>This whole thing is giving me WizardLM2 vibes.</title>
    <updated>2025-07-12T19:09:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt; &lt;img alt="This whole thing is giving me WizardLM2 vibes." src="https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104" title="This whole thing is giving me WizardLM2 vibes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kn56m7cgshcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly42e5</id>
    <title>Interesting info about Kimi K2</title>
    <updated>2025-07-12T16:05:34+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt; &lt;img alt="Interesting info about Kimi K2" src="https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423" title="Interesting info about Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt; &lt;p&gt;Source: @rasbt on X&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/klm2b78lvgcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyaozv</id>
    <title>Moonshot AI just made their moonshot</title>
    <updated>2025-07-12T20:46:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt; &lt;img alt="Moonshot AI just made their moonshot" src="https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd" title="Moonshot AI just made their moonshot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Screenshot: &lt;a href="https://openrouter.ai/moonshotai"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Announcement: &lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95q67pnr9icf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
</feed>
