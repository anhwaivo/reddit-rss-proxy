<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-05T14:24:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lrv48g</id>
    <title>Day 10/50: Building a Small Language Model from Scratch - What is Model Distillation?</title>
    <updated>2025-07-04T22:28:05+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Day 10/50: Building a Small Language Model from Scratch — What is Model Distillation?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;This is one of my favorite topics. I’ve always wanted to run large models (several billion parameters, like DeepSeek 671b) or at least make my smaller models behave as intelligently and powerfully as those massive, high-parameter models. But like many of us, I don’t always have the hardware to run those resource-intensive models. But what if we could transfer the knowledge of a large model to a smaller one? That’s the whole idea of model distillation.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;What is Model Distillation?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Model distillation is a technique in which a large, complex model (referred to as the teacher) transfers its knowledge to a smaller, simpler model (referred to as the student). The goal is to make the student model perform almost as well as the teacher, but with fewer resources.&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Think of it like this:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;A PhD professor (teacher model) teaches a high school student (student model) everything they know, without the student having to go through a decade of research.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Why Do We Need Model Distillation?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Large models are:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Expensive to run&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Hard to deploy on edge devices&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Distillation solves this by:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Lowering memory/compute usage&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Maintaining competitive accuracy&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How Does Model Distillation Work?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;There are three main components:&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Teacher Model&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: A large, pre-trained model with high performance.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Student Model&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: A smaller model, which we aim to train to mimic the teacher.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Soft Targets&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Instead of just learning from the ground-truth labels, the student also learns from the teacher’s probability distribution over classes (logits), which carries extra information&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Let me break it down in simple language. In the case of traditional training, the model learns from hard labels. For example, if the correct answer is “Cat,” the label is simply 1 for “Cat” and 0 for everything else.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;However, in model distillation, the student also learns from the teacher’s soft predictions, which means it not only knows the correct answer but also how confident the teacher is about each possible answer.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If you are still unclear about it, let me provide a simpler example.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s say the task is image classification.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Image:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Picture of a cat&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Hard label (ground truth):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;“Cat” → 1&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;All other classes → 0&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Teacher model’s prediction (soft label):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;“Cat” → 85%&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;“Dog” → 10%&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;“Fox” → 4%&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;“Rabbit” → 1%&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Instead of learning only “This is a Cat”, the student model also learns that:&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;“The teacher is very confident it’s a cat, but it’s also somewhat similar to a dog or a fox.”&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;This additional information helps students learn more nuanced decision boundaries, making them more innovative and generalizable, even with fewer parameters.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;To sum up, Distillation allows the student to model learning not just what the teacher thinks is correct, but also how confident the teacher is across all options; this is what we call learning from soft targets.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Types of Knowledge Distillation&lt;/h1&gt; &lt;p&gt;&lt;em&gt;There is more than one way to pass knowledge from a teacher to a student. Let’s look at the main types:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1. Logit-based Distillation (Hinton et al.):&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;This is the method introduced by Geoffrey Hinton, the father of deep learning.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Here, the student doesn’t just learn from the correct label, but from the full output of the teacher (called logits), which contains rich information about how confident the teacher is in each class.&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Think of it like learning how the teacher thinks, not just what the final answer is.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2. Feature-based Distillation:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Instead of copying the final output, the student attempts to mimic the intermediate representations (such as hidden layers) of the teacher model.&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Imagine learning how the teacher breaks down and analyzes the problem step by step, rather than just their final conclusion.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;This is useful when you want the student to develop a similar internal understanding to that of the teacher.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;3. Response-based Distillation:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;This one is more straightforward; the student is trained to match the teacher’s final output, often without worrying about logits or hidden features.&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;It’s like learning to copy the teacher’s answer sheet during a test — not the most comprehensive learning, but sometimes good enough for quick tasks!&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Real-World Applications — Why Distillation Matters&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Mobile Devices:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Want to run BERT or GPT on your phone without needing a cloud GPU? Distilled models make this possible by reducing the size of large models while preserving much of their power.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Autonomous Vehicles:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Edge devices in self-driving cars can’t afford slow, bulky models. Distilled vision models enable faster, real-time decisions without requiring a massive compute stack in the trunk.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Chatbots and Virtual Assistants:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;For real-time conversations, low latency is key. Distilled language models offer fast responses while maintaining low memory and compute usage, making them ideal for customer service bots or AI tutors.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Limitations and Challenges &lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;1. Performance Gap:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Despite the best efforts, a student model may not accurately match the teacher’s performance, especially on complex tasks that require fine-grained reasoning.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;2. Architecture Mismatch:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;If the student model is too different from the teacher in design, it may struggle to “understand” what the teacher is trying to teach.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;3. Training Overhead:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Training a good student model still takes time, data, and effort; it’s not a simple copy-paste job. And sometimes, tuning distillation hyperparameters (such as temperature or alpha) can be tricky.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Popular Tools and Frameworks &lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Hugging Face:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Models like DistilBERT are smaller and faster versions of BERT, trained via distillation.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;TinyML:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;This focuses on deploying distilled models on ultra-low-power device&lt;/em&gt;&lt;strong&gt;&lt;em&gt;s,&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;such as microcontrollers, think smartwatches or IoT sensors.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;OpenVINO / TensorRT:&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;These are optimization toolkits by Intel and NVIDIA that pair well with distilled models to extract every last bit of performance from them on CPUs and GPUs.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;&lt;em&gt;I was genuinely amazed when I first learned about model distillation.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;In my case, I applied model distillation while building a model specifically for the DevOps field. I had a set of DevOps-related questions, but I didn’t have high-quality answers. So, I used GPT-o3 (yes, it did cost me) to generate expert-level responses. Once I had those, I used them to train a smaller model that could perform well without relying on GPT o3 every time. I’ll share the code for this in a future post.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Even DeepSeek has mentioned using model distillation as part of their training strategy for smaller models&lt;/em&gt; &lt;a href="https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html"&gt;https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html&lt;/a&gt;&lt;em&gt;. It’s a great example of how powerful this technique can be.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Distillation initially felt like a complex idea, but I’ve done my best to break it down into simple language.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrv48g/day_1050_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrv48g/day_1050_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrv48g/day_1050_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T22:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls0d8u</id>
    <title>speech, app studio, hosting - all local and seemless(ish) | my toy: bplus Server</title>
    <updated>2025-07-05T03:21:12+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls0d8u/speech_app_studio_hosting_all_local_and/"&gt; &lt;img alt="speech, app studio, hosting - all local and seemless(ish) | my toy: bplus Server" src="https://preview.redd.it/toexu80x1zaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc03891c2dcbeac4c24ad26bd6c710c99bf02bd7" title="speech, app studio, hosting - all local and seemless(ish) | my toy: bplus Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hopefully I uploaded everything correctly and haven't embarrassed myself..:&lt;br /&gt; &lt;a href="https://github.com/mrhappynice/bplus-server"&gt;https://github.com/mrhappynice/bplus-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My little toy. Just talk into the mic. hit gen. look at code, is it there?? hit create, page is hosted and live.&lt;br /&gt; also app manager(edit, delete, create llm-ready context) and manual app builder.&lt;br /&gt; Gemini connection added also, select model. Local through LM Studio(port 1234) should be able to just change url for Ollama etc.. &lt;/p&gt; &lt;p&gt;Voice is through Whisper server port 5752. Piper TTS(cmd line exe) also have browser speech through Web Speech API(ehh..) &lt;/p&gt; &lt;p&gt;mdChat and pic-chat are special WIP and blocked from the app manager. I'm forgetting about 22 things.&lt;br /&gt; Hopefully everything is working for ya. p e a c e &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/toexu80x1zaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls0d8u/speech_app_studio_hosting_all_local_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls0d8u/speech_app_studio_hosting_all_local_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T03:21:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsazjq</id>
    <title>Looking for open-source tool to blur entire bodies by gender in videos/images</title>
    <updated>2025-07-05T14:21:08+00:00</updated>
    <author>
      <name>/u/DayOk2</name>
      <uri>https://old.reddit.com/user/DayOk2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for an open‑source AI tool that can run locally on my computer (CPU only, no GPU) and process videos and images with the following functionality:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The tool should take a video or image as input and output the same video/image with these options for blurring: &lt;ul&gt; &lt;li&gt;Blur the entire body of all men.&lt;/li&gt; &lt;li&gt;Blur the entire body of all women.&lt;/li&gt; &lt;li&gt;Blur the entire bodies of both men and women.&lt;/li&gt; &lt;li&gt;Always blur the entire bodies of anyone whose gender is ambiguous or unrecognized, regardless of the above options, to avoid misclassification.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;The rest of the video or image should remain completely untouched and retain original quality. For videos, the audio must be preserved exactly.&lt;/li&gt; &lt;li&gt;The tool should be a command‑line program.&lt;/li&gt; &lt;li&gt;It must run on a typical computer with CPU only (no GPU required).&lt;/li&gt; &lt;li&gt;I plan to process one video or image at a time.&lt;/li&gt; &lt;li&gt;I understand processing may take time, but ideally it would run as fast as possible, aiming for under about 2 minutes for a 10‑minute video if feasible.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My main priorities are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ease of use.&lt;/li&gt; &lt;li&gt;Reliable gender detection (with ambiguous people always blurred automatically).&lt;/li&gt; &lt;li&gt;Running fully locally without complicated setup or programming skills.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To be clear, I want the tool to blur the entire body of the targeted people (not just faces, but full bodies) while leaving everything else intact.&lt;/p&gt; &lt;p&gt;Does such a tool already exist? If not, are there open‑source components I could combine to build this? Explain clearly what I would need to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DayOk2"&gt; /u/DayOk2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsazjq/looking_for_opensource_tool_to_blur_entire_bodies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsazjq/looking_for_opensource_tool_to_blur_entire_bodies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsazjq/looking_for_opensource_tool_to_blur_entire_bodies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T14:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrbwmz</id>
    <title>Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search</title>
    <updated>2025-07-04T06:36:12+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt; &lt;img alt="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" src="https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a18a6b5aef4f1d48310d2918ee6ff6f6c5943c2" title="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/CAE"&gt;https://github.com/MVPandey/CAE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I'm a terrible frontend dev)&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/dqws3fzgysaf1.gif"&gt;https://i.redd.it/dqws3fzgysaf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates multiple response candidates at each conversation state&lt;/li&gt; &lt;li&gt;Simulates how conversations might unfold down each branch (using the LLM to predict user responses)&lt;/li&gt; &lt;li&gt;Scores each trajectory on metrics like empathy, goal achievement, coherence&lt;/li&gt; &lt;li&gt;Uses MCTS with UCB1 to efficiently explore the most promising paths&lt;/li&gt; &lt;li&gt;Selects the response that leads to the best expected outcome&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FastAPI backend with async SQLAlchemy (PostgreSQL)&lt;/li&gt; &lt;li&gt;Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible endpoint&lt;/li&gt; &lt;li&gt;Dual-purpose: works as both a standard chat API and on-demand analysis engine&lt;/li&gt; &lt;li&gt;No agentic framework dependencies &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)&lt;/li&gt; &lt;li&gt;Branch pruning is naive - just threshold-based instead of something smarter like progressive widening&lt;/li&gt; &lt;li&gt;Memory usage grows with tree size - haven't implemented node recycling yet&lt;/li&gt; &lt;li&gt;The pgvector embedding code is there but commented out (wanted semantic search over conversation history)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Originally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.&lt;/p&gt; &lt;p&gt;Example output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.&lt;/p&gt; &lt;p&gt;Curious if anyone's tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.&lt;/p&gt; &lt;p&gt;Anyway, please let me know any thoughts, criticisms, feedback, etc! :) &lt;/p&gt; &lt;p&gt;I also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T06:36:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls1hd2</id>
    <title>Will this ever be fixed? RP repetition</title>
    <updated>2025-07-05T04:29:35+00:00</updated>
    <author>
      <name>/u/Blizado</name>
      <uri>https://old.reddit.com/user/Blizado</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From time to time, often months between it. I start a roleplay with a local LLM and when I do this I chat for a while. And since two years I run every time into the same issue: After a while the roleplay turned into a &amp;quot;how do I fix the LLM from repeating itself too much&amp;quot; or into a &amp;quot;Post an answer, wait for the LLM answer, edit the answer more and more&amp;quot; game.&lt;/p&gt; &lt;p&gt;I really hate this crap. I want to have fun and not want to always closely looking what the LLM answers and compare it the previous answer so that the LLM never tend to go down this stupid repeating rabbit hole...&lt;/p&gt; &lt;p&gt;One idea for a solution that I have would be to use the LLM answer an let it check that one with another prompt itself, let it compare with maybe the last 10 LLM answers before that one and let it rephrase the answer when some phrases are too similar.&lt;/p&gt; &lt;p&gt;At least that would be my first quick idea which could work. Even when it would make the answer time even longer. But for that you would need to write your own &amp;quot;Chatbot&amp;quot; (well, on that I work from time to time a bit - and such things hold be also back from it).&lt;/p&gt; &lt;p&gt;Run into that problem minutes ago and it ruined my roleplay, again. This time I used Mistral 3.2, but it didn't really matter what LLM I use. It always tend to slowly repeate stuff before you really notice it without analyzing every answer (what already would ruin the RP). It is especially annoying because the first hour or so (depends on the LLM and the settings) it works without any problems and so you can have a lot of fun.&lt;/p&gt; &lt;p&gt;What are your experiences when you do longer roleplay or maybe even endless roleplays you continue every time? I love to do this, but that ruins it for me every time.&lt;/p&gt; &lt;p&gt;And before anyone comes with that up: no, any setting that should avoid repetion did not fix that problem, It only delays it at best, but it didn't disappear.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blizado"&gt; /u/Blizado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls1hd2/will_this_ever_be_fixed_rp_repetition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls1hd2/will_this_ever_be_fixed_rp_repetition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls1hd2/will_this_ever_be_fixed_rp_repetition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T04:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls7vmb</id>
    <title>Multi GPUs?</title>
    <updated>2025-07-05T11:37:42+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current state of multi GPU use in local UIs? For example, GPUs such as 2x RX570/580/GTX1060, GTX1650, etc... I ask for future reference of the possibility of having twice VRam amount or an increase since some of these can still be found for half the price of a RTX.&lt;/p&gt; &lt;p&gt;In case it's possible, pairing AMD GPU with Nvidia one is a bad idea? And if pairing a ~8gb Nvidia with an RTX to hit nearly 20gb or more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T11:37:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrvlsx</id>
    <title>How and why is Llama so behind the other models at coding and UI/UX? Who is even using it?</title>
    <updated>2025-07-04T22:52:32+00:00</updated>
    <author>
      <name>/u/idwiw_wiw</name>
      <uri>https://old.reddit.com/user/idwiw_wiw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrvlsx/how_and_why_is_llama_so_behind_the_other_models/"&gt; &lt;img alt="How and why is Llama so behind the other models at coding and UI/UX? Who is even using it?" src="https://b.thumbs.redditmedia.com/ky0nsIz_RJXchcUklbRQzW-JFzxABDUvpQ9m5bMNp2o.jpg" title="How and why is Llama so behind the other models at coding and UI/UX? Who is even using it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on the this &lt;a href="https://www.designarena.ai/leaderboard"&gt;benchmark for coding and UI/UX&lt;/a&gt;, the Llama models are absolutely horrendous when it comes to build websites, apps, and other kinds of user interfaces. &lt;/p&gt; &lt;p&gt;How is Llama this bad and Meta so behind on AI compared to everyone else? No wonder they're trying to poach every top AI researcher out there. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.designarena.ai/models/llama-4-scout"&gt;Llama Examples&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idwiw_wiw"&gt; /u/idwiw_wiw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lrvlsx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrvlsx/how_and_why_is_llama_so_behind_the_other_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrvlsx/how_and_why_is_llama_so_behind_the_other_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T22:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls663p</id>
    <title>Asking LLMs data visualized as plots</title>
    <updated>2025-07-05T09:43:46+00:00</updated>
    <author>
      <name>/u/injeolmi-bingsoo</name>
      <uri>https://old.reddit.com/user/injeolmi-bingsoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fixed title: Asking LLMs for data visualized as plots&lt;/p&gt; &lt;p&gt;Hi, I'm looking for an app (e.g. LM Studio) + LLM solution that allows me to visualize LLM-generated data.&lt;/p&gt; &lt;p&gt;I often ask LLM questions that returns some form of numerical data. For example, I might ask &amp;quot;what's the world's population over time&amp;quot; or &amp;quot;what's the population by country in 2000&amp;quot;, which might return me a table with some data. This data is better visualized as a plot (e.g. bar graph).&lt;/p&gt; &lt;p&gt;Are there models that might return plots (which I guess is a form of image)? I am aware of [&lt;a href="https://github.com/nyanp/chat2plot%5D(chat2plot)"&gt;https://github.com/nyanp/chat2plot](chat2plot)&lt;/a&gt;, but are there others? Are there ones which can simply plug into a generalist app like LM Studio (afaik, LM Studio doesn't output graphics. Is that true?)?&lt;/p&gt; &lt;p&gt;I'm pretty new to self-hosted local LLMs so pardon me if I'm missing something obvious!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/injeolmi-bingsoo"&gt; /u/injeolmi-bingsoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls663p/asking_llms_data_visualized_as_plots/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls663p/asking_llms_data_visualized_as_plots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls663p/asking_llms_data_visualized_as_plots/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T09:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls8sk9</id>
    <title>Are there any autoregressive image gen models I can run locally on a 9070 XT/RAM?</title>
    <updated>2025-07-05T12:30:25+00:00</updated>
    <author>
      <name>/u/jojokingxp</name>
      <uri>https://old.reddit.com/user/jojokingxp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all, are there any models that work like gpt image 1 that I can run on an AMD GPU or on RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jojokingxp"&gt; /u/jojokingxp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8sk9/are_there_any_autoregressive_image_gen_models_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8sk9/are_there_any_autoregressive_image_gen_models_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8sk9/are_there_any_autoregressive_image_gen_models_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrjy15</id>
    <title>Anyone else feel like working with LLM libs is like navigating a minefield ?</title>
    <updated>2025-07-04T14:22:03+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've worked about 7 years in software development companies, and it's &amp;quot;easy&amp;quot; to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.&lt;/p&gt; &lt;p&gt;Problem with Django? Update it, the bug was probably fixed ages ago.&lt;/p&gt; &lt;p&gt;With LLMs it's an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I'm skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.&lt;/p&gt; &lt;p&gt;Example from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unmerged bugfix that makes it &lt;strong&gt;UNUSABLE&lt;/strong&gt; &lt;a href="https://github.com/vllm-project/vllm/pull/19084"&gt;https://github.com/vllm-project/vllm/pull/19084&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8 wasn't working, I had to make the PR myself &lt;a href="https://github.com/vllm-project/vllm/pull/19420"&gt;https://github.com/vllm-project/vllm/pull/19420&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR &lt;a href="https://github.com/vllm-project/vllm/pull/20488"&gt;https://github.com/vllm-project/vllm/pull/20488&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.&lt;/p&gt; &lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt; &lt;p&gt;And I'm not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can't download yourself and that EXPLODE at the slightest conflict: &lt;/p&gt; &lt;p&gt;&lt;code&gt;driver versions &amp;lt;----&amp;gt; torch version &amp;lt;-----&amp;gt; vLLM version&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It's driving me insane.&lt;/p&gt; &lt;p&gt;I don't understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrmxn7</id>
    <title>llama : add high-throughput mode by ggerganov · Pull Request #14363 · ggml-org/llama.cpp</title>
    <updated>2025-07-04T16:26:56+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"&gt; &lt;img alt="llama : add high-throughput mode by ggerganov · Pull Request #14363 · ggml-org/llama.cpp" src="https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6fff29b1956872ed77766a108a404db46b43026" title="llama : add high-throughput mode by ggerganov · Pull Request #14363 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14363"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T16:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrqoul</id>
    <title>i made a script to train your own transformer model on a custom dataset on your machine</title>
    <updated>2025-07-04T19:04:17+00:00</updated>
    <author>
      <name>/u/samas69420</name>
      <uri>https://old.reddit.com/user/samas69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;over the last couple of years we have seen LLMs become super duper popular and some of them are small enough to run on consumer level hardware, but in most cases we are talking about pre-trained models that can be used only in inference mode without considering the full training phase. Something that i was cuorious about tho is what kind of performance i could get if i did everything, including the full training without using other tools like lora or quantization, on my own everyday machine so i made a script that does exactly that, the script contains also a file (config.py) that can be used to tune the hyperparameters of the architecture so that anyone running it can easily set them to have the largest model as possible with their hardware (in my case with the model in the script and with a 12gb 3060 i can train about 50M params, 300M with smaller batch and mixed precision) here is the repo &lt;a href="https://github.com/samas69420/transformino"&gt;https://github.com/samas69420/transformino&lt;/a&gt; , to run the code the only thing you'll need is a dataset in the form of a csv file with a column containing the text that will be used for training (tweets, sentences from a book etc), the project also have a very low number of dependencies to make it more easy to run (you'll need only pytorch, pandas and tokenizers), every kind of feedback would be appreciated &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samas69420"&gt; /u/samas69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrqoul/i_made_a_script_to_train_your_own_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T19:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lsaczg</id>
    <title>Utilize iGPU (AMD Radeon 780m) even if the dGPU is running via MUX switch</title>
    <updated>2025-07-05T13:51:24+00:00</updated>
    <author>
      <name>/u/panther_ra</name>
      <uri>https://old.reddit.com/user/panther_ra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;br /&gt; I'm wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.&lt;br /&gt; The whole idea that I can use idling iGPU for the AI tasks (small 7b models).&lt;br /&gt; The MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).&lt;br /&gt; I've a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.&lt;br /&gt; I know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panther_ra"&gt; /u/panther_ra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T13:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrzrmd</id>
    <title>Best model at the moment for 128GB M4 Max</title>
    <updated>2025-07-05T02:44:52+00:00</updated>
    <author>
      <name>/u/Xx_DarDoAzuL_xX</name>
      <uri>https://old.reddit.com/user/Xx_DarDoAzuL_xX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Recently got myself a brand new M4 Max 128Gb ram Mac Studio. &lt;/p&gt; &lt;p&gt;I saw some old posts about the best models to use with this computer, but I am wondering if that has changed throughout the months/years. &lt;/p&gt; &lt;p&gt;Currently, what is the best model and settings to use with this machine? &lt;/p&gt; &lt;p&gt;Cheers! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xx_DarDoAzuL_xX"&gt; /u/Xx_DarDoAzuL_xX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrzrmd/best_model_at_the_moment_for_128gb_m4_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T02:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls9jvu</id>
    <title>Which open source LLM has the most genuine sense of humor?</title>
    <updated>2025-07-05T13:10:47+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm genuinely struggling with everything out there in terms of making me smile and general joke quality. If there is such a model, at what settings should it run? (temp/top_k etc). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls9jvu/which_open_source_llm_has_the_most_genuine_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T13:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsf6x</id>
    <title>OCRFlux-3B</title>
    <updated>2025-07-04T20:21:20+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt; &lt;img alt="OCRFlux-3B" src="https://external-preview.redd.it/x9gxRnW-oFgiJds7kCEygtLLuK_ZzX-0pJcvDDyr2xk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5dcd5f194d87db7612ad57b0c05d46dc6f0cfae3" title="OCRFlux-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the HF repo:&lt;/p&gt; &lt;p&gt;&amp;quot;OCRFlux is a multimodal large language model based toolkit for converting PDFs and images into clean, readable, plain Markdown text. It aims to push the current state-of-the-art to a significantly higher level.&amp;quot;&lt;/p&gt; &lt;p&gt;Claims to beat other models like olmOCR and Nanonets-OCR-s by a substantial margin. Read online that it can also merge content spanning multiple pages such as long tables. There's also a docker container with the full toolkit and a github repo. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ChatDOC/OCRFlux-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsf6x/ocrflux3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls95oj</id>
    <title>Apple MLX Quantizations Royal Rumble 🔥</title>
    <updated>2025-07-05T12:50:36+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt; &lt;img alt="Apple MLX Quantizations Royal Rumble 🔥" src="https://a.thumbs.redditmedia.com/-1Fa5pMoUdufyX7EbInbdiYRv8uh6lx6DYYAentaN_0.jpg" title="Apple MLX Quantizations Royal Rumble 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-8B model using Winogrande as benchmark.&lt;br /&gt; DWQ and 5bit rule! &lt;/p&gt; &lt;p&gt;🥇 dwq – 68.82%&lt;br /&gt; 🥈 5bit – 68.51%&lt;br /&gt; 🥉 6bit – 68.35%&lt;br /&gt; bf16 – 67.64%&lt;br /&gt; dynamic – 67.56%&lt;br /&gt; 8bit – 67.56%&lt;br /&gt; 4bit – 66.30%&lt;br /&gt; 3bit – 63.85%&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3"&gt;https://preview.redd.it/95nyy1fby1bf1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6402294cedb1bdfc338ea34983203e7118188a3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls95oj/apple_mlx_quantizations_royal_rumble/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls8c2s</id>
    <title>Aveni Labs releases FinLLM technical report: a 7B domain-specific model for financial services outperforming some frontier LLMs</title>
    <updated>2025-07-05T12:04:41+00:00</updated>
    <author>
      <name>/u/Ok-Cryptographer9361</name>
      <uri>https://old.reddit.com/user/Ok-Cryptographer9361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read the &lt;a href="https://aveni.ai/wp-content/uploads/2025/05/Aveni-Detect-Combined-Case-Study.pdf"&gt;FinLLM technical report&lt;/a&gt; from Aveni Labs. It’s a 7B parameter language model built specifically for UK financial services, trained with regulatory alignment and fine-tuned for tasks like compliance monitoring, adviser QA, and KYC review.&lt;/p&gt; &lt;p&gt;Key points that stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms GPT-4o mini, Gemini 1.5 Flash, and LLaMA-based models on financial domain tasks like tabular data analysis, multi-turn customer dialogue, long-context reasoning, and document QA&lt;/li&gt; &lt;li&gt;Built using a filtering pipeline called Finance Classifier 2.0 that selects high-quality, in-domain training data (regulatory guidance, advice transcripts, etc.)&lt;/li&gt; &lt;li&gt;Open 1B and 7B variants designed for fine-tuning and secure deployment in VPC or on-prem environments&lt;/li&gt; &lt;li&gt;Optimized for agentic RAG setups where traceability and source-grounding are required&lt;/li&gt; &lt;li&gt;Benchmarked using their own dataset, AveniBench, which focuses on real FS tasks like consumer vulnerability detection and conduct risk spotting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They are also working on a 30B version, but the current 7B model is already matching or beating much larger models in this domain.&lt;/p&gt; &lt;p&gt;Anyone else here working on small or mid-scale domain-specific models in regulated industries? Curious how others are handling fine-tuning and evaluation for high-risk applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Cryptographer9361"&gt; /u/Ok-Cryptographer9361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls8c2s/aveni_labs_releases_finllm_technical_report_a_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T12:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrss4u</id>
    <title>THUDM/GLM-4.1V-9B-Thinking looks impressive</title>
    <updated>2025-07-04T20:37:49+00:00</updated>
    <author>
      <name>/u/ConfidentTrifle7247</name>
      <uri>https://old.reddit.com/user/ConfidentTrifle7247</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt; &lt;img alt="THUDM/GLM-4.1V-9B-Thinking looks impressive" src="https://preview.redd.it/62vkwepq4xaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ded30c7e3562f37aa41502883d7aa8b656c68551" title="THUDM/GLM-4.1V-9B-Thinking looks impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking"&gt;https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentTrifle7247"&gt; /u/ConfidentTrifle7247 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/62vkwepq4xaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:37:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri12r</id>
    <title>Great price on a 5090</title>
    <updated>2025-07-04T12:53:29+00:00</updated>
    <author>
      <name>/u/psdwizzard</name>
      <uri>https://old.reddit.com/user/psdwizzard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt; &lt;img alt="Great price on a 5090" src="https://preview.redd.it/1en1lic1uuaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7df49758108c1feb283e4286654e01dbd232a219" title="Great price on a 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About to pull the trigger on this one I can't believe how cheap it is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psdwizzard"&gt; /u/psdwizzard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1en1lic1uuaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T12:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls3gho</id>
    <title>Open source tool for generating training datasets from text files and pdf for fine-tuning language models.</title>
    <updated>2025-07-05T06:36:35+00:00</updated>
    <author>
      <name>/u/Idonotknow101</name>
      <uri>https://old.reddit.com/user/Idonotknow101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey yall I made a new open-source tool.&lt;/p&gt; &lt;p&gt;It's an app that &lt;strong&gt;creates training data for AI models from your text and PDFs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to make your own AI smarter. The data comes out ready for different models.&lt;/p&gt; &lt;p&gt;Super simple, super useful, and it's all open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Idonotknow101"&gt; /u/Idonotknow101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MonkWarrior08/Dataset_Generator_for_Fine-tuning?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls3gho/open_source_tool_for_generating_training_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T06:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrz5uy</id>
    <title>Got some real numbers how llama.cpp got FASTER over last 3-months</title>
    <updated>2025-07-05T02:08:31+00:00</updated>
    <author>
      <name>/u/AggressiveHunt2300</name>
      <uri>https://old.reddit.com/user/AggressiveHunt2300</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I am author of Hyprnote(&lt;a href="https://github.com/fastrepl/hyprnote"&gt;https://github.com/fastrepl/hyprnote&lt;/a&gt;) - privacy-first notepad for meetings. We regularly test out the AI models we use in various devices to make sure it runs well.&lt;/p&gt; &lt;p&gt;When testing MacBook, Qwen3 1.7B is used, and for Windows, Qwen3 0.6B is used. (All Q4 KM)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5828"&gt;b5828&lt;/a&gt;(newer) .. &lt;a href="https://github.com/ggml-org/llama.cpp/tree/b5162"&gt;b5162&lt;/a&gt;(older)&lt;/p&gt; &lt;p&gt;Thinking of writing lot longer blog post with lots of numbers &amp;amp; what I learned during the experiment. Please let me know if that is something you guys are interested in.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;OS&lt;/th&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="left"&gt;RAM&lt;/th&gt; &lt;th align="left"&gt;Compute&lt;/th&gt; &lt;th align="left"&gt;Prefill Tok/s&lt;/th&gt; &lt;th align="left"&gt;Gen Tok/s&lt;/th&gt; &lt;th align="left"&gt;Median Load (ms)&lt;/th&gt; &lt;th align="left"&gt;Prefill RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Gen RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;Load RAM (MB)&lt;/th&gt; &lt;th align="left"&gt;SHA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook Pro 14-inch&lt;/td&gt; &lt;td align="left"&gt;macOS 15.3.2&lt;/td&gt; &lt;td align="left"&gt;Apple M2 Pro&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;Metal&lt;/td&gt; &lt;td align="left"&gt;615.20&lt;/td&gt; &lt;td align="left"&gt;21.69&lt;/td&gt; &lt;td align="left"&gt;362.52&lt;/td&gt; &lt;td align="left"&gt;2332.28&lt;/td&gt; &lt;td align="left"&gt;2337.67&lt;/td&gt; &lt;td align="left"&gt;2089.56&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;571.85&lt;/td&gt; &lt;td align="left"&gt;21.43&lt;/td&gt; &lt;td align="left"&gt;372.32&lt;/td&gt; &lt;td align="left"&gt;2341.77&lt;/td&gt; &lt;td align="left"&gt;2347.05&lt;/td&gt; &lt;td align="left"&gt;2102.27&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HP EliteBook 660 16-inch G11&lt;/td&gt; &lt;td align="left"&gt;Windows 11.24H2&lt;/td&gt; &lt;td align="left"&gt;Intel Core Ultra 7 155U&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;162.52&lt;/td&gt; &lt;td align="left"&gt;14.05&lt;/td&gt; &lt;td align="left"&gt;1533.99&lt;/td&gt; &lt;td align="left"&gt;3719.23&lt;/td&gt; &lt;td align="left"&gt;3641.65&lt;/td&gt; &lt;td align="left"&gt;3535.43&lt;/td&gt; &lt;td align="left"&gt;b5828&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;148.52&lt;/td&gt; &lt;td align="left"&gt;12.89&lt;/td&gt; &lt;td align="left"&gt;2487.26&lt;/td&gt; &lt;td align="left"&gt;3719.96&lt;/td&gt; &lt;td align="left"&gt;3642.34&lt;/td&gt; &lt;td align="left"&gt;3535.24&lt;/td&gt; &lt;td align="left"&gt;b5162&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveHunt2300"&gt; /u/AggressiveHunt2300 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrz5uy/got_some_real_numbers_how_llamacpp_got_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T02:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls70r2</id>
    <title>Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance</title>
    <updated>2025-07-05T10:43:05+00:00</updated>
    <author>
      <name>/u/d5dq</name>
      <uri>https://old.reddit.com/user/d5dq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt; &lt;img alt="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" src="https://external-preview.redd.it/Hmgn73CQ0ArpZT9jmmMJLBLX21JxLwuOBVd0t3yUiJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86ebf97301bc00e90b9b236ebf2a2bb13dae2a1a" title="Impact of PCIe 5.0 Bandwidth on GPU Content Creation Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/d5dq"&gt; /u/d5dq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pugetsystems.com/labs/articles/impact-of-pcie-5-0-bandwidth-on-gpu-content-creation-performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls70r2/impact_of_pcie_50_bandwidth_on_gpu_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T10:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrsx20</id>
    <title>How RAG actually works — a toy example with real math</title>
    <updated>2025-07-04T20:44:15+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most RAG explainers jump into theories and scary infra diagrams. Here’s the tiny end-to-end demo that can easy to understand for me:&lt;/p&gt; &lt;p&gt;Suppose we have a documentation like this: &amp;quot;Boil an egg. Poach an egg. How to change a tire&amp;quot;&lt;/p&gt; &lt;h1&gt;Step 1: Chunk&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;S0: &amp;quot;Boil an egg&amp;quot; S1: &amp;quot;Poach an egg&amp;quot; S2: &amp;quot;How to change a tire&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Embed&lt;/h1&gt; &lt;p&gt;After the words “Boil an egg” pass through a pretrained transformer, the model compresses its hidden states into a single 4-dimensional vector; each value is just one coordinate of that learned “meaning point” in vector space.&lt;/p&gt; &lt;p&gt;Toy demo values:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;V0 = [ 0.90, 0.10, 0.00, 0.10] # “Boil an egg” V1 = [ 0.88, 0.12, 0.00, 0.09] # “Poach an egg” V2 = [-0.20, 0.40, 0.80, 0.10] # “How to change a tire” &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Real models spit out 384-D to 3072-D vectors; 4-D keeps the math readable.)&lt;/p&gt; &lt;h1&gt;Step 3: Normalize&lt;/h1&gt; &lt;p&gt;Put every vector on the unit sphere:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Normalised (unit-length) vectors V0̂ = [ 0.988, 0.110, 0.000, 0.110] # 0.988² + 0.110² + 0.000² + 0.110² ≈ 1.000 → 1 V1̂ = [ 0.986, 0.134, 0.000, 0.101] # 0.986² + 0.134² + 0.000² + 0.101² ≈ 1.000 → 1 V2̂ = [-0.217, 0.434, 0.868, 0.108] # (-0.217)² + 0.434² + 0.868² + 0.108² ≈ 1.001 → 1 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 4: Index&lt;/h1&gt; &lt;p&gt;Drop V0^,V1^,V2^ into a similarity index (FAISS, Qdrant, etc.).&lt;br /&gt; Keep a side map &lt;code&gt;{0:S0, 1:S1, 2:S2}&lt;/code&gt; so IDs can turn back into text later.&lt;/p&gt; &lt;h1&gt;Step 5: Similarity Search&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;User asks&lt;/strong&gt;&lt;br /&gt; “Best way to cook an egg?”&lt;/p&gt; &lt;p&gt;We embed this sentence and normalize it as well, which gives us something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ = [0.989, 0.086, 0.000, 0.118] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we need to find the vector that’s &lt;em&gt;closest&lt;/em&gt; to this one.&lt;br /&gt; The most common way is cosine similarity — often written as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = (A ⋅ B) / (‖A‖ × ‖B‖) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But since we already normalized all vectors,&lt;br /&gt; ‖A‖ = ‖B‖ = 1 → so the formula becomes just:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cos(θ) = A ⋅ B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This means we just need to calculate the &lt;strong&gt;dot product&lt;/strong&gt; between the user input vector and each stored vector.&lt;br /&gt; If two vectors are exactly the same, dot product = 1.&lt;br /&gt; So we sort by which ones have values closest to 1 - higher = more similar.&lt;/p&gt; &lt;p&gt;Let’s calculate the scores (example, not real)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Vi^ ⋅ V0̂ = (0.989)(0.988) + (0.086)(0.110) + (0)(0) + (0.118)(0.110) ≈ 0.977 + 0.009 + 0 + 0.013 = 0.999 Vi^ ⋅ V1̂ = (0.989)(0.986) + (0.086)(0.134) + (0)(0) + (0.118)(0.101) ≈ 0.975 + 0.012 + 0 + 0.012 = 0.999 Vi^ ⋅ V2̂ = (0.989)(-0.217) + (0.086)(0.434) + (0)(0.868) + (0.118)(0.108) ≈ -0.214 + 0.037 + 0 + 0.013 = -0.164 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So we find that sentence 0 (“Boil an egg”) and sentence 1 (“Poach an egg”)&lt;br /&gt; are both very close to the user input.&lt;/p&gt; &lt;p&gt;We &lt;strong&gt;retrieve those two as context&lt;/strong&gt;, and pass them to the LLM.&lt;br /&gt; Now the LLM has relevant info to answer accurately, instead of guessing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrsx20/how_rag_actually_works_a_toy_example_with_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T20:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ls5b89</id>
    <title>Powerful 4B Nemotron based finetune</title>
    <updated>2025-07-05T08:43:38+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt; &lt;img alt="Powerful 4B Nemotron based finetune" src="https://external-preview.redd.it/HPVfFVlOpp4pNbOJ94txPWQsg8plop9RTeb6Vvswqrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d882c129728e2bb772cd8f145ea68d43d0c6637" title="Powerful 4B Nemotron based finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I present to you &lt;strong&gt;Impish_LLAMA_4B&lt;/strong&gt;, one of the most powerful roleplay \ adventure finetunes at its size category.&lt;/p&gt; &lt;p&gt;TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;incredibly powerful&lt;/strong&gt; roleplay model for the size. It has &lt;strong&gt;sovl !&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Does &lt;strong&gt;Adventure&lt;/strong&gt; very well for such size!&lt;/li&gt; &lt;li&gt;Characters have &lt;strong&gt;agency&lt;/strong&gt;, and might surprise you! &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B#roleplay-examples-this-character-is-availbe-here"&gt;See the examples in the logs&lt;/a&gt; 🙂&lt;/li&gt; &lt;li&gt;Roleplay &amp;amp; Assistant data used plenty of &lt;strong&gt;16K&lt;/strong&gt; examples.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very responsive&lt;/strong&gt;, feels 'in the moment', kicks &lt;strong&gt;far above&lt;/strong&gt; its weight. You might forget it's a &lt;strong&gt;4B&lt;/strong&gt; if you squint.&lt;/li&gt; &lt;li&gt;Based on a lot of the data in &lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B"&gt;Impish_Magic_24B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Super long context&lt;/strong&gt; as well as context attention for &lt;strong&gt;4B&lt;/strong&gt;, personally tested for up to &lt;strong&gt;16K&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Can run on &lt;strong&gt;Raspberry Pi 5&lt;/strong&gt; with ease.&lt;/li&gt; &lt;li&gt;Trained on over &lt;strong&gt;400m tokens&lt;/strong&gt; with highlly currated data that was tested on countless models beforehand. And some new stuff, as always.&lt;/li&gt; &lt;li&gt;Very decent assistant.&lt;/li&gt; &lt;li&gt;Mostly &lt;strong&gt;uncensored&lt;/strong&gt; while retaining plenty of intelligence.&lt;/li&gt; &lt;li&gt;Less &lt;strong&gt;positivity&lt;/strong&gt; &amp;amp; &lt;strong&gt;uncensored&lt;/strong&gt;, &lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;Negative_LLAMA_70B&lt;/a&gt; style of data, adjusted for &lt;strong&gt;4B&lt;/strong&gt;, with serious upgrades. Training data contains combat scenarios. And it &lt;strong&gt;shows&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt;Trained on &lt;strong&gt;extended 4chan dataset&lt;/strong&gt; to add humanity, quirkiness, and naturally— less positivity, and the inclination to... argue 🙃&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Short length&lt;/strong&gt; response (1-3 paragraphs, usually 1-2). CAI Style.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the model card for more details &amp;amp; character cards for Roleplay \ Adventure:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, currently hosting it on Horde at an extremely high availability, likely less than 2 seconds queue, even under maximum load (~&lt;strong&gt;3600&lt;/strong&gt; tokens per second, &lt;strong&gt;96 threads&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ga4ihkf1q0bf1.png?width=1086&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d387a56cd2c4029a1f36db3df13c627e6d9f11cd"&gt;Horde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;~3600 tokens per second, 96 threads)Would love some feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ls5b89/powerful_4b_nemotron_based_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-05T08:43:38+00:00</published>
  </entry>
</feed>
