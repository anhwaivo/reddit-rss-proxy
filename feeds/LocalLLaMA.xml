<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-27T21:06:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n1cm2w</id>
    <title>monkeSearch has now become a bit smarter! a call for contributors to make this project more easy to adapt.</title>
    <updated>2025-08-27T09:46:12+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/monkesearch/monkeSearch"&gt;https://github.com/monkesearch/monkeSearch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I released monkeSearch this week and I've been receiving great response on the tool. monkeSearch is essentially fully local natural language file search engine based on qwen0.6b (for now), and it works pretty well with no finetuning etc. with just 400~ lines of code.&lt;br /&gt; This post is also a call for asking for contributors to help me continue this project to become more polished in terms of usage (GUI development + installation etc.) and also for people to build onto the base and give me suggestions and make it more smarter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1cm2w/monkesearch_has_now_become_a_bit_smarter_a_call/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1cm2w/monkesearch_has_now_become_a_bit_smarter_a_call/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1cm2w/monkesearch_has_now_become_a_bit_smarter_a_call/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T09:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1kunk</id>
    <title>Running GLM 4.5 2 bit quant on 80GB VRAM and 128GB RAM</title>
    <updated>2025-08-27T15:54:24+00:00</updated>
    <author>
      <name>/u/Jaswanth04</name>
      <uri>https://old.reddit.com/user/Jaswanth04</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I recently upgraded my system to have 80 GB VRAM, with 1 5090 and 2 3090s. I have a 128GB DDR4 RAM.&lt;/p&gt; &lt;p&gt;I am trying to run unsloth GLM 4.5 2 bit on the machine and I am getting around 4 to 5 tokens per sec.&lt;/p&gt; &lt;p&gt;I am using the below command,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/home/jaswant/Documents/llamacpp/llama.cpp/llama-server \ --model unsloth/GLM-4.5-GGUF/UD-Q2_K_XL/GLM-4.5-UD-Q2_K_XL-00001-of-00003.gguf \ --alias &amp;quot;unsloth/GLM&amp;quot; \ -c 32768 \ -ngl 999 \ -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; \ -fa \ --temp 0.6 \ --top-p 1.0 \ --top-k 40 \ --min-p 0.05 \ --threads 32 --threads-http 8 \ --cache-type-k f16 --cache-type-v f16 \ --port 8001 \ --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is the 4-5 tokens per sec expected for my hardware ? or can I change the command so that I can get a better speed ?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaswanth04"&gt; /u/Jaswanth04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kunk/running_glm_45_2_bit_quant_on_80gb_vram_and_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kunk/running_glm_45_2_bit_quant_on_80gb_vram_and_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kunk/running_glm_45_2_bit_quant_on_80gb_vram_and_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1e7q1</id>
    <title>The fastest real time TTS you used that doesn't sacrifice quality and is easy to set up?</title>
    <updated>2025-08-27T11:17:11+00:00</updated>
    <author>
      <name>/u/learninggamdev</name>
      <uri>https://old.reddit.com/user/learninggamdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, looking for a TTS option, that is fast and doesn't sacrifice quality. I looked into orpheus but it's a pain to set up and get it working and it also requires like a lot of vRAM, a bit more than 48GB IIRC unless I am wrong.&lt;br /&gt; I looked into Kokoro, but it's super robotic, and the ones that are fast like KittenTTS are the same.&lt;/p&gt; &lt;p&gt;Looking for something that does real time streaming, under 400ms ideally.&lt;/p&gt; &lt;p&gt;Any recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learninggamdev"&gt; /u/learninggamdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1e7q1/the_fastest_real_time_tts_you_used_that_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1e7q1/the_fastest_real_time_tts_you_used_that_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1e7q1/the_fastest_real_time_tts_you_used_that_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T11:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n12aqj</id>
    <title>Deepseek changes their API price again</title>
    <updated>2025-08-27T00:10:24+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"&gt; &lt;img alt="Deepseek changes their API price again" src="https://preview.redd.it/x6keqt10fglf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8ec2bcfd599ff48e74e4fe29bfdc5460aeaec90" title="Deepseek changes their API price again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is far less attractive tbh. Basically they said R1 and V3 were going with a price now of 0.07 (0.56 cache miss) and 1.12, now that 1.12 is now 1.68. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x6keqt10fglf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T00:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1potw</id>
    <title>[Project Release] Running Meta Llama 3B on Intel NPU with OpenVINO-genai</title>
    <updated>2025-08-27T18:53:05+00:00</updated>
    <author>
      <name>/u/Spiritual-Ad-5916</name>
      <uri>https://old.reddit.com/user/Spiritual-Ad-5916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1potw/project_release_running_meta_llama_3b_on_intel/"&gt; &lt;img alt="[Project Release] Running Meta Llama 3B on Intel NPU with OpenVINO-genai" src="https://external-preview.redd.it/PZDgThMSB8bav25Yik-Cvtq4se2RshL0SgT_QDOj4ok.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=408062ba6069c7dc2c052872e7988165a4230326" title="[Project Release] Running Meta Llama 3B on Intel NPU with OpenVINO-genai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just finished my new open-source project and wanted to share it here. I managed to get Meta Llama &lt;strong&gt;Chat&lt;/strong&gt; running &lt;strong&gt;locally&lt;/strong&gt; on my Intel Core Ultra laptopâ€™s &lt;strong&gt;NPU&lt;/strong&gt; using &lt;strong&gt;OpenVINO GenAI&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;ðŸ”§ &lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exported the HuggingFace model with &lt;code&gt;optimum-cli&lt;/code&gt; â†’ OpenVINO IR format&lt;/li&gt; &lt;li&gt;Quantized it to &lt;strong&gt;INT4/FP16&lt;/strong&gt; for NPU acceleration&lt;/li&gt; &lt;li&gt;Packaged everything neatly into a GitHub repo for others to try&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;âš¡ &lt;strong&gt;Why itâ€™s interesting:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No GPU required â€” just the &lt;strong&gt;Intel NPU&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;100% &lt;strong&gt;offline&lt;/strong&gt; inference&lt;/li&gt; &lt;li&gt;Meta Llama runs surprisingly well when optimized&lt;/li&gt; &lt;li&gt;A good demo of OpenVINO GenAI for students/newcomers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n1potw/video/hseva1f6zllf1/player"&gt;https://reddit.com/link/1n1potw/video/hseva1f6zllf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ“‚ Repo link: [&lt;a href="https://github.com/balaragavan2007/Meta_Llama_on_intel_NPU"&gt;balaragavan2007/Meta_Llama_on_intel_NPU: This is how I made MetaLlama 3b LLM running on NPU of Intel Ultra processor&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual-Ad-5916"&gt; /u/Spiritual-Ad-5916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1potw/project_release_running_meta_llama_3b_on_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1potw/project_release_running_meta_llama_3b_on_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1potw/project_release_running_meta_llama_3b_on_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T18:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0us6p</id>
    <title>Nous Research presents Hermes 4</title>
    <updated>2025-08-26T19:06:53+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt; &lt;img alt="Nous Research presents Hermes 4" src="https://external-preview.redd.it/NQUFFcCjHt1BJkc3XZx_qrQGOmxnmvDswSz5yNpH4xs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=963a55e599f5d49840779052d831759babb45c21" title="Nous Research presents Hermes 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: &lt;a href="https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728"&gt;HF collection&lt;/a&gt;&lt;br /&gt; My long-awaited open-source masterpiece&lt;/p&gt; &lt;p&gt;&lt;a href="https://hermes4.nousresearch.com"&gt;https://hermes4.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2508.18255"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.nousresearch.com/"&gt;Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T19:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0tgrr</id>
    <title>nano-banana is a MASSIVE jump forward in image editing</title>
    <updated>2025-08-26T18:16:15+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"&gt; &lt;img alt="nano-banana is a MASSIVE jump forward in image editing" src="https://preview.redd.it/7kcykqmxnelf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71a63e7a49527931c15a14e3dbb88e861587ab4" title="nano-banana is a MASSIVE jump forward in image editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7kcykqmxnelf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T18:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1k51r</id>
    <title>ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples</title>
    <updated>2025-08-27T15:27:10+00:00</updated>
    <author>
      <name>/u/AdventurousSwim1312</name>
      <uri>https://old.reddit.com/user/AdventurousSwim1312</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k51r/archifactory_benchmark_slm_architecture_on/"&gt; &lt;img alt="ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples" src="https://external-preview.redd.it/JJYAwEsa6SPFqvAYJPcxetVbTITZ1NUPhPUkc0pUTvo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5c886cef488fec470732221ccbc53cb1dff1d6b" title="ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tp0m1roqyklf1.png?width=1120&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1027434f6a1bb8906f4ccac541634f120a07c26"&gt;35M Parameters : GQA vs Mamba vs Retnet vs RWKV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.&lt;/p&gt; &lt;p&gt;One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.&lt;/p&gt; &lt;p&gt;On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.&lt;/p&gt; &lt;p&gt;For that purpose, i create ArchiFactory, a simple (&amp;lt;500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.&lt;/p&gt; &lt;p&gt;Included:&lt;/p&gt; &lt;p&gt;- simple modular architecture to be sure to compare similar stuff&lt;/p&gt; &lt;p&gt;- complete optimized training loop using pytorch lightning&lt;/p&gt; &lt;p&gt;- fp8 training (can achieve &amp;lt;20min training on 5090 grade GPU)&lt;/p&gt; &lt;p&gt;- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.&lt;/p&gt; &lt;p&gt;- guidelines to test integrate new modules&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/gabrielolympie/ArchiFactory"&gt;https://github.com/gabrielolympie/ArchiFactory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousSwim1312"&gt; /u/AdventurousSwim1312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k51r/archifactory_benchmark_slm_architecture_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k51r/archifactory_benchmark_slm_architecture_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k51r/archifactory_benchmark_slm_architecture_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1hro7</id>
    <title>How to train a Language Model to run on RP2040 locally</title>
    <updated>2025-08-27T13:56:52+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 2 days in a hackathon getting a transformers model to run on a TinyPico 8MB.&lt;/p&gt; &lt;p&gt;Day #1 was spent finding the most optimal architecture &amp;amp; hyper-parameter&lt;/p&gt; &lt;p&gt;Day #2 was spent spinning GPUs to train the actual models (20$ spent on GPU)&lt;/p&gt; &lt;p&gt;I thought I might share what I did and someone else could scale it up further!&lt;/p&gt; &lt;p&gt;Current progress: Due to RP2040 memory fragmentation, we can only fit 256 vocabulary in the model, meaning the dataset curation is quite intensive&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1hro7/how_to_train_a_language_model_to_run_on_rp2040/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1hro7/how_to_train_a_language_model_to_run_on_rp2040/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1hro7/how_to_train_a_language_model_to_run_on_rp2040/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T13:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1m8xh</id>
    <title>Pair a vision grounding model with a reasoning LLM with Cua</title>
    <updated>2025-08-27T16:45:35+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1m8xh/pair_a_vision_grounding_model_with_a_reasoning/"&gt; &lt;img alt="Pair a vision grounding model with a reasoning LLM with Cua" src="https://external-preview.redd.it/czhjcTlybG5jbGxmMbzuqtV-zsPSC2s-Lu_18m-UGy8cX2XwaXvrFiOhDTxh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36155e6a7dff8ec2cbd8990b3651a22c0ff9a6e6" title="Pair a vision grounding model with a reasoning LLM with Cua" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans.&lt;/p&gt; &lt;p&gt;The problem: every GUI model speaks a different dialect. â€¢ some want pixel coordinates â€¢ others want percentages â€¢ a few spit out cursed tokens like &amp;lt;|loc095|&amp;gt;&lt;/p&gt; &lt;p&gt;We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.:&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;anthropic/claude-3-5-sonnet-20241022&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;But hereâ€™s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) â†’&lt;/p&gt; &lt;p&gt;agent = ComputerAgent( model=&amp;quot;huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o&amp;quot;, tools=[computer] )&lt;/p&gt; &lt;p&gt;This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working together.&lt;/p&gt; &lt;p&gt;Two specialists beat one generalist. Weâ€™ve got a ready-to-run notebook demo - curious what combos you all will try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5ayaaquncllf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1m8xh/pair_a_vision_grounding_model_with_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1m8xh/pair_a_vision_grounding_model_with_a_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1k2zg</id>
    <title>4 Months of Droidrun: How we started the Mobile Agent Race</title>
    <updated>2025-08-27T15:25:03+00:00</updated>
    <author>
      <name>/u/Sleyn7</name>
      <uri>https://old.reddit.com/user/Sleyn7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Back in April, I shared an early demo of DroidRun a side project we built to let AI agents interact with Android phones like real users. &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/xiZ7mbJ967"&gt;https://www.reddit.com/r/LocalLLaMA/s/xiZ7mbJ967&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Originally, it was just a tool to automate app usage and collect structured market intelligence. No UI. No docs. No product. Just a working prototype.&lt;/p&gt; &lt;p&gt;Then things escalated. We posted a short demo. It went viral. Within 48 hours, we hit 2,000+ GitHub stars. Shortly after, we closed our first funding round.&lt;/p&gt; &lt;p&gt;Other teams started entering the space. A few copied our approach. A Chinese university lab briefly overtook us on benchmarks. But we kept building and open-sourced everything.&lt;/p&gt; &lt;p&gt;We launched DroidRun on Product Hunt in July and to our surprise, we became Product of the Day. It was a huge moment that confirmed this new category PhoneUse agents was real. Since then, weâ€™ve been focused on turning a prototype into a framework and building an actual ecosystem around it.&lt;/p&gt; &lt;p&gt;I just wanted to thank all of you guys that were early supporters of this journey! Without you there wouldn't be such a strong community driving this category forward. So if you are interested in mobile Agents i would encourage you to join us, as this is just the beginning of PhoneUse.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sleyn7"&gt; /u/Sleyn7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k2zg/4_months_of_droidrun_how_we_started_the_mobile/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k2zg/4_months_of_droidrun_how_we_started_the_mobile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1k2zg/4_months_of_droidrun_how_we_started_the_mobile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1ciob</id>
    <title>2x5090 in Enthoo Pro 2 Server Edition</title>
    <updated>2025-08-27T09:40:17+00:00</updated>
    <author>
      <name>/u/arstarsta</name>
      <uri>https://old.reddit.com/user/arstarsta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ciob/2x5090_in_enthoo_pro_2_server_edition/"&gt; &lt;img alt="2x5090 in Enthoo Pro 2 Server Edition" src="https://preview.redd.it/7nx941hs8jlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a04cfdc54944678df8f971e0c40a0ed999536ec0" title="2x5090 in Enthoo Pro 2 Server Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arstarsta"&gt; /u/arstarsta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7nx941hs8jlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ciob/2x5090_in_enthoo_pro_2_server_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ciob/2x5090_in_enthoo_pro_2_server_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T09:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1kwp7</id>
    <title>I Built an Ollama Powered AI Tool that Found 40+ Live API Keys on GitHub Gists</title>
    <updated>2025-08-27T15:56:35+00:00</updated>
    <author>
      <name>/u/chocolateUI</name>
      <uri>https://old.reddit.com/user/chocolateUI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a side project I've been working on that turned out to be both fascinating and a little alarming. It's called Keyscan, and it's an AI-powered tool I built to scan GitHub Gists for exposed API keys. It uses Ollama under the hood, and you can run the tool on your own devices to search for API keys.&lt;/p&gt; &lt;p&gt;The idea came to me while I was working on another project and was looking at someone's gist. As I was reading the gist, I was struck by a random thought: What would happen if I searched for &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; on GitHub Gists? Would I actually find a real API key?&lt;/p&gt; &lt;p&gt;Turns out, yes. On the first page of results was a gist containing a Groq API key. I tested the key using curl, and to my surprise, it was live. I alerted the owner, but the whole experience stuck with me. How many other keys were out there, sitting in public gists?&lt;/p&gt; &lt;p&gt;So, a month later, I decided to stop wondering and start building. Over the course of a few days, I put together Keyscan. Keyscan uses a combination of the GitHub Gists API, a local LLM (Ollama), and some custom verification logic to identify and validate exposed API keys. The tool works in roughly three phases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Fetching: Searches Gists for specific keywords and file types, and fetches file contents.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Classification: Preprocesses file contents into lines, and uses an LLM to determine if a line contains an API key and identifies the provider.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Verification: Tests the key against the provider's API to see if it's live.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I ran Keyscan on a list of 100 keywords over two days and scanned around 2,500 Gists. In the end, I found over 40 live API keys, including keys for OpenAI, Mistral, Gemini, Groq, and much more.&lt;/p&gt; &lt;p&gt;One of the most ridiculous finds was a .env file where someone asked Claude to collate all their API keys and then uploaded the file to Gists. Yes, most of the keys were live.&lt;/p&gt; &lt;p&gt;If you would like to read more about Keyscan and my findings, do check out my Medium article.&lt;/p&gt; &lt;p&gt;&lt;a href="https://liaogg.medium.com/keyscan-eaa3259ba510"&gt;https://liaogg.medium.com/keyscan-eaa3259ba510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Keyscan is also completely open source on GitHub. I'm also looking for contributors who can help expand the current file type modules. Here is the link:&lt;/p&gt; &lt;p&gt;Let me know what you think about my project! I'd love to hear your feedback or ideas for improving Keyscan. Sorry for self-promotion, I think my project is worth a look.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chocolateUI"&gt; /u/chocolateUI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwp7/i_built_an_ollama_powered_ai_tool_that_found_40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwp7/i_built_an_ollama_powered_ai_tool_that_found_40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwp7/i_built_an_ollama_powered_ai_tool_that_found_40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1rssb</id>
    <title>[open source] We built a better reranker and open sourced it.</title>
    <updated>2025-08-27T20:13:26+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our research team just released the best performing and most efficient reranker out there, and it's available now as an open weight model on HuggingFace. Rerankers are critical in context engineering: they improve retrieval accuracy, and help you make the best use of limited context, whether for RAG or another use case.&lt;/p&gt; &lt;p&gt;Reranker v2 was designed specifically for agentic RAG, supports instruction following, and is multilingual.&lt;/p&gt; &lt;p&gt;Along with this, we're also open source our eval set, which allows you to reproduce our benchmark results. Back in March, when we introduced the world's first instruction-following reranker, it was SOTA on BEIR. After observing reranker use in production, we created an evaluation dataset that better matches real world use - focusing on QA-focused tests from several benchmarks. By releasing these datasets, we are also advancing instruction-following reranking evaluation, where high-quality benchmarks are currently limited.&lt;/p&gt; &lt;p&gt;Now all the weights for reranker V2 are live on HuggingFace: 1B, 2B, and 6B parameter models. I've been having fun building demos with earlier versions, like a reranker-based MCP server selector. Excited to try this out with the latest version!&lt;/p&gt; &lt;p&gt;Please give it a try and let us know what you think. Links to learn more in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1rssb/open_source_we_built_a_better_reranker_and_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T20:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n190vf</id>
    <title>NVIDIA Jet-Nemotron : 53x Faster Hybrid-Architecture Language Model Series</title>
    <updated>2025-08-27T05:53:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA Jet-Nemotron is a new LLM series which is about 50x faster for inferencing. The model introduces 3 main concept :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PostNAS&lt;/strong&gt;: a new search method that tweaks only attention blocks on top of pretrained models, cutting massive retraining costs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JetBlock&lt;/strong&gt;: a dynamic linear attention design that filters value tokens smartly, beating older linear methods like Mamba2 and GLA.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Attention&lt;/strong&gt;: keeps a few full-attention layers for reasoning, replaces the rest with JetBlocks, slashing memory use while boosting throughput.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/hu_JfJSqljo"&gt;https://youtu.be/hu_JfJSqljo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/html/2508.15884v1"&gt;https://arxiv.org/html/2508.15884v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n190vf/nvidia_jetnemotron_53x_faster_hybridarchitecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n190vf/nvidia_jetnemotron_53x_faster_hybridarchitecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n190vf/nvidia_jetnemotron_53x_faster_hybridarchitecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T05:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1h6xx</id>
    <title>Local Inference for Very Large Models - a Look at Current Options</title>
    <updated>2025-08-27T13:33:58+00:00</updated>
    <author>
      <name>/u/HvskyAI</name>
      <uri>https://old.reddit.com/user/HvskyAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I've been considering upgrading my hardware to run larger models locally, and thought I might get some thoughts from the community. &lt;em&gt;Fair warning - this is a bit of a hardware rant&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Currently, I'm running 2 x 3090 (48GB VRAM), and hence using EXL2/3 quants of ~70B models quite happily. They are leagues ahead of where the SOTA was a couple of years ago, and they fulfill most general use cases quite well. &lt;/p&gt; &lt;p&gt;That being said, there are increasingly larger and more capable MoE models releasing with open weights. Deepseek R1/V3 (671B32A), Kimi K2 (1000B/1T32A), GLM 4.5 (355B32A), Qwen 3 (235B22A)... &lt;/p&gt; &lt;p&gt;Being on a consumer board with an AM4 chip and DDR4 memory, going with GGUF/hybrid inference completely tanks my TG speeds. Therefore, I find myself looking at solutions to run these very large MoE models locally, and none of them seem particularly appealing: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Simply add more 3090's:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The VRAM price-to-performance ratio on these cards is unmatched, and they remain a mainstay for inference long after the 4090 and 5090 have released. Running two of these myself, I'm very happy with them. &lt;/p&gt; &lt;p&gt;But there are limitations to simply adding more and more 3090's. For one, at 24GB per card, one simply runs out of PCIe lanes on a consumer board. Yes, you could run Oculink and bifurcate with a lot of risers, but let's do the math here; a Q_4_K_M quant of Deepseek R1 comes in at 404GB for the weights alone. That's roughly 404 / 24 = 16.833..., or approximately 17 cards &lt;em&gt;before&lt;/em&gt; considering context, display output, embedding models, etc. &lt;/p&gt; &lt;p&gt;Even with a 2.22-bit dynamic quant from Unsloth, that's 183 / 24 = 7.625, so eight cards plus context and system overhead. &lt;/p&gt; &lt;p&gt;I mean, I could bifurcate, but I do think that's pushing it on an AM4 board. Even on something like the latest Threadripper Pro boards, you'd still be looking at bifurcation to fit enough cards for any reasonable quant. &lt;/p&gt; &lt;p&gt;This is before considering the other big issue - power consumption. Sure, PCIe bandwidth doesn't matter much for inference once the model is loaded, so bifurcation is no big deal. But 17+ cards on a single machine? Yes, the cards can be power limited to ~150W/card without impacting inference speed much, but that's still 17 x 150 = 2550W at &lt;em&gt;minimum&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;The power efficiency does not scale with these cards as we go into higher VRAM ranges, and physically interfacing enough cards becomes an issue. Otherwise, they're great. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Go&lt;/strong&gt; &lt;strong&gt;with a server motherboard, add fast multi-channel RAM, and run hybrid inference:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This seems like the most sane of the options. Granted, I'm not too knowledgeable about workstation/server hardware, so perhaps some better informed individuals could chime in here. &lt;/p&gt; &lt;p&gt;Assuming that multi-channel DDR5 memory is a priority for running MoE, something like the latest gen EPYC processors appear to meet the criteria; 12-channel DDR5, 128 PCIe 5.0 lanes, and plenty of memory capacity. Per-socket memory bandwidth is fairly reasonable on these, as well. &lt;/p&gt; &lt;p&gt;My concerns with hybrid inference are prompt processing speeds (I've heard that they can be slower, although it's difficult to get a hold of actual benchmark examples for specific configurations), cost of the system (the chips themselves are costly, and the board and memory are not cheap, either), and the fact that this all still requires some degree of GPU acceleration. &lt;/p&gt; &lt;p&gt;I suppose I just don't know enough about what to look for when it comes to server hardware. Memory bandwidth is a priority, but does the core/thread count and clock speed matter much for hybrid inference? &lt;/p&gt; &lt;p&gt;Some of the EPYC 9000-series chips are surprisingly well-priced relative to the memory and PCIe lanes offered, whereas they also go up to $10K without any notable increase in these areas. Surely I'm missing something here, and input would be appreciated. &lt;/p&gt; &lt;p&gt;Anyways, even with MoE models and selective management of experts, GPU acceleration is needed for acceptable TG speeds, which brings me to my next option. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Get GPUs with more VRAM per Card:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;So this would be something like the RTX 6000 Ada, RTX 6000 Pro (Blackwell), and so on. They're fast, have lots of VRAM, and are more power-efficient for inference purposes, where one is memory-bound as opposed to compute-bound (in a local enthusiast context, that is). &lt;/p&gt; &lt;p&gt;The RTX 6000 Pro in particular is appealing. 96GB of GDDR7 VRAM with a 512-bit memory bus means something around ~1.8 TB/s of memory bandwidth. Dual-slot form factor and 600W comes out to about the same power usage as power-limited 3090s for equivalent VRAM &lt;em&gt;before&lt;/em&gt; any power limiting. &lt;/p&gt; &lt;p&gt;Great option, then. Just get a few of these, right? &lt;/p&gt; &lt;p&gt;It's $9K per card, which comes out to around $93.75/GB of VRAM, whereas a used 3090 at $600 comes out to $25/GB. Yes, it's faster, and also dodges some of the aforementioned issues with having an entire rack of 3090s, but that's still quite a high premium to be paying - nearly 4x the cost on a per-GB basis. &lt;/p&gt; &lt;p&gt;I suppose the other option would be something like multiple modded 48GB 4090Ds from China, which I see are available for 23,000 HKD, or ~$3K. Apparently the VBIOS works with stock Nvidia firmware, but at this price ($62.5/GB) and a 384-bit memory bus, just like a 3090, I don't see much of an argument for these aside from the potential energy savings. &lt;/p&gt; &lt;p&gt;So the ideal solution is to just stuff 4+ RTX 6000 Pros into an EPYC server, but that would be extremely costly... After doing the breakdown on these, I do see why people still opt for power-limited 3090s. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. M3 Ultra w/ 512GB unified memory:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;This brings me to a - relatively - more budget-friendly option; an Apple Mac Studio with an M3 Ultra maxed out at 512GB unified memory comes in at around $10K, and would be able to fit R1 at 4-bits. The cost/memory ratio here is only barely matched by the 3090s ($600 x 17 = $10,200), and this is before considering a host system to house so many GPUs. The power efficiency is also significantly better. &lt;/p&gt; &lt;p&gt;The limitations are that TTFT (Time to First Token) is abysmal on these systems, the ecosystem for MLX and Metal are lacking in comparison to CUDA, and the machine is not modifiable or expandable in the future. &lt;/p&gt; &lt;p&gt;This option is appealing, if for no other reason than the fact that it is likely to cause the least headaches and work straight out of the box. That being said, my current machine is a water-cooled frankenstein of a PC, so the fact that I can't slot in an extra NVMe drive into a machine that costs $10K is a bit off-putting. &lt;/p&gt; &lt;p&gt;I've also only seen a few users reporting their experiences with Apple silicon, and it appears to be quite slow when the context fills up. Combine this with the fact that I prefer Linux, and have grown used to working with Nvidia-compatible back ends, and it looks like a bit of a band-aid fix and a dead end. &lt;/p&gt; &lt;p&gt;If anyone here is running maxed out M-series chips with any success, I'd love to hear how it's going for you. It's an elegant option, if somewhat limited in future scope. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Give up local inference, and just rent on the cloud:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;All this talk of ten-thousand dollar hardware and a dozen graphic cards makes me think of the ongoing electricity bill, which does beg the question - why not just go with a cloud rental/API? &lt;/p&gt; &lt;p&gt;The economics are undeniably in favor of this option, particular for the largest of the aforementioned models. Host an instance on Runpod and do your inference there, and only pay by the hour. Even better, go with an API provider and pay by the &lt;em&gt;token&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Think about how long it would take to even out on a $10K+ machine at the current rates that Deepseek's official API is charging. I mean, how much inference do you perform annually, really?&lt;/p&gt; &lt;p&gt;That being said, this is &lt;em&gt;local&lt;/em&gt; llama, and I think everyone here prefers to keep their information local and their model under their own control rather than outsourcing to a third party. It may be cost-inefficient, but if it's between paying a subscription and letting all my thoughts/code/documents go through OpenAI/Anthropic/Deepseek servers versus building a ridiculous machine that doubles as a room heater in the winter... &lt;/p&gt; &lt;p&gt;Well, I may be on the wrong side of history here, but sign me up for the latter. I'm staying local, and I'm willing to spend some cash to do it. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;So that's been a short overview of the options I see as available for local inference of very large models. Some are more viable than others, some more elegant than others, and some are much more expensive than others. &lt;/p&gt; &lt;p&gt;At the end of the day, if it performs well, then it's good. However, there are multiple ways to go about a task such as this. &lt;/p&gt; &lt;p&gt;Anyone with lots of 3090s, an EPYC board, a maxed out M-series chip, or anything else that can run massive MoE models locally - I'd be interested to hear your thoughts and experiences. &lt;/p&gt; &lt;p&gt;To the community at large, I'd like to hear where people are at with their local inference rigs, and what option here is most future-proof or appealing to you, and for what reasons. &lt;/p&gt; &lt;p&gt;Any and all input is welcome. &lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HvskyAI"&gt; /u/HvskyAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1h6xx/local_inference_for_very_large_models_a_look_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1h6xx/local_inference_for_very_large_models_a_look_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1h6xx/local_inference_for_very_large_models_a_look_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T13:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1oz10</id>
    <title>gpt-oss:120b running on an AMD 7800X3D CPU and a 7900XTX GPU</title>
    <updated>2025-08-27T18:26:21+00:00</updated>
    <author>
      <name>/u/PaulMaximumsetting</name>
      <uri>https://old.reddit.com/user/PaulMaximumsetting</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1oz10/gptoss120b_running_on_an_amd_7800x3d_cpu_and_a/"&gt; &lt;img alt="gpt-oss:120b running on an AMD 7800X3D CPU and a 7900XTX GPU" src="https://external-preview.redd.it/OHhxcThuejF1bGxmMYmYrWenOKzAjZFX7ODXsoNVhBW8BdQuVGzO-0hl-pk1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea5182abff3e8407a996e33379c2a0efd568efca" title="gpt-oss:120b running on an AMD 7800X3D CPU and a 7900XTX GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a quick demo of gpt-oss:120b running on an AMD 7800X3D CPU and a 7900XTX GPU. Approximately 21GB of VRAM and 51GB of system RAM are being utilized.&lt;/p&gt; &lt;p&gt;System Specifications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: AMD 7800X3D CPU&lt;/li&gt; &lt;li&gt;GPU: AMD 7900 XTX (24GB)&lt;/li&gt; &lt;li&gt;RAM: DDR5 running at 5200Mhz (Total system memory is nearly 190GB)&lt;/li&gt; &lt;li&gt;OS: Linux Mint&lt;/li&gt; &lt;li&gt;Interface: OpenWebUI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Performance: &lt;strong&gt;Averaging 7.48 tokens per second and 139 prompt tokens per second.&lt;/strong&gt; While not the fastest setup, it offers a relatively affordable option for building your own local deployment for these larger models. Not to mention there's plenty of room for additional context; however, keep in mind that a larger context window may slow things down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaulMaximumsetting"&gt; /u/PaulMaximumsetting &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiftmmz1ullf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1oz10/gptoss120b_running_on_an_amd_7800x3d_cpu_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1oz10/gptoss120b_running_on_an_amd_7800x3d_cpu_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T18:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1kils</id>
    <title>Free 1,000 CPU + 100 GPU hours for testers</title>
    <updated>2025-08-27T15:41:35+00:00</updated>
    <author>
      <name>/u/Ok_Post_149</name>
      <uri>https://old.reddit.com/user/Ok_Post_149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve always had a hard time getting data scientists and analyst to scale their code in the cloud. Most of the time theyâ€™d hand it off to DevOps, which created a massive backlog and DevOps would get spread super thin.&lt;/p&gt; &lt;p&gt;I built cluster compute software that lets any Python developer deploy to huge clusters (10k vCPUs, 1k GPUs) with a single function. You can bring your own Docker image, set hardware requirements, run jobs as background tasks so you can fire and forget, and responses are fast. You can call a million simple functions in a couple seconds.&lt;/p&gt; &lt;p&gt;Itâ€™s &lt;a href="https://github.com/Burla-Cloud/burla"&gt;open source&lt;/a&gt; and Iâ€™m still making install easier, but I also have a few managed versions. If you want to test I'll cover 1,000 CPU and 100 GPU hours. Hereâ€™s a tweet of me running it on a 4k vCPU cluster to screenshot 30k arXiv PDFs and push them to GCS: &lt;a href="https://x.com/infra_scale_5/status/1938024103744835961"&gt;https://x.com/infra_scale_5/status/1938024103744835961&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love some testers.&lt;/p&gt; &lt;p&gt;*core use cases are really meant for embarrassingly parallel workloads*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Post_149"&gt; /u/Ok_Post_149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kils/free_1000_cpu_100_gpu_hours_for_testers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kils/free_1000_cpu_100_gpu_hours_for_testers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kils/free_1000_cpu_100_gpu_hours_for_testers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1nnuw</id>
    <title>Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud.</title>
    <updated>2025-08-27T17:38:03+00:00</updated>
    <author>
      <name>/u/TeamEarly</name>
      <uri>https://old.reddit.com/user/TeamEarly</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"&gt; &lt;img alt="Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud." src="https://external-preview.redd.it/dW44NmZ0amZpbGxmMQuGtiBYImp4y5TE0Jp1MRc4u7lsWJVoGir-kxQWUhhu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=860470108798c3775427c269351d94be3f33b0f3" title="Elmer lets you use your locally-hosted models from anywhere, all relayed privately from your Mac to your iPhone via your personal iCloud." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering putting Elmer on TestFlight. It's an iOS/Mac app combo that lets you use your locally-hosted AI models &amp;amp; services (Ollama, LM Studio, ComfyUI) from anywhere, using your iPhone. &lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Remote access to your local AI setup via secure CloudKit relay&lt;/li&gt; &lt;li&gt;Auto-discovery: Just run the Mac app, iPhone finds it automatically&lt;/li&gt; &lt;li&gt;Multi-service: Works with Ollama, LM Studio, ComfyUI, and custom endpoints&lt;/li&gt; &lt;li&gt;No port forwarding: Uses your personal iCloud for secure tunneling between devices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Perfect for when you want to access your local setup's compute while mobile, without the complexity of VPNs or exposing ports. I'm still working on it but thinking of doing a TesFlight soon! &lt;/p&gt; &lt;p&gt;I'm curious if anyone has opinion about the relay strategy? I considered options like Cloudflare Tunnels, but iCloud felt most private. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamEarly"&gt; /u/TeamEarly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e5knbtjfillf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1nnuw/elmer_lets_you_use_your_locallyhosted_models_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1n6wr</id>
    <title>Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!</title>
    <updated>2025-08-27T17:20:26+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"&gt; &lt;img alt="Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!" src="https://external-preview.redd.it/fQ68o495vGTQI0wPf3lbyqrUgPukCFvd0dIhZUXV0Is.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571a08519f9d8c16a4098391bb0cbae0b5a43e5b" title="Drummer's GLM Steam 106B A12B v1 - A finetune of GLM Air aimed to improve creativity, flow, and roleplaying!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Stop me if you have already seen this...&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1n6wr/drummers_glm_steam_106b_a12b_v1_a_finetune_of_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1myth</id>
    <title>OpenAI has launched HealthBench on HuggingFace</title>
    <updated>2025-08-27T17:12:06+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"&gt; &lt;img alt="OpenAI has launched HealthBench on HuggingFace" src="https://b.thumbs.redditmedia.com/Cd1CiYSAFAOcmX901eEWMbsHfHloFXn3PMvvDhm3VkI.jpg" title="OpenAI has launched HealthBench on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/datasets/openai/healthbench"&gt;https://huggingface.co/datasets/openai/healthbench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n1myth"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1myth/openai_has_launched_healthbench_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T17:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1kwy4</id>
    <title>Smuggling Nvidia GPUs to China</title>
    <updated>2025-08-27T15:56:51+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"&gt; &lt;img alt="Smuggling Nvidia GPUs to China" src="https://external-preview.redd.it/YjBh51k4f4z8_hT8ywyQGSPmo_QffcXvjGcCE4rlHns.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af98a8faa60a778d7426ad70272313ccf7bded36" title="Smuggling Nvidia GPUs to China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The assembly process works this way â€” Nvidia designs the silicon (done all over the world, but theyâ€™re headquartered in California), and TSMC manufactures and fabricates the silicon in Taiwan. Then, Chinese companies manufacture â€” and sometimes engineer through contract â€” the cooling solutions, the PCB (printed circuit board), and source all the capacitors and voltage regulator components. Everything that makes one of these devices â€” pretty much everything â€” is sourced in China.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Very insightful interview, especially for those who did not have time to watch the entire video.&lt;/p&gt; &lt;p&gt;Personally I find the repair/recycle capability (aka &amp;quot;keeping silicon in circulation&amp;quot; - the way Steve describes it) to be way more significant factor than export bans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/how-gpus-get-smuggled-to-china"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1kwy4/smuggling_nvidia_gpus_to_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T15:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1amux</id>
    <title>Hugging Face has reached two million models.</title>
    <updated>2025-08-27T07:35:26+00:00</updated>
    <author>
      <name>/u/sstainsby</name>
      <uri>https://old.reddit.com/user/sstainsby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"&gt; &lt;img alt="Hugging Face has reached two million models." src="https://preview.redd.it/6basw10amilf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e12973c2fb3ede2accf2e8ae76cc63010a6ac51" title="Hugging Face has reached two million models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sstainsby"&gt; /u/sstainsby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6basw10amilf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1amux/hugging_face_has_reached_two_million_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T07:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1ece5</id>
    <title>TheDrummer is on fire!!!</title>
    <updated>2025-08-27T11:23:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="/u/TheLocalDrummer"&gt;u/TheLocalDrummer&lt;/a&gt; published lots of new models (finetunes) in the last days:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/GLM-Steam-106B-A12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Behemoth-X-123B-v2-GGUF"&gt;https://huggingface.co/TheDrummer/Behemoth-X-123B-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF"&gt;https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-12B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-4B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF"&gt;https://huggingface.co/TheDrummer/Gemma-3-R1-27B-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF"&gt;https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF"&gt;https://huggingface.co/TheDrummer/RimTalk-Mini-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are looking for something new to try - this is definitely the moment!&lt;/p&gt; &lt;p&gt;if you want more in progress models, please check discord and &lt;a href="https://huggingface.co/BeaverAI"&gt;https://huggingface.co/BeaverAI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1ece5/thedrummer_is_on_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T11:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1paeu</id>
    <title>What you think it will be..</title>
    <updated>2025-08-27T18:37:50+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt; &lt;img alt="What you think it will be.." src="https://preview.redd.it/68wbznvowllf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f674727eff3b01ce03cc3be22d7a1f41fa83009d" title="What you think it will be.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68wbznvowllf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1paeu/what_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T18:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
