<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-12T13:34:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hyvfjq</id>
    <title>What do you think of AI employees?</title>
    <updated>2025-01-11T13:03:35+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seeing a surge in start-ups and large enterprises building AI employees.&lt;/p&gt; &lt;p&gt;A good number of well-funded start-ups are building AI SDRs, SWEs, marketing agents, and Customer success agents. Even Salesforce is working on AgentForce to create no-code salesforce automation agents.&lt;/p&gt; &lt;p&gt;This trend is growing faster than I thought; dozens of start-ups are probably in YC this year.&lt;/p&gt; &lt;p&gt;I’m not sure if any of them are in production doing the jobs in the real world, and also, these agents may require a dozen integrations to be anywhere close to being functional.&lt;/p&gt; &lt;p&gt;As much as I like LLMs, they still don’t seem capable of handling edge cases in real-world jobs. They may be suitable for building automated pipelines for tightly scoped tasks, but replacing humans seems far-fetched.&lt;/p&gt; &lt;p&gt;Salesforce Chairman Mark Benioff even commented on not hiring human employees anymore; though it could be their sneaky marketing, it shows their intent.&lt;/p&gt; &lt;p&gt;What do you think of this AI employee in general the present and future? I would love to hear your thoughts if you’re building something simillar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyrml</id>
    <title>[Mini Rant] Are LLMs trapped in English and the assistant paradigms?</title>
    <updated>2025-01-11T15:52:33+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;br /&gt; It feels like we’re trapped in two mainstream paradigms, and it’s starting to get on my nerves. Let me explain:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLMs (too) focused on English&lt;/strong&gt;&lt;br /&gt; We’re seeing more and more models—Qwen, Mistral, Llama 3.x, etc.—that claim “multilingual” abilities. And if you look closely, everyone approaches the problem differently. However, my empirical scenarios often fail to deliver a good experience with those LLMs, even at a 70B scale.&lt;br /&gt; Yes, I understand English reaches the largest audience, but by focusing everything on English, we’re limiting the nuanced cultural and stylistic richness of other languages (French, Spanish, Italian, etc.).&lt;br /&gt; As a result, we rarely see new “styles” or modes of reasoning outside of English.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The “assistant” obsession&lt;/strong&gt;&lt;br /&gt; Everyone wants to build a conversation assistant. Sure, it’s a popular use case,&lt;br /&gt; but it kind of locks us into a single format: a Q&amp;amp;A flow with a polite, self-censored style.&lt;br /&gt; We forget these are token generators that could be tweaked for creative text manipulation or other forms of generation.&lt;br /&gt; I really wish we’d explore more diverse use cases: scenario generation, data-to-text, or other conversation protocols that aren’t so uniform.&lt;/p&gt; &lt;p&gt;I understand that model publishers invest significant resources into performing benchmarks and enhancing multilingual capabilities. For instance, Aya Expanse by Cohere For AI represents a notable advancement in this area. Despite these efforts, in real-world scenarios, I’ve never been able to achieve the same level of performance in French as in English with open-source models. Conversely, closed-source models maintain a more consistent performance across languages, which is frustrating because I’d prefer using open-source models.&lt;/p&gt; &lt;p&gt;Am I the only one who feels we’re stuck between “big English-only LLMs” and “conversation assistant” paradigms? I think there’s so much potential out there for better multilingual support and more interesting use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzlwox</id>
    <title>Any EU based people/companies that build and sell AI servers?</title>
    <updated>2025-01-12T12:45:05+00:00</updated>
    <author>
      <name>/u/EternalOptimister</name>
      <uri>https://old.reddit.com/user/EternalOptimister</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most people that build custom machines are in the US. I am considering to buy/build servers with older/alternative/used GPUs to decrease the overall cost.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EternalOptimister"&gt; /u/EternalOptimister &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlwox/any_eu_based_peoplecompanies_that_build_and_sell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlwox/any_eu_based_peoplecompanies_that_build_and_sell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlwox/any_eu_based_peoplecompanies_that_build_and_sell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T12:45:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzlxul</id>
    <title>You don't need agents for code AI, according to Agentless. Is anybody using this approach?</title>
    <updated>2025-01-12T12:47:07+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You don’t need fancy agent tools to solve complex code problems. Agentless is a non-agent framework used by OpenAI to get high accuracy on the SWE Bench with o3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlxul/you_dont_need_agents_for_code_ai_according_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlxul/you_dont_need_agents_for_code_ai_according_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlxul/you_dont_need_agents_for_code_ai_according_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T12:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu2dh</id>
    <title>LocalGLaDOS - running on a real LLM-rig</title>
    <updated>2025-01-11T11:34:21+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt; &lt;img alt="LocalGLaDOS - running on a real LLM-rig" src="https://external-preview.redd.it/EfE2n_bbhcmfaS9RbA5FtQq7jGIahU2UIGm8g-a1Uag.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ce4ca891cbd89dfa15f29ba5ffa968064f42e85" title="LocalGLaDOS - running on a real LLM-rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-GHKTocDF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyils</id>
    <title>Nvidia 50x0 cards are not better than their 40x0 equivalents</title>
    <updated>2025-01-11T15:40:50+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking closely at the specs, I found 40x0 equivalents for the new 50x0 cards except for 5090. Interestingly, all 50x0 cards are not as energy efficient as the 40x0 cards. Obviously, GDDR7 is the big reason for the significant boost in memory bandwidth for 50x0.&lt;/p&gt; &lt;p&gt;Unless you really need FP4 and DLSS4, there are not that strong a reason to buy the new cards. For the 4070Super/5070 pair, the former can be 15% faster in prompt processing and the latter is 33% faster in inference. If you value prompt processing, it might even make sense to buy the 4070S.&lt;/p&gt; &lt;p&gt;As I mentioned in another thread, this gen is more about memory upgrade than the actual GPU upgrade.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;4070 Super&lt;/th&gt; &lt;th align="left"&gt;5070&lt;/th&gt; &lt;th align="left"&gt;4070Ti Super&lt;/th&gt; &lt;th align="left"&gt;5070Ti&lt;/th&gt; &lt;th align="left"&gt;4080 Super&lt;/th&gt; &lt;th align="left"&gt;5080&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;141.93&lt;/td&gt; &lt;td align="left"&gt;123.37&lt;/td&gt; &lt;td align="left"&gt;176.39&lt;/td&gt; &lt;td align="left"&gt;175.62&lt;/td&gt; &lt;td align="left"&gt;208.9&lt;/td&gt; &lt;td align="left"&gt;225.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;220&lt;/td&gt; &lt;td align="left"&gt;250&lt;/td&gt; &lt;td align="left"&gt;285&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;320&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;656.12&lt;/td&gt; &lt;td align="left"&gt;493.49&lt;/td&gt; &lt;td align="left"&gt;618.93&lt;/td&gt; &lt;td align="left"&gt;585.39&lt;/td&gt; &lt;td align="left"&gt;652.8&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GB/s&lt;/td&gt; &lt;td align="left"&gt;504&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;896&lt;/td&gt; &lt;td align="left"&gt;736&lt;/td&gt; &lt;td align="left"&gt;960&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Price at Launch&lt;/td&gt; &lt;td align="left"&gt;$599&lt;/td&gt; &lt;td align="left"&gt;$549&lt;/td&gt; &lt;td align="left"&gt;$799&lt;/td&gt; &lt;td align="left"&gt;$749&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz254t</id>
    <title>New finetune Negative_LLAMA_70B</title>
    <updated>2025-01-11T18:20:50+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's January 2025, and still, there are very few models out there that have successfully tackled LLM's positivity bias. &lt;strong&gt;LLAMA 3.3&lt;/strong&gt; was received in the community with mixed feelings. It is an exceptional assistant, and superb at instruction following (&lt;strong&gt;highest IFEVAL&lt;/strong&gt; to date, and by a large margin too.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem-&lt;/strong&gt; it is very predictable, dry, and of course, plaugued with positivity bias like all other LLMs. &lt;strong&gt;Negative_LLAMA_70B&lt;/strong&gt; is &lt;strong&gt;not&lt;/strong&gt; an unalignment-focused model (even though it's pretty uncensored), but it is my attempt to address positivity bias while keeping the exceptional intelligence of the &lt;strong&gt;LLAMA 3.3 70B&lt;/strong&gt; base model. Is the base 3.3 smarter than my finetune? I'm pretty sure it is, however, Negative_LLAMA_70B is still pretty damn smart.&lt;/p&gt; &lt;p&gt;The model was &lt;strong&gt;NOT&lt;/strong&gt; overcooked with unalignment, so it won't straight up throw morbid or depressing stuff at you, but if you were to ask it to write a story, or engage in an RP, you would notice &lt;strong&gt;slightly&lt;/strong&gt; darker undertones. In a long trip, the character takes in a story- their legs will be hurt and would feel tired, in &lt;strong&gt;Roleplay&lt;/strong&gt; when you seriously piss off a character- it might hit you (without the need to explicitly prompt such behavior in the character card).&lt;/p&gt; &lt;p&gt;Also, &lt;strong&gt;toxic-dpo&lt;/strong&gt; and other morbid unalignment datasets were &lt;strong&gt;not&lt;/strong&gt; used. I did include a private dataset that should allow total freedom in both &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt;, and quite a lot of various assistant-oriented tasks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B?not-for-all-audiences=true#tldr"&gt;&lt;/a&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Less positivity bias&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;exceptionally good&lt;/strong&gt; at following the character card.&lt;/li&gt; &lt;li&gt;Characters feel more &lt;strong&gt;'alive'&lt;/strong&gt;, and will occasionally &lt;strong&gt;initiate stuff on their own&lt;/strong&gt; (without being prompted to, but fitting to their character).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong ability&lt;/strong&gt; to comprehend and roleplay &lt;strong&gt;uncommon physical and mental characteristics&lt;/strong&gt;. TL;DR Strong Roleplay &amp;amp; Creative writing abilities. Less positivity bias. Very smart assistant with low refusals. exceptionally good at following the character card. Characters feel more 'alive', and will occasionally initiate stuff on their own (without being prompted to, but fitting to their character). Strong ability to comprehend and roleplay uncommon physical and mental characteristics.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B"&gt;https://huggingface.co/SicariusSicariiStuff/Negative_LLAMA_70B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz254t/new_finetune_negative_llama_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmpuq</id>
    <title>VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?</title>
    <updated>2025-01-12T13:31:43+00:00</updated>
    <author>
      <name>/u/SpudMonkApe</name>
      <uri>https://old.reddit.com/user/SpudMonkApe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt; &lt;img alt="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" src="https://external-preview.redd.it/aphKSMbfvfDHStraL4JSGgDfke__oze-3mdG_k4jOVQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37b50e3ca1b1a72567f853cc77c80c80b325c53a" title="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpudMonkApe"&gt; /u/SpudMonkApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcmag.com/news/vlc-media-player-to-use-ai-to-generate-subtitles-for-videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzew4v</id>
    <title>Speculative decoding isn't coming to ollama anytime soon, any alternatives?</title>
    <updated>2025-01-12T04:36:53+00:00</updated>
    <author>
      <name>/u/ServeAlone7622</name>
      <uri>https://old.reddit.com/user/ServeAlone7622</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to this recently &lt;a href="https://github.com/ollama/ollama/pull/8134"&gt;rejected PR&lt;/a&gt; ollama isn't going to bring draft models and speculative decoding in any time soon. I'd very much like to have this feature. I tried it out on mlx and it seems to be more than a token speed up. It seems to take the &amp;quot;voice&amp;quot; of the draft model and integrate it into the larger model. I guess this is a type of steering?&lt;/p&gt; &lt;p&gt;Imagine giving something like small stories to a 128k context model!&lt;/p&gt; &lt;p&gt;In any event, I'd use mlx but my use case isn't purely apple. &lt;/p&gt; &lt;p&gt;Does anyone have suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ServeAlone7622"&gt; /u/ServeAlone7622 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzew4v/speculative_decoding_isnt_coming_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T04:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzka74</id>
    <title>Are there any base models (not chat or instruction tuned) with vision support?</title>
    <updated>2025-01-12T10:53:52+00:00</updated>
    <author>
      <name>/u/ElectricalAngle1611</name>
      <uri>https://old.reddit.com/user/ElectricalAngle1611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. also links to ggufs would be nice if they exist!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalAngle1611"&gt; /u/ElectricalAngle1611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzka74/are_there_any_base_models_not_chat_or_instruction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzka74/are_there_any_base_models_not_chat_or_instruction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzka74/are_there_any_base_models_not_chat_or_instruction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T10:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X: &lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt;hf: &lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview&lt;/a&gt; blog: &lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;https://novasky-ai.github.io/posts/sky-t1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzlyf5</id>
    <title>MCPAdapt leverage 650+ MCP server as tools in any agentic framework.</title>
    <updated>2025-01-12T12:48:10+00:00</updated>
    <author>
      <name>/u/gaarll</name>
      <uri>https://old.reddit.com/user/gaarll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there,&lt;/p&gt; &lt;p&gt;I just open-sourced a repository that allows to seamlessly integrate MCP server as tools in any agentic framework starting with smolagents: &lt;a href="https://github.com/grll/mcpadapt"&gt;https://github.com/grll/mcpadapt&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have a look let me know what you think and join me in adapting MCP servers for other frameworks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaarll"&gt; /u/gaarll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzlyf5/mcpadapt_leverage_650_mcp_server_as_tools_in_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T12:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzi6ed</id>
    <title>What can i do with 6GB of VRAM ?</title>
    <updated>2025-01-12T08:16:41+00:00</updated>
    <author>
      <name>/u/vsh46</name>
      <uri>https://old.reddit.com/user/vsh46</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had built a small rig for mining and learning AI in 2019.&lt;/p&gt; &lt;p&gt;Following are the system specs :- - GPU: Asus GTX 1660 Ti (6GB VRAM) - CPU: Intel Core i5-9400F - RAM: Corsair 8GB - SSD: WD Blue NAND SATA (250GB) - HDD: WD Blue SATA (1TB)&lt;/p&gt; &lt;p&gt;I wanted to make use of this system by running some local llm with high toks / sec with decent use for tasks like tool calling, instruction following, coding and basic home assistant based QnA. Mainly planning to use this system like a jarvis at home which does basic tasks but is very good at those.&lt;/p&gt; &lt;p&gt;Any recommendations of some LLMs or Frameworks that can help me achieve this would really help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vsh46"&gt; /u/vsh46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzi6ed/what_can_i_do_with_6gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T08:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz0n8c</id>
    <title>GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</title>
    <updated>2025-01-11T17:15:58+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt; &lt;img alt="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" src="https://external-preview.redd.it/fWekNX9cjJo2NgR6zTyYnqvItoILS5GvTDAQC2foz30.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13f97a5793ce6881c43646a9bce53d9dbbf16b98" title="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzcxby</id>
    <title>moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others.</title>
    <updated>2025-01-12T02:46:02+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt; &lt;img alt="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." src="https://preview.redd.it/c73tl9ko7hce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecac7f710d04af5ce2b21d2e0825db84a321f21" title="moondream CAPTCHAs test. It's surprisingly accurate at solving rotation CAPTCHAs, but not so much for the others." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c73tl9ko7hce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzcxby/moondream_captchas_test_its_surprisingly_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz2rar</id>
    <title>Why we don't know researchers behind DeepSeek?</title>
    <updated>2025-01-11T18:48:03+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero interviews, zero social activity. Zero group photos, none about us page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz2rar/why_we_dont_know_researchers_behind_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hze1xk</id>
    <title>6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!</title>
    <updated>2025-01-12T03:48:19+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt; &lt;img alt="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" src="https://external-preview.redd.it/eDd2Nndjb3ppaGNlMUVpVcA3yZ4wjFwvAE4TdXYq4bpkJwG-QulsV1F4T0eu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=475a3c9c78a9d27249a2c72bcf664f75b7f9639c" title="6x AMD Instinct Mi60 AI Server vs Llama 405B + vLLM + Open-WebUI - Impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r3w7zbozihce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hze1xk/6x_amd_instinct_mi60_ai_server_vs_llama_405b_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T03:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz7l7d</id>
    <title>OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model</title>
    <updated>2025-01-11T22:23:27+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt; &lt;img alt="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" src="https://preview.redd.it/nhsep8z3xfce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b460c09bdabd5d02e8e8e46757749c4711b75f" title="OpenAI is losing money , meanwhile qwen is planning voice mode , imagine if they manage to make o1 level model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhsep8z3xfce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz7l7d/openai_is_losing_money_meanwhile_qwen_is_planning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T22:23:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz5caf</id>
    <title>Tutorial: Run Moondream 2b's new gaze detection on any video</title>
    <updated>2025-01-11T20:42:31+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt; &lt;img alt="Tutorial: Run Moondream 2b's new gaze detection on any video" src="https://external-preview.redd.it/a2VmczhmdHllZmNlMTF40J1mEmizgXzWsZQRgxJwv14NVEzVGBQqF-uixs9J.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6d357b58a592f06ed596b1615e185d70bfedfdf" title="Tutorial: Run Moondream 2b's new gaze detection on any video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i9ofbftyefce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz5caf/tutorial_run_moondream_2bs_new_gaze_detection_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T20:42:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzd3xs</id>
    <title>Qwen releases Qwen Chat (online)</title>
    <updated>2025-01-12T02:56:09+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chat.qwenlm.ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzd3xs/qwen_releases_qwen_chat_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T02:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzfjmp</id>
    <title>Parking Systems analysis and Report Generation with Computer vision and Ollama</title>
    <updated>2025-01-12T05:15:40+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt; &lt;img alt="Parking Systems analysis and Report Generation with Computer vision and Ollama " src="https://external-preview.redd.it/ZDUxcHMwOGt5aGNlMZNdfj6QUni_z9Bf_NJiTzUymfkgPwnfSrss06zjR7A1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f892ce79dd89dd0ae0171dc7ac8e70e942f8504" title="Parking Systems analysis and Report Generation with Computer vision and Ollama " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2tf8yz7kyhce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T05:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz97my</id>
    <title>they don’t know how good gaze detection is on moondream</title>
    <updated>2025-01-11T23:38:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt; &lt;img alt="they don’t know how good gaze detection is on moondream" src="https://external-preview.redd.it/anBia3RnaGhhZ2NlMSTi0DO1FtxEm4mYFQVOtZR8uuj4lv59wjB_E-Pc4Mjr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3206ad0d968f5e06525f4113c574566e35551fb1" title="they don’t know how good gaze detection is on moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xgysp5nhagce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T23:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzany5</id>
    <title>We are an AI company now!</title>
    <updated>2025-01-12T00:47:37+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt; &lt;img alt="We are an AI company now!" src="https://preview.redd.it/0yl0970umgce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53963d0db45722eea8467f27c91ca48e5a7cf6fc" title="We are an AI company now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0yl0970umgce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T00:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzkw3f</id>
    <title>DeepSeek V3 is the gift that keeps on giving!</title>
    <updated>2025-01-12T11:37:25+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt; &lt;img alt="DeepSeek V3 is the gift that keeps on giving!" src="https://preview.redd.it/fj10nizoujce1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b294748a76dfcaf7f0f25300479cd3ea3b25308" title="DeepSeek V3 is the gift that keeps on giving!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fj10nizoujce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T11:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz28ld</id>
    <title>Bro whaaaat?</title>
    <updated>2025-01-11T18:24:57+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt; &lt;img alt="Bro whaaaat?" src="https://preview.redd.it/cwi5l2ziqece1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6895d12163dd294798940a5c5b6368da7f91b2f" title="Bro whaaaat?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwi5l2ziqece1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:24:57+00:00</published>
  </entry>
</feed>
