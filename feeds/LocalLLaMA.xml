<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-10T07:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j7iaz4</id>
    <title>WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp; Qdrant</title>
    <updated>2025-03-09T21:33:40+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"&gt; &lt;img alt="WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp;amp; Qdrant" src="https://external-preview.redd.it/S9bl2NwR9T_6QLlvwFTmOUTu7C-3tJMOo7MGY2kvTlc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a94f23393f1d8c9c0739f49ff1c6b1de91d3f233" title="WebRAgent: A Retrieval-Augmented Generation (RAG) Web App Built with Ollama &amp;amp; Qdrant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dkruyt/WebRAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7iaz4/webragent_a_retrievalaugmented_generation_rag_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T21:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ny65</id>
    <title>Microsoft Phi-4 and Phi-4 Multi Modal Instruct</title>
    <updated>2025-03-10T02:02:51+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some interesting observations with Phi-4.&lt;br /&gt; Looks like when they went from the original 14B to the smaller 5B, a lot of capabilities were degraded - some of it is expected given the smaller size, but I was surprised how much of a differential exists.&lt;/p&gt; &lt;p&gt;More details here:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=nJDSZD8zVVE"&gt;https://www.youtube.com/watch?v=nJDSZD8zVVE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ny65/microsoft_phi4_and_phi4_multi_modal_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ny65/microsoft_phi4_and_phi4_multi_modal_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ny65/microsoft_phi4_and_phi4_multi_modal_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T02:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72gw5</id>
    <title>AMD May Bring ROCm Support On Windows Operating System As AMDâ€™s Vice President Nods For It</title>
    <updated>2025-03-09T07:30:47+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/"&gt;https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T07:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7s3rg</id>
    <title>Zonos Install issues. I've followed a few tutorials and followed them to a T but I keep getting this error when I run 1, install-uv-qinglong Any idea how to get this to work?</title>
    <updated>2025-03-10T06:08:14+00:00</updated>
    <author>
      <name>/u/NatCanDo</name>
      <uri>https://old.reddit.com/user/NatCanDo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s3rg/zonos_install_issues_ive_followed_a_few_tutorials/"&gt; &lt;img alt="Zonos Install issues. I've followed a few tutorials and followed them to a T but I keep getting this error when I run 1, install-uv-qinglong Any idea how to get this to work?" src="https://preview.redd.it/kqsjylhozsne1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecc99f49fbd0c63b2f184ee38acf9a7996ff764a" title="Zonos Install issues. I've followed a few tutorials and followed them to a T but I keep getting this error when I run 1, install-uv-qinglong Any idea how to get this to work?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NatCanDo"&gt; /u/NatCanDo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqsjylhozsne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s3rg/zonos_install_issues_ive_followed_a_few_tutorials/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s3rg/zonos_install_issues_ive_followed_a_few_tutorials/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T06:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6yxdr</id>
    <title>Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf (and Thinking/Reasoning MOES...) ... 34+ new models (Lllamas, Qwen - MOES and not Moes..)</title>
    <updated>2025-03-09T03:46:25+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From David_AU ;&lt;/p&gt; &lt;p&gt;First two models based on Qwen's off the charts &amp;quot;QwQ 32B&amp;quot; model just released, with some extra power. Detailed instructions, and examples at each repo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Model, Free thinker, Extra Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular, Not so Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AND Qwen/Llama Thinking/Reasoning MOES - all sizes, shapes ...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;34&lt;/strong&gt; reasoning/thinking models (example generations, notes, instructions etc):&lt;/p&gt; &lt;p&gt;Includes Llama 3,3.1,3.2 and Qwens, DeepSeek/QwQ/DeepHermes in MOE and NON MOE config plus others:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is an interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf"&gt;https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For Qwens (12 models) only (Moes and/or Enhanced):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde"&gt;https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Separate source / full precision sections/collections at main repo here:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;656 Models, in 27 collections:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU"&gt;https://huggingface.co/DavidAU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LORAs for Deepseek / DeepHermes - &amp;gt; Turn any Llama 8b into a thinking model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Several LORAs for Llama 3, 3.1 to convert an 8B Llama model to &amp;quot;thinking/reasoning&amp;quot;, detailed instructions included on each LORA repo card. Also Qwen, Mistral Nemo, and Mistral Small adapters too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-reasoning-adapters-loras-any-model-to-reasoning-67bdb1a7156a97f6ec42ce36"&gt;https://huggingface.co/collections/DavidAU/d-au-reasoning-adapters-loras-any-model-to-reasoning-67bdb1a7156a97f6ec42ce36&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Special service note for Lmstudio users:&lt;/p&gt; &lt;p&gt;The issue with QwQs (32B from Qwen and mine 35B) re: Templates/Jinja templates has been fixed. Make sure you update to build 0.3.12 ; otherwise manually select CHATML template to work with the new QwQ models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T03:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79oma</id>
    <title>Why ate we not seeing much desktop apps developed with local AI integration,by smaller developers?</title>
    <updated>2025-03-09T15:13:54+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean there is huge market out there and there are infinite categories of desktop apps that can benefit from inyegrating local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r1sm</id>
    <title>Understanding context length and memory usage</title>
    <updated>2025-03-10T04:56:51+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried using QwQ Q4 with the default (2k) context length in Ollama and it Ollama ps shows 23GB of memory used. When I changed it to 16k (16384), the memory used changed to 68GB! That's a lot more than I expected. Is there a way to understand how context affects VRAM usage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r1sm/understanding_context_length_and_memory_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r1sm/understanding_context_length_and_memory_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r1sm/understanding_context_length_and_memory_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T04:56:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7fviw</id>
    <title>is anyone else getting extremely nerfed results for qwq?</title>
    <updated>2025-03-09T19:46:40+00:00</updated>
    <author>
      <name>/u/Mr_Cuddlesz</name>
      <uri>https://old.reddit.com/user/Mr_Cuddlesz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im running qwq fp16 on my local machine but it seems to be performing much worse vs. qwq on qwen chat. is anyone else experiencing this? i am running this: &lt;a href="https://ollama.com/library/qwq:32b-fp16"&gt;https://ollama.com/library/qwq:32b-fp16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Cuddlesz"&gt; /u/Mr_Cuddlesz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T19:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7baw1</id>
    <title>What GPU do you use for 32B/70B models, and what speed do you get?</title>
    <updated>2025-03-09T16:28:29+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What GPU are you using for 32B or 70B models? How fast do they run in tokens per second?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T16:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7ep0m</id>
    <title>12V-2x6 Power Connector Cooks At Over 150Â°C With A "Water-Cooled" NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage</title>
    <updated>2025-03-09T18:56:09+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"&gt; &lt;img alt="12V-2x6 Power Connector Cooks At Over 150Â°C With A &amp;quot;Water-Cooled&amp;quot; NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage" src="https://external-preview.redd.it/Lr1gQ1IkpnQxgTpcgRzn8EDKNAzaJLo3Qv8Ts8UVing.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ff7729a068308654c949a0e21e53869bc166978" title="12V-2x6 Power Connector Cooks At Over 150Â°C With A &amp;quot;Water-Cooled&amp;quot; NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/12v-2x6-power-connector-cooks-at-over-150c-with-a-water-cooled-nvidia-geforce-rtx-5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7ep0m/12v2x6_power_connector_cooks_at_over_150c_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79ev6</id>
    <title>Open WebUi + Tailscale = Beauty</title>
    <updated>2025-03-09T15:01:05+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I might be late to this party but just wanted to advertise for anyone who needs a nudge, if you have a good solution for running local LLMs but find it difficult to take it everywhere with you, or find the noise of fans whirring up distracting to you or others around you, you should check this out.&lt;/p&gt; &lt;p&gt;I've been using Open Web UI for ages as my front end for Ollama and it is fantastic. When I was at home I could even use it on my phone via the same network.&lt;/p&gt; &lt;p&gt;At work a coworker recently suggested I look into Tailscale and wow I am blown away by this. In short, you can easily create your own VPN and never have to worry about setting up static IPs or VIPs or NAT traversal or port forwarding. Basically a simple installer on any device (including your phones).&lt;/p&gt; &lt;p&gt;With that done, you can then (for example) connect your phone directly to the Open WebUI you have running on your desktop at home from anywhere in the world, from any connection, and never have to think about the connectivity again. All e2e encrypted. Mesh network no so single point of failure. &lt;/p&gt; &lt;p&gt;Is anyone else using this? I searched and saw some side discussions but not a big dedicated thread recently.&lt;/p&gt; &lt;p&gt;10/10 experience and HIGHLY recommended to give it a try. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j75xpm</id>
    <title>Dumb question - I use Claude 3.5 A LOT, what setup would I need to create a comparable local solution?</title>
    <updated>2025-03-09T11:45:31+00:00</updated>
    <author>
      <name>/u/Friendly_Signature</name>
      <uri>https://old.reddit.com/user/Friendly_Signature</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a hobbyist coder that is now working on bigger personal builds. (I was Product guy and Scrum master for AGES, now I am trying putting the policies I saw around me enforced on my own personal build projects). &lt;/p&gt; &lt;p&gt;Loving that I am learning by DOING my own CI/CD, GitHub with apps and Actions, using Rust instead of python, sticking to DDD architecture, TD development, etc&lt;/p&gt; &lt;p&gt;I spend a lot on Claude, maybe enough that I could justify a decent hardware purchase. It seems the new Mac Studio M3 Ultra pre-config is aimed directly at this market?&lt;/p&gt; &lt;p&gt;Any feedback welcome :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly_Signature"&gt; /u/Friendly_Signature &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T11:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7p21s</id>
    <title>Thinking is challenging (how to run deepseek and qwq)</title>
    <updated>2025-03-10T03:00:52+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, when I want a webui I use oobabooga, when I need an api I run vllm or llama.cpp and when I feel creative I use and abuse of silly tavern. Call me old school if you wantðŸ¤™ &lt;/p&gt; &lt;p&gt;But with these thinking models there's a catch. The &amp;lt;thinking&amp;gt; part should be displayed to the user but should not be incorporated in the context for the next message in a multi-turn conversation.&lt;/p&gt; &lt;p&gt;As far as I know no webui does that, there is may be a possibility with open-webui, but I don't understand it very well (yet?).&lt;/p&gt; &lt;p&gt;How do you do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7p21s/thinking_is_challenging_how_to_run_deepseek_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7p21s/thinking_is_challenging_how_to_run_deepseek_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7p21s/thinking_is_challenging_how_to_run_deepseek_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T03:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7q3ms</id>
    <title>I've uploaded new Wilmer users, and made another tutorial vid showing setup plus ollama hotswapping multiple 14b models on a single RTX 4090</title>
    <updated>2025-03-10T03:59:59+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright folks, so a few days back I was talking about some of my development workflows using Wilmer and had promised to try to get those released this weekend, as well as a video on how to use them, and also again showing the Ollama model hot-swapping so that a single 4090 can run as many 14-24b models as you have hard drive space for. I finished just in time lol&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/v2xYQCHZwJM"&gt;The tutorial vid on Youtube&lt;/a&gt; &lt;em&gt;(pop to the 34 minute mark to see a quick example of the wikipedia workflow)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;For the hotswapping: I show it in the video, but basically every node in the workflow can hit a different LLM API, right? So if you have 10 nodes, you could hit 10 different APIs. With Ollama, you can just keep hitting the same API endpoint (say 127.0.0.1:11434), but each time you send a different model name. That will cause Ollama to unload the previous model, and load a new model. So even with 24GB of VRAM, you could have a workflow that uses a bunch of 8-24b models, and swaps them out on each node. Gives a little freedom to do more complex stuff with.&lt;/p&gt; &lt;p&gt;I've added 6 new example users to the &lt;a href="https://github.com/someoddcodeguy/wilmerAI/"&gt;WilmerAI &lt;/a&gt;repository, set with the models that I use for development/testing on my 24GB VRAM windows machine and all set up with Ollama multi-modal image support (they also should be able to handle multiple images in 1 message, instead of just 1 image at a time):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;coding-complex-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is a long workflow. Really long. Using my real model lineup on the M2 Ultra, this workflow can take as long as 30-40 minutes to finish. But the result is usually worth it. I've had this one resolve issues o3-mini-high could not, and I've had 4o, o1, and o3-mini-high rate a lot of its work as better than 4o and o1.&lt;/li&gt; &lt;li&gt;I generally kick this guy off at the start of working on something, or when something gets really frustrating, but otherwise don't use often.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-reasoning-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This workflow is heavily built around a reasoning workflow. Second heaviest hitter; takes a while with QwQ, but worth the result&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-dual-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is generally 2 non-reasoning models. Much faster than the first two.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;coding-single-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is just 1 model, usually my coder like Qwen2.5 32b Coder.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;general-multi-model&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is a general purpose model, usually something mid-range like Qwen2.5 32b Instruct&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;general-offline-wikipedia&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;This is the general purpose model but injects a full text wikipedia article to help with factual responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These are 6 of the 11 or so Wilmer instances I keep running to help with development; another 2 instances are two more general models: large (for factual answers like Qwen2.5 72b Instruct) and large-rag (something with high IF scores like Llama 3.3 70b Instruct).&lt;/p&gt; &lt;p&gt;Additionally, I've added a new Youtube tutorial video, which walks through downloading Wilmer, setting up a user, running it, and hitting it with a curl command.&lt;/p&gt; &lt;p&gt;Anyhow, hope this stuff is helpful! Eventually Roland will be in a spot that I can release it, but that's still a bit away. I apologize if there are any mistakes or issues in these users; after QwQ came out I completely reworked some of these workflows to make use of that model, so I hope it helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q3ms/ive_uploaded_new_wilmer_users_and_made_another/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T03:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j77rkb</id>
    <title>I've made Deepseek R1 think in Spanish</title>
    <updated>2025-03-09T13:35:34+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"&gt; &lt;img alt="I've made Deepseek R1 think in Spanish" src="https://preview.redd.it/fx6kdf0w2one1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9ef3b20bb9a2eb6b38c32bf652878fd0dc519c3" title="I've made Deepseek R1 think in Spanish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally it only thinks in English (or in Chinese if you prompt in Chinese). So with this prompt I'll put in the comments its CoT is entirely in Spanish. I should note that I am not a native Spanish speaker. It was an experiment for me because normally it doesn't think in other languages even if you prompt so, but this prompt works. It should be applicable to other languages too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fx6kdf0w2one1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j77rkb/ive_made_deepseek_r1_think_in_spanish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T13:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j79obx</id>
    <title>Local Deep Research Update - I worked on your requested features and got also help from you</title>
    <updated>2025-03-09T15:13:32+00:00</updated>
    <author>
      <name>/u/ComplexIt</name>
      <uri>https://old.reddit.com/user/ComplexIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Runs 100% locally with Ollama or OpenAI-API Endpoint/vLLM - only search queries go to external services (Wikipedia, arXiv, DuckDuckGo, The Guardian) when needed. Works with the same models as before (Mistral, DeepSeek, etc.).&lt;/p&gt; &lt;p&gt;Quick install:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;&lt;code&gt;https://github.com/LearningCircuit/local-deep-research&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ollama pull mistral&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;python&lt;/code&gt; &lt;a href="http://main.py"&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As many of you requested, I've added several new features to the Local Deep Research tool:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Auto Search Engine Selection&lt;/strong&gt;: The system intelligently selects the best search source based on your query (&lt;strong&gt;Wikipedia&lt;/strong&gt; for facts, &lt;strong&gt;arXiv&lt;/strong&gt; for academic content, your &lt;strong&gt;local documents&lt;/strong&gt; when relevant)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local RAG Support&lt;/strong&gt;: You can now create custom document collections for different topics and search through your own files along with online sources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In-line Citations&lt;/strong&gt;: Added better citation handling as requested&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Search Engines&lt;/strong&gt;: &lt;strong&gt;Now supports Wikipedia, arXiv, DuckDuckGo, The Guardian, and your local document collections&lt;/strong&gt; - it is easy for you to add your own search engines if needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;: A new web UI makes it easier to start research, track progress, and view results - it is created by a contributor(&lt;strong&gt;HashedViking&lt;/strong&gt;)!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you for all the contributions, feedback, suggestions, and stars - they've been essential in improving the tool!&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://github.com/LearningCircuit/local-deep-research/blob/main/examples/2008-finicial-crisis.md"&gt;https://github.com/LearningCircuit/local-deep-research/blob/main/examples/2008-finicial-crisis.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexIt"&gt; /u/ComplexIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T15:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j5kl</id>
    <title>Build a low cost (&lt;1300â‚¬) deep learning rig</title>
    <updated>2025-03-09T22:10:46+00:00</updated>
    <author>
      <name>/u/yachty66</name>
      <uri>https://old.reddit.com/user/yachty66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt; &lt;img alt="Build a low cost (&amp;lt;1300â‚¬) deep learning rig" src="https://external-preview.redd.it/kz76DidpUO1jZDbyqLId1mAwEm0aIMpcywQeyNnxncY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed352f3d0dd50f0d25d7b383b5540a87749fcfe1" title="Build a low cost (&amp;lt;1300â‚¬) deep learning rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all.&lt;/p&gt; &lt;p&gt;It's the first time for me building a computer - my goal was to make the build as cheap as possible while still having good performance, and the RTX 3090 FE seemed to be giving the best bang for the buck.&lt;/p&gt; &lt;p&gt;I used these parts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: RTX 3090 FE (used)&lt;/li&gt; &lt;li&gt;CPU: Intel i5 12400F&lt;/li&gt; &lt;li&gt;Motherboard: Asus PRIME B660M-K D4&lt;/li&gt; &lt;li&gt;RAM: Corsair Vengeance LPX 32GB (2x16GB)&lt;/li&gt; &lt;li&gt;Storage: WD Green SN3000 500GB NVMe&lt;/li&gt; &lt;li&gt;PSU: MSI MAG A750GL PCIE5 750W&lt;/li&gt; &lt;li&gt;CPU Cooler: ARCTIC Freezer 36&lt;/li&gt; &lt;li&gt;Case Fan: ARCTIC P12 PWM&lt;/li&gt; &lt;li&gt;Case: ASUS Prime AP201 MicroATX&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole build cost me less than 1,300â‚¬.&lt;/p&gt; &lt;p&gt;I have a more detailed explanation of how I did things and the links to the parts in my GitHub repo: &lt;a href="https://github.com/yachty66/aicomputer"&gt;https://github.com/yachty66/aicomputer&lt;/a&gt;. I might continue the project to make affordable AI computers available for people like students, so the GitHub repo is actively under development.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zimdxgommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9358dc0882ceaa3d89f49184369ee8bf7d949224"&gt;https://preview.redd.it/zimdxgommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9358dc0882ceaa3d89f49184369ee8bf7d949224&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hkorlhommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=afc00171321ec28ef30820102f42b5e1d3ad6fa2"&gt;https://preview.redd.it/hkorlhommqne1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=afc00171321ec28ef30820102f42b5e1d3ad6fa2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/36jb4oommqne1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ceb0b30bac9df00d21636347a3368efbcb55539"&gt;https://preview.redd.it/36jb4oommqne1.jpg?width=3840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ceb0b30bac9df00d21636347a3368efbcb55539&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yachty66"&gt; /u/yachty66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j5kl/build_a_low_cost_1300_deep_learning_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7q0fw</id>
    <title>What are some useful tasks I can perform with smaller (&lt; 8b) local models?</title>
    <updated>2025-03-10T03:54:51+00:00</updated>
    <author>
      <name>/u/binarySolo0h1</name>
      <uri>https://old.reddit.com/user/binarySolo0h1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am new to the AI scenes and I can run smaller local ai models on my machine. So, what are some things that I can use these local models for. They need not be complex. Anything small but useful to improve everyday development workflow is good enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binarySolo0h1"&gt; /u/binarySolo0h1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7q0fw/what_are_some_useful_tasks_i_can_perform_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T03:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7g30z</id>
    <title>When will Llama 4, Gemma 3, or Qwen 3 be released?</title>
    <updated>2025-03-09T19:55:53+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When do you guys think these SOTA models will be released? It's been like forever so do anything of you know if there is a specific date in which they will release the new models? Also, what kind of New advancements do you think these models will bring to the AI industry, how will they be different from our old models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T19:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7t18m</id>
    <title>Framework and DIGITS suddenly seem underwhelming compared to the 512GB Unified Memory on the new Mac.</title>
    <updated>2025-03-10T07:15:44+00:00</updated>
    <author>
      <name>/u/Common_Ad6166</name>
      <uri>https://old.reddit.com/user/Common_Ad6166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was holding out on purchasing a FrameWork desktop until we could see what kind of performance the DIGITS would get when it comes out in May. But now that Apple has announced the new M4 Max/ M3 Ultra Mac's with 512 GB Unified memory, the 128 GB options on the other two seem paltry in comparison. &lt;/p&gt; &lt;p&gt;Are we actually going to be locked into the Apple ecosystem for another decade? This can't be true!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common_Ad6166"&gt; /u/Common_Ad6166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7t18m/framework_and_digits_suddenly_seem_underwhelming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T07:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7dzao</id>
    <title>QWQ low score in Leaderboard, what happened?</title>
    <updated>2025-03-09T18:25:08+00:00</updated>
    <author>
      <name>/u/ipechman</name>
      <uri>https://old.reddit.com/user/ipechman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt; &lt;img alt="QWQ low score in Leaderboard, what happened?" src="https://preview.redd.it/77rco6vfipne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce1eb44fa048f06ef55f271c38be8e206c4ed0a5" title="QWQ low score in Leaderboard, what happened?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ipechman"&gt; /u/ipechman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/77rco6vfipne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7dzao/qwq_low_score_in_leaderboard_what_happened/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T18:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7s1ef</id>
    <title>Why Isn't There a Real-Time AI Translation App for Smartphones Yet?</title>
    <updated>2025-03-10T06:03:28+00:00</updated>
    <author>
      <name>/u/spbxspb</name>
      <uri>https://old.reddit.com/user/spbxspb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the advancements in AI, especially in language models and real-time processing, why donâ€™t we have a truly seamless AI-powered translation app for smartphones? Something that works offline, translates speech in real-time with minimal delay, and supports multiple languages fluently.&lt;/p&gt; &lt;p&gt;Most current apps either require an internet connection, have significant lag, or struggle with natural-sounding translations. Given how powerful AI has become, it feels like we should already have a Star Trek-style universal translator by now.&lt;/p&gt; &lt;p&gt;Is it a technical limitation, a business decision, or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spbxspb"&gt; /u/spbxspb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7s1ef/why_isnt_there_a_realtime_ai_translation_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T06:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7j6cg</id>
    <title>&lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast</title>
    <updated>2025-03-09T22:11:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt; &lt;img alt="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" src="https://external-preview.redd.it/ZDFiNmN0NHptcW5lMdBuqabr-hQLmYC8Qi5X9EdtbTx_2YZ-hAZhcsR_hrB1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2014a6f010867f98da226a97f756cd7d035b3cb" title="&amp;lt;70B models aren't ready to solo codebases yet, but we're gaining momentum and fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2wo0b8lqmqne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7j6cg/70b_models_arent_ready_to_solo_codebases_yet_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T22:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7n2s5</id>
    <title>Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl</title>
    <updated>2025-03-10T01:18:27+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt; &lt;img alt="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" src="https://b.thumbs.redditmedia.com/aZROr3LtwGC89EWOHjMHoIbCvPQTB8Fs1jwIfYjEb8U.jpg" title="Manus turns out to be just Claude Sonnet + 29 other tools, Reflection 70B vibes ngl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1"&gt;https://preview.redd.it/yi72dtz4krne1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f8e56eaed3a6e5eee634567caa3f64f1fbcc2f1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Dorialexander/status/1898719861284454718"&gt;https://x.com/Dorialexander/status/1898719861284454718&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837"&gt;https://preview.redd.it/6b84agl7krne1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce944225e1c92f7d0d466c896fdf2a80c667837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jianxliao/status/1898861051183349870"&gt;https://x.com/jianxliao/status/1898861051183349870&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7n2s5/manus_turns_out_to_be_just_claude_sonnet_29_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T01:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7r47l</id>
    <title>I just made an animation of a ball bouncing inside a spinning hexagon</title>
    <updated>2025-03-10T05:01:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt; &lt;img alt="I just made an animation of a ball bouncing inside a spinning hexagon" src="https://external-preview.redd.it/aHcybDc4eW5tc25lMWpXkBeJA0bkbXxKyNPWYhDqX6Z4Wwq4cQiczMXRiEBU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1910662e66472f313e9a9c19401be8a1be2f181a" title="I just made an animation of a ball bouncing inside a spinning hexagon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cy79860omsne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-10T05:01:09+00:00</published>
  </entry>
</feed>
