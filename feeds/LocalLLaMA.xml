<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-04T18:07:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j329e9</id>
    <title>ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild</title>
    <updated>2025-03-04T04:07:23+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"&gt; &lt;img alt="ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild" src="https://external-preview.redd.it/Iv7H9o8LgDHbszU_vTFdZ8XMQmmCZhgShm1FiEoXFQ4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a13486bc9da50e971c1852f3ecbc2b560e6b2f4b" title="ktransformers troll rig R1 671B UD-Q2_K_XL on 96GB RAM in the wild" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4ucmn3b44x4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j329e9/ktransformers_troll_rig_r1_671b_udq2_k_xl_on_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:07:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bq60</id>
    <title>acord: a daemon for AI inference</title>
    <updated>2025-03-04T14:15:52+00:00</updated>
    <author>
      <name>/u/stanimirov</name>
      <uri>https://old.reddit.com/user/stanimirov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Self promotion, but could be useful... in some time&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We've been working on a software suite for AI inference. One of its components is acord: a daemon with a WebSocket interface for AI inference... well, to be precise it's more general than that. It's a daemon for abstract compute operations, but we &lt;em&gt;are&lt;/em&gt; building it with AI inference in mind. In short, think ollama, but not bound to llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/alpaca-core/acord"&gt;https://github.com/alpaca-core/acord&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's currently alpha: a preview. This means that nothing works, but it does so in precisely the way we want it to. Still, we created some moderately flashy demos.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stable diffusion: &lt;a href="https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/sd-screencap.webp"&gt;https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/sd-screencap.webp&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Llama chat: &lt;a href="https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/llama-screencap.webp"&gt;https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/llama-screencap.webp&lt;/a&gt;&lt;/li&gt; &lt;li&gt;cli-assist (copilot-cli-like example): &lt;a href="https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/cli-assist-ps.webp"&gt;https://media.githubusercontent.com/media/alpaca-core/acord/master/doc/cli-assist-ps.webp&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the following weeks we intend to work on a full use vertical and hopefully create a usable release by the end of April. For now we would love some input:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you think it's a good idea?&lt;/li&gt; &lt;li&gt;What do you want to use this for?&lt;/li&gt; &lt;li&gt;What features would you like from such software?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stanimirov"&gt; /u/stanimirov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bq60/acord_a_daemon_for_ai_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bq60/acord_a_daemon_for_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bq60/acord_a_daemon_for_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gcsp</id>
    <title>Getting Started Advice</title>
    <updated>2025-03-04T17:33:39+00:00</updated>
    <author>
      <name>/u/Shabuwa</name>
      <uri>https://old.reddit.com/user/Shabuwa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just getting started playing around with LLM’s and the amount of resources i’m finding (both videos and posts) are great, but somewhat overwhelming. &lt;/p&gt; &lt;p&gt;Does anyone have any recommendations on a youtube playlist or maybe a post that really lays out the groundwork from deciding what model to use, training, fine tuning, etc. and beyond?&lt;/p&gt; &lt;p&gt;For reference I have a 5080 and 48gb of ram. My understanding is that i’ll be capped somewhere in the range of 8-14B models given my hardware but I’m honestly a bit co fused on how to start after installing a model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shabuwa"&gt; /u/Shabuwa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gcsp/getting_started_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gcsp/getting_started_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gcsp/getting_started_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3789k</id>
    <title>How to run hardware accelerated Ollama on integrated GPU, like Radeon 780M on Linux.</title>
    <updated>2025-03-04T09:44:57+00:00</updated>
    <author>
      <name>/u/Sensitive-Leather-32</name>
      <uri>https://old.reddit.com/user/Sensitive-Leather-32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For hardware acceleration you could use either &lt;strong&gt;ROCm&lt;/strong&gt; or &lt;strong&gt;Vulkan&lt;/strong&gt;. Ollama devs &lt;strong&gt;don't want to merge Vulkan&lt;/strong&gt; integration, so better use ROCm if you can. It has slightly worse performance, but is easier to run.&lt;/p&gt; &lt;p&gt;If you still need Vulkan, you can &lt;a href="https://github.com/whyvl/ollama-vulkan"&gt;find a fork here&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Installation&lt;/h1&gt; &lt;p&gt;I am running Archlinux, so installed ollama and ollama-rocm. Rocm dependencies are installed automatically.&lt;/p&gt; &lt;p&gt;You can also follow &lt;a href="https://github.com/ollama/ollama/blob/ddb6dc81c26721a08453f1db7f2727076e97dabc/docs/tutorials/amd-igpu-780m.md"&gt;this guide for other distributions&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Override env&lt;/h1&gt; &lt;p&gt;If you have &amp;quot;unsupported&amp;quot; GPU, set &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.2&lt;/code&gt; in &lt;code&gt;/etc/systemd/system/ollama.service.d/override.conf&lt;/code&gt; this way:&lt;/p&gt; &lt;p&gt;&lt;code&gt;[Service]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Environment=&amp;quot;your env value&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;then run &lt;code&gt;sudo systemctl daemon-reload &amp;amp;&amp;amp; sudo systemctl restart ollama.service&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For different GPUs you may need to try different override values like 9.0.0, 9.4.6. Google them.)&lt;/p&gt; &lt;h1&gt;APU fix patch&lt;/h1&gt; &lt;p&gt;You probably need &lt;a href="https://github.com/ollama/ollama/pull/6282"&gt;this &lt;/a&gt;&lt;a href="https://github.com/ollama/ollama/pull/6282"&gt;patch&lt;/a&gt; until it gets merged. There is a &lt;a href="https://gitlab.com/zhaose233/ollama-rocm-package/-/jobs/9282301273/artifacts/browse"&gt;repo with CI with patched packages&lt;/a&gt; for Archlinux.&lt;/p&gt; &lt;h1&gt;Increase GTT size&lt;/h1&gt; &lt;p&gt;If you want to run big models with a bigger context, you have to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ixy5kf/comment/mfoptfw/?context=3"&gt;set GTT size according to this guide&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Amdgpu kernel bug&lt;/h1&gt; &lt;p&gt;Later during high GPU load I got freezes and graphics restarts with the &lt;a href="https://gist.github.com/AntonIXO/3f49ce8453766a4c07c8920c4c9d2b01"&gt;following logs in dmesg&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The only way to fix it is to build a &lt;a href="https://lore.kernel.org/lkml/20241127114638.11216-1-lamikr@gmail.com/T/"&gt;kernel with this patch&lt;/a&gt;. Use b4 am [&lt;a href="mailto:20241127114638.11216-1-lamikr@gmail.com"&gt;20241127114638.11216-1-lamikr@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:20241127114638.11216-1-lamikr@gmail.com"&gt;20241127114638.11216-1-lamikr@gmail.com&lt;/a&gt;) to get the latest version.&lt;/p&gt; &lt;h1&gt;Performance tips&lt;/h1&gt; &lt;p&gt;You can also set these env valuables to get better generation speed:&lt;/p&gt; &lt;p&gt;HSA_ENABLE_SDMA=0&lt;br /&gt; HSA_ENABLE_COMPRESSION=1&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention"&gt;OLLAMA_FLASH_ATTENTION=1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache"&gt;OLLAMA_KV_CACHE_TYPE=q8_0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Specify max context with: OLLAMA_CONTEXT_LENGTH=16382 # 16k (move context - more ram)&lt;/p&gt; &lt;p&gt;OLLAMA_NEW_ENGINE - does not work for me.&lt;/p&gt; &lt;p&gt;Now you got HW accelerated LLMs on your APUs🎉 Check it with &lt;strong&gt;ollama ps&lt;/strong&gt; and &lt;strong&gt;amdgpu_top&lt;/strong&gt; utility.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Leather-32"&gt; /u/Sensitive-Leather-32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3789k/how_to_run_hardware_accelerated_ollama_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T09:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37ns6</id>
    <title>Future of Phi-4-multimodal</title>
    <updated>2025-03-04T10:16:18+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for a good multilingual model for summarizing texts, writing answer e-mails that can run on a consumer laptop with 6-8GB VRAM.&lt;/p&gt; &lt;p&gt;Phi-4-multimodal looked very promising to me and I was excited to see a multimodal LLM with such capacities and such a small size.&lt;/p&gt; &lt;p&gt;But it seems like implementing a llama.cpp support is more complicated than expected: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11292#issuecomment-2692445044"&gt;https://github.com/ggml-org/llama.cpp/pull/11292#issuecomment-2692445044&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What will this mean for support, tools, fine-tunes etc. Will using this model lead to having a niche LLM where I have to implement solutuons (e.g. work with agents) all by myself (like using Rocm a few years ago) or is it not a big thing that there is no llama.cpp support?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37ns6/future_of_phi4multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3479c</id>
    <title>I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection</title>
    <updated>2025-03-04T06:00:27+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"&gt; &lt;img alt="I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection" src="https://b.thumbs.redditmedia.com/FJNdfekndsHi-w5W0N_AnNYGW8A2CD2BuNGLk_uW8iI.jpg" title="I'm working on a open source UI coding tool with artifacts, cli, agent actions, and github connection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j3479c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3479c/im_working_on_a_open_source_ui_coding_tool_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T06:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gctr</id>
    <title>Locally hosted homicidal escape room leveraging local inference, agentic workflows, TTS, IOT, beer, and friends</title>
    <updated>2025-03-04T17:33:41+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Hope everyone is having a good Tuesday. I started a project last year and faced some challenges and I’m wondering if some of you want to help with some creative solutions?! If I find the time to work on the project further, I’d love to share the codebase as a thank you or hopefully this inspires some to come up with something similar.&lt;/p&gt; &lt;p&gt;Current stack: 3080ti, Phi-3.5.mini-instruct (FP16), Langchain, Home assistant, tortoise-tts, IOT devices: mainly LED lights.&lt;/p&gt; &lt;p&gt;How it worked was the model with a cloned GLaDOS voice from portal is a sociopathic AI that has control of the room we are in along with the life systems and environment. She was given a brief overview of the objects in the room and generated an escape game on the fly.&lt;/p&gt; &lt;p&gt;This was communicated by LED lights by brightness and color. She would turn the heat up to 200F degrees if we answered a riddle, puzzle, or question wrong. Or just did it anyway because she wanted to.&lt;/p&gt; &lt;p&gt;For the agentic workflow I wrote a simple queue and task system that separate agents worked on. Mostly updating environment variables — like if we were drowning already or not. This could’ve of been a deterministic api call, but I wanted to do it anyway 🤣 The countdowns and other dynamic wild things created on the fly were insane haha!&lt;/p&gt; &lt;p&gt;We communicated by a terminal (laptop) in the center of the room. It was fun for about an hour, but the limitations were apparent.&lt;/p&gt; &lt;p&gt;Challenges: Larger context the model would lose focus and repeat tooling calls. This could be fixed using a newer model and finetuning.&lt;/p&gt; &lt;p&gt;The model’s safeguards still work even when prompted appropriately. Preventing actual fun from a deranged homicidal sociopathic LLM trying to torture and kill us. Maybe abliteration? &lt;a href="https://huggingface.co/blog/mlabonne/abliteration"&gt;https://huggingface.co/blog/mlabonne/abliteration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gctr/locally_hosted_homicidal_escape_room_leveraging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:33:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j345eq</id>
    <title>AMD Rocm User Forum</title>
    <updated>2025-03-04T05:57:04+00:00</updated>
    <author>
      <name>/u/Nerina23</name>
      <uri>https://old.reddit.com/user/Nerina23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"&gt; &lt;img alt="AMD Rocm User Forum" src="https://external-preview.redd.it/y_FINKO8XMlAJhEf7w8V__pSsdOlGq2_AygT_2N_tmE.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90054cd4dd3bf4dc8ffabe4326ea716b454230eb" title="AMD Rocm User Forum" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fingers crossed for competition to the Nvidia Dominance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nerina23"&gt; /u/Nerina23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AMD/status/1896709832629158323"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j345eq/amd_rocm_user_forum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T05:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2usb0</id>
    <title>Is qwen 2.5 coder still the best?</title>
    <updated>2025-03-03T22:02:25+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anything better been released for coding? (&amp;lt;=32b parameters)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3c4ey</id>
    <title>🌡️ LLM Thermometer</title>
    <updated>2025-03-04T14:34:45+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt; &lt;img alt="🌡️ LLM Thermometer" src="https://a.thumbs.redditmedia.com/Q4kNq15yRUaNS3c9JbpuoSTOIwk_3gvnIr2E-bhqCr4.jpg" title="🌡️ LLM Thermometer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I'm working on a toy/research project called &amp;quot;LLM Thermometer&amp;quot; that tries to answer:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can we infer the temperature setting used by an LLM just by analyzing its outputs?&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What it does&lt;/h1&gt; &lt;p&gt;The tool:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates multiple responses using the same prompt at different temperatures&lt;/li&gt; &lt;li&gt;Measures semantic similarity between responses using embeddings&lt;/li&gt; &lt;li&gt;Visualizes the relationship between temperature and response similarity&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Early results suggest this works :) Higher temperatures consistently produce more diverse responses (lower similarity scores), while lower temperatures generate more consistent outputs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lpjjw2f9nome1.png?width=2876&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3f58c5b93ee33483b62ccaa7838acaadf0ccba8"&gt;Similarity between generated texts with the same temperature levels from the prompt: \&amp;quot;What will technology look like in 2050?\&amp;quot; Cooler colors (blue) correspond to lower temperature values, while warmer colors (red) correspond to higher temperature values (range from 0 to 1 with steps of 0.1).&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Technical details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Uses OpenAI Python SDK but can connect to any compatible API endpoint&lt;/li&gt; &lt;li&gt;Works with local models via vLLM's OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Auto generates detailed reports with visualizations&lt;/li&gt; &lt;li&gt;Reports available at: &lt;a href="https://s1m0n38.github.io/llm-thermometer/"&gt;https://s1m0n38.github.io/llm-thermometer/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for prompt suggestions&lt;/h1&gt; &lt;p&gt;I'd appreciate suggestions for prompts that might produce interesting temperature-dependent variations! I've tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;What will technology look like in 2050?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;What's the meaning of life?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;What are the ethical implications of widespread AI adoption?&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Write a creative story with six paragraphs.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else should I try? Especially interested in prompts where temperature differences produce qualitatively different responses beyond just variation in wording.&lt;/p&gt; &lt;p&gt;Any feedback or ideas welcome!&lt;/p&gt; &lt;p&gt;The project is on GitHub at &lt;a href="https://github.com/S1M0N38/llm-thermometer/"&gt;https://github.com/S1M0N38/llm-thermometer/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3c4ey/llm_thermometer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:34:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3elmz</id>
    <title>Chain-of-Experts: Unlocking the Communication Power of MoEs</title>
    <updated>2025-03-04T16:23:21+00:00</updated>
    <author>
      <name>/u/finallyifoundvalidUN</name>
      <uri>https://old.reddit.com/user/finallyifoundvalidUN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We propose Chain-of-Experts (CoE), which fundamentally changes sparse Large Language Model (LLM) processing by implementing sequential communication between intra-layer experts within Mixture-of-Experts (MoE) models.&lt;/p&gt; &lt;p&gt;Mixture-of-Experts (MoE) models process information independently in parallel between experts and have high memory requirements. CoE introduces an iterative mechanism enabling experts to &amp;quot;communicate&amp;quot; by processing tokens on top of outputs from other experts.&lt;/p&gt; &lt;p&gt;Experiments show that CoE significantly outperforms previous MoE models in multiple aspects:&lt;/p&gt; &lt;p&gt;Performance: CoE with 2x iterations reduces Math validation loss from 1.20 to 1.12 Scaling: 2x iterations matches performance of 3x expert selections, outperforming layer scaling Efficiency: 17.6% lower memory usage with equivalent performance Flexibility: 823x increase in expert combinations, improving utilization, communication, and specialization&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZihanWang314/coe"&gt;https://github.com/ZihanWang314/coe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finallyifoundvalidUN"&gt; /u/finallyifoundvalidUN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T16:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32y7c</id>
    <title>Split brain (Update) - What I've learned and will improve</title>
    <updated>2025-03-04T04:43:51+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt; &lt;img alt="Split brain (Update) - What I've learned and will improve" src="https://external-preview.redd.it/UWvmtQPs4ScGH0IthYKdfU1hrMW7JnkAzdMKFse7jL0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c740e7b82c11757b66468f76734733c2aa704f1c" title="Split brain (Update) - What I've learned and will improve" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a update post to the last one &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;Here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have uploaded a inference page to the code I had previously discussed. &lt;a href="https://github.com/alientony/Split-brain/blob/main/inference-app.py"&gt;Inference&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can download the fusion layer here. &lt;a href="https://huggingface.co/Alienanthony/Splitbrain_Fusion_model"&gt;Fusion layer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original models can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;https://huggingface.co/meta-llama/Llama-3.2-1B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;So far the inference has been fascinating. Unfortunately I have only had the original gpt4all dataset on hand for training. (800mb) &lt;/p&gt; &lt;p&gt;Including I have learned that if you're doing to use a fused layer for differentiation for one model output you should probably make another. So moving forward I will update the training and attempt again. &lt;/p&gt; &lt;p&gt;BUT I am extremely fascinated by this new crazy system.&lt;/p&gt; &lt;p&gt;As you can see below. While we did not give the model on the left &amp;quot;Describe the history of chocolate chip cookies.&amp;quot; it does begin to think in that direction within it's &amp;quot;Think&amp;quot; space. &lt;/p&gt; &lt;p&gt;I have been able to replicate this sort of &amp;quot;thought directions&amp;quot; multiple times but it is very erratic. As both models are actually not on the same playing field due to the dependency in the way the architecture functions and it is asymmetrical rather than mirrored.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1"&gt;https://preview.redd.it/m19l6vxpklme1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdbd8edda0fb1b9a6b03d5922cf233fa462911a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One major issue I need to fix is the fused layer to realign the model on the right to produce usable tokens.&lt;/p&gt; &lt;p&gt;I also need a larger dataset as this will give more of a wider branch of training for the &amp;quot;sharing of info&amp;quot; across models but I find these results majorly agreeable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4"&gt;https://preview.redd.it/ckwixc26rlme1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4164c7d255c32c1c00275f121437c96a65eef5b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32y7c/split_brain_update_what_ive_learned_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38265</id>
    <title>16Gb GPU around $300 mark, low idle power. Does it exist?</title>
    <updated>2025-03-04T10:45:25+00:00</updated>
    <author>
      <name>/u/Rxunique</name>
      <uri>https://old.reddit.com/user/Rxunique</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got a 4x V100 16gb rig on during the day only, now looking for a low power GPU to keep on 24/7. needing to run 14B model on ollama and immich image recognition.&lt;/p&gt; &lt;p&gt;It for 2U server, so ideally blower cards, but capable of modifying a RTX FAN as well.&lt;/p&gt; &lt;p&gt;P100 idles 40W which is too much for 24/7, P4 only has 8gb. Most RTX with 16gb ram are not that cheap. 12GB card will struggle with 14B model.&lt;/p&gt; &lt;p&gt;Basically something like a half price Tesla T4 or A2 but doesn't have to be as good, or as small. Because for similar budget I might as well add a bit more and wait for brand new RTX50xx or new RX9070 which in theory should idel around 10w ish&lt;/p&gt; &lt;p&gt;I've been searching a while, does such GPU existing or am I searching for unobtanium &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rxunique"&gt; /u/Rxunique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j38265/16gb_gpu_around_300_mark_low_idle_power_does_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3929v</id>
    <title>When will Meta AI get a Llama upgrade already?</title>
    <updated>2025-03-04T11:52:51+00:00</updated>
    <author>
      <name>/u/Cheetah3051</name>
      <uri>https://old.reddit.com/user/Cheetah3051</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been stuck at 3.2 for months&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheetah3051"&gt; /u/Cheetah3051 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T11:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j34snr</id>
    <title>I open sourced my project to analyze your years of Apple Health data with Local Llama</title>
    <updated>2025-03-04T06:40:45+00:00</updated>
    <author>
      <name>/u/Fit_Chair2340</name>
      <uri>https://old.reddit.com/user/Fit_Chair2340</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt; &lt;img alt="I open sourced my project to analyze your years of Apple Health data with Local Llama" src="https://external-preview.redd.it/lLsIvxvl6coJk3dd69rue5IjQS1mwSUfJjRAQiI0jak.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0705f2c7bb303cf84d8b222fd9c148b2417ea6ce" title="I open sourced my project to analyze your years of Apple Health data with Local Llama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around and found out that you can export all your Apple health data. I've been wearing an Apple watch for 8 years and whoop for 3 years. I always check my day to day and week to week stats but I never looked at the data over the years. What if I could send this data to A.I. for analysis? But I also don't want to send my private data to a public LLM. What if I could run the analysis locally?&lt;/p&gt; &lt;p&gt;I exported my data and there was 989MB of data! So I needed to write some code to break this down. The code takes in your export data and gives you options to look at Steps, Distance, Heart rate, Sleep and more. It gave me some cool charts and you can use local llama to run the A.I. analysis!&lt;/p&gt; &lt;p&gt;I was really stressed at work last 2 years.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563"&gt;https://preview.redd.it/65612e77cmme1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=784225ea3990427860e9abb16fcd60eec3da2563&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It gave me some CRAZY insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seasonal Anomalies: While there's a general trend of higher activity in spring/summer, some of your most active periods occurred during winter months, particularly in December and January of recent years.&lt;/li&gt; &lt;li&gt;Reversed Weekend Pattern: Unlike most people who are more active on weekends, your data shows consistently lower step counts on weekends, suggesting your physical activity is more tied to workdays than leisure time.&lt;/li&gt; &lt;li&gt;COVID Impact: There's a clear signature of the pandemic in your data, with more erratic step patterns and changed workout routines during 2020-2021, followed by a distinct recovery pattern in late 2021.&lt;/li&gt; &lt;li&gt;Morning Consistency: Your most successful workout periods consistently occur in morning hours, with these sessions showing better heart rate performance compared to other times.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run this on your own computer. No one can access your data. &lt;a href="https://github.com/krumjahn/applehealth"&gt;&lt;strong&gt;Here's the link to the project.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you need more guidance on how to run it (not a programmer), &lt;a href="https://rumjahn.com/how-i-used-a-i-to-analyze-8-years-of-apple-health-fitness-data-to-uncover-actionable-insights/"&gt;check out my detailed instructions here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If people like this, I will make a web app version so you can run it without using code. Give this a like if you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit_Chair2340"&gt; /u/Fit_Chair2340 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j34snr/i_open_sourced_my_project_to_analyze_your_years/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T06:40:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fkax</id>
    <title>LLM Quantization Comparison</title>
    <updated>2025-03-04T17:02:00+00:00</updated>
    <author>
      <name>/u/dat1-co</name>
      <uri>https://old.reddit.com/user/dat1-co</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt; &lt;img alt="LLM Quantization Comparison" src="https://external-preview.redd.it/CGbzq4JDmMgH-DT2hVt-MPGAGrs7Io3E0dHabckY9J8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbd8d1effbe5db86d79260e6b8463c84c31b3a11" title="LLM Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat1-co"&gt; /u/dat1-co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dat1.co/blog/llm-quantization-comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bhzb</id>
    <title>Survival Specialist Fine-tune (Llama 3.1- 8B)</title>
    <updated>2025-03-04T14:05:03+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt; &lt;img alt="Survival Specialist Fine-tune (Llama 3.1- 8B)" src="https://external-preview.redd.it/85ZgfWZxijLN2bSUGD1P0g3arLnrKn-sjIsrvAEBZ2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65ea7c4f77726004c20e1a7fce60c95196efde1f" title="Survival Specialist Fine-tune (Llama 3.1- 8B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lolzinventor/Meta-Llama-3.1-8B-SurviveV3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37yhi</id>
    <title>Just 2B R1 like model achieved "aha" moment in vision task!!!</title>
    <updated>2025-03-04T10:38:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt; &lt;img alt="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" src="https://external-preview.redd.it/FrHOpKWMdqORPFSxLwpGvRx_uoQMNXcCJsvQiFYTNWw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2604aa322cfdf8200f3a6d6f9f972bb244dac34" title="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:38:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3byj5</id>
    <title>ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models</title>
    <updated>2025-03-04T14:27:00+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt; &lt;img alt="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" src="https://external-preview.redd.it/zs_VjubpLgixBxipH26-eVw1VDZXtfTYy2chbOoXyyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2bad2c795210312fd91bc4ff5801de93d52e3cad" title="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance’s Doubao Large Model Team, in collaboration with the M-A-P open-source community, has announced the release of SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning capabilities of large language models (LLMs) across 285 graduate-level disciplines. This dataset encompasses 26,529 multiple-choice questions, offering a rigorous assessment of LLM performance.&lt;br /&gt; &lt;a href="https://github.com/SuperGPQA/SuperGPQA"&gt;Github&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/m-a-p/SuperGPQA"&gt;HuggingFace&lt;/a&gt; &lt;a href="https://www.arxiv.org/abs/2502.14739"&gt;Paper&lt;/a&gt; &lt;a href="https://supergpqa.github.io"&gt;Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed"&gt;https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fq3ne63lmome1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e93f86a69d8ef6992f76c640ccbafb72504b045"&gt;Performance on SuperGPQA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3dqea5umome1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66a03b81db1827e82a1db111db6581188a2f4f06"&gt;LLM Performance Across Different Categories&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3emu0</id>
    <title>Open Source Claude Code (Actual Repo Converted from Binary)</title>
    <updated>2025-03-04T16:24:41+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/dnakov/claude-code/tree/main"&gt;Claude Code Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dnakov/anon-kode"&gt;Fork to enable OpenAI Compatibility&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: This is a decompiled version of Claude's code, not officially open-source. The original title may have been misleading—use at your own discretion.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3emu0/open_source_claude_code_actual_repo_converted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T16:24:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38499</id>
    <title>DiffRhythm - ASLP-lab: generate full songs (4 min) with vocals</title>
    <updated>2025-03-04T10:49:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/ASLP-lab/DiffRhythm"&gt;https://huggingface.co/spaces/ASLP-lab/DiffRhythm&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894"&gt;https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ASLP-lab"&gt;https://github.com/ASLP-lab&lt;/a&gt;&lt;br /&gt; Paper: DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion: &lt;a href="https://arxiv.org/abs/2503.01183"&gt;https://arxiv.org/abs/2503.01183&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32p97</id>
    <title>Qwen 32b coder instruct can now drive a coding agent fairly well</title>
    <updated>2025-03-04T04:29:49+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt; &lt;img alt="Qwen 32b coder instruct can now drive a coding agent fairly well" src="https://external-preview.redd.it/aDJ4N25hdXlvbG1lMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b763684453c2eb0539d13912eebe98f2d438296" title="Qwen 32b coder instruct can now drive a coding agent fairly well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2000d3tolme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bldn</id>
    <title>C4AI Aya Vision</title>
    <updated>2025-03-04T14:09:38+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt; &lt;img alt="C4AI Aya Vision" src="https://external-preview.redd.it/2FtgBIdrUTjZVqld2wWXrJgxCyutz4lA4knvupaJc-g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cffd187923d62691d6d92d4dd9ba2db2a4f098" title="C4AI Aya Vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/CohereForAI/c4ai-aya-vision-67c4ccd395ca064308ee1484"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gahy</id>
    <title>NVIDIA’s GeForce RTX 4090 With 96GB VRAM Reportedly Exists; The GPU May Enter Mass Production Soon, Targeting AI Workloads.</title>
    <updated>2025-03-04T17:31:10+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/"&gt;https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly highly interested. If this will be true.&lt;/p&gt; &lt;p&gt;Price around 6k. &lt;/p&gt; &lt;p&gt;Source; &amp;quot;The user did confirm that the one with a 96 GB VRAM won't guarantee stability and that its cost, due to a higher VRAM, will be twice the amount you would pay on the 48 GB edition. As per the user, this is one of the reasons why the factories are considering making only the 48 GB edition but may prepare the 96 GB in about 3-4 months.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:31:10+00:00</published>
  </entry>
</feed>
