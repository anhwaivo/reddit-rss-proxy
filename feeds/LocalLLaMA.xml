<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-13T23:24:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mon8it</id>
    <title>Woah. Letta vs Mem0. (For AI memory nerds)</title>
    <updated>2025-08-12T22:34:04+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"&gt; &lt;img alt="Woah. Letta vs Mem0. (For AI memory nerds)" src="https://preview.redd.it/8sl96y461oif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2350ded0c596ea924dac589bbe58ff31eb68579" title="Woah. Letta vs Mem0. (For AI memory nerds)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m an absolute AI memory nerd, and have probably read every proposal made about memory, and demoed virtually all of the professional solutions out there. But I’m absolutely stunned to see Letta basically call out Mem0 like a WWE feud. To be clear: I do not have any kind of affiliation with any memory company (beyond my own, which is not a memory company per se), but Letta (which began as MemGPT) are in many ways the OGs in this space. So, in this tiny corner of AI nerd land, this is a fairly wild smack down to watch. Just posting this in case any other memory heads are paying attention. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8sl96y461oif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mon8it/woah_letta_vs_mem0_for_ai_memory_nerds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T22:34:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpe9p2</id>
    <title>GLM 4.5 Air, local setup issues, vllm and llama.cpp</title>
    <updated>2025-08-13T19:36:17+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I seem to be not quite able to match GLM 4.5 Air model output between what's running on &lt;a href="http://chat.z.ai/bigmodel.cn"&gt;chat.z.ai/bigmodel.cn&lt;/a&gt; and my local 4x RTX3090 vllm/llama.cpp setup. I tried cpatonn/GLM-4.5-Air-AWQ-4bit, QuantTrio/GLM-4.5-Air-AWQ-FP16Mix, unsloth/GLM-4.5-Air-GGUF (q4_k_m, ud-q4_k_xl, ud-q5_k_xl, ud-q6_k_xl) - all under &amp;quot;normal&amp;quot; sampler defaults and the suggested temperature of 0.7). One very obvious prompt is just this short question:&lt;/p&gt; &lt;p&gt;&amp;gt; How to benchmark perplexity with llama.cpp?&lt;/p&gt; &lt;p&gt;On my local setup it leads to a lot of ruminating/attention problems (example: &lt;a href="https://pastebin.com/yaNdWNFb"&gt;https://pastebin.com/yaNdWNFb&lt;/a&gt; more than 200 lines), every single attempt on any of the tried quants and both on vllm (AWQ quants) and llama.cpp (gguf quants). On zai/bigmodel the prompt leads on every attempt to a comparatively concise reasoning output (see: &lt;a href="https://pastebin.com/9GSyR1Dz"&gt;https://pastebin.com/9GSyR1Dz&lt;/a&gt; less than 60 lines).&lt;/p&gt; &lt;p&gt;Very much appreciated, if someone who also runs GLM 4.5 Air locally could try that prompt and report whether the output is similar to zai/bigmodel, or like the one I get. If similar to zai/bigmodel, please share your local setup details (inference hardware, drivers, inference engine, versions, arguments, used model incl. quantization, etc.). Many thanks!&lt;/p&gt; &lt;p&gt;btw: having an additional strange issue with vllm and concurrent requests; seemingly only with GLM 4.5 Air quants and only if multiple requests run simultaneously I end up with responses like this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;lt;think&amp;gt;Okay, the user just sent a simple &amp;quot;Hi&amp;quot; as their first message. HmmHello! How can I assist you today?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is without reasoning parser, just to make it more visible that the model fails to produce the closing &amp;lt;/think&amp;gt; tag and just &amp;quot;continues&amp;quot; mid-thought with the message content &amp;quot;Hello! ...&amp;quot;. If the glm45 reasoning parser is used it gets confused as well, i.e., the message content ends up in the reasoning_content and the message content is empty.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpe9p2/glm_45_air_local_setup_issues_vllm_and_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpe9p2/glm_45_air_local_setup_issues_vllm_and_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpe9p2/glm_45_air_local_setup_issues_vllm_and_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:36:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpee0x</id>
    <title>GPT OSS 120b 34th on Simple bench, roughly on par with Llama 3.3 70b</title>
    <updated>2025-08-13T19:40:46+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simple-bench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpee0x/gpt_oss_120b_34th_on_simple_bench_roughly_on_par/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:40:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mov3d9</id>
    <title>I tried the Jan-v1 model released today and here are the results</title>
    <updated>2025-08-13T04:41:17+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"&gt; &lt;img alt="I tried the Jan-v1 model released today and here are the results" src="https://b.thumbs.redditmedia.com/Vlwsqp7XrYSffFPbRsbOOK6OJG64K1FIWlwv_HqoDjw.jpg" title="I tried the Jan-v1 model released today and here are the results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Search tool was brave. Tried 3 searches and its broken - the chat screenshots are attached and summarized below&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the GDP of the US?:&lt;/strong&gt; Gave me a growth rate number, not the GDP figure itself.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the popilation of the world?:&lt;/strong&gt; Got stuck in loop searching for the same thing and then thinking. I waited for several minutes, gave up and stopped it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Whats the size of the Jan AI team and where are they based?:&lt;/strong&gt; Same thing.. This time I let it go on for over 5 minutes and was just in a loop. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mov3d9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mov3d9/i_tried_the_janv1_model_released_today_and_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T04:41:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mox183</id>
    <title>[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs</title>
    <updated>2025-08-13T06:34:17+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"&gt; &lt;img alt="[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs" src="https://preview.redd.it/nclxmfireqif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3b16e237e84f24c17c50122327bd2265e9514a10" title="[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously shared the open‑source library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt; &lt;a href="https://docstrange.nanonets.com"&gt;&lt;strong&gt;https://docstrange.nanonets.com&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear feedbacks! &lt;/p&gt; &lt;p&gt;Original Post - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nclxmfireqif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mox183/update_docstrange_structured_data_extraction_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T06:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpdjx9</id>
    <title>Case study: hybrid SSM + sparse-attention LM that holds up at 32k ctx (w/ sane throughput)</title>
    <updated>2025-08-13T19:09:07+00:00</updated>
    <author>
      <name>/u/gpu_mamba</name>
      <uri>https://old.reddit.com/user/gpu_mamba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Would love to hear your thoughts on this in the comments. Anything I should play around w next?&lt;/p&gt; &lt;p&gt;TLDR: I swapped ~40% of self-attn layers for Mamba-style SSM blocks and added a block-shifted local attention pattern and a bunch of tiny global tokens. On a 1.3B LM trained ~300B tokens, I’m seeing ~1.35× more tokens/s and ~40% lower VRAM at 32k context vs a same-size vanilla Transformer, with basically iso perplexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I tried this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Long context is still rough: O(N&lt;sup&gt;2)&lt;/sup&gt; attn, tiny batches, random OOMs. Pure SSMs are fast but can miss global mixing. My hypothesis: use SSM for cheap local mixing and keep just enough attention for routing/global hops&lt;/p&gt; &lt;p&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interleave: 4 SSM blocks -&amp;gt; 1 global-attn layer, repeat through depth.&lt;/li&gt; &lt;li&gt;Block-shifted local attn: 512-token windows. each layer shifts/dilates so tokens meet fresh neighbors deeper in the net.&lt;/li&gt; &lt;li&gt;Mini-globals: 8 learned “global” tokens per layer that let info jump far without full attention.&lt;/li&gt; &lt;li&gt;less important parts: RMSNorm everywhere, SwiGLU MLPs, RoPE only on the attention layers. SSM uses a selective scan w/ gating (chunk len 1k) and a fused kernel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;training setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;machines: 8 nodes × 8×H100 80GB (64 GPUs total), NVSwitch inside nodes, fast IB (3.2TB/s) between nodes.&lt;/li&gt; &lt;li&gt;provider: TensorPool (solid on-demand multinode availability)&lt;/li&gt; &lt;li&gt;dist: torch &amp;amp; DeepSpeed ZeRO-3; Flash-Attn2 on the global layers; grad-ckpt on SSM+MLP. Pure data parallel (model small enough).&lt;/li&gt; &lt;li&gt;I/O: WebDataset tar shards, per-node NVMe cache, 6–8 loader workers/GPU to keep the pipes fed.&lt;/li&gt; &lt;li&gt;Curriculum: context 4k -&amp;gt; 8k -&amp;gt;16k -&amp;gt; 32k, packed samples.&lt;/li&gt; &lt;li&gt;Optim: AdamW (β1/β2=0.9/0.95), wd=0.1, cosine, 3k warmup, bf16. μP-ish MLP scaling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;numbers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;perplexity (The Pile val): 5.92 → 5.88 (basically iso)&lt;/li&gt; &lt;li&gt;Long-ctx retrieval (Needle @32k): 72% → 96% hit rate&lt;/li&gt; &lt;li&gt;BookSum long ROUGE-L: +0.9&lt;/li&gt; &lt;li&gt;HumanEval: 27.4 → 27.6 (noise) -Peak VRAM @32k, bs=1/GPU: ~69GB → ~41GB.&lt;/li&gt; &lt;li&gt;Tokens/s/GPU @32k: ~+35%.&lt;/li&gt; &lt;li&gt;scaling efficiency @64 GPUs: ~88–92% once comms tuned.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;final thoughts&lt;/strong&gt; I’d be happy to share my DeepSpeed/NCCL configs and a tiny ~130M “mini-BSMT” toy run if yall wanna try on a single node first. Also curious if anyone’s tried MoE-on-MLP with SSM-dense blocks. This seems like the next obvious move for quality without destroying latency. YMMV, but for long-doc tasks this hybrid felt like the right trade-off: keeps global routing, drops most of the O(N²) pain.&lt;/p&gt; &lt;p&gt;Cheers!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpu_mamba"&gt; /u/gpu_mamba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdjx9/case_study_hybrid_ssm_sparseattention_lm_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdjx9/case_study_hybrid_ssm_sparseattention_lm_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdjx9/case_study_hybrid_ssm_sparseattention_lm_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:09:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mowlg2</id>
    <title>Multi-Token Prediction(MTP) in llama.cpp</title>
    <updated>2025-08-13T06:07:57+00:00</updated>
    <author>
      <name>/u/UpperParamedicDude</name>
      <uri>https://old.reddit.com/user/UpperParamedicDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15225"&gt;https://github.com/ggml-org/llama.cpp/pull/15225&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The dev says they're pretty new to ML outside of python so patience is required. It's only a draft for now but i felt like i need to share it with you folks, maybe some of you have the required knowledge and skills to help them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UpperParamedicDude"&gt; /u/UpperParamedicDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mowlg2/multitoken_predictionmtp_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T06:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpb0mo</id>
    <title>Dual GPU Setup for LLMs – Notes from a Newbie</title>
    <updated>2025-08-13T17:36:06+00:00</updated>
    <author>
      <name>/u/DrRamorey</name>
      <uri>https://old.reddit.com/user/DrRamorey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some learnings I made the hard way. These points might be obvious to some, but I wasn’t fully aware of them before I built my LLM workstation. Hopefully this helps other newbies like me.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;I was using my AMD RX 6800 mostly for LLM workloads and wanted more VRAM to test larger models. I built a PC to accommodate two GPUs for this use case.&lt;br /&gt; The plan was to use my RX 6800 plus a newer GPU. I knew it should be an AMD card, and the RX 9070 XT seemed like the best value.&lt;br /&gt; I’m still an amateur with LLMs—mostly using them in LM Studio—but I’ve started experimenting with dedicated servers and Docker setups.&lt;/p&gt; &lt;h1&gt;Learning 1 - You can’t assume gaming benchmarks reflect LLM performance&lt;/h1&gt; &lt;p&gt;Standard benchmarks like 3DMark, Heaven, or Superposition showed my new 9070 XT was &lt;strong&gt;51–64% faster&lt;/strong&gt; than my old card. I kind of expected similar gains in LLM performance.&lt;/p&gt; &lt;p&gt;That was clearly not the case. Here are my &lt;code&gt;llama-bench&lt;/code&gt; results (ROCm):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 -sm none ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 9070 XT, gfx1201 (0x1201), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;sm&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1420.48 ± 4.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;47.74 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;947.02 ± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 9070 XT is 50&lt;strong&gt;% faster&lt;/strong&gt; in prompt parsing, but only &lt;strong&gt;10% faster&lt;/strong&gt; in token generation.&lt;br /&gt; That was really disappointing.&lt;/p&gt; &lt;p&gt;It’s a different picture for image generation (results below are generation times in seconds; lower is better).&lt;br /&gt; As this isn’t my main interest, I only did very basic testing. ComfyUI had some weird issues with the 9070 XT but worked flawlessly with the RX 6800 (as of July 2025).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;RX 6800&lt;/th&gt; &lt;th align="left"&gt;RX 9070 XT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Stable Diffusion 3.5 simple 1024x1024&lt;/td&gt; &lt;td align="left"&gt;115&lt;/td&gt; &lt;td align="left"&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDXL simple 1024x1024&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Flux schnell 1024x1024&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Flux checkpoint 1024x1024&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;61&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As my 9070 XT was also &lt;strong&gt;way too loud&lt;/strong&gt;, I returned it and picked up a second-hand RX 6800 XT. It’s only slightly faster than my old card, but &lt;strong&gt;€450 cheaper&lt;/strong&gt; than the 9070 XT.&lt;/p&gt; &lt;p&gt;Lesson: ignore standard gaming benchmarks when choosing a GPU for LLMs.&lt;br /&gt; Check LLM-specific benchmark lists like &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt; and pick a GPU that matches your existing one if you’re not going for identical models.&lt;/p&gt; &lt;h1&gt;Learning 2 - Two GPUs do not double LLM token generation performance&lt;/h1&gt; &lt;p&gt;This should be obvious, but I never really thought about it and assumed overall performance would scale with two GPUs.&lt;/p&gt; &lt;p&gt;Wrong again.&lt;br /&gt; The main benefit of the second GPU is &lt;strong&gt;extra VRAM&lt;/strong&gt;. Larger models can be split across both GPUs—but performance is actually worse than with a single card (see next point).&lt;/p&gt; &lt;h1&gt;Learning 3 - Splitting models across two GPUs can tank performance&lt;/h1&gt; &lt;p&gt;Using the same model as before, now with the RX 6800 XT:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 -sm none` ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 6800 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;sm&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1070.86 ± 2.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;44.78 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;875.96 ± 1.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;none&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.25 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 6800 XT is only &lt;strong&gt;3% faster&lt;/strong&gt; than my old RX 6800 in token generation.&lt;/p&gt; &lt;p&gt;Now splitting the model across both cards:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m gemma-3-12B-it-qat-GGUF/gemma-3-12B-it-QAT-Q4_0.gguf -mg 0,1 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 ROCm devices: Device 0: AMD Radeon RX 6800 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 Device 1: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32 &lt;/code&gt;&lt;/pre&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;964.56 ± 2.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;31.92 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;962.75 ± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 12B Q4_0&lt;/td&gt; &lt;td align="left"&gt;6.41 GiB&lt;/td&gt; &lt;td align="left"&gt;11.77 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;31.90 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Interpretation (my guess):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt parsing (&lt;code&gt;pp512&lt;/code&gt;) seems truly parallelized—the performance is roughly the average of both cards.&lt;/li&gt; &lt;li&gt;Token generation (&lt;code&gt;tg128&lt;/code&gt;) is slower than a single card. This makes sense: both cards must work in sync, so there’s extra overhead—likely from synchronization and maybe PCIe bandwidth limits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In my case, splitting gave me &lt;strong&gt;26% lower token generation speed&lt;/strong&gt; compared to my slowest card.&lt;br /&gt; The upside: I now have 32GB of VRAM for bigger models.&lt;/p&gt; &lt;h1&gt;Learning 3 - Consumer hardware (mainboard and case) pitfalls&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Mainboard&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You need &lt;strong&gt;two physical PCIe x16 slots&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;These must support &lt;strong&gt;lane splitting&lt;/strong&gt; (x16 → x8/x8). Some boards don’t support this or use weird splits (x8/x1). x8 speeds didn’t cause me performance issues (even in gaming), but x4 or x1 would likely bottleneck.&lt;/li&gt; &lt;li&gt;Slot spacing matters—many modern GPUs are 3+ slots thick, which can block the second slot.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I underestimated the heat and noise from two GPUs.&lt;/li&gt; &lt;li&gt;With one GPU, my airflow-oriented build was fine.&lt;/li&gt; &lt;li&gt;Adding the second GPU was a &lt;strong&gt;massive change&lt;/strong&gt;—temps spiked and noise went from “barely there” to “annoying constant GPU roar.”&lt;/li&gt; &lt;li&gt;If your build sits on your desk, fan performance and possibly sound-dampening panels become very important.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m still learning, and most of this is based on my own trial and error.&lt;br /&gt; If I’ve misunderstood something, overlooked a better method, or drawn the wrong conclusions, I’d appreciate corrections.&lt;br /&gt; Feel free to share your own benchmarks, tweaks, or experiences so others (including me) can learn from them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrRamorey"&gt; /u/DrRamorey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpb0mo/dual_gpu_setup_for_llms_notes_from_a_newbie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpdzgz</id>
    <title>Which is the best small model to fine-tune?</title>
    <updated>2025-08-13T19:25:33+00:00</updated>
    <author>
      <name>/u/IllustriousSearch411</name>
      <uri>https://old.reddit.com/user/IllustriousSearch411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I have started to learn more about fine-tuning, LLMs and Lora recently, I have already tried to fine-tune some models using unsloth + lora on google colab, mostly Llama 3 8B, and got some OK results, but I feel it could be a lot better.&lt;/p&gt; &lt;p&gt;I have a Nvidia 3060 12gb VRAM available, which is probably better than the colab GPU, the dataset I am using for this is pretty small too, around 1400 entries and I want to fine-tune a model that can perform better than what I have previously done, I was thinking about qwen models which seems to be on top of discussions recently but I dont really know which one to choose.&lt;/p&gt; &lt;p&gt;If anyone needs more info comment here that I can add more context to the post, and I appreciate any intereactions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllustriousSearch411"&gt; /u/IllustriousSearch411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdzgz/which_is_the_best_small_model_to_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdzgz/which_is_the_best_small_model_to_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpdzgz/which_is_the_best_small_model_to_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T19:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1ras</id>
    <title>[Beta] Local TTS Studio with Kokoro, Kitten TTS, and Piper built in, completely in JavaScript (930+ voices to choose from)</title>
    <updated>2025-08-13T11:24:08+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! Last week, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;I posted&lt;/a&gt; a Kitten TTS web demo that it seemed like a lot of people liked, so I decided to take it a step further and add Piper and Kokoro to the project! The project lets you load Kitten TTS, Piper Voices, or Kokoro completely in the browser, 100% local. It also has a quick preview feature in the voice selection dropdowns.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://clowerweb.github.io/tts-studio/"&gt;&lt;strong&gt;Online Demo&lt;/strong&gt;&lt;/a&gt; (GitHub Pages)&lt;/h1&gt; &lt;p&gt;Repo (Apache 2.0): &lt;a href="https://github.com/clowerweb/tts-studio"&gt;https://github.com/clowerweb/tts-studio&lt;/a&gt;&lt;br /&gt; One-liner Docker installer: &lt;code&gt;docker pull ghcr.io/clowerweb/tts-studio:latest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;Kitten TTS standalone&lt;/a&gt; was also updated to include a bunch of your feedback including bug fixes and requested features! There's also a &lt;a href="https://clowerweb.github.io/piper-tts-web-demo/"&gt;Piper standalone&lt;/a&gt; available.&lt;/p&gt; &lt;p&gt;Lemme know what you think and if you've got any feedback or suggestions!&lt;/p&gt; &lt;p&gt;If this project helps you save a few GPU hours, please consider &lt;a href="https://github.com/sponsors/clowerweb"&gt;grabbing me a coffee!&lt;/a&gt; ☕&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1ras/beta_local_tts_studio_with_kokoro_kitten_tts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4vxe</id>
    <title>Plant UML Generator LLM finetune</title>
    <updated>2025-08-13T13:44:53+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt; &lt;img alt="Plant UML Generator LLM finetune" src="https://external-preview.redd.it/A2vjjq6KSpHPh1Yu7sh0j0dqmFW8S4lQdPeKAxnYi1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ffbbaa449ca9e4ad6cfa5fac6ce6ffc23c3275b" title="Plant UML Generator LLM finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Introducing pumlGenV2-1: The AI That Visualizes Complex Ideas as PlantUML Diagrams&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;Give it a complex question—whether about &lt;strong&gt;architecture&lt;/strong&gt;, &lt;strong&gt;philosophical debates&lt;/strong&gt;, or &lt;strong&gt;historical events&lt;/strong&gt;—and it generates a structured PlantUML diagram with (all input text is treated as a question):&lt;br /&gt; ✔ &lt;strong&gt;Logical relationships&lt;/strong&gt; between concepts&lt;br /&gt; ✔ &lt;strong&gt;Hierarchical grouping&lt;/strong&gt; (packages, components)&lt;br /&gt; ✔ &lt;strong&gt;Annotations &amp;amp; notes&lt;/strong&gt; for clarity&lt;br /&gt; ✔ &lt;strong&gt;Color-coding &amp;amp; legends&lt;/strong&gt; for readability&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;h1&gt;1️⃣ Philosophy Made Visual&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Can AI achieve true understanding, or is it just pattern recognition?&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Philosophy Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maps consciousness, semantics, and computational limits&lt;/li&gt; &lt;li&gt;Contrasts human vs. machine cognition&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2️⃣ Technical Architecture in Seconds&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; &lt;em&gt;&amp;quot;Diagram a microservices e-commerce system with Kubernetes, Kafka, and Istio.&amp;quot;&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Output:&lt;/strong&gt;&lt;br /&gt; Microservices Diagram&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shows frontend/backend/services&lt;/li&gt; &lt;li&gt;Visualizes async flows (RabbitMQ, Kafka)&lt;/li&gt; &lt;li&gt;Includes security (mTLS, Istio)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-domain&lt;/strong&gt;: Works for tech, philosophy, history, law&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precise syntax&lt;/strong&gt;: Generates valid PlantUML code&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context-aware&lt;/strong&gt;: Adds relevant notes and structure&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Best for analytical questions (not narratives/stories)&lt;/li&gt; &lt;li&gt;Output may need minor tweaks for custom styling&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chrisrutherford/pumlGenV2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4vxe/plant_uml_generator_llm_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpgkap</id>
    <title>Why it’s a mistake to ask chatbots about their mistakes</title>
    <updated>2025-08-13T21:02:24+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;The tendency to ask AI bots to explain themselves reveals widespread misconceptions about how they work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/"&gt;https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpgkap/why_its_a_mistake_to_ask_chatbots_about_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T21:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpa1ew</id>
    <title>I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!</title>
    <updated>2025-08-13T17:00:27+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt; &lt;img alt="I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!" src="https://external-preview.redd.it/l26S5nt-U8KfOY_GWKDJSOzb_2xdSlDoF2CnOuQm95k.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d46f203692b0157fc900d760b8c60f96cb3a1eee" title="I am building a semantic file search engine using Qwen0.6b with recently released LangExtract tool!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;br /&gt; I am a long time lurker and quite active here on LocalLlama. I am starting to build a tool called 'monkeSearch'. Essentially an open source local file search engine where you can type english sentences or even broken keywords related to your file (typing like a &amp;quot;monke&amp;quot; essentially) and you get the closest matches listed, and without needing a beefy system in order to run this locally.&lt;br /&gt; Instead of remembering exact filenames or navigating through folder hierarchies, you can search with queries like &amp;quot;photos from wedding 3 weeks ago&amp;quot; or &amp;quot;resume pdf last month.&amp;quot; The whole tool is supposed to be offline, and aimed at running on potato pc too. &lt;/p&gt; &lt;p&gt;So basically, when you aim at searching a file, we have specific things in mind when we are given the freedom to search for stuff in semantic language, and we can segregate the input query in 3 domains: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Filetype: most deterministic thing we have in our mind while searching&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Temporal data (time related) for example: x days ago, x weeks ago, last x months&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Misc. data: for example file name, file path which can be anything.&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The idea had a lot of iterations, and I was planning to do all of the heavylifting without any ML at all, with just bare algorithms and pattern matching (yup i am crazy) &lt;/p&gt; &lt;p&gt;But a few days back, Google released LangExtract, and it was exactly what I could have dreamt of, and I started playing around with it.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;input query: &amp;quot;web dev code&amp;quot; LangExtract: Processing, current=12 chars, processed=12 chars: [00:00] ✓ Extraction processing complete ✓ Extracted 1 entities (1 unique types) • Time: 0.65s • Speed: 18 chars/sec • Chunks: 1 FILE TYPE INDICATORS: - 'web dev code' → ts, js, html &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see above, I am using Qwen 0.6b running locally, and the base model performs surprisingly well (works perfectly in 90% of the cases). Finetuning it would result in better results. I have the dataset generation script ready for finetuning too. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lnvkw3sngtif1.png?width=4430&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85aee20b6c0af82b04c83dcabe1d2dc20fa25a41"&gt;architecture planning &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Okay so the above diagram covers the planning I did for the idea, and it has a lot of stuff to implement.&lt;br /&gt; I have already started working on the services locally. And it's a lot of work for one guy to do.&lt;br /&gt; &lt;a href="https://github.com/monkesearch"&gt;https://github.com/monkesearch&lt;/a&gt;&lt;br /&gt; This project has multiple service components and I'd love to collaborate with others interested in: &lt;/p&gt; &lt;p&gt;* NLP/semantic processing implementation&lt;/p&gt; &lt;p&gt;* Database optimization (currently planning SQLite)&lt;/p&gt; &lt;p&gt;* Frontend development for the query interface&lt;/p&gt; &lt;p&gt;* Testing!!&lt;/p&gt; &lt;p&gt;* Performance optimization for large file systems&lt;/p&gt; &lt;p&gt;If you're interested in contributing or have suggestions for the architecture, let's discuss below. I'm particularly interested in feedback on the semantic tagging approach and ideas for optimizing the overall system for real-time file processing.&lt;/p&gt; &lt;p&gt;Thanks for reading, and looking forward to building this with the community!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpa1ew/i_am_building_a_semantic_file_search_engine_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:00:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp7cfe</id>
    <title>awesome-private-ai: all things for your AI data sovereign</title>
    <updated>2025-08-13T15:20:08+00:00</updated>
    <author>
      <name>/u/tdi</name>
      <uri>https://old.reddit.com/user/tdi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi just wanted to show - I have created this list. Been working on those topics recently and will be expanding it even more.&lt;br /&gt; &lt;a href="https://github.com/tdi/awesome-private-ai"&gt;https://github.com/tdi/awesome-private-ai&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tdi"&gt; /u/tdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp7cfe/awesomeprivateai_all_things_for_your_ai_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T15:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp92nc</id>
    <title>Flash Attention massively accelerate gpt-oss-120b inference speed on Apple silicon</title>
    <updated>2025-08-13T16:24:53+00:00</updated>
    <author>
      <name>/u/DaniDubin</name>
      <uri>https://old.reddit.com/user/DaniDubin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share my observation and experience with gpt-oss-120b (unsloth/gpt-oss-120b-GGUF, F16).&lt;br /&gt; I am running it via LM Studio (latest v0.3.23), my hardware config is Mac Studio M4 Max (16c/40g) with 128GB of unified memory. &lt;/p&gt; &lt;p&gt;My main complaint against gpt-oss-120b was its inference speed, once the context window get filled up, it was dropping from 35-40 to 10-15 t/s when the context was around 15K only.&lt;/p&gt; &lt;p&gt;Now I noticed that by default Flash Attention is turned off. Once I turn it on via LM Studio model's configuration, I got ~50t/s with the context window at 15K, instead of the usual &amp;lt;15t/s.&lt;/p&gt; &lt;p&gt;Has anyone else tried to run this model with Flash Attention? Is there any trade-offs in model's accuracy? In my *very* limited testing I didn't notice any. I did not know that it can speed up so much the inference speed. I also noticed that Flash Attention is only available with GGUF quants, not on MLX.&lt;/p&gt; &lt;p&gt;Would like to hear your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniDubin"&gt; /u/DaniDubin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T16:24:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6it6</id>
    <title>now it can turn your PDFs and docs into clean fine tuning datasets</title>
    <updated>2025-08-13T14:48:26+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt; &lt;img alt="now it can turn your PDFs and docs into clean fine tuning datasets" src="https://external-preview.redd.it/3sG_aaHa7N5A_uKldFg_ckXPZRKSagJ4eq_vlsxxQ-g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b05b86b865816d2d239bd9c679d5afbf3fd0461" title="now it can turn your PDFs and docs into clean fine tuning datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4z271b5usif1.png?width=1812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4d98143bf7d60e382b53787e3ce6eb6272f8c8"&gt;The flow on how it generates datasets using local resources&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mp6it6/video/hhwtavqwusif1/player"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;repo is here &lt;a href="https://github.com/Datalore-ai/datalore-localgen-cli"&gt;https://github.com/Datalore-ai/datalore-localgen-cli&lt;/a&gt;&lt;/p&gt; &lt;p&gt;a while back I posted here about a terminal tool I made during my internship that could generate fine tuning datasets from real world data using deep research.&lt;br /&gt; after that post, I got quite a few dms and some really thoughtful feedback. thank you to everyone who reached out.&lt;/p&gt; &lt;p&gt;also, it got around 15 stars on GitHub which might be small but it was my first project so I am really happy about it. thanks to everyone who checked it out.&lt;/p&gt; &lt;p&gt;one of the most common requests was if it could work on local resources instead of only going online.&lt;br /&gt; so over the weekend I built a separate version that does exactly that.&lt;/p&gt; &lt;p&gt;you point it to a local file like a pdf, docx, jpg or txt and describe the dataset you want. it extracts the text, finds relevant parts with semantic search, applies your instructions through a generated schema, and outputs the dataset.&lt;/p&gt; &lt;p&gt;I am planning to integrate this into the main tool soon so it can handle both online and offline sources in one workflow.&lt;/p&gt; &lt;p&gt;if you want to see some example datasets it generated, feel free to dm me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6it6/now_it_can_turn_your_pdfs_and_docs_into_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp6y0e</id>
    <title>Thankful to r/localllama, Swapped from Manus to a local setup</title>
    <updated>2025-08-13T15:04:36+00:00</updated>
    <author>
      <name>/u/Proof_Dog6506</name>
      <uri>https://old.reddit.com/user/Proof_Dog6506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"&gt; &lt;img alt="Thankful to r/localllama, Swapped from Manus to a local setup" src="https://preview.redd.it/0zp95auywsif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db6545a173cd1158bbe18fa4ac2296625343d76b" title="Thankful to r/localllama, Swapped from Manus to a local setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw a post here a while back about running multi‑agent setups locally. At the time I was still subbed to Manus and figured I'd just stick with what I knew.&lt;/p&gt; &lt;p&gt;Last week I decided to actually try it after seeing it mentioned again and… the OS community is fire tbh. Found an open‑source tool that runs entirely on my machine, does the same workflows (even better) I used Manus for, and I can tweak it however I want.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before vs After:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Before: $40/month, cloud‑only, occasional downtime&lt;/li&gt; &lt;li&gt;After: $0, local‑first, tweakable, private, running with ollama and self‑hosted models, have full control of search (human in the loop)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Props to whoever originally posted about this, you might have just saved me a subscription. Massive thanks to LocalLLaMA for putting this on my radar. Here's the post I found that kicked this off for me: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else made the switch?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof_Dog6506"&gt; /u/Proof_Dog6506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0zp95auywsif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp6y0e/thankful_to_rlocalllama_swapped_from_manus_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T15:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp4gwl</id>
    <title>Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985</title>
    <updated>2025-08-13T13:28:20+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt; &lt;img alt="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" src="https://external-preview.redd.it/Bvw60PvhPgoef0Ng9Djae_QLUotq8vncLfnhqt8cL74.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=218782160b09032a3d5202434bc6cfb1ccd15a36" title="Beelink GTR9 Pro Mini PC Launched: 140W AMD Ryzen AI MAX+ 395 APU, 128 GB LPDDR5x 8000 MT/s Memory, 2 TB Crucial SSD, Dual 10GbE LAN For $1985" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-gtr9-pro-mini-pc-launched-140w-amd-ryzen-ai-max-395-128-gb-dual-10gbe-1985-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp4gwl/beelink_gtr9_pro_mini_pc_launched_140w_amd_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T13:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1moz341</id>
    <title>gpt-oss-120B most intelligent model that fits on an H100 in native precision</title>
    <updated>2025-08-13T08:46:18+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt; &lt;img alt="gpt-oss-120B most intelligent model that fits on an H100 in native precision" src="https://preview.redd.it/4okvse7e2rif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=943876a00ac037e2110c919f54e46c6e6d4303b4" title="gpt-oss-120B most intelligent model that fits on an H100 in native precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting analysis thread: &lt;a href="https://x.com/artificialanlys/status/1952887733803991070"&gt;https://x.com/artificialanlys/status/1952887733803991070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4okvse7e2rif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moz341/gptoss120b_most_intelligent_model_that_fits_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T08:46:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpez1p</id>
    <title>Added locally generated dialogue + voice acting to my game!</title>
    <updated>2025-08-13T20:02:24+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt; &lt;img alt="Added locally generated dialogue + voice acting to my game!" src="https://external-preview.redd.it/N2szZGNtMzRldWlmMZmQp7O5BpjYg7UqegAgE9IdgP7TYx8Szh9dJVqIheQu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eccefbf13830c4a641bc7633ab5e9b01c2c86540" title="Added locally generated dialogue + voice acting to my game!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t1qgim34euif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpez1p/added_locally_generated_dialogue_voice_acting_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T20:02:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp1j7e</id>
    <title>Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp</title>
    <updated>2025-08-13T11:12:30+00:00</updated>
    <author>
      <name>/u/csixtay</name>
      <uri>https://old.reddit.com/user/csixtay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt; &lt;img alt="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" src="https://preview.redd.it/j7hi9xgjrrif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=328e2e9fc9cd738d0907c1394e77c1ec12b827b3" title="Peak safety theater: gpt-oss-120b refuses to discuss implementing web search in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csixtay"&gt; /u/csixtay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7hi9xgjrrif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp1j7e/peak_safety_theater_gptoss120b_refuses_to_discuss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T11:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpabh1</id>
    <title>Matrix-Game 2.0 — first open-source, real-time, long-sequence interactive world model. 25 FPS, minutes-long interaction</title>
    <updated>2025-08-13T17:10:32+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Skywork_ai/status/1955237399912648842?t=hsxnA2t2FyKxRsSRBCJ1kA&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpabh1/matrixgame_20_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp2wq3</id>
    <title>There is a new text-to-image model named nano-banana</title>
    <updated>2025-08-13T12:20:32+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt; &lt;img alt="There is a new text-to-image model named nano-banana" src="https://preview.redd.it/jmw88evj4sif1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53c768eb9781ffe9119d98e2a2e9f3c88c8adab5" title="There is a new text-to-image model named nano-banana" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jmw88evj4sif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp2wq3/there_is_a_new_texttoimage_model_named_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T12:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpayu9</id>
    <title>Fully local Qwen 2.5 Omni realtime agent (sees + talks).... tested it by cooking dinner</title>
    <updated>2025-08-13T17:34:17+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpayu9/fully_local_qwen_25_omni_realtime_agent_sees/"&gt; &lt;img alt="Fully local Qwen 2.5 Omni realtime agent (sees + talks).... tested it by cooking dinner" src="https://external-preview.redd.it/NTd2N2twdnRtdGlmMXnk9Y57JoTaUjqhxZO6PSdtyiDJseK_nxh9m4-CzVh9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46112b14a42011bc134293e7412cb409a7ebf206" title="Fully local Qwen 2.5 Omni realtime agent (sees + talks).... tested it by cooking dinner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, I love Qwen 2.5 Omni. Pushed it pretty hard to get a fully local AI agent running that can see (webcam) and talk back in real-time. Super impressed with how fast and versatile it is overall. &lt;/p&gt; &lt;p&gt;I got a full local pipeline working and had it help me come up with dinner ideas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: Webcam feed processed frame-by-frame&lt;/li&gt; &lt;li&gt;Reasoning: Open Qwen model interpreting what it sees&lt;/li&gt; &lt;li&gt;Output: Overlayed AI response in ~1 second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some rough edges I ran into:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Conversations: Handles single questions fine, but back-and-forths make it go dumb fast&lt;/li&gt; &lt;li&gt;Hallucinations: More than I expected&lt;/li&gt; &lt;li&gt;Audio input: Needs really clean audio; played guitar in front of it and asked it to identify chords… didn’t go well lol&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Still, it works well and is 100% local. Can't wait for Qwen 3.0 Omni release. I'll leave the repo in the comments for those who want to mess with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m9ttqovtmtif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpayu9/fully_local_qwen_25_omni_realtime_agent_sees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpayu9/fully_local_qwen_25_omni_realtime_agent_sees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T17:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mp5bjc</id>
    <title>God I love Qwen and llamacpp so much!</title>
    <updated>2025-08-13T14:01:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt; &lt;img alt="God I love Qwen and llamacpp so much!" src="https://external-preview.redd.it/YWE3eDdxZG5tc2lmMRvVg1psIEfKedgCcU_ySdSE0fdUxqG9M3HUjgrx1S5i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8afab7c45ab87f6ac2ce8db445bb27de25840096" title="God I love Qwen and llamacpp so much!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local batch inference with qwen3 30B Instruct on a single RTX3090, 4 requests in parallel &lt;/p&gt; &lt;p&gt;Gonna use it to mass process some data to generate insights about our platform usage&lt;/p&gt; &lt;p&gt;I feel like I'm hitting my limits here and gonna need a multi GPU setup soon 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ur3oxzhnmsif1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mp5bjc/god_i_love_qwen_and_llamacpp_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T14:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
