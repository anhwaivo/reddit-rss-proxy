<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-07T23:50:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mk92k4</id>
    <title>gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF · Hugging Face</title>
    <updated>2025-08-07T19:10:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"&gt; &lt;img alt="gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF · Hugging Face" src="https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8ef5dae58a1931f159f19948400500dc5e8110f" title="gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T19:10:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjw40a</id>
    <title>Nonescape: SOTA AI-Image Detection Model (Open-Source)</title>
    <updated>2025-08-07T10:08:24+00:00</updated>
    <author>
      <name>/u/e3ntity_</name>
      <uri>https://old.reddit.com/user/e3ntity_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjw40a/nonescape_sota_aiimage_detection_model_opensource/"&gt; &lt;img alt="Nonescape: SOTA AI-Image Detection Model (Open-Source)" src="https://preview.redd.it/6p2s5uidnkhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcd836239c046a643a71f476cd112af2a16585e7" title="Nonescape: SOTA AI-Image Detection Model (Open-Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model Info&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Nonescape just open-sourced two AI-image detection models: a full model with SOTA accuracy and a mini 80MB model that can run in-browser.&lt;/p&gt; &lt;p&gt;Demo (works with images+videos): &lt;a href="https://www.nonescape.com"&gt;https://www.nonescape.com&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/aediliclabs/nonescape"&gt;https://github.com/aediliclabs/nonescape&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The models detect the latest AI-images (including diffusion images, deepfakes, and GANs)&lt;/li&gt; &lt;li&gt;Trained on 1M+ images representative of the internet&lt;/li&gt; &lt;li&gt;Includes Javascript/Python libraries to run the models&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/e3ntity_"&gt; /u/e3ntity_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6p2s5uidnkhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjw40a/nonescape_sota_aiimage_detection_model_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjw40a/nonescape_sota_aiimage_detection_model_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T10:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkaxrx</id>
    <title>On the topic of graphs</title>
    <updated>2025-08-07T20:22:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"&gt; &lt;img alt="On the topic of graphs" src="https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8651ed9c62cd9f04f57ff3ed7c3f0971fcdc97f" title="On the topic of graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdhwce4vonhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:22:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk74wq</id>
    <title>Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models</title>
    <updated>2025-08-07T17:58:02+00:00</updated>
    <author>
      <name>/u/agentcubed</name>
      <uri>https://old.reddit.com/user/agentcubed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt; &lt;img alt="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" src="https://b.thumbs.redditmedia.com/TJcHxiajHwTxrtNoLptpQfkKcoLceehEHpbxyUdzAnI.jpg" title="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models that needs expensive hardware&lt;br /&gt; R1 670b, 37b active&lt;br /&gt; Kimi K2 1t, 32b active&lt;br /&gt; Qwen3 235b, 22b active&lt;/p&gt; &lt;p&gt;Models that are local-friendly&lt;br /&gt; GLM 4.5 Air 106b, 12b active (very pushing it but fine)&lt;br /&gt; Qwen3 14b&lt;br /&gt; oss 120b, 5b active&lt;br /&gt; Qwen3 30b, 3b active&lt;br /&gt; oss 20b, 3b active&lt;/p&gt; &lt;p&gt;Why are people expecting gpt-oss to be better than R1 K2 or Qwen 235b?&lt;/p&gt; &lt;p&gt;I would rather have a model that I can actually run locally than a good model that needs providers. To be clear, I hate and won't use gpt-oss, but because it's censored and not because models many times larger are better.&lt;/p&gt; &lt;p&gt;I LOVED Qwen3 30b/3b was local-friendly and fast and nobody compared it to bigger models, but when OpenAI releases a local model and suddenly everyone is comparing it to non-local models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47a39c27091e2beb9233c783989cf7305269027e"&gt;Graph of all local-friendly models (GLM Air would be tough)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agentcubed"&gt; /u/agentcubed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke7ef</id>
    <title>120B runs awesome on just 8GB VRAM!</title>
    <updated>2025-08-07T22:32:04+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU (~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt; &lt;p&gt;You can offload just the attention layers to GPU (requiring about 5GB of VRAM) for fast prefill.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache for the sequence&lt;/li&gt; &lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt; &lt;li&gt;Routing tables&lt;/li&gt; &lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt; &lt;p&gt;This yields an amazing snappy system for a 120B model! Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt; &lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;with 5GB of vram usage!&lt;/p&gt; &lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk87kd</id>
    <title>Polymarket</title>
    <updated>2025-08-07T18:38:21+00:00</updated>
    <author>
      <name>/u/V4ldeLund</name>
      <uri>https://old.reddit.com/user/V4ldeLund</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk87kd/polymarket/"&gt; &lt;img alt="Polymarket" src="https://preview.redd.it/puuand3c6nhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fd560e7d342a8544893e31baa753e80a4522002" title="Polymarket" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's too much winning Sam, please stop /s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/V4ldeLund"&gt; /u/V4ldeLund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/puuand3c6nhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk87kd/polymarket/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk87kd/polymarket/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T18:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk7r1g</id>
    <title>Trained an 41M HRM-Based Model to generate semi-coherent text!</title>
    <updated>2025-08-07T18:20:52+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"&gt; &lt;img alt="Trained an 41M HRM-Based Model to generate semi-coherent text!" src="https://b.thumbs.redditmedia.com/5IXZKHsgxD2_snxB5qYDZsSXRsrSDWyvbqoNOIrkjvM.jpg" title="Trained an 41M HRM-Based Model to generate semi-coherent text!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk7r1g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T18:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke83e</id>
    <title>Oss20b creative writing</title>
    <updated>2025-08-07T22:32:53+00:00</updated>
    <author>
      <name>/u/Upbeat5840</name>
      <uri>https://old.reddit.com/user/Upbeat5840</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke83e/oss20b_creative_writing/"&gt; &lt;img alt="Oss20b creative writing" src="https://preview.redd.it/be1mdlfecohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da2c4f7db63119767e9d55eee818f60cfb65d94f" title="Oss20b creative writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was curious so I decided to run some custom software to see what type of creative writing 20b could pull off. My opinion is that its creativity is much wider than the latest qwen. That one kept trying to insist we were going to be telling a ghost story. I ran the world building portion of the prompting with 20b and got three plausible interesting worlds that might be fun to explore. Chose one and had it generate five books. I like to test the long horizon capabilities while saturating the context windows. &lt;/p&gt; &lt;p&gt;Anyway anyone else trying this with these models? I’m curious if you are getting the consistently repeating phrases I’m seeing. &lt;/p&gt; &lt;p&gt;“You have potential” “you can’t do this alone” it’s always a green shield. It always brings up orchards. Etc&lt;/p&gt; &lt;p&gt;You can read the first three chapters on my LinkedIn article. I’m on my phone and LinkedIn doesn’t play well so I can’t copy it out easily. &lt;/p&gt; &lt;p&gt;Story Summary: Mira Larkspur, driven by tremors from Mount Ardent, uncovers a glowing inscription that reveals an ancient fault line. Her journey to prevent a world-shattering sundering leads her to forge alliances with key figures from three distinct realms: the smith Durgan Ironhand, the elder Alarion Greenroot, and the sky-ship captain Riven Skyward. Together, they create a &amp;quot;Stabilizer&amp;quot; device by combining the unique magical disciplines of their realms—metal runes, leaf-breath resin, and aether-dust—to harness a crystal core. Despite sabotage attempts by the rogue caster Elias Thorn and the merchant Lydia Grey, Mira and her allies successfully use the device to seal the fissure. Her victory leads to the creation of the Guild of Balance, a new organization dedicated to safeguarding the world, with Mira elected as its first steward, ready to face new threats on the horizon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.linkedin.com/pulse/openai-oss-20b-writing-jeremy-harper-ktnac"&gt;https://www.linkedin.com/pulse/openai-oss-20b-writing-jeremy-harper-ktnac&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upbeat5840"&gt; /u/Upbeat5840 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/be1mdlfecohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke83e/oss20b_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke83e/oss20b_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk5n89</id>
    <title>HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide</title>
    <updated>2025-08-07T17:02:37+00:00</updated>
    <author>
      <name>/u/Tango-Down766</name>
      <uri>https://old.reddit.com/user/Tango-Down766</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt; &lt;img alt="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" src="https://b.thumbs.redditmedia.com/-NXaX5EHmxxb7GBcWRIp5vrkgUBJaiSRrpm9inzqlEM.jpg" title="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://civitaiarchive.com/"&gt;https://civitaiarchive.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tango-Down766"&gt; /u/Tango-Down766 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk5n89"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjju67</id>
    <title>No, no, no, wait - on a second thought, I KNOW the answer!</title>
    <updated>2025-08-06T23:11:24+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt; &lt;img alt="No, no, no, wait - on a second thought, I KNOW the answer!" src="https://preview.redd.it/zs8aeebxdhhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb8196976261024587d9462ed2ceb999cbda98af" title="No, no, no, wait - on a second thought, I KNOW the answer!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, I know my prompt itself is flawed - let me clarify that I don't side with any country in this regard and just wanted to test for the extent of &amp;quot;SAFETY!!1&amp;quot; in OpenAI's new model. I stumbled across this funny reaction here.&lt;/p&gt; &lt;p&gt;Model: GPT-OSS 120b (High reasoning mode), default system prompt, no further context on the official GPT-OSS website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zs8aeebxdhhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T23:11:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk4kt0</id>
    <title>Be careful in selecting providers on openrouter</title>
    <updated>2025-08-07T16:22:25+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"&gt; &lt;img alt="Be careful in selecting providers on openrouter" src="https://preview.redd.it/o9dqe3l9imhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63498a33a88373227cb3e4dd804ff112b545e323" title="Be careful in selecting providers on openrouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o9dqe3l9imhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T16:22:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk7u6i</id>
    <title>GPT 5 seems worse than Gemini in head-to-head</title>
    <updated>2025-08-07T18:24:11+00:00</updated>
    <author>
      <name>/u/nypdk</name>
      <uri>https://old.reddit.com/user/nypdk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7u6i/gpt_5_seems_worse_than_gemini_in_headtohead/"&gt; &lt;img alt="GPT 5 seems worse than Gemini in head-to-head" src="https://b.thumbs.redditmedia.com/BkR-ESFutx328ilHuO5vEwJk72Q5qEOzvUlacIGXlSo.jpg" title="GPT 5 seems worse than Gemini in head-to-head" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jcuuuedh3nhf1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54ca5f632741b68b51c12ebeb30ec2d9cf56976b"&gt;https://preview.redd.it/jcuuuedh3nhf1.png?width=1032&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=54ca5f632741b68b51c12ebeb30ec2d9cf56976b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This image from &lt;a href="http://lmarena.ai/leaderboard/text"&gt;lmarena.ai/leaderboard/text&lt;/a&gt; shows that Gemini beats GPT-5, and that the winrates for Gemini are still higher. Not really sure what the hype is around the model in this case, especially when companies can fine tune to fit benchmarks. This is really the only thing that matters, and gemini still has higher WR in battles (66% vs 62%)&lt;/p&gt; &lt;p&gt;UPDATE: It looks like they pulled that image LOL. Not sure if the elo is just being flat out manipulated then but here is a similarish plot for one of the subcategories:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text/english"&gt;https://lmarena.ai/leaderboard/text/english&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nypdk"&gt; /u/nypdk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7u6i/gpt_5_seems_worse_than_gemini_in_headtohead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7u6i/gpt_5_seems_worse_than_gemini_in_headtohead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk7u6i/gpt_5_seems_worse_than_gemini_in_headtohead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T18:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk6mnf</id>
    <title>GPT - 5 graph</title>
    <updated>2025-08-07T17:38:58+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6mnf/gpt_5_graph/"&gt; &lt;img alt="GPT - 5 graph" src="https://preview.redd.it/gq9em5jyvmhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfe8461fc8e2d885741740dc1c86129fc37b05dd" title="GPT - 5 graph" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw another like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gq9em5jyvmhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6mnf/gpt_5_graph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6mnf/gpt_5_graph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjub4z</id>
    <title>llama.cpp HQ</title>
    <updated>2025-08-07T08:14:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"&gt; &lt;img alt="llama.cpp HQ" src="https://preview.redd.it/d15gp2d33khf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=356bf4bfc9f7c3e2c9fc089431a35c0a3300f0d2" title="llama.cpp HQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d15gp2d33khf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjub4z/llamacpp_hq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T08:14:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk3rj1</id>
    <title>Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop</title>
    <updated>2025-08-07T15:52:01+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt; &lt;img alt="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" src="https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f53b202537c26df370b20f3e2c66f92c5b25828" title="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While the setup looks über cool, the software is still not ready to make good use of the hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N5xhOqlvRh4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T15:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk6z83</id>
    <title>GPT5 reveal plots be like (obviously a made-up tweet, don't believe what you see on the internet)</title>
    <updated>2025-08-07T17:52:00+00:00</updated>
    <author>
      <name>/u/AuspiciousApple</name>
      <uri>https://old.reddit.com/user/AuspiciousApple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6z83/gpt5_reveal_plots_be_like_obviously_a_madeup/"&gt; &lt;img alt="GPT5 reveal plots be like (obviously a made-up tweet, don't believe what you see on the internet)" src="https://preview.redd.it/2y8c20g1ymhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86626757ac1473a3442080ec17bff52b0ba72e6c" title="GPT5 reveal plots be like (obviously a made-up tweet, don't believe what you see on the internet)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AuspiciousApple"&gt; /u/AuspiciousApple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2y8c20g1ymhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6z83/gpt5_reveal_plots_be_like_obviously_a_madeup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk6z83/gpt5_reveal_plots_be_like_obviously_a_madeup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk61k5</id>
    <title>GPT 5 pricing</title>
    <updated>2025-08-07T17:17:08+00:00</updated>
    <author>
      <name>/u/sruly_</name>
      <uri>https://old.reddit.com/user/sruly_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk61k5/gpt_5_pricing/"&gt; &lt;img alt="GPT 5 pricing" src="https://preview.redd.it/erzhspvwrmhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a31a204228ed97ee7f89f2a4281b676d7a6dd615" title="GPT 5 pricing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pricing found here &lt;a href="https://openai.com/api/"&gt;https://openai.com/api/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sruly_"&gt; /u/sruly_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erzhspvwrmhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk61k5/gpt_5_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk61k5/gpt_5_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkcwiv</id>
    <title>OpenAI open washing</title>
    <updated>2025-08-07T21:38:35+00:00</updated>
    <author>
      <name>/u/gwyngwynsituation</name>
      <uri>https://old.reddit.com/user/gwyngwynsituation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they’re not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior. And that concludes my rant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gwyngwynsituation"&gt; /u/gwyngwynsituation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T21:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk26rk</id>
    <title>Llama.cpp now supports GLM 4.5 Air</title>
    <updated>2025-08-07T14:52:12+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt; &lt;img alt="Llama.cpp now supports GLM 4.5 Air" src="https://b.thumbs.redditmedia.com/jawkehNzIT0a-enbiD4fQc_KPJ-dSoMI8t5allPhBfU.jpg" title="Llama.cpp now supports GLM 4.5 Air" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from our hero sammcj&lt;/p&gt; &lt;p&gt;Pictured, Cuda v1.45 engine in LM Studio. (the cuda 12 1.44 runtime still not working--the GLM 4.5 PR was merged in the past 8 hours or so).&lt;/p&gt; &lt;p&gt;As an aside, my initial vibe is it is far too wordy and overthinks, though, and gpt oss 120b is better and also faster in pure t/s but that's very much early vibe so take with a heavy dose of salt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk26rk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T14:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk9qxe</id>
    <title>Fixed the SWE-bench graph:</title>
    <updated>2025-08-07T19:36:45+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk9qxe/fixed_the_swebench_graph/"&gt; &lt;img alt="Fixed the SWE-bench graph:" src="https://b.thumbs.redditmedia.com/V9X-wElhTzujCwtEHzajd2V4dcql8pBZRAbgoyQZpuY.jpg" title="Fixed the SWE-bench graph:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk9qxe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk9qxe/fixed_the_swebench_graph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk9qxe/fixed_the_swebench_graph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T19:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxx6j</id>
    <title>GPT-OSS is Another Example Why Companies Must Build a Strong Brand Name</title>
    <updated>2025-08-07T11:49:08+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please, for the love of God, convince me that GPT-OSS is the best open-source model that exists today. I dare you to convince me. There's no way the GPT-OSS 120B is better than Qwen-235B-A22B-2507, let alone DeepSeek R1. So why do 90% of YouTubers, and even Two Minute Papers (a guy I respect), praise GPT-OSS as the most beautiful gift to humanity any company ever gave? &lt;/p&gt; &lt;p&gt;It's not even multimodal, and they're calling it a gift? WTF for? Isn't that the same coriticim when Deepseek-R1 was released, that it was text-based only? In about 2 weeks, Alibaba released a video model (Wan2.2) , an image model (Qwen-Image) that are the best open-source models in their categories, two amazing 30B models that are super fast and punch above their weight, and two incredible 4B models – yet barely any YouTubers covered them. Meanwhile, OpenAI launches a rather OK model and hell broke loose everywhere. How do you explain this? I can't find any rational explanation except OpenAI built a powerful brand name.&lt;/p&gt; &lt;p&gt;When DeepSeek-R1 was released, real innovation became public – innovation GPT-OSS clearly built upon. How can a model have 120 Experts all stable without DeepSeek's paper? And to make matters worse, OpenAI dared to show their 20B model trained for under $500K! As if that's an achievement when DeepSeek R1 cost just $5.58 million – 89x cheaper than OpenAI's rumored budgets. &lt;/p&gt; &lt;p&gt;Remember when every outlet (especially American ones) criticized DeepSeek: 'Look, the model is censored by the Communist Party. Do you want to live in a world of censorship?' Well, ask GPT-OSS about the Ukraine war and see if it answers you. The hypocrisy is rich. User &lt;a href="/u/Final_Wheel_7486"&gt;u/Final_Wheel_7486&lt;/a&gt; posted about this.&lt;/p&gt; &lt;p&gt;I'm not a coder or mathematician, and even if I were, these models wouldn't help much – they're too limited. So I DON'T CARE ABOUT CODING SCORES ON BENCHMARKS. Don't tell me 'these models are very good at coding' as if a 20B model can actually code. Coders are a niche group. We need models that help average people.&lt;/p&gt; &lt;p&gt;This whole situation reminds me of that greedy guy who rarely gives to charity, then gets praised for doing the bare minimum when he finally does.&lt;/p&gt; &lt;p&gt;I am notsaying the models OpenAI released are bad, they simply aren't. But, what I am saying is that the hype is through the roof for an OK product. I want to hear your thoughts. &lt;/p&gt; &lt;p&gt;P.S. OpenAI fanboys, please keep it objective and civil!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T11:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk8bh1</id>
    <title>caught in 4K</title>
    <updated>2025-08-07T18:42:23+00:00</updated>
    <author>
      <name>/u/JP_525</name>
      <uri>https://old.reddit.com/user/JP_525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk8bh1/caught_in_4k/"&gt; &lt;img alt="caught in 4K" src="https://b.thumbs.redditmedia.com/YQrgzZ0f8iGzesSaaIQmX6IpiMlZjzskYA1keLu6UKk.jpg" title="caught in 4K" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JP_525"&gt; /u/JP_525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk8bh1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk8bh1/caught_in_4k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk8bh1/caught_in_4k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T18:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkf543</id>
    <title>To all GPT-5 posts</title>
    <updated>2025-08-07T23:11:59+00:00</updated>
    <author>
      <name>/u/Danny_Davitoe</name>
      <uri>https://old.reddit.com/user/Danny_Davitoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt; &lt;img alt="To all GPT-5 posts" src="https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1" title="To all GPT-5 posts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please. I don’t care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny_Davitoe"&gt; /u/Danny_Davitoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v08gwidjohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkavhy</id>
    <title>random bar chart made by Qwen3-235B-A22B-2507</title>
    <updated>2025-08-07T20:19:55+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt; &lt;img alt="random bar chart made by Qwen3-235B-A22B-2507" src="https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd853635222d78299767b459957da8a9ae9f30b5" title="random bar chart made by Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rka3lhpnonhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk5ti0</id>
    <title>Hilarious chart from GPT-5 Reveal</title>
    <updated>2025-08-07T17:08:57+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5ti0/hilarious_chart_from_gpt5_reveal/"&gt; &lt;img alt="Hilarious chart from GPT-5 Reveal" src="https://preview.redd.it/ewx61i9gqmhf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce6f821baade0cb741dbab09472eb1f7eb1d04a5" title="Hilarious chart from GPT-5 Reveal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewx61i9gqmhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5ti0/hilarious_chart_from_gpt5_reveal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5ti0/hilarious_chart_from_gpt5_reveal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
