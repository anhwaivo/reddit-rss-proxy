<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-10T22:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jw7hs8</id>
    <title>Seeking Advice fintuning</title>
    <updated>2025-04-10T20:10:54+00:00</updated>
    <author>
      <name>/u/Gold-Artichoke-9288</name>
      <uri>https://old.reddit.com/user/Gold-Artichoke-9288</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, i am still new to fine tuning trying to learn by doing projects.&lt;/p&gt; &lt;p&gt;Currently im trying to fine tune a model with unsloth, i found a dataset in hugging face and have done the first project, the results were fine (based on training and evaluation loss).&lt;/p&gt; &lt;p&gt;So in my second project i decided to prepare my own data, i have pdf files with plain text and im trying to transform them into a question answer format as i read somewhere that this format is necessary to fine tune models. I find this a bit odd as acquiring such format could be nearly impossible.&lt;/p&gt; &lt;p&gt;So i came up with two approaches, i extracted the text from the files into small chnuks. First one is to use some nlp technics and pre trained model to generate questions or queries based on those chnuks results were terrible maybe im doing something wrong but idk. Second one was to only use one feature which is the chunks only 215 row . Dataset shape is (215, 1) I trained it on 2000steps and notice an overfitting by measuring the loss of both training and testing test loss was 3 point something and traing loss was 0.00…somthing.&lt;/p&gt; &lt;p&gt;My questions are: - How do you prepare your data if you have pdf files with plain text my case (datset about law) - what are other evaluation metrics you do - how do you know if your model ready for real world deployment &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gold-Artichoke-9288"&gt; /u/Gold-Artichoke-9288 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7hs8/seeking_advice_fintuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7hs8/seeking_advice_fintuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7hs8/seeking_advice_fintuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T20:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1hph</id>
    <title>Should we add real people to lmarena?</title>
    <updated>2025-04-10T16:02:23+00:00</updated>
    <author>
      <name>/u/Economy_Apple_4617</name>
      <uri>https://old.reddit.com/user/Economy_Apple_4617</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a reference point, a sort of new Turing test What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Apple_4617"&gt; /u/Economy_Apple_4617 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1hph/should_we_add_real_people_to_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1hph/should_we_add_real_people_to_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1hph/should_we_add_real_people_to_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw0ieg</id>
    <title>AMD AI395 + 128GB - Inference Use case</title>
    <updated>2025-04-10T15:20:56+00:00</updated>
    <author>
      <name>/u/SecuredStealth</name>
      <uri>https://old.reddit.com/user/SecuredStealth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;I'm heard a lot of pros and cons for the AI395 from AMD with at most 128GB RAM (Framework, GMKtec). Of course prompt processing speeds are unknown, and probably dense models won't function well as the memory bandwidth isn't that great. I'm curious to know if this build will be useful for inferencing use cases. I don't plan to do any kind of training or fine tuning. I don't plan to make elaborate prompts, but I do want to be able to use higher quants and RAG. I plan to make general purpose prompts, as well some focussed on scripting. Is this build still going to prove useful or is it just money wasted? I enquire about wasted money because the pace of development is fast and I don't want a machine which is totally obsolete in a year from now due to newer innovations. &lt;/p&gt; &lt;p&gt;I have limited space at home so a full blown desktop with multiple 3090s is not going to work out. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SecuredStealth"&gt; /u/SecuredStealth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0ieg/amd_ai395_128gb_inference_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0ieg/amd_ai395_128gb_inference_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0ieg/amd_ai395_128gb_inference_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw4yqv</id>
    <title>What is the best scraper tool right now? Firecrawl is great, but I want to explore more options</title>
    <updated>2025-04-10T18:25:56+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using Firecrawl lately (which is great), but I’m more curious what others are using right now for a scalable scraping like large sites or dynamic contents . I am familiar with the old-school BeautifulSoup/Selenium way but i kind of feel left out on a reliable scrapper tool.&lt;/p&gt; &lt;p&gt;Are there any newer frameworks or scrapers that stand out right now?&lt;/p&gt; &lt;p&gt;Would love to hear some recommendation or experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw4yqv/what_is_the_best_scraper_tool_right_now_firecrawl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw4yqv/what_is_the_best_scraper_tool_right_now_firecrawl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw4yqv/what_is_the_best_scraper_tool_right_now_firecrawl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T18:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvpwgc</id>
    <title>Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 “is coming in hours”</title>
    <updated>2025-04-10T04:52:17+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"&gt; &lt;img alt="Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 “is coming in hours”" src="https://external-preview.redd.it/95PFso5FRvaqkffIUa3P3b8toY1_H2zLUQj524lS6zs.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d55dbb224ea9b043b89c3770cb1097d7dc7459" title="Bindu Reddy, CEO of AbacusAI (LiveBench) states Qwen 3 “is coming in hours”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/bindureddy/status/1910185483545776630?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvpwgc/bindu_reddy_ceo_of_abacusai_livebench_states_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T04:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvtn1t</id>
    <title>"Dragontail" model at LMarena is a potential beast</title>
    <updated>2025-04-10T09:19:23+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious if anyone has any suspicions about the true identity behind the Dragontail model at LMArena. From what I've seen so far, this mysterious model performs on par with top-tier models like o3-mini-high and claude-3-7-sonnet-20250219-thinking-32k, but what it sets it apart from them is that it consistently delivers the correct answers (tedious mathematical problems). Sadly, open weights models such as DeepSeek V3 or R1, Llama4, Cohere's, are not even close to be able to solve them. There is also a (slightly worse) Shadebrook model that I suspect is also related to it.&lt;/p&gt; &lt;p&gt;Does anyone have any theories or insights about which model might actually be powering this beast?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvtn1t/dragontail_model_at_lmarena_is_a_potential_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T09:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw7b63</id>
    <title>Today, what are the go to front-ends for training LoRAs and fine-tuning?</title>
    <updated>2025-04-10T20:03:28+00:00</updated>
    <author>
      <name>/u/X3liteninjaX</name>
      <uri>https://old.reddit.com/user/X3liteninjaX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, been out of the game for a while so I'm hoping someone could direct me to whatever front ends are most popular these days that offer LoRA training and ideally fine-tuning. I still have oobabooga's text-gen-webui installed if that is still popular.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/X3liteninjaX"&gt; /u/X3liteninjaX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7b63/today_what_are_the_go_to_frontends_for_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7b63/today_what_are_the_go_to_frontends_for_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw7b63/today_what_are_the_go_to_frontends_for_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T20:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw2aph</id>
    <title>Llama 4 Japanese Evals</title>
    <updated>2025-04-10T16:36:21+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Llama 4 didn't explicitly call out CJK support, they did claim stronger overall multi-lingual capabilities with &amp;quot;10x more multilingual tokens than Llama 3&amp;quot; and &amp;quot;pretraining on 200 languages.&amp;quot;&lt;/p&gt; &lt;p&gt;Since I had some H100 nodes available and my eval suite was up and running, I ran some testing on both Maverick FP8 and Scout on the &lt;a href="https://blog.vllm.ai/2025/04/05/llama4.html"&gt;inference-validated vLLM v0.8.3 release&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those that are just interested in the results. Here's how Maverick does, compared against the same models that Meta uses in their announcement blog, but w/ a bit of spice - Llama 3.1 405B, and the best Japanese models I've tested so far, quasar-alpha and gpt-4.5 (which at list price, costs &amp;gt;$500 to eval! BTW, shout out to &lt;a href="/u/MrKeys_X"&gt;/u/MrKeys_X&lt;/a&gt; for contributing some credits towards testing gpt-4.5):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model Name&lt;/th&gt; &lt;th&gt;Shaberi AVG&lt;/th&gt; &lt;th&gt;ELYZA 100&lt;/th&gt; &lt;th&gt;JA MT Bench&lt;/th&gt; &lt;th&gt;Rakuda&lt;/th&gt; &lt;th&gt;Tengu&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;openrouter/quasar-alpha&lt;/td&gt; &lt;td&gt;&lt;strong&gt;9.20&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;9.41&lt;/td&gt; &lt;td&gt;9.01&lt;/td&gt; &lt;td&gt;9.42&lt;/td&gt; &lt;td&gt;&lt;strong&gt;8.97&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-4.5-preview-2025-02-27&lt;/td&gt; &lt;td&gt;9.19&lt;/td&gt; &lt;td&gt;&lt;strong&gt;9.50&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.85&lt;/td&gt; &lt;td&gt;&lt;strong&gt;9.56&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-4o-2024-11-20&lt;/td&gt; &lt;td&gt;9.15&lt;/td&gt; &lt;td&gt;9.34&lt;/td&gt; &lt;td&gt;&lt;strong&gt;9.10&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;9.55&lt;/td&gt; &lt;td&gt;8.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;deepseek-ai/DeepSeek-V3-0324&lt;/td&gt; &lt;td&gt;8.98&lt;/td&gt; &lt;td&gt;9.22&lt;/td&gt; &lt;td&gt;8.68&lt;/td&gt; &lt;td&gt;9.24&lt;/td&gt; &lt;td&gt;8.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gemini-2.0-flash&lt;/td&gt; &lt;td&gt;8.83&lt;/td&gt; &lt;td&gt;8.75&lt;/td&gt; &lt;td&gt;8.77&lt;/td&gt; &lt;td&gt;9.48&lt;/td&gt; &lt;td&gt;8.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8&lt;/td&gt; &lt;td&gt;8.64&lt;/td&gt; &lt;td&gt;8.54&lt;/td&gt; &lt;td&gt;8.81&lt;/td&gt; &lt;td&gt;9.14&lt;/td&gt; &lt;td&gt;8.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.1-405B-Instruct-FP8&lt;/td&gt; &lt;td&gt;8.41&lt;/td&gt; &lt;td&gt;8.52&lt;/td&gt; &lt;td&gt;8.42&lt;/td&gt; &lt;td&gt;9.07&lt;/td&gt; &lt;td&gt;7.63&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And here's Scout results. I didn't test Gemini 2.0 Flash Lite, but threw in a few other small models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model Name&lt;/th&gt; &lt;th&gt;Shaberi AVG&lt;/th&gt; &lt;th&gt;ELYZA 100&lt;/th&gt; &lt;th&gt;JA MT Bench&lt;/th&gt; &lt;th&gt;Rakuda&lt;/th&gt; &lt;th&gt;Tengu&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;google/gemma-3-27b-it&lt;/td&gt; &lt;td&gt;&lt;strong&gt;8.53&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.53&lt;/td&gt; &lt;td&gt;8.71&lt;/td&gt; &lt;td&gt;8.85&lt;/td&gt; &lt;td&gt;&lt;strong&gt;8.03&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistralai/Mistral-Small-3.1-24B-Instruct-2503&lt;/td&gt; &lt;td&gt;8.51&lt;/td&gt; &lt;td&gt;&lt;strong&gt;8.56&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.63&lt;/td&gt; &lt;td&gt;9.12&lt;/td&gt; &lt;td&gt;7.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;microsoft/phi-4&lt;/td&gt; &lt;td&gt;8.48&lt;/td&gt; &lt;td&gt;8.49&lt;/td&gt; &lt;td&gt;8.65&lt;/td&gt; &lt;td&gt;9.11&lt;/td&gt; &lt;td&gt;7.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;google/gemma-3-12b-it&lt;/td&gt; &lt;td&gt;8.48&lt;/td&gt; &lt;td&gt;8.34&lt;/td&gt; &lt;td&gt;8.67&lt;/td&gt; &lt;td&gt;9.02&lt;/td&gt; &lt;td&gt;7.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.1-405B-Instruct-FP8&lt;/td&gt; &lt;td&gt;8.41&lt;/td&gt; &lt;td&gt;8.52&lt;/td&gt; &lt;td&gt;8.42&lt;/td&gt; &lt;td&gt;9.07&lt;/td&gt; &lt;td&gt;7.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-4-Scout-17B-16E-Instruct&lt;/td&gt; &lt;td&gt;8.35&lt;/td&gt; &lt;td&gt;8.07&lt;/td&gt; &lt;td&gt;8.54&lt;/td&gt; &lt;td&gt;8.94&lt;/td&gt; &lt;td&gt;7.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;8.28&lt;/td&gt; &lt;td&gt;8.09&lt;/td&gt; &lt;td&gt;&lt;strong&gt;8.76&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.88&lt;/td&gt; &lt;td&gt;7.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;shisa-ai/shisa-v2-llama-3.1-8b-preview&lt;/td&gt; &lt;td&gt;8.10&lt;/td&gt; &lt;td&gt;7.58&lt;/td&gt; &lt;td&gt;8.32&lt;/td&gt; &lt;td&gt;&lt;strong&gt;9.22&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;7.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.1-8B-Instruct&lt;/td&gt; &lt;td&gt;7.34&lt;/td&gt; &lt;td&gt;6.95&lt;/td&gt; &lt;td&gt;7.67&lt;/td&gt; &lt;td&gt;8.36&lt;/td&gt; &lt;td&gt;6.40&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For absolute perf, Gemma 3 27B and Mistral Small 3.1 beat out Scout, and Phi 4 14B and Gemma 3 12B are actually amazing for their size (and outscore not just Scout, but Llama 3.1 405B.&lt;/p&gt; &lt;p&gt;If you want to read more about the evals themselves, and see some of the custom evals we're developing and those results (role playing, instruction following), check out a blog post I made here: &lt;a href="https://shisa.ai/posts/llama4-japanese-performance/"&gt;https://shisa.ai/posts/llama4-japanese-performance/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2aph/llama_4_japanese_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2aph/llama_4_japanese_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2aph/llama_4_japanese_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:36:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9cn3</id>
    <title>Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes</title>
    <updated>2025-04-10T21:29:24+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"&gt; &lt;img alt="Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes" src="https://preview.redd.it/6tbbef5js2ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660231214b036de1e98f805d80a58cd827280c9f" title="Fiction.liveBench: new Grok 3 scores are solid, llama 4 scores improved after vllm fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6tbbef5js2ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9cn3/fictionlivebench_new_grok_3_scores_are_solid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw91nh</id>
    <title>Orpheus TTS released multilingual support</title>
    <updated>2025-04-10T21:16:12+00:00</updated>
    <author>
      <name>/u/YearnMar10</name>
      <uri>https://old.reddit.com/user/YearnMar10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn’t find a thread on this here so far.&lt;/p&gt; &lt;p&gt;CanopyAI released new models for their Orpheus TTS model for different languages.&lt;/p&gt; &lt;p&gt;LANGUAGE(S) - French - German - Mandarin - Korean - Hindi - Spanish + Italian&lt;/p&gt; &lt;p&gt;More info here: &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;https://github.com/canopyai/Orpheus-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here: &lt;a href="https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba"&gt;https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here: &lt;a href="https://canopylabs.ai/releases/orpheus_can_speak_any_language"&gt;https://canopylabs.ai/releases/orpheus_can_speak_any_language&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also released a training guide, and there are already some finetunes floating around on HF and the first gguf versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YearnMar10"&gt; /u/YearnMar10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw91nh/orpheus_tts_released_multilingual_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:16:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvxi5f</id>
    <title>New coding model DeepCoder-14B-Preview</title>
    <updated>2025-04-10T13:09:25+00:00</updated>
    <author>
      <name>/u/mrskeptical00</name>
      <uri>https://old.reddit.com/user/mrskeptical00</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"&gt; &lt;img alt="New coding model DeepCoder-14B-Preview" src="https://external-preview.redd.it/YkKjX2lPMDEhqwzwRn4cpEMg8i531yZ0i7k6psvFYo8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b9a413e72aba377960a99362f5936db3ce66d4b" title="New coding model DeepCoder-14B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A joint collab between the Agentica team and Together AI based on finetune of DeepSeek-R1-Distill-Qwen-14B. They claim it’s as good at o3-mini. &lt;/p&gt; &lt;p&gt;HuggingFace URL: &lt;a href="https://huggingface.co/agentica-org/DeepCoder-14B-Preview"&gt;https://huggingface.co/agentica-org/DeepCoder-14B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF"&gt;https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrskeptical00"&gt; /u/mrskeptical00 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.together.ai/blog/deepcoder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvxi5f/new_coding_model_deepcoder14bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T13:09:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw3olo</id>
    <title>B200 vs H100 Training Benchmark: Up to 57% Faster Throughput</title>
    <updated>2025-04-10T17:33:26+00:00</updated>
    <author>
      <name>/u/igorsusmelj</name>
      <uri>https://old.reddit.com/user/igorsusmelj</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorsusmelj"&gt; /u/igorsusmelj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.lightly.ai/blog/nvidia-b200-vs-h100"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw3olo/b200_vs_h100_training_benchmark_up_to_57_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw3olo/b200_vs_h100_training_benchmark_up_to_57_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T17:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvx7kj</id>
    <title>Who is winning the GPU race??</title>
    <updated>2025-04-10T12:55:29+00:00</updated>
    <author>
      <name>/u/Senior-Raspberry-929</name>
      <uri>https://old.reddit.com/user/Senior-Raspberry-929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released the new tpu, 23x faster than the best supercomputer (that's what they claim).&lt;/p&gt; &lt;p&gt;What exactly is going on? Is nvidia still in the lead? who is competing with nvidia?&lt;/p&gt; &lt;p&gt;Apple seems like a very strong competitor, does apple have a chance?&lt;/p&gt; &lt;p&gt;Google is also investing in chips and released the most powerful chip, are they winning the race?&lt;/p&gt; &lt;p&gt;How is nvidia still holding strong? what makes nvidia special? they seem like they are falling behind apple and google.&lt;/p&gt; &lt;p&gt;I need someone to explain the entire situation with ai gpus/cpus&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior-Raspberry-929"&gt; /u/Senior-Raspberry-929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx7kj/who_is_winning_the_gpu_race/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx7kj/who_is_winning_the_gpu_race/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvx7kj/who_is_winning_the_gpu_race/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T12:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1g2a</id>
    <title>A slop forensics toolkit for LLMs: computing over-represented lexical profiles and inferring similarity trees</title>
    <updated>2025-04-10T16:00:36+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1g2a/a_slop_forensics_toolkit_for_llms_computing/"&gt; &lt;img alt="A slop forensics toolkit for LLMs: computing over-represented lexical profiles and inferring similarity trees" src="https://b.thumbs.redditmedia.com/zoJ26g_SPkE1NaH7mLwIHl5BIKSdXRusHlTvaFx1FoQ.jpg" title="A slop forensics toolkit for LLMs: computing over-represented lexical profiles and inferring similarity trees" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Releasing a few tools around LLM slop (over-represented words &amp;amp; phrases).&lt;/p&gt; &lt;p&gt;It uses stylometric analysis to surface repetitive words &amp;amp; n-grams which occur more often in LLM output compared to human writing.&lt;/p&gt; &lt;p&gt;Also borrowing some bioinformatics tools to infer similarity trees from these slop profiles, treating the presence/absence of lexical features as &amp;quot;mutations&amp;quot; to infer relationships.&lt;/p&gt; &lt;p&gt;- compute a &amp;quot;slop profile&amp;quot; of over-represented words &amp;amp; phrases for your model&lt;/p&gt; &lt;p&gt;- uses bioinformatics tools to infer similarity trees&lt;/p&gt; &lt;p&gt;- builds canonical slop phrase lists&lt;/p&gt; &lt;p&gt;Github repo: &lt;a href="https://github.com/sam-paech/slop-forensics"&gt;https://github.com/sam-paech/slop-forensics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing"&gt;https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jw1g2a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1g2a/a_slop_forensics_toolkit_for_llms_computing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1g2a/a_slop_forensics_toolkit_for_llms_computing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:00:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvw91v</id>
    <title>Notes on Llama 4: The hits, the misses, and the disasters</title>
    <updated>2025-04-10T12:05:25+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Llama 4 is here, but definitely not in the shape everyone wanted. There’s only negative sentiment towards it. Nobody seems to say good things about it except for a few Meta employees.&lt;/p&gt; &lt;p&gt;They seriously rushed the launch, but I am still not sure why. If the models were bad, why not postpone it? Was it something to do with tariffs, the anticipation of Monday market crash, to cushion their stock? &lt;/p&gt; &lt;p&gt;The entire launch was muddled with controversies, from poor models and false claims to bungled-up benchmarks. But are there any good Llama 4 models? If you search hard enough, there are a few.&lt;/p&gt; &lt;p&gt;Here is an overview of the Llama 4 models.&lt;/p&gt; &lt;h1&gt;The Hits&lt;/h1&gt; &lt;p&gt;There’s a very few good things about the Llama 4 models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;10 million context window in Scout and 1 million in Maverick. Good at the needle in the haystack tests I have done.&lt;/li&gt; &lt;li&gt;The Maverick seems to be a model created for agentic use cases, and it performs well on the function-calling benchmarks.&lt;/li&gt; &lt;li&gt;It’s very fast and cheap, again compliments function calling use cases.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Misses&lt;/h1&gt; &lt;p&gt;A lot of misses, indeed&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Starting with a restrictive, not-so-open-source Llama Licence. Still a mystery why it is when Deepseek models are MIT.&lt;/li&gt; &lt;li&gt;The 400b Maverick doesn’t justify its size. I'm not sure why they went with 17b active parameters; it’s worse than QwQ 32b in reasoning.&lt;/li&gt; &lt;li&gt;It neither offers the best code gen, writing, or reasoning.&lt;/li&gt; &lt;li&gt;The biggest miss is that there is no paper, no system card, just a blog post. Everyone looked up to Meta for this, and now they have botched this.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Disasters&lt;/h1&gt; &lt;p&gt;They are not recovering from this ever again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They literally gamed the Lmsys the sloppiest benchmark just to appear good. It’s sad at this point. I'm not sure if they cooked up other benchmarks mentioned in their release blog post.&lt;/li&gt; &lt;li&gt;Meta has tarnished their image again. They had the people's mandate, and they chose to squander it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Being a long-time Llama appreciator, the Llama 4 launch was such a letdown. It would have been still fine and forgotten if it was just a bad model, but cooking up benchmarks to appear that they are still in the AI race is horrible. &lt;/p&gt; &lt;p&gt;Full write-up on the Llama 4 launch here: &lt;a href="https://composio.dev/blog/notes-on-llama-4-the-hits-the-misses-and-the-disasters/"&gt;Notes on Llama 4: The Hits, the Misses, and the Disasters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to know your opinions on Llama 4 and would be interested to hear if you found anything good with these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvw91v/notes_on_llama_4_the_hits_the_misses_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T12:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw6cdk</id>
    <title>Openai New Memory feature is just Vector Search?</title>
    <updated>2025-04-10T19:23:39+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't get what's the big deal about this?&lt;/p&gt; &lt;p&gt;they are simply creating the embeddings for past chats and doing a vector search and adding chunks to context for every prompt right?&lt;/p&gt; &lt;p&gt;I've (we've) made this stuff 3 years ago, I don't get it, what am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw6cdk/openai_new_memory_feature_is_just_vector_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T19:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw09wz</id>
    <title>OpenAI releasing o3 full and o4 mini soon</title>
    <updated>2025-04-10T15:11:04+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw09wz/openai_releasing_o3_full_and_o4_mini_soon/"&gt; &lt;img alt="OpenAI releasing o3 full and o4 mini soon" src="https://preview.redd.it/hhfn6im1x0ue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f766150be4927f7c071cc0e4d87666562a35aa1" title="OpenAI releasing o3 full and o4 mini soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hhfn6im1x0ue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw09wz/openai_releasing_o3_full_and_o4_mini_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw09wz/openai_releasing_o3_full_and_o4_mini_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1n27</id>
    <title>Introducing ZR1-1.5B, a small but powerful reasoning model for math and code</title>
    <updated>2025-04-10T16:08:44+00:00</updated>
    <author>
      <name>/u/retrolione</name>
      <uri>https://old.reddit.com/user/retrolione</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retrolione"&gt; /u/retrolione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.zyphra.com/post/introducing-zr1-1-5b-a-small-but-powerful-math-code-reasoning-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1n27/introducing_zr115b_a_small_but_powerful_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1n27/introducing_zr115b_a_small_but_powerful_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9fba</id>
    <title>Macbook Pro M4 Max inference speeds</title>
    <updated>2025-04-10T21:32:31+00:00</updated>
    <author>
      <name>/u/SufficientRadio</name>
      <uri>https://old.reddit.com/user/SufficientRadio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"&gt; &lt;img alt="Macbook Pro M4 Max inference speeds" src="https://preview.redd.it/bms6abl5s2ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa3ff737f60bcdc49e3d199033b76539d0bc294d" title="Macbook Pro M4 Max inference speeds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had trouble finding this kind of information when I was deciding on what Macbook to buy so putting this out there to help future purchase decisions:&lt;/p&gt; &lt;p&gt;Macbook Pro 16&amp;quot; M4 Max 36gb 14‑core CPU, 32‑core GPU, 16‑core Neural&lt;/p&gt; &lt;p&gt;During inference, cpu/gpu temps get up to 103C and power draw is about 130W.&lt;/p&gt; &lt;p&gt;36gb ram allows me to comfortably load these models and still use my computer as usual (browsers, etc) without having to close every window. However, I do no need to close programs like Lightroom and Photoshop to make room.&lt;/p&gt; &lt;p&gt;Finally, the nano texture glass is worth it...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SufficientRadio"&gt; /u/SufficientRadio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bms6abl5s2ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9fba/macbook_pro_m4_max_inference_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:32:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw0c2i</id>
    <title>Llama 4 Maverick scores on seven independent benchmarks</title>
    <updated>2025-04-10T15:13:37+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"&gt; &lt;img alt="Llama 4 Maverick scores on seven independent benchmarks" src="https://b.thumbs.redditmedia.com/_L8mk4hZtxyzWyaFtsc3lifBL2BFQ1j3lLTdiCYP1ho.jpg" title="Llama 4 Maverick scores on seven independent benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;Extended NYT Connections&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;Creative Short Story Writing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/confabulations/"&gt;Confabulations/Hallucinations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/generalization/"&gt;Thematic Generalization&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/elimination_game/"&gt;Elimination Game&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/step_game/"&gt;Step Race Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/pgg_bench/"&gt;Public Goods Game&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jw0c2i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw0c2i/llama_4_maverick_scores_on_seven_independent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw2tbk</id>
    <title>So, Quasar Alpha might actually be OpenAI's model</title>
    <updated>2025-04-10T16:58:12+00:00</updated>
    <author>
      <name>/u/-Cacique</name>
      <uri>https://old.reddit.com/user/-Cacique</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"&gt; &lt;img alt="So, Quasar Alpha might actually be OpenAI's model" src="https://preview.redd.it/3xztmaoxf1ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0979fac46ea6a31825df6db93efc19bf9cce1a9b" title="So, Quasar Alpha might actually be OpenAI's model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cacique"&gt; /u/-Cacique &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3xztmaoxf1ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw2tbk/so_quasar_alpha_might_actually_be_openais_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jvs66w</id>
    <title>Qwen Dev: Qwen3 not gonna release "in hours", still need more time</title>
    <updated>2025-04-10T07:29:02+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt; &lt;img alt="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" src="https://preview.redd.it/3kcfx9xnmyte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd7d81fbe37029fcaaed94d7eab854f4dea3442" title="Qwen Dev: Qwen3 not gonna release &amp;quot;in hours&amp;quot;, still need more time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3kcfx9xnmyte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jvs66w/qwen_dev_qwen3_not_gonna_release_in_hours_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T07:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw9upz</id>
    <title>Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present “Both Sides”</title>
    <updated>2025-04-10T21:51:41+00:00</updated>
    <author>
      <name>/u/WanderingStranger0</name>
      <uri>https://old.reddit.com/user/WanderingStranger0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"&gt; &lt;img alt="Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present “Both Sides”" src="https://external-preview.redd.it/03uwcLJN0ZpAJgNqUGMgr-4V8aM1Tl_IIQXCSo0btJA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1a58da6c992c3251df315afbe32e193f0741c63" title="Facebook Pushes Its Llama 4 AI Model to the Right, Wants to Present “Both Sides”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WanderingStranger0"&gt; /u/WanderingStranger0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/facebook-pushes-its-llama-4-ai-model-to-the-right-wants-to-present-both-sides/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw9upz/facebook_pushes_its_llama_4_ai_model_to_the_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T21:51:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1l9n</id>
    <title>ByteDance just released the technical report for Seed-Thinking-v1.5</title>
    <updated>2025-04-10T16:06:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"&gt; &lt;img alt="ByteDance just released the technical report for Seed-Thinking-v1.5" src="https://preview.redd.it/zok3h2gu61ue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82891e270a5455a5741a8218234c99a25e56cc02" title="ByteDance just released the technical report for Seed-Thinking-v1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance just released the technical report for Seed-Thinking-v1.5, which is also an inference model trained using reinforcement learning. Based on the scores, it outperforms DeepSeek-R1 and is at a level close to Gemini-2.5-Pro and O3-mini-high. &lt;/p&gt; &lt;p&gt;However, I've searched everywhere and haven't found where the model is. I'm uncertain if they will release the weights. Once it's released, I will test it immediately.&lt;/p&gt; &lt;p&gt;Technical report link: &lt;a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5"&gt;https://github.com/ByteDance-Seed/Seed-Thinking-v1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zok3h2gu61ue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1l9n/bytedance_just_released_the_technical_report_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T16:06:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jw1e6b</id>
    <title>Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they’ll drop Qwen3 today)</title>
    <updated>2025-04-10T15:58:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"&gt; &lt;img alt="Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they’ll drop Qwen3 today)" src="https://b.thumbs.redditmedia.com/ZuvbVVx37QGz7TIEX4U5keZjwMgFyohptWI2tF28Stc.jpg" title="Can we all agree that Qwen has the best LLM mascot? (not at all trying to suck up so they’ll drop Qwen3 today)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jw1e6b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jw1e6b/can_we_all_agree_that_qwen_has_the_best_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-10T15:58:22+00:00</published>
  </entry>
</feed>
