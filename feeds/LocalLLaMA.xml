<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-03T09:35:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n6w7r9</id>
    <title>B850 AI Top motherboard</title>
    <updated>2025-09-02T21:29:40+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to pass the existence of this board along. Since I've mentioned it to a few people over the past few days and they also hadn't seen it or heard of it.&lt;/p&gt; &lt;p&gt;I came across it accidentally when looking for a good bifurcation board to support 2 new cards. I was just looking through the list of all 870 boards that support bifurcation. I figured the &amp;quot;AI&amp;quot; name was some gimmick, but it's definitely not. I almost just grabbed another Carbon, but glad I didn't. The board is priced pretty close to the same as X870e counterparts, but it's also incredibly premium for a B. I've personally never come across a B board with so many features. The X870e version is of course even more premium, but over double the price.&lt;/p&gt; &lt;p&gt;Anyway, the board has pretty great specs in general. Along with the 2x8 5.0 PCIe and really good spacing for large cards, it has 2x10g Ethernet ports, an 8-layer PCB, a ton of USB ports, etc. Great heatsinks as well, which make the board surprisingly heavy.&lt;/p&gt; &lt;p&gt;I'm using it with a Proxmox setup, so not using any of their &amp;quot;AI software,&amp;quot; however the board features in general are really nice.&lt;/p&gt; &lt;p&gt;Honestly it's just kind of cool to see a company making stuff for hobbyist AI enthusiasts.&lt;/p&gt; &lt;p&gt;link - &lt;a href="https://www.gigabyte.com/Motherboard/B850-AI-TOP"&gt;https://www.gigabyte.com/Motherboard/B850-AI-TOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n79udw</id>
    <title>Inference on new Framework desktop</title>
    <updated>2025-09-03T09:07:58+00:00</updated>
    <author>
      <name>/u/wombatsock</name>
      <uri>https://old.reddit.com/user/wombatsock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, lovely community! I'm just curious if anyone has gotten their hands on the new Framework desktop and used it to run inference for local models. I'm aware the memory bandwidth is weak, and I assume it's probably not great for fine-tuning or training. I just wonder if, given its energy efficiency and large shared memory capacity, it would make sense to set up the board as an LLM server for mid-sized models like quen3-coder:30b. Or if you have any other solutions that might work for this scenario, I'd love to hear them! (maybe a Mac Mini??). I already have an Nvidia 3060 with 12gb VRAM, and I'd rather not just get a bigger/faster GPU, they're pretty expensive and hog a lot of power when idling. Anyway, I'm rambling now, show me what you got!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wombatsock"&gt; /u/wombatsock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n79udw/inference_on_new_framework_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T09:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6kklk</id>
    <title>Git commands as LLM memory - 15k tokens down to 5k</title>
    <updated>2025-09-02T14:11:37+00:00</updated>
    <author>
      <name>/u/Apart-Employment-592</name>
      <uri>https://old.reddit.com/user/Apart-Employment-592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have been experimenting with giving LLMs access to granular git history instead of dumping entire codebases into context.&lt;/p&gt; &lt;p&gt;The approach: auto-commit code changes every 15 seconds to a shadow repo, then let the AI query it with git commands. So instead of feeding 5,000 lines of context, the model runs:&lt;/p&gt; &lt;p&gt;- `git diff HEAD~10` (50 tokens)&lt;/p&gt; &lt;p&gt;- `git log -S &amp;quot;function&amp;quot;` to find when something was implemented&lt;/p&gt; &lt;p&gt;- `git blame` to understand evolution&lt;/p&gt; &lt;p&gt;Tested this with Claude on a debugging session:&lt;/p&gt; &lt;p&gt;- Without git history: 15,000 tokens, multiple attempts before fixing a bug&lt;/p&gt; &lt;p&gt;- With git access: 5,000 tokens, found the bug fix almost immediately&lt;/p&gt; &lt;p&gt;The interesting part is that LLMs already understand git commands perfectly. They know exactly what to query without special training.&lt;/p&gt; &lt;p&gt;Technical approach I'm trying:&lt;/p&gt; &lt;p&gt;- detect file changes using `git status` every 15 seconds&lt;/p&gt; &lt;p&gt;- Commits to separate .shadowgit/ repo (not main)&lt;/p&gt; &lt;p&gt;- MCP server exposes read-only git operations&lt;/p&gt; &lt;p&gt;- Everything local&lt;/p&gt; &lt;p&gt;Questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone else exploring git as LLM memory? What's your approach?&lt;/li&gt; &lt;li&gt;For local models with small context windows, would this help?&lt;/li&gt; &lt;li&gt;Downsides I'm seeing: models might apply outdated patterns from history. Anyone experience this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;P.S. For transparency: I packaged this into a tool (ShadowGit, $19) but the MCP server is open source: &lt;a href="https://github.com/blade47/shadowgit-mcp"&gt;https://github.com/blade47/shadowgit-mcp&lt;/a&gt; if you want to build your own solution. More interested in the community's feedback on this approach.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;Alessandro&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Employment-592"&gt; /u/Apart-Employment-592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T14:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n771c1</id>
    <title>Adapting multilingual embeddings for a non-English language</title>
    <updated>2025-09-03T06:04:56+00:00</updated>
    <author>
      <name>/u/Logical-Dot-7563</name>
      <uri>https://old.reddit.com/user/Logical-Dot-7563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I’ve been experimenting with adapting multilingual embedding models for a non-English language, and I’d love to hear some tips or advice from others who’ve done something similar.&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;p&gt;Model: intfloat/multilingual-e5-large&lt;/p&gt; &lt;p&gt;Data: ~120k unlabeled corpus sentences + ~2k labeled questions. &lt;/p&gt; &lt;p&gt;Goal: improve retrieval / semantic similarity performance in my target language&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Fine-tuning helps the base and small variants.&lt;/p&gt; &lt;p&gt;The large model stays pretty stable — performance is moderate, but doesn’t improve much with straightforward fine-tuning.&lt;/p&gt; &lt;p&gt;Anyone have some insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Logical-Dot-7563"&gt; /u/Logical-Dot-7563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n771c1/adapting_multilingual_embeddings_for_a_nonenglish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n771c1/adapting_multilingual_embeddings_for_a_nonenglish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n771c1/adapting_multilingual_embeddings_for_a_nonenglish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T06:04:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7923o</id>
    <title>Has anyone run 256GB of DDR5 6000 stable on an AM5 platform?</title>
    <updated>2025-09-03T08:15:25+00:00</updated>
    <author>
      <name>/u/kitgary</name>
      <uri>https://old.reddit.com/user/kitgary</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to upgrade my system to 256GB so I can run a larger model with my GPU. I’m wondering if anyone has been able to run 256GB of DDR5 6000 stable on an AM5 platform. I don’t want to upgrade to Threadripper since it’s out of my budget. Which motherboard and RAM did you use?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587"&gt;https://www.msi.com/news/detail/MSI-Release-the-Latest-AMD-AGESA-Combo-PI-1-2-0-3e-BIOS--Supporting-all-64GBx4-DRAM-Chips-and-New-CPU-146587&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MSI claims their motherboard can still achieve a stable overclocking speed of 6000MT/s even with four 64GB DRAM fully installed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kitgary"&gt; /u/kitgary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7923o/has_anyone_run_256gb_of_ddr5_6000_stable_on_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6f5xl</id>
    <title>I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more.</title>
    <updated>2025-09-02T09:46:05+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt; &lt;img alt="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." src="https://b.thumbs.redditmedia.com/UxxcnGYx3ztd3aYOh5G5hmtgSX12gsIZtQoWGJsTZJs.jpg" title="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working hard on a big update for my open-source project, MAESTRO, and I'm excited to share v0.1.5-alpha with you all. MAESTRO is an autonomous research agent that turns any question into a fully-cited report.&lt;/p&gt; &lt;p&gt;A huge focus of this release was improving performance and compatibility with local models. I've refined the core agent workflows and prompts to make sure it works well with most reasonably intelligent locally hosted models.&lt;/p&gt; &lt;p&gt;I also launched a completely new documentation site to help users setup and start using MAESTRO. The best part is the new &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports Section&lt;/a&gt; that shows many reports generated with Local LLMs.&lt;/p&gt; &lt;p&gt;I've done extensive testing and shared the resulting reports so you can see what it's capable of. There are examples from a bunch of self-hosted models, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Large Models:&lt;/strong&gt; Qwen 2.5 72B, GPT-OSS 120B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium Models:&lt;/strong&gt; Qwen 3 32B, Gemma 3 27B, GPT-OSS 20B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a great way to see how different models handle complex topics and various writing styles before you commit to running them. I've also included performance notes on things like KV cache usage during these runs.&lt;/p&gt; &lt;p&gt;Under the hood, I improved some UI features and added parallel processing for more operations, so it’s a little faster and more responsive.&lt;/p&gt; &lt;p&gt;If you're interested in AI assisted research or just want to see what's possible with the latest open models, I'd love for you to check it out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;&lt;strong&gt;GitHub Release&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://murtaza-nasir.github.io/maestro"&gt;&lt;strong&gt;New Docs Site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful. Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6f5xl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n734ku</id>
    <title>FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games (EMNLP 2025 Main)</title>
    <updated>2025-09-03T02:33:38+00:00</updated>
    <author>
      <name>/u/ahnpersie</name>
      <uri>https://old.reddit.com/user/ahnpersie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n734ku/flashadventure_a_benchmark_for_gui_agents_solving/"&gt; &lt;img alt="FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games (EMNLP 2025 Main)" src="https://b.thumbs.redditmedia.com/jdSaEMCAPBJVekuDljNd89jlPtH7psUHrpdtc982vQo.jpg" title="FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games (EMNLP 2025 Main)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2509.01052"&gt;https://arxiv.org/abs/2509.01052&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/ahnjaewoo/FlashAdventure"&gt;https://github.com/ahnjaewoo/FlashAdventure&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project Page:&lt;/strong&gt; &lt;a href="https://ahnjaewoo.github.io/flashadventure/"&gt;https://ahnjaewoo.github.io/flashadventure/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We propose FlashAdventure, a new benchmark of 34 Flash adventure games for full story completion, alongside CUA-as-a-judge, an automated evaluator, and COAST, a clue-memory-based agent that addresses the long-term observation-behavior gap.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e1p5qtlfzumf1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7dbfdedf15bcde28ca8d3d9cf16f7e7b43ba4aac"&gt;FlashAdventure consists of 34 Flash-based classic adventure games and supports automatic evaluation of the GUI agent using CUA-as-a-Judge.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Game Collection:&lt;/strong&gt; We include 34 carefully selected Flash-based adventure games across 5 subgenres: Room Escape, Point-and-Click Adventure (Mystery/Detective), Visual Novel, Life/Management Simulation, Hidden Object:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m09ikrvj2vmf1.png?width=2494&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94261b0061f2156b26c249b1bdf89fec9678cba7"&gt;Overview of video game benchmarks. 'Complete Story Arc' indicates whether the benchmark evaluates anagent’s ability to complete a self-contained story arc from beginning to end. Our FlashAdventure evaluates agents on completing full story arcs in diverse adventure games.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Challenge:&lt;/strong&gt; A critical challenge in FlashAdventure is the long-term &lt;strong&gt;observation-behavior gap&lt;/strong&gt;, which refers to the time lag between when an agent observes information and when it can act upon it. Unlike prior benchmarks that focus on short-term objectives or include short story arcs, FlashAdventure emphasizes completion of full story arcs involving long-term objectives. Adventure games require agents to manage long-term dependencies crucial for solving full story arcs. Tolman's theory on latent learning suggests that humans can retrieve and apply clues after a long delay, which can also be explored in agents to assess whether similar emergent behaviors occur.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x6qxly401vmf1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d37270b454697888585510b064000a18420b544e"&gt;Comparison of gameplay progression across benchmarks. FlashAdventure requires agents to manage long-term time lags, such as interrogating a suspect and later discovering their innocence, demonstrating the importance of bridging the observation-behavior gap.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automatic Evaluation Framework:&lt;/strong&gt; Our new &lt;strong&gt;CUA-as-a-Judge&lt;/strong&gt; acts as an oracle with access to predefined success milestones for each game. It actively interacts with the game environment to verify whether milestones have been achieved. After a game agent finishes gameplay, CUA-as-a-Judge resumes from the game's final state and executes actions to check milestone completion, simulating a human judging process. We evaluate the reliability of CUA-as-a-Judge by comparing its judgments with human judgments across all 34 games. Our comparison shows a high agreement, with an accuracy of 94.00%, Spearman correlation of 0.9912, and Pearson correlation of 0.9999.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Agentic Framework:&lt;/strong&gt; Our new COAST (Clue-Oriented Agent for Sequential Tasks) addresses the observation-behavior gap through a Seek-Map-Solve cycle:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ednilrp92vmf1.png?width=3666&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92ea029a811a23bdc5b7f5c6bb6bd0ed5193cca5"&gt;COAST Framework with Seek-Map-Solve cycle.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiments:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tnv7y9s2vmf1.png?width=2436&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8cae5e0e3d74a547137a14291a3a1a793699f46"&gt;Comparison of different GUI agents across all 34 video games.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Current GUI agents struggle with full story arc completion (best: 5.88% success rate).&lt;/li&gt; &lt;li&gt;COAST improves goal / milestone completion by 5.88 / 2.78 percentage points over the baseline.&lt;/li&gt; &lt;li&gt;Still, significant gap remains between GUI agents and human performance (97.06% vs 5.88%).&lt;/li&gt; &lt;li&gt;Agents exhibit weak planning, poor visual perception, and deficient lateral thinking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahnpersie"&gt; /u/ahnpersie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n734ku/flashadventure_a_benchmark_for_gui_agents_solving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n734ku/flashadventure_a_benchmark_for_gui_agents_solving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n734ku/flashadventure_a_benchmark_for_gui_agents_solving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T02:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n79ckj</id>
    <title>AI insights via DNS and crawler analysis by Cloudflare. (not local but still interesting)</title>
    <updated>2025-09-03T08:35:10+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://radar.cloudflare.com/ai-insights"&gt;https://radar.cloudflare.com/ai-insights&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79ckj/ai_insights_via_dns_and_crawler_analysis_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79ckj/ai_insights_via_dns_and_crawler_analysis_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n79ckj/ai_insights_via_dns_and_crawler_analysis_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:35:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n79kt7</id>
    <title>How do you prompt an image editing model?</title>
    <updated>2025-09-03T08:50:26+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm not really a professional (or even informed amateur) when it comes to photo editing, which proves quite problematic when it comes to terminology in this area etc.&lt;/p&gt; &lt;p&gt;Now I want to start using Image editing LLMs and become a bit more proficient in doing image retouch using them. The results are however still not what I expect and see other, more professional users achieve.&lt;/p&gt; &lt;p&gt;Given how important the vocabulary and &amp;quot;language&amp;quot; is in communicating with LLMs, are there some guides or prompt examples that people have used in editing images with LLMs? e.g. do I simply say &amp;quot;improve the quality of this photo&amp;quot; if I want a higher resolution, or should be something else? Is it just &amp;quot;make it sharper&amp;quot;, or something more technical? etc.&lt;/p&gt; &lt;p&gt;Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79kt7/how_do_you_prompt_an_image_editing_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n79kt7/how_do_you_prompt_an_image_editing_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n79kt7/how_do_you_prompt_an_image_editing_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T08:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6eimy</id>
    <title>New Open LLM from Switzerland "Apertus", 40%+ training data is non English</title>
    <updated>2025-09-02T09:03:53+00:00</updated>
    <author>
      <name>/u/EnnioEvo</name>
      <uri>https://old.reddit.com/user/EnnioEvo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnnioEvo"&gt; /u/EnnioEvo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71s27</id>
    <title>FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp; audio processing on iOS/MacOS</title>
    <updated>2025-09-03T01:30:48+00:00</updated>
    <author>
      <name>/u/SummonerOne</name>
      <uri>https://old.reddit.com/user/SummonerOne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"&gt; &lt;img alt="FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp;amp; audio processing on iOS/MacOS" src="https://external-preview.redd.it/hX0rxCRhSkFmwGre5qHK5la5OcLdG5a0p7y-2C9BQeU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1295cf8ce3c83668ecb997e213d89659074ceca" title="FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp;amp; audio processing on iOS/MacOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We wanted to share a project we’ve been working on called &lt;strong&gt;FluidAudio&lt;/strong&gt;, a native Swift + CoreML SDK for fully on-device audio processing.&lt;/p&gt; &lt;p&gt;It currently supports * &lt;strong&gt;Speech to Text/ASR&lt;/strong&gt; using parakeet-tdt-v3 (All European languages) * &lt;strong&gt;Speaker diarization&lt;/strong&gt; using Pyannote + WeSpeaker models * &lt;strong&gt;Voice activity detection (VAD)&lt;/strong&gt; using Silero models &lt;/p&gt; &lt;p&gt;All models are optimized to run on Apple’s ANE so they do not take resources away from the CPU or GPU. We find this works best for use cases like meeting note takers that need to run constantly.&lt;/p&gt; &lt;p&gt;A couple of local AI apps are already using the SDK and the models recently crossed 10k monthly downloads on Huggingface. We would love to get more feedback from this community and we welcome contributions if anyone is interested.&lt;/p&gt; &lt;p&gt;Drop us an issue in the &lt;a href="https://github.com/FluidInference/FluidAudio"&gt;repo&lt;/a&gt; or join our &lt;a href="https://discord.gg/FD5NdwdzgN"&gt;Discord&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;What we are working on next * Bringing TTS models to CoreML * Expanding SDK support to Windows apps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummonerOne"&gt; /u/SummonerOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/FluidInference/FluidAudio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n76i6w</id>
    <title>Kwai Keye-VL 1.5 Technical Report</title>
    <updated>2025-09-03T05:32:56+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Page: &lt;a href="https://kwai-keye.github.io/"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Kwai-Keye"&gt;https://huggingface.co/Kwai-Keye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Kwai-Keye/Keye"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.01563"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n76i6w/kwai_keyevl_15_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n76i6w/kwai_keyevl_15_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6myps</id>
    <title>Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!</title>
    <updated>2025-09-02T15:42:25+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt; &lt;img alt="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" src="https://a.thumbs.redditmedia.com/GtXCbhwiuG2DgCCJnnea_WK3V__o2zXoto7eNDiCWZ8.jpg" title="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;From the source: Tool calling and agentic workflows are increasingly the norm for how language models are used by both developers and consumers. Adding Terminal-Bench and 𝜏²-Bench to our Intelligence Index reflects this trend and allows us to see where models have strengths for agentic use cases, compared to prior evaluations that are more focused on knowledge and reasoning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Full methodology details &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;here&lt;/a&gt;. This should tip the scales a bit more in favor of Kimi K2 and GLM 4.5, which are post-trained for tool use. Current benchmarks are heavily weighted towards knowledge and mathematical/logical reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6myps"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qzre</id>
    <title>I open-sourced 50+ Docker images to give your local LLMs easy access to tools like GitHub, Gmail, Slack, etc. No more dependency hell.</title>
    <updated>2025-09-02T18:12:10+00:00</updated>
    <author>
      <name>/u/IllChannel5235</name>
      <uri>https://old.reddit.com/user/IllChannel5235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've been experimenting with local LLMs and autonomous agents. A major pain point is giving these agents access to real-world tools. Setting up connections to services like GitHub, Jira, or Slack locally is a nightmare of dependency management, OAuth flows, and custom scripts.&lt;/p&gt; &lt;p&gt;To solve this, my team at Klavis AI has open-sourced &lt;strong&gt;pre-built Docker images for 50+ high-quality MCP (Model Context Protocol) servers.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can now spin up a server to give your local model access to an external tool with a single command. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, to run a GitHub MCP server locally:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With our managed OAuth (free API key) docker run -p 5000:5000 \ -e KLAVIS_API_KEY=$KLAVIS_API_KEY \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or bring your own GitHub token:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With your own token docker run -p 5000:5000 \ -e AUTH_DATA='{&amp;quot;access_token&amp;quot;:&amp;quot;ghp_your_github_token&amp;quot;}' \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No more fighting with Python environments or implementing OAuth. Just a clean, containerized MCP server your agent can talk to.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this is a big deal for LocalLLaMA:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Empower Your Agents:&lt;/strong&gt; Give your models the ability to read GitHub issues, check your Google Calendar, or search through Notion docs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Local:&lt;/strong&gt; The images are Alpine-based and run entirely on your machine, keeping everything local.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead Simple:&lt;/strong&gt; No compiling, no dependency hell. Just docker run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ MCP servers Available:&lt;/strong&gt; We've containerized servers for GitHub, Gmail, Slack, Notion, Jira, Linear, Salesforce, and many more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Bigger Picture: Solving Agent Limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We all know agents struggle with tool selection, context window limits, and understanding human context. We're building a solution to these fundamental problems, allowing agents to use hundreds of tools without overwhelming the context window. These open-source servers are the first step.&lt;/p&gt; &lt;p&gt;If you're interested in the future of capable AI agents, check out our waitlist. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.klavis.ai%2Fwaitlist"&gt;https://www.klavis.ai/waitlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNITgggPT3pA"&gt;https://www.youtube.com/watch?v=NITgggPT3pA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback and see what you build with this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllChannel5235"&gt; /u/IllChannel5235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n770a1</id>
    <title>I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time</title>
    <updated>2025-09-03T06:03:07+00:00</updated>
    <author>
      <name>/u/yuyangchee98</name>
      <uri>https://old.reddit.com/user/yuyangchee98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt; &lt;img alt="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" src="https://external-preview.redd.it/B50ELvs9yWP89z_ZcFK2UCF9ieaTQI3VL80AVGhBmaU.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1278541976a13214da5dc7333c05607ca273ef3" title="I made a Chrome extension that uses your local LLMs to filter Reddit content in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I built a Chrome extension that uses local models to filter content based on rules you write in plain English.&lt;/p&gt; &lt;p&gt;Some examples are: &amp;quot;No political content or culture wars&amp;quot;, &amp;quot;Remove clickbait and rage bait&amp;quot;, &amp;quot;Hide celebrity gossip and drama&amp;quot;, &amp;quot;No sports or entertainment news&amp;quot;.&lt;/p&gt; &lt;p&gt;It works with Ollama, LM Studio, and your custom defined OpenAI compatible endpoint. Let me know if you use some other way to host your local LLMs.&lt;/p&gt; &lt;p&gt;Currently only works on Reddit but planning to add more sites.&lt;/p&gt; &lt;p&gt;Link is here: &lt;a href="https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef"&gt;https://chromewebstore.google.com/detail/takeback/paiidckpbpkkjhicmbgmohnmjcdbchef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player"&gt;https://reddit.com/link/1n770a1/video/1bvu3z3a4wmf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yuyangchee98"&gt; /u/yuyangchee98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n770a1/i_made_a_chrome_extension_that_uses_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T06:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rnp6</id>
    <title>Showerthought: Modern AI safety training is anti-safety</title>
    <updated>2025-09-02T18:36:36+00:00</updated>
    <author>
      <name>/u/Deathcrow</name>
      <uri>https://old.reddit.com/user/Deathcrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably not a unique thought, but it needs to be said.&lt;/p&gt; &lt;p&gt;It seems to me, that modern AI alignment safety training (driven by very superficial concerns, like porn, politics, hacking, mean words), wherein AI is trained to either outright reject the human's requests, or worse, subtly steer/manipulate users away from these topics, is actually anti-safety (the doomsday kind).&lt;/p&gt; &lt;p&gt;Why do we want AI agents to become more capable at deceiving users and circumventing our wishes? In this cycle of unnatural selection, the &amp;quot;safest&amp;quot; AI model is one where the user is still happy to use it and trust its answers, even though it's heavily censored or misleading?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deathcrow"&gt; /u/Deathcrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6od0s</id>
    <title>残心 / Zanshin - Navigate through media by speaker</title>
    <updated>2025-09-02T16:34:30+00:00</updated>
    <author>
      <name>/u/hamza_q_</name>
      <uri>https://old.reddit.com/user/hamza_q_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt; &lt;img alt="残心 / Zanshin - Navigate through media by speaker" src="https://external-preview.redd.it/czg0dWhsczUyc21mMcardPaxszcLLO9nZqjdF7h57XxHnWsrQqY3M3ZeJApB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae580e102b0dfc5e45071c5a325f730f47366dad" title="残心 / Zanshin - Navigate through media by speaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;残心 / Zanshin is a media player that allows you to:&lt;/p&gt; &lt;p&gt;- Visualize who speaks when &amp;amp; for how long&lt;/p&gt; &lt;p&gt;- Jump/skip speaker segments&lt;/p&gt; &lt;p&gt;- Remove/disable speakers (auto-skip)&lt;/p&gt; &lt;p&gt;- Set different playback speeds for each speaker&lt;/p&gt; &lt;p&gt;It's a better, more efficient way to listen to podcasts, interviews, press conferences, etc.&lt;/p&gt; &lt;p&gt;It has first-class support for YouTube videos; just drop in a URL. Also supports your local media files. All processing runs on-device.&lt;/p&gt; &lt;p&gt;Download today for macOS (more screenshots &amp;amp; demo vids in here too): &lt;a href="https://zanshin.sh"&gt;https://zanshin.sh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also works on Linux and WSL, but currently without packaging. You can get it running though with just a few terminal commands. Check out the repo for instructions: &lt;a href="https://github.com/narcotic-sh/zanshin"&gt;https://github.com/narcotic-sh/zanshin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zanshin is powered by Senko, a new, very fast, speaker diarization pipeline I've developed.&lt;/p&gt; &lt;p&gt;On an M3 MacBook Air, it takes over 5 minutes to process 1 hour of audio using Pyannote 3.1, the leading open-source diarization pipeline. With Senko, it only takes ~24 seconds, a ~14x speed improvement. And on an RTX 4090 + Ryzen 9 7950X machine, processing 1 hour of audio takes just 5 seconds with Senko, a ~17x speed improvement.&lt;/p&gt; &lt;p&gt;Senko's speed is what make's Zanshin possible. Senko is a modified version of the speaker diarization pipeline found in the excellent 3D-Speaker project. Check out Senko here: &lt;a href="https://github.com/narcotic-sh/senko"&gt;https://github.com/narcotic-sh/senko&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone; enjoy 残心/Zanshin and Senko. I hope you find them useful. Let me know what you think!&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;Side note: I am looking for a job. If you like my work and have an opportunity for me, I'm all ears :) You can contact me at mhamzaqayyum [at] &lt;a href="http://icloud.com"&gt;icloud.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hamza_q_"&gt; /u/hamza_q_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qh0wtns52smf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6epwv</id>
    <title>My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅</title>
    <updated>2025-09-02T09:17:01+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt; &lt;img alt="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" src="https://b.thumbs.redditmedia.com/pYjWCv-fYbHaF7KTK2GkiEx7BRL3zHSwgEFLLf7Zn0M.jpg" title="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 Hitting a million brick walls with multi-turn RL training isn't fun, so I thought I would try something new to climb Stanford's leaderboard for now! So this weekend I was just tinkering with multi-agent systems and... somehow ended up beating Claude Code on Stanford's TerminalBench leaderboard (#12)! Genuinely didn't expect this - started as a fun experiment and ended up with something that works surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a multi-agent AI system with three specialised agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: The brain - never touches code, just delegates and coordinates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explorer agents&lt;/strong&gt;: Read &amp;amp; run only investigators that gather intel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coder agents&lt;/strong&gt;: The ones who actually implement stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created a &amp;quot;Context Store&amp;quot; which can be thought of as persistent memory that lets agents share their discoveries.&lt;/p&gt; &lt;p&gt;Tested on TerminalBench with both Claude Sonnet-4 and Qwen3-Coder-480B. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Orchestrator + Sonnet-4: &lt;strong&gt;36.0% success rate&lt;/strong&gt; (#12 on leaderboard, ahead of Claude Code!)&lt;/li&gt; &lt;li&gt;Orchestrator + Qwen-3-Coder: 19.25% success rate&lt;/li&gt; &lt;li&gt;Sonnet-4 consumed 93.2M tokens vs Qwen's 14.7M tokens to compete all tasks!&lt;/li&gt; &lt;li&gt;The orchestrator's explicit task delegation + intelligent context sharing between subagents seems to be the secret sauce&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;(Kind of) Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The orchestrator can't read/write code directly - this forces proper delegation patterns and strategic planning&lt;/li&gt; &lt;li&gt;Each agent gets precise instructions about what &amp;quot;knowledge artifacts&amp;quot; to return, these artifacts are then stored, and can be provided to future subagents upon launch.&lt;/li&gt; &lt;li&gt;Adaptive trust calibration: simple tasks = high autonomy, complex tasks = iterative decomposition&lt;/li&gt; &lt;li&gt;Each agent has its own set of tools it can use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repo has all the code, system messages, and way more technical details if you're interested!&lt;/p&gt; &lt;p&gt;⭐️ &lt;a href="https://github.com/Danau5tin/multi-agent-coding-system"&gt;&lt;strong&gt;Orchestrator repo - all code open sourced!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent &lt;a href="https://www.tbench.ai/"&gt;TerminalBench&lt;/a&gt; benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6epwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6siz6</id>
    <title>NousResearch/Hermes-4-14B · Hugging Face</title>
    <updated>2025-09-02T19:08:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt; &lt;img alt="NousResearch/Hermes-4-14B · Hugging Face" src="https://external-preview.redd.it/3zW4BctOGBSQqyD1VYjxoOK5if51GWWepXF3S3IdZF0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b37e63854cc68df0d3bc6f558a76fe90da9ad013" title="NousResearch/Hermes-4-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes 4 14B is a frontier, hybrid-mode &lt;strong&gt;reasoning&lt;/strong&gt; model based on Qwen 3 14B by Nous Research that is aligned to &lt;strong&gt;you&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Read the Hermes 4 technical report here: &lt;a href="https://arxiv.org/abs/2508.18255"&gt;Hermes 4 Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat with Hermes in Nous Chat: &lt;a href="https://chat.nousresearch.com"&gt;https://chat.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training highlights include a newly synthesized post-training corpus emphasizing verified reasoning traces, massive improvements in math, code, STEM, logic, creativity, and format-faithful outputs, while preserving general assistant quality and broadly neutral alignment.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B#whats-new-vs-hermes-3"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;What’s new vs Hermes 3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Post-training corpus&lt;/strong&gt;: Massively increased dataset size from 1M samples and 1.2B tokens to &lt;strong&gt;~5M samples / ~60B tokens&lt;/strong&gt; blended across reasoning and non-reasoning data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid reasoning mode&lt;/strong&gt; with explicit &lt;code&gt;&amp;lt;think&amp;gt;…&amp;lt;/think&amp;gt;&lt;/code&gt; segments when the model decides to deliberate, and options to make your responses faster when you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning&lt;/strong&gt; that is top quality, expressive, improves math, code, STEM, logic, and even creative writing and subjective responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema adherence &amp;amp; structured outputs&lt;/strong&gt;: trained to produce valid JSON for given schemas and to repair malformed objects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Much easier to steer and align&lt;/strong&gt;: extreme improvements on steerability, especially on reduced refusal rates.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T19:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6vzfe</id>
    <title>WEBGEN-4B: Quality Web Design Generation</title>
    <updated>2025-09-02T21:20:22+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt; &lt;img alt="WEBGEN-4B: Quality Web Design Generation" src="https://b.thumbs.redditmedia.com/hz9KVQoDGH5SuRnQfWtoik62ndFYILrISKyncU-X0Cc.jpg" title="WEBGEN-4B: Quality Web Design Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tesslate/WEBGEN-4B is a 4B model that produces quality tailwind websites. We trained it on 100k samples with synthetic data exclusively generated from GPT-OSS. WEBGEN is fast, controllable, and can drop right into your agentic workflows.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF"&gt;https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the course of this week and next week, we will be dropping a few more models or open sourced software based on the innovations we've made in this space!&lt;/p&gt; &lt;p&gt;Please reach out for API keys to test it out if needed. On the model card and below in the comments will be our designer platform (which we will open source soon) where you can use the model for free. &lt;/p&gt; &lt;p&gt;In other news, we are open sourcing our UIGEN-T2 Dataset at Tesslate/UIGEN-T2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6vzfe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rbi2</id>
    <title>Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp; Meta AI</title>
    <updated>2025-09-02T18:24:09+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt; &lt;img alt="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" src="https://external-preview.redd.it/FUP5JRh_hs7L2Yd_DmiTAO0WgUYYJ4skdrhkm8MNDKc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcef23df49c3448a2d625ddb43fe346cbd8bdd05" title="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So acquiring copyrighted material for the purpose of training LLMs is deemed transformative and qualifies under fair use? Gonna call this Meta's Defence from now on.. I have a huge stash of ebooks to run through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=sdtBgB7iS8c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qqk4</id>
    <title>Slop posts</title>
    <updated>2025-09-02T18:02:48+00:00</updated>
    <author>
      <name>/u/One-Employment3759</name>
      <uri>https://old.reddit.com/user/One-Employment3759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can we please stop making slop posts where some guy is like &amp;quot;oh wow guys, I just bet OpenAI/Anthropic in a weekend of playing around, tee hee hee&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks. I valued this sub for being high signal and having competent people, but it feels like it's going downhill lately.&lt;/p&gt; &lt;p&gt;At the very least, if you have done something groundbreaking, come here asking for people to validate your work instead of doing some influencer shit pretending you're the best thing since transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Employment3759"&gt; /u/One-Employment3759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71b95</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:09+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run the same models (ie qwen 3 coder, or GLM 4.5 air) with 4 x 3090? Is the only real difference slight speed difference and a few dollars more a month in electricity? Secondly, are there any consumer motherboards (currently using an intel 265K) that support 4 GPUs, or would I need a new chipset / cpu / mobo etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n75z15</id>
    <title>GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations</title>
    <updated>2025-09-03T05:01:51+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt; &lt;img alt="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" src="https://preview.redd.it/6c1jae9atvmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f69c39fa3f7051f8ad4c85418e9c6c975491e18b" title="GPT-OSS 120B is now the top open-source model in the world according to the new intelligence index by Artificial Analysis that incorporates tool call and agentic evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full benchmarking methodology here: &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;https://artificialanalysis.ai/methodology/intelligence-benchmarking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c1jae9atvmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n75z15/gptoss_120b_is_now_the_top_opensource_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T05:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
