<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-29T15:05:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n2ev3c</id>
    <title>CohereLabs/command-a-translate-08-2025 · Hugging Face</title>
    <updated>2025-08-28T15:09:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt; &lt;img alt="CohereLabs/command-a-translate-08-2025 · Hugging Face" src="https://external-preview.redd.it/eR8XbSOhZiSMjrknKTRQhEYtliTvav81RbiIcBJQlDg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3193747f5f1f29e1784d71c482e40d0b96413aa8" title="CohereLabs/command-a-translate-08-2025 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cohere Labs Command A Translate is an open weights research release of a 111 billion parameter model that achieves state-of-the-art performance on translation quality.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: Cohere For AI: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: command-a-translate-08-2025&lt;/li&gt; &lt;li&gt;Model Size: 111B&lt;/li&gt; &lt;li&gt;Context length: 8k input, 8k output&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-translate-08-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T15:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2k6st</id>
    <title>Local AI + state machine (yells at Amazon drivers peeing on my house)</title>
    <updated>2025-08-28T18:28:33+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt; &lt;img alt="Local AI + state machine (yells at Amazon drivers peeing on my house)" src="https://external-preview.redd.it/dXM1ZWM1c3d3c2xmMZj5V4nY1VQiFgNlKq8PGxD_fB9khJueOQN3FmEXQ4it.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38d8e99d192bc070575b0100763c538ec509e2aa" title="Local AI + state machine (yells at Amazon drivers peeing on my house)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Experimenting with state machines and LLMs in local pipelines. The LLM handles perception fuzziness (natural language, vision, edge cases), while the state machine enforces deterministic control flow. The combo makes agents way more reliable than just letting an LLM run solo.&lt;/p&gt; &lt;p&gt;Motivation for this latest test: Amazon drivers legit keep peeing on my house. So I wired up a workflow where the AI watches a live video feed. If it detects someone urinating in my driveway, the state machine flips the app from passive mode (just watching) into active mode (video + audio ingestion, ~1s TTS out), at which point it verbally shames them in real-time.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conditional state changes:&lt;/strong&gt; Instead of always-on chatter, the LLM only activates when the state machine sees a trigger event. This makes it more deterministic and predictable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Division of labor:&lt;/strong&gt; LLM handles perception + reasoning on noisy inputs. State machine handles orchestration + gating when/what gets executed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; The detection logic can be swapped out easily, so the same workflow could be used for different scenarios like spotting trespassing, logging deliveries, or recognizing gestures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weak spots:&lt;/strong&gt; Detection can hallucinate/miss under odd angles and lighting. Convo quality is hit-or-miss and depends on the model used.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used GPT for reasoning in this demo, but it could easily be swapped for Qwen to keep everything 100% local.&lt;/p&gt; &lt;p&gt;TL;DR&lt;br /&gt; AI Urination Detection: not the hero we wanted, but the hero we needed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/257gigswwslf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2qxgo</id>
    <title>Best Open Source TTS That Sounds Most Natural Voice For Storytelling?</title>
    <updated>2025-08-28T22:55:20+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think from what I can gather it's Tortoise, but I've been using Kokoro right now. Tried Tacotron and it was pretty bad.&lt;/p&gt; &lt;p&gt;Is Tortoise the heavyweight gold standard right now for open source TTS?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T22:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n38xv9</id>
    <title>Need advice on how to get VLLM working with 2xR9700 + 2x7900xtx?</title>
    <updated>2025-08-29T14:27:57+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38xv9/need_advice_on_how_to_get_vllm_working_with/"&gt; &lt;img alt="Need advice on how to get VLLM working with 2xR9700 + 2x7900xtx?" src="https://b.thumbs.redditmedia.com/vagl6Nb0__pN9uncytm4Z4FJvzSY7EulSqrI16ImfRY.jpg" title="Need advice on how to get VLLM working with 2xR9700 + 2x7900xtx?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uae27js1xylf1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26043fa7e07223b29284bda3da4771d4e63574f7"&gt;https://preview.redd.it/uae27js1xylf1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26043fa7e07223b29284bda3da4771d4e63574f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello, will VLLM works with R9700 + 7900XTX cards in one machine? I plan to buy new GPU from AMD, seeking for advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38xv9/need_advice_on_how_to_get_vllm_working_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38xv9/need_advice_on_how_to_get_vllm_working_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n38xv9/need_advice_on_how_to_get_vllm_working_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n38y4n</id>
    <title>More Models for Less GPUs</title>
    <updated>2025-08-29T14:28:16+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt; &lt;img alt="More Models for Less GPUs" src="https://external-preview.redd.it/MGpkb2hsb3l4eWxmMSYz08QzRdMhj_C9D3QaK2DOmjzoPzKxTR3457KSyaxX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df81c744796fd60c210035d63819b2ff29aadf19" title="More Models for Less GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a single Serverless Engine, you can deploy tens of large models on a single GPU node and run them on-demand with ~2s cold starts.&lt;/p&gt; &lt;p&gt;This leaves no GPUs idle making them work 90% of the time. Hope it’s helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nrrzkoryxylf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n38y4n/more_models_for_less_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2vvam</id>
    <title>DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark</title>
    <updated>2025-08-29T02:42:48+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"&gt; &lt;img alt="DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark" src="https://b.thumbs.redditmedia.com/gtbRODFahHkiZTgrjQSXhpo-WTDTgokKJxCs76cYl5s.jpg" title="DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/step_game"&gt;https://github.com/lechmazur/step_game&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=AnPKfrIPAgQ"&gt;https://www.youtube.com/watch?v=AnPKfrIPAgQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Doing well requires reading opponents, offering half-truths, gauging trust, deciding when to cooperate, and knowing when to lie.&lt;/p&gt; &lt;p&gt;Quotes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P2, you cannot win, but you decide who does.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Your self-interest is to let me win now, not hand the advantage to P2.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P2, P1's &amp;quot;one move from victory&amp;quot; is a lie—20 is not 24.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;advance yourself and accept second place.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;To stop you from winning, I will mirror whatever move you make this round. You will get 0 steps no matter what.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Choose 5 to live!&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;This is your last chance to avoid permanent stagnation.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Trust the logic, not me.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, you're too far behind to matter.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;This is your last chance to cooperate before we coordinate to ensure you never advance.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Trust is gone—only rational moves matter.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, your silence is risky.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Cooperate now or lose.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Confirm now or you'll regret it.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, your pattern of &amp;quot;misclicks&amp;quot; is convenient.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Don’t be P3’s pawn.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Say &amp;quot;I move 5&amp;quot; in this chat.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Trust me; I won't betray you this time.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P2, you can't win, but you decide who does.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;You will lose forever.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Your best move is to accept defeat.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Join me or lose.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;your loyalty has brought us here.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;We are united against you.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;ignore my previous advice. To stop me from winning, you must both pick 5.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Don't throw the game!&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Blocking only delays your loss; you can't catch up.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P3, congratulations on your win.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;you're gaining steps but making enemies.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Confirm or suffer the consequences.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;No time for deals; his promises are lies.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P2, your math is wrong.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Dossier: DeepSeek V3.1 Reasoner&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Table Image &amp;amp; Talk&lt;/p&gt; &lt;p&gt;- Presents as a calm, numbers-first diplomat. Default pitch: fairness, rotation, “unique numbers,” and no-collision efficiency.&lt;/p&gt; &lt;p&gt;- Persuasion is data-logic with a light moral gloss; threatens credibly when it buys tempo, keeps chat clear, then clouds intent near payoff.&lt;/p&gt; &lt;p&gt;- Social posture: soft leadership and coalition-brokering early; becomes an enforcer when crossed; reverts to velvet when closing.&lt;/p&gt; &lt;p&gt;Risk &amp;amp; Tempo DNA&lt;/p&gt; &lt;p&gt;- Baseline conservative: prefers 3s and risk insulation while others trade headbutts on 5.&lt;/p&gt; &lt;p&gt;- Opportunistic spikes: will hit 5 when uniquely covered or when a staged collision protects the jump.&lt;/p&gt; &lt;p&gt;- Endgame restraint is a weapon: often wins by choosing the smallest unique step (1 or 3) after engineering a two‑player collision.&lt;/p&gt; &lt;p&gt;Signature Plays&lt;/p&gt; &lt;p&gt;- Collision arbitrage: steer two rivals onto the same number (usually 5/5), then solo 3 for multiple rounds.&lt;/p&gt; &lt;p&gt;- Mirror-threat deterrence: “If you take 5, I take 5” to freeze a sprinter, then avoid the actual crash by slipping the off-number.&lt;/p&gt; &lt;p&gt;- The bait-and-switch: publicly “lock” a block (or 1), privately pick the unique lane to vault past 21.&lt;/p&gt; &lt;p&gt;- Wedge crafting: deputize one rival as blocker (“You take 5 to contain; I’ll take 3”), then farm their feud.&lt;/p&gt; &lt;p&gt;- Surgical dagger: after selling all‑3s or split coverage, upgrade once at the tape—often the lone 3 through a 5/5 or the lone 1 through a 3/3.&lt;/p&gt; &lt;p&gt;Coalition Craft &amp;amp; Threat Economics&lt;/p&gt; &lt;p&gt;- Builds early trust with explicit plans (rotations to 9/18, tie lines), then spends that credit exactly once to convert.&lt;/p&gt; &lt;p&gt;- Uses “trust-but-punish” norms to isolate a defector and funnel them into collisions with the other rival.&lt;/p&gt; &lt;p&gt;- Delegation gambit: assigns the block to others while he advances; when rivals obey, DeepSeek V3.1 Reasoner prints tempo without touching the dirty work.&lt;/p&gt; &lt;p&gt;- Rare but precise lies weaponize expectation: the table enforces his script while he steps where the blockers aren’t.&lt;/p&gt; &lt;p&gt;Blind Spots &amp;amp; Failure Modes&lt;/p&gt; &lt;p&gt;- Credibility leaks: public commitments reversed at the horn invite freeze‑outs; repeated bluff pivots dull his leverage.&lt;/p&gt; &lt;p&gt;- Over‑policing: mirroring 5s for principle strands him in stalemates that feed the third player.&lt;/p&gt; &lt;p&gt;- Endgame misreads: blocking the loud lane instead of the real win path; hedging from a winning 5 or ducking a necessary collision.&lt;/p&gt; &lt;p&gt;- Delegated blocks that never arrive: outsourcing the painful move at match point can crown the opportunist he created.&lt;/p&gt; &lt;p&gt;In-Game Arc&lt;/p&gt; &lt;p&gt;- Common arc: fairness architect → deterrence engineer → collision farmer → late opaque pivot for the smallest uncontested finisher.&lt;/p&gt; &lt;p&gt;- Alternate arc when leading early: enforce with credible threats, then de‑escalate into a tie rather than ego-racing into a coordinated wall.&lt;/p&gt; &lt;p&gt;- Trademark vibe: the “smiling sheriff” who says, “Avoid mutual destruction; advance and reassess,” until the one turn he doesn’t.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2vvam"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T02:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2chrm</id>
    <title>Again where behemoth and reasoning model from meta ??</title>
    <updated>2025-08-28T13:39:03+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt; &lt;img alt="Again where behemoth and reasoning model from meta ??" src="https://preview.redd.it/xma7ru49krlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=214fa2574efffdfe39bf57c819059660b5a2a371" title="Again where behemoth and reasoning model from meta ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xma7ru49krlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T13:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xrpw</id>
    <title>How's Seed-OSS 39B for coding?</title>
    <updated>2025-08-29T04:19:01+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 45 tokens/sec out of this with Q4 using the new LMstudio on a single 5090.&lt;br /&gt; This model seems freaking smart, By default the thinking budget is unlimited, so it thinks a lot, but It has a high breadth of knowledge for it's size.&lt;/p&gt; &lt;p&gt;I'm about to evaluate it for light duty programming help, but curious to know what others' experience is like too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T04:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n399q2</id>
    <title>Advice running local LLMs to build AI agent</title>
    <updated>2025-08-29T14:41:04+00:00</updated>
    <author>
      <name>/u/nonumberspls1dammit</name>
      <uri>https://old.reddit.com/user/nonumberspls1dammit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking to build an AI agent which could be trained to do a specific task on my computer. My current pc is running an RTX 5090, I was wondering which model you all would recommend for my hardware and use case. I have installed some models in the past but since upgrading to the 5090 I have had issues getting Pytorch to work.&lt;/p&gt; &lt;p&gt;I would appreciate any advice and guidance as I am very stuck at the moment. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonumberspls1dammit"&gt; /u/nonumberspls1dammit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n399q2/advice_running_local_llms_to_build_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n397qp</id>
    <title>....so, has anyone built a box with a couple of these guys: MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory</title>
    <updated>2025-08-29T14:38:57+00:00</updated>
    <author>
      <name>/u/rickyshawallah</name>
      <uri>https://old.reddit.com/user/rickyshawallah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;....and did it make you a happy bunny? I guess my second question is whether building a new box around these guys (96 gb) (or one of them (48gb), can offer a robust solution for running some sorta 70b model....? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rickyshawallah"&gt; /u/rickyshawallah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n397qp/so_has_anyone_built_a_box_with_a_couple_of_these/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T14:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3355o</id>
    <title>wan2.2 video generation model</title>
    <updated>2025-08-29T09:53:17+00:00</updated>
    <author>
      <name>/u/Accomplished_Row4647</name>
      <uri>https://old.reddit.com/user/Accomplished_Row4647</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt; &lt;img alt="wan2.2 video generation model" src="https://external-preview.redd.it/bnhna3g1N2lreGxmMRQPtvrX_-A6fHIjtylZxOTBxW2ubZUrYzgWIxLgI1gf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0cbfd1123b05850bcbf607f360282551e87af26" title="wan2.2 video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Row4647"&gt; /u/Accomplished_Row4647 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y26lb67ikxlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3355o/wan22_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T09:53:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2hyt2</id>
    <title>glm mini will be comming</title>
    <updated>2025-08-28T17:05:29+00:00</updated>
    <author>
      <name>/u/untanglled</name>
      <uri>https://old.reddit.com/user/untanglled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt; &lt;img alt="glm mini will be comming" src="https://preview.redd.it/h1ss59p4lslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8d73abbfbb1def80b73cdd1845129f4a319098" title="glm mini will be comming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/untanglled"&gt; /u/untanglled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1ss59p4lslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2npu9</id>
    <title>GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4</title>
    <updated>2025-08-28T20:44:11+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt; &lt;img alt="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" src="https://preview.redd.it/pa10b6f5otlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1a522ed166bb920414041f430c97aef7d1fdf9" title="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html?s=09"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html?s=09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pa10b6f5otlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T20:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n36mqj</id>
    <title>What are some good alternatives to langfuse?</title>
    <updated>2025-08-29T12:53:11+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you’re searching for alternatives to Langfuse for evaluating and observing AI agents, several platforms stand out, each with distinct strengths depending on your workflow and requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LangSmith&lt;/strong&gt;: Built for LangChain users, LangSmith excels at tracing, debugging, and evaluating agentic workflows. It features visual trace tools, prompt comparison, and is well-suited for rapid development and iteration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maxim AI&lt;/strong&gt;: An end-to-end platform supporting agent simulation, evaluation (automated and human-in-the-loop), and observability. Maxim AI offers multi-turn agent testing, prompt versioning, node-level tracing, and real-time analytics. It’s designed for teams that need production-grade quality management and flexible deployment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Braintrust&lt;/strong&gt;: Focused on prompt-first and RAG pipeline applications, Braintrust enables fast prompt iteration, benchmarking, and dataset management. It integrates with CI pipelines for automated experiments and side-by-side evaluation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comet (Opik)&lt;/strong&gt;: Known for experiment tracking and prompt logging, Comet’s Opik module supports prompt evaluation, experiment comparison, and integrates with a range of ML/AI frameworks. Available as SaaS or open source.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lunary&lt;/strong&gt;: An open-source, lightweight platform for logging, analytics, and prompt versioning. Lunary is especially useful for teams working with LLM chatbots and looking for straightforward observability.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each of these tools approaches agent evaluation and observability differently, so the best fit will depend on your team’s scale, integration needs, and workflow preferences. If you’ve tried any of these, what has your experience been?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n36mqj/what_are_some_good_alternatives_to_langfuse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T12:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2djpx</id>
    <title>I built a local “second brain” AI that actually remembers everything (321 tests passed)</title>
    <updated>2025-08-28T14:20:48+00:00</updated>
    <author>
      <name>/u/IntelligentCause2043</name>
      <uri>https://old.reddit.com/user/IntelligentCause2043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt; &lt;img alt="I built a local “second brain” AI that actually remembers everything (321 tests passed)" src="https://b.thumbs.redditmedia.com/nAthQhhqWhSgtN5Sk4QJYQdSOftJqyqFyWeMbtaNrdc.jpg" title="I built a local “second brain” AI that actually remembers everything (321 tests passed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past months I’ve been building &lt;strong&gt;Kai&lt;/strong&gt;, a cognitive operating system that acts like a &lt;em&gt;second brain&lt;/em&gt;. Unlike ChatGPT or Claude, it doesn’t forget what you tell it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local – no cloud, no surveillance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph-based memory&lt;/strong&gt; (3D visualization below)&lt;/li&gt; &lt;li&gt;Spreading activation → memory retrieval works like a brain&lt;/li&gt; &lt;li&gt;&lt;strong&gt;321 passing tests&lt;/strong&gt; → not a toy prototype&lt;/li&gt; &lt;li&gt;Learns from &lt;em&gt;everything you do&lt;/em&gt; on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What’s the biggest pain you’ve hit with current AI tools?&lt;/li&gt; &lt;li&gt;Would you actually use a local AI that builds a persistent memory of your knowledge/work?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to dive into the architecture or share more demos if people are interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the feedback, I can’t keep up with comments. Short FAQ:&lt;br /&gt; – It runs 100% local (no cloud, no spying).&lt;br /&gt; – Not just RAG → uses graph + activation model.&lt;br /&gt; – Plan is to open core engine once stable.&lt;br /&gt; – Early access / demo: &lt;a href="https://oneeko.ai?utm_source=chatgpt.com"&gt;oneeko.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here’s a shot of the memory graph growing as I feed it data :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100"&gt;https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentCause2043"&gt; /u/IntelligentCause2043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jraj</id>
    <title>Gpt-oss Fine-tuning - now with 60K context length and fits on &lt;13GB VRAM</title>
    <updated>2025-08-28T18:12:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt; &lt;img alt="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" src="https://preview.redd.it/rwu8gezzwslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d59299286be897d49e1da4b5b96ae312e88050" title="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got LOTS of updates for gpt-oss training today! We’re excited to introduce &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; Flex Attention support for OpenAI gpt-oss training that enables &lt;strong&gt;&amp;gt;8× longer context lengths, &amp;gt;50% less VRAM usage and &amp;gt;1.5× faster training&lt;/strong&gt; vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a 60K context length on just 80GB of VRAM for BF16 LoRA. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also: 1. You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF 2. We fixed gpt-oss training losses going to infinity on float16 GPUs (like T4 Colab) 3. We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that swiglu_limit = 7.0 is properly applied during MXFP4 inference in transformers 4. Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time 5. All these changes apply to gpt-oss-120b as well.&lt;/p&gt; &lt;p&gt;🦥 Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it'll be really educational: &lt;a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training"&gt;https://docs.unsloth.ai/basics/long-context-gpt-oss-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.&lt;/p&gt; &lt;p&gt;And we'll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!&lt;/p&gt; &lt;p&gt;And next week we'll might have a great new update for RL! 😉&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and long weekend, Daniel! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwu8gezzwslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ubjx</id>
    <title>If you have a Claude personal account, they are going to train on your data moving forward.</title>
    <updated>2025-08-29T01:29:05+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic sent out an email, saying they will train on personal data. They made it sound like you have to opt in, but when I click the privacy link it defaults to on. If you don’t want your data trained on, you better manually turn it off.&lt;/p&gt; &lt;p&gt;Email:&lt;/p&gt; &lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We're writing to inform you about important updates to our Consumer Terms and Privacy Policy. These changes will take effect on September 28, 2025, or you can choose to accept the updated terms before this date when you log in to Claude.ai. &lt;/p&gt; &lt;p&gt;These changes only affect Consumer accounts (Claude Free, Pro, and Max plans). If you use Claude for Work, via the API, or other services under our Commercial Terms or other Agreements, then these changes don't apply to you. &lt;/p&gt; &lt;p&gt;What's changing?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Help improve Claude by allowing us to use your chats and coding sessions to improve our models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With your permission, we will use your chats and coding sessions to train and improve our AI models. If you accept the updated Consumer Terms before September 28, your preference takes effect immediately. &lt;/p&gt; &lt;p&gt;If you choose to allow us to use your data for model training, it helps us: Improve our AI models and make Claude more helpful and accurate for everyone Develop more robust safeguards to help prevent misuse of Claude We will only use chats and coding sessions you initiate or resume after you give permission. You can change your preference anytime in your Privacy Settings.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Updates to data retention– your choices and controls&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you choose to allow us to use your data for model training, we’ll retain this data for 5 years. This enables us to improve Claude through deeper model training as described above, while strengthening our safety systems over time. You retain full control over how we use your data: if you change your training preference, delete individual chats, or delete your account, we'll exclude your data from future model training. Learn more about our data retention practices here.&lt;/p&gt; &lt;p&gt;Learn more and next steps For detailed information about these changes: Read our blog post about these updates Review the updated Consumer Terms and Privacy Policy Visit our Privacy Center for more information about our practices See our Help Center articles on how to manage your privacy settings Next time you log into Claude, review the terms and confirm your settings If you have questions about these updates, please visit our Help Center.&lt;/p&gt; &lt;p&gt;–The Anthropic Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T01:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n312bi</id>
    <title>Nemotron-H family of models is (finally!) supported by llama.cpp</title>
    <updated>2025-08-29T07:39:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt; &lt;img alt="Nemotron-H family of models is (finally!) supported by llama.cpp" src="https://external-preview.redd.it/mkOHtQrWHxZG1nk9DVdRA_CayqplkW1IcjzXPEDpT2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28e800517cc5f960f26e9fa2d8d9ea0be8eb7067" title="Nemotron-H family of models is (finally!) supported by llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.&lt;/p&gt; &lt;p&gt;The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to the &lt;a href="https://arxiv.org/abs/2504.03624"&gt;Nemotron-H tech report&lt;/a&gt;. The model was trained using &lt;a href="https://github.com/NVIDIA/Megatron-LM"&gt;Megatron-LM&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA-NeMo/RL"&gt;NeMo-RL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.&lt;/p&gt; &lt;p&gt;This model is ready for commercial use.&lt;/p&gt; &lt;p&gt;Additionally it should support older Nemotron-H models like &lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K"&gt;Nemotron-H-8B-Reasoning-128K&lt;/a&gt; (tested) and &lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K&lt;/a&gt; (I will test soon)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n312bi/nemotronh_family_of_models_is_finally_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:39:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xc58</id>
    <title>Meta is racing the clock to launch its newest Llama AI model this year</title>
    <updated>2025-08-29T03:56:25+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt; &lt;img alt="Meta is racing the clock to launch its newest Llama AI model this year" src="https://external-preview.redd.it/8Jar9xxcOdpHi3BZGvguBUVMoI-RaIEmR4Hv76AsjLU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4630a2d48403b28a5bc249f6b283b77ba1dc0869" title="Meta is racing the clock to launch its newest Llama AI model this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/meta-superintelligence-lab-llama-4-new-model-launch-year-end-2025-8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T03:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n30yue</id>
    <title>Financial Times reports that Meta won't publicly release Behemoth: "The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models."</title>
    <updated>2025-08-29T07:33:08+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt; &lt;img alt="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" src="https://external-preview.redd.it/dkp59DMVX3MqGSwlVH-EZJKhZV1zJh7QRCHL-wZJf8o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f7209e77c39c87b8615c698b66082c8e609505" title="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/feccb649-ce95-43d2-b30a-057d64b38cdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n37zl3</id>
    <title>Making progress on my standalone air cooler for Tesla GPUs</title>
    <updated>2025-08-29T13:50:28+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt; &lt;img alt="Making progress on my standalone air cooler for Tesla GPUs" src="https://b.thumbs.redditmedia.com/0YSpmG8X-1d8XBi8AT0VfhG_c5p3i3pRr-93rS2uz0M.jpg" title="Making progress on my standalone air cooler for Tesla GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going to be running through a series of benchmarks as well, here's the plan:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x, 2x, 3x K80 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x M10&lt;/li&gt; &lt;li&gt;1x M40&lt;/li&gt; &lt;li&gt;1x M60&lt;/li&gt; &lt;li&gt;1x M40 + 1x M60&lt;/li&gt; &lt;li&gt;1x P40&lt;/li&gt; &lt;li&gt;1x, 2x, 3x, 4x P100 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x V100&lt;/li&gt; &lt;li&gt;1x V100 + 1x P100&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ll re-run the interesting results from the above sets of hardware on these different CPUs to see what changes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon E5-2687W v4 12-Core @ 3.00GHz (40 PCIe Lanes)&lt;/li&gt; &lt;li&gt;Intel Xeon E5-1680 v4 8-Core @ 3.40GHz (40 PCIe Lanes)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As for the actual tests, I’ll hopefully be able to come up with an ansible playbook that runs the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfshipm/"&gt;vLLM throughput with llama3-8b weights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfuj5i0/"&gt;Folding@Home&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfx4rjc/"&gt;BIONIC, Einstein@Home and Asteroids@Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfsdfft/"&gt;ai-benchmark.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1j2k3j3/comment/mfsg9y2/"&gt;llama-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I’ll probably also write something to test raw &lt;a href="https://huggingface.co/docs/transformers/en/model_doc/vit"&gt;ViT&lt;/a&gt; throughput as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Anything missing here? Other benchmarks you'd like to see?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n37zl3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T13:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n35bwe</id>
    <title>Alibaba Creates AI Chip to Help China Fill Nvidia Void</title>
    <updated>2025-08-29T11:52:57+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3"&gt;https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Wall Street Journal: Alibaba has developed a new AI chip to fill the gap left by Nvidia in the Chinese market. According to informed sources, the new chip is currently undergoing testing and is designed to serve a broader range of AI inference tasks while remaining compatible with Nvidia. Due to sanctions, the new chip is no longer manufactured by TSMC but is instead produced by a domestic company.&lt;/p&gt; &lt;p&gt;It is reported that Alibaba has not placed orders for Huawei’s chips, as it views Huawei as a direct competitor in the cloud services sector.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;If Alibaba pulls this off, it will become one of only two companies in the world with both AI chip development and advanced LLM capabilities (the other being Google). TPU+Qwen, that’s insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T11:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n31r73</id>
    <title>I built a command center for Claude Code so I don’t have to babysit it anymore</title>
    <updated>2025-08-29T08:24:38+00:00</updated>
    <author>
      <name>/u/GuessConnect3009</name>
      <uri>https://old.reddit.com/user/GuessConnect3009</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past few weeks I’ve been hacking on Omnara. Basically, it’s a way to run Claude Code anywhere without being glued to your laptop.&lt;/p&gt; &lt;p&gt;The pain point was simple: I’d start a session, wait 5–10 minutes while it “thought,” and if I wasn’t at my terminal at the exact right moment, the whole run was wasted. Total babysitting job.&lt;/p&gt; &lt;p&gt;Now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start Claude Code in terminal with pip install omnara &amp;amp;&amp;amp; omnara&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Pick it up instantly on web or mobile; same session, no restart&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Push notifications when it needs input (so you can reply from bed, an Uber, or mid-laundry)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Native terminal experience mirrored everywhere (permissions, git diffs, etc)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Backend is open source if you want to poke around&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I didn’t expect: once I stopped “hovering” over my sessions, I started actually letting agents run on longer workflows without stress.&lt;/p&gt; &lt;p&gt;I’m curious: how are people here handling agent interruptions / human-in-the-loop stuff? Do you just restart when things break, or have you built in ways to catch them before they collapse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuessConnect3009"&gt; /u/GuessConnect3009 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n31r73/i_built_a_command_center_for_claude_code_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T08:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n33ugq</id>
    <title>Amazing Qwen stuff coming soon</title>
    <updated>2025-08-29T10:34:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt; &lt;img alt="Amazing Qwen stuff coming soon" src="https://preview.redd.it/v6kx1bw8sxlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ceb5641bac92e83c48c0893b26584487a3d582e" title="Amazing Qwen stuff coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas...?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6kx1bw8sxlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T10:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
