<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-21T10:40:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k3unyo</id>
    <title>SOTA Quantitative Spatial Reasoning Performance from 3B VLM</title>
    <updated>2025-04-20T19:38:37+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3unyo/sota_quantitative_spatial_reasoning_performance/"&gt; &lt;img alt="SOTA Quantitative Spatial Reasoning Performance from 3B VLM" src="https://a.thumbs.redditmedia.com/WpiQBH7DJOgacCoqjOG_hB3yG6f-lkBcVCrp8ifj2L0.jpg" title="SOTA Quantitative Spatial Reasoning Performance from 3B VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Updated &lt;strong&gt;SpaceThinker&lt;/strong&gt; docs to include a live demo, .gguf weights, and evaluation using &lt;a href="https://andrewliao11.github.io/spatial_prompt/"&gt;Q-Spatial-Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This 3B VLM scores on par with the closed, frontier model APIs compared in the project.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Space:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/remyxai/SpaceThinker-Qwen2.5VL-3B"&gt;https://huggingface.co/spaces/remyxai/SpaceThinker-Qwen2.5VL-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B"&gt;https://huggingface.co/remyxai/SpaceThinker-Qwen2.5VL-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Colab:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1buEe2QC4_pnrJwQ9XyRAH7RfaIa6pbex?usp=sharing"&gt;https://colab.research.google.com/drive/1buEe2QC4_pnrJwQ9XyRAH7RfaIa6pbex?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k3unyo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3unyo/sota_quantitative_spatial_reasoning_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3unyo/sota_quantitative_spatial_reasoning_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T19:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k45m2k</id>
    <title>Best programming reasoning trace datasets?</title>
    <updated>2025-04-21T05:02:15+00:00</updated>
    <author>
      <name>/u/klawisnotwashed</name>
      <uri>https://old.reddit.com/user/klawisnotwashed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Just read the s1: simple test-time scaling paper from Stanford. $30 and 26 minutes to train a small reasoning model. Would love to try replicating their efforts for a coding model specifically and benchmark it. Any ideas on where to get some good reasoning data for programming for this project? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klawisnotwashed"&gt; /u/klawisnotwashed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45m2k/best_programming_reasoning_trace_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45m2k/best_programming_reasoning_trace_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k45m2k/best_programming_reasoning_trace_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T05:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3n4u1</id>
    <title>I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne</title>
    <updated>2025-04-20T13:58:12+00:00</updated>
    <author>
      <name>/u/Jattoe</name>
      <uri>https://old.reddit.com/user/Jattoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt; &lt;img alt="I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne" src="https://a.thumbs.redditmedia.com/MK3_Ar5JxQPHqjerMJ0s5Kt_ypxo56WxUgndoco5GA8.jpg" title="I REALLY like Gemma3 for writing--but it keeps renaming my characters to Dr. Aris Thorne" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use it for rewrites of my own writing, not for original content, but moreso stylistic ideas and such, and it's the best so far.&lt;/p&gt; &lt;p&gt;But it has some weird information in there, I'm guessing perhaps as a thumbprint? It's such a shame because if it wasn't for this dastardly Dr. Aris Thorne and whatever crop of nonsenses that are shoved into the pot in order to make such a thing repetitive despite different prompts... Well, it'd be just about the best Google has ever produced, perhaps even better than the refined Llamas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jattoe"&gt; /u/Jattoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n4u1/i_really_like_gemma3_for_writingbut_it_keeps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T13:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49an5</id>
    <title>Multilingual RAG: are the documents retrieved correctly ?</title>
    <updated>2025-04-21T09:23:00+00:00</updated>
    <author>
      <name>/u/Difficult_Face5166</name>
      <uri>https://old.reddit.com/user/Difficult_Face5166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;It might be a stupid question but for multi-lingual RAG, are all documents extracted &amp;quot;correctly&amp;quot; with the retriever ? i.e. if my query is in English, will the retriever only end up retrieving top k documents in English by similarity and will ignore documents in other languages ? Or will it consider other by translation or by the fact that embeddings create similar vector (or very near) for same word in different languages and therefore any documents are considered for top k ?&lt;/p&gt; &lt;p&gt;I would like to mix documents in French and English and I was wondering if I need to do two vector databases separately or mixed ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult_Face5166"&gt; /u/Difficult_Face5166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49an5/multilingual_rag_are_the_documents_retrieved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49an5/multilingual_rag_are_the_documents_retrieved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49an5/multilingual_rag_are_the_documents_retrieved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3fdqa</id>
    <title>I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!</title>
    <updated>2025-04-20T05:23:56+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"&gt; &lt;img alt="I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!" src="https://external-preview.redd.it/ZTh6bHV5dXpjeHZlMYnlOxWogVD-LlbQBY7Zy9u919rxevDxwZbMMM1phMmn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01477954af0e59cfd452236b1012f46967be20f6" title="I spent 5 months building an open source AI note taker that uses only local AI models. Would really appreciate it if you guys could give me some feedback!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey community! I recently open-sourced &lt;strong&gt;Hyprnote&lt;/strong&gt; — a smart notepad built for people with back-to-back meetings.&lt;/p&gt; &lt;p&gt;In a nutshell, Hyprnote is a note-taking app that listens to your meetings and &lt;strong&gt;creates an enhanced version by combining the raw notes with context from the audio&lt;/strong&gt;. It runs on local AI models, so you don’t have to worry about your data going anywhere.&lt;/p&gt; &lt;p&gt;Hope you enjoy the project!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2njzhyuzcxve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T05:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3n7od</id>
    <title>PocketPal</title>
    <updated>2025-04-20T14:01:55+00:00</updated>
    <author>
      <name>/u/Illustrious-Dot-6888</name>
      <uri>https://old.reddit.com/user/Illustrious-Dot-6888</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"&gt; &lt;img alt="PocketPal" src="https://preview.redd.it/rfaxzunvxzve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aff8aa5797ead7a7b17086a66b51c1f74f7bbff" title="PocketPal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying my Donald system prompt with Gemma&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Dot-6888"&gt; /u/Illustrious-Dot-6888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rfaxzunvxzve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3n7od/pocketpal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T14:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49ydt</id>
    <title>What is the best way to extract subtitle form video in 2025 ?</title>
    <updated>2025-04-21T10:08:57+00:00</updated>
    <author>
      <name>/u/Tomtun_rd</name>
      <uri>https://old.reddit.com/user/Tomtun_rd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am attempting to extract subtitles in my local language from a video. I used EasyOCR, but the accuracy is poor. Are there more effective techniques available, or would an LLM-based model be a better option?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tomtun_rd"&gt; /u/Tomtun_rd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49ydt/what_is_the_best_way_to_extract_subtitle_form/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49ydt/what_is_the_best_way_to_extract_subtitle_form/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49ydt/what_is_the_best_way_to_extract_subtitle_form/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T10:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3ygru</id>
    <title>[Release] GPU Benchmark - Compare your Stable Diffusion performance globally</title>
    <updated>2025-04-20T22:38:23+00:00</updated>
    <author>
      <name>/u/yachty66</name>
      <uri>https://old.reddit.com/user/yachty66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just released &lt;strong&gt;GPU Benchmark&lt;/strong&gt;, a simple open-source tool that measures how many Stable Diffusion images your GPU can generate in 5 minutes and compares your results with others worldwide on our leaderboard.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs Stable Diffusion for exactly 5 minutes&lt;/li&gt; &lt;li&gt;Counts how many images your GPU can generate&lt;/li&gt; &lt;li&gt;Tracks GPU temperature (max and average)&lt;/li&gt; &lt;li&gt;Anonymously submits results to a global leaderboard sorted by country&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why I made this:&lt;/h1&gt; &lt;p&gt;I was selling GPUs on eBay Kleinanzeigen and found the existing GPU health checks to be bad; specifically, there were no benchmark tools that specifically run on AI.&lt;/p&gt; &lt;h1&gt;Installation is super simple:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install gpu-benchmark &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;And running it is even simpler:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;gpu-benchmark &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The benchmark takes about 5 minutes after initial model loading. You can view all results on our online &lt;a href="https://www.unitedcompute.ai/gpu-benchmark"&gt;benchmark results&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Compatible with:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Any CUDA-compatible NVIDIA GPU&lt;/li&gt; &lt;li&gt;Python&lt;/li&gt; &lt;li&gt;Requires internet for result submission (but you can run offline too)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to hear your feedback and see your results! Has anyone else been looking for something like this?&lt;/p&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/yachty66/gpu-benchmark"&gt;project Github website&lt;/a&gt; for more info as well.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: This is completely free and open-source - just a tool I built because I thought the community might find it useful.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yachty66"&gt; /u/yachty66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3ygru/release_gpu_benchmark_compare_your_stable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3ygru/release_gpu_benchmark_compare_your_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3ygru/release_gpu_benchmark_compare_your_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T22:38:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3jpal</id>
    <title>Trying to create a Sesame-like experience Using Only Local AI</title>
    <updated>2025-04-20T10:34:52+00:00</updated>
    <author>
      <name>/u/fagenorn</name>
      <uri>https://old.reddit.com/user/fagenorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"&gt; &lt;img alt="Trying to create a Sesame-like experience Using Only Local AI" src="https://external-preview.redd.it/d3VlZmk5dm93eXZlMdxX11AfgZTEMF7oSAzFAlLSpvlezRf_S3o9RpaxpyHo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec75d44b379dfe02094858707c3beff78197228a" title="Trying to create a Sesame-like experience Using Only Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a personal project I've been working on in my freetime. I'm trying to build an interactive, voice-driven avatar. Think sesame but the full experience running locally.&lt;/p&gt; &lt;p&gt;The basic idea is: my voice goes in -&amp;gt; gets transcribed locally with Whisper -&amp;gt; that text gets sent to the Ollama api (along with history and a personality prompt) -&amp;gt; the response comes back -&amp;gt; gets turned into speech with a local TTS -&amp;gt; and finally animates the Live2D character (lipsync + emotions).&lt;/p&gt; &lt;p&gt;My main goal was to see if I could get this whole thing running smoothly locally on my somewhat old GTX 1080 Ti. Since I also like being able to use latest and greatest models + ability to run bigger models on mac or whatever, I decided to make this work with ollama api so I can just plug and play that.&lt;/p&gt; &lt;p&gt;I shared the initial release around a month back, but since then I have been working on V2 which just makes the whole experience a tad bit nicer. A big added benefit is also that the whole latency has gone down.&lt;br /&gt; I think with time, it might be possible to get the latency down enough that you could havea full blown conversation that feels instantanious. The biggest hurdle at the moment as you can see is the latency causes by the TTS.&lt;/p&gt; &lt;p&gt;The whole thing's built in C#, which was a fun departure from the usual Python AI world for me, and the performance has been pretty decent.&lt;/p&gt; &lt;p&gt;Anyway, the code's here if you want to peek or try it: &lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;https://github.com/fagenorn/handcrafted-persona-engine&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fagenorn"&gt; /u/fagenorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x8koh8vowyve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3jpal/trying_to_create_a_sesamelike_experience_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T10:34:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3kuqb</id>
    <title>Hopes for cheap 24GB+ cards in 2025</title>
    <updated>2025-04-20T11:52:23+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before AMD launched their 9000 series GPUs I had hope they would understand the need for a high VRAM GPU but hell no. They are either stupid or not interested in offering AI capable GPUs: Their 9000 series GPUs both have 16 GB VRAM, down from 20 and 24GB from the previous(!) generation of 7900 XT and XTX.&lt;/p&gt; &lt;p&gt;Since it takes 2-3 years for a new GPU generation does this mean no hope for a new challenger to enter the arena this year or is there something that has been announced and about to be released in Q3 or Q4?&lt;/p&gt; &lt;p&gt;I know there is this AMD AI Max and Nvidia Digits, but both seem to have low memory bandwidth (even too low for MoE?)&lt;/p&gt; &lt;p&gt;Is there no chinese competitor who can flood the market with cheap GPUs that have low compute but high VRAM?&lt;/p&gt; &lt;p&gt;EDIT: There is Intel, they produce their own chips, they could offer something. Are they blind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3kuqb/hopes_for_cheap_24gb_cards_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T11:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3l728</id>
    <title>AMD preparing RDNA4 Radeon PRO series with 32GB memory on board</title>
    <updated>2025-04-20T12:13:00+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"&gt; &lt;img alt="AMD preparing RDNA4 Radeon PRO series with 32GB memory on board" src="https://external-preview.redd.it/j-nxBdzwljPlN5seurBLaB3DFUJJUSe9zBrJeVFzQtc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a798518e71babd3749ea83f0d2ad7aae5967f3f" title="AMD preparing RDNA4 Radeon PRO series with 32GB memory on board" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-preparing-radeon-pro-series-with-navi-48-xtw-gpu-and-32gb-memory-on-board"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3l728/amd_preparing_rdna4_radeon_pro_series_with_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T12:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3up8v</id>
    <title>What’s Your Go-To Local LLM Setup Right Now?</title>
    <updated>2025-04-20T19:40:17+00:00</updated>
    <author>
      <name>/u/techblooded</name>
      <uri>https://old.reddit.com/user/techblooded</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with a few models for summarizing Reddit/blog posts and some light coding tasks, but I keep getting overwhelmed by the sheer number of options and frameworks out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techblooded"&gt; /u/techblooded &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3up8v/whats_your_goto_local_llm_setup_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3up8v/whats_your_goto_local_llm_setup_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3up8v/whats_your_goto_local_llm_setup_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T19:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k44g1f</id>
    <title>best local llm to run locally</title>
    <updated>2025-04-21T03:51:48+00:00</updated>
    <author>
      <name>/u/Different-Put5878</name>
      <uri>https://old.reddit.com/user/Different-Put5878</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, so having gotten myself a top notch computer ( at least for me), i wanted to get into llm's locally and was kinda dissapointed when i compared the answers quaIity having used gpt4.0 on openai. Im very conscious that their models were trained on hundreds of millions of hardware so obviously whatever i can run on my gpu will never match. What are some of the smartest models to run locally according to you guys?? I been messing around with lm studio but the models sems pretty incompetent. I'd like some suggestions of the better models i can run with my hardware.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;cpu: amd 9950x3d&lt;/p&gt; &lt;p&gt;ram: 96gb ddr5 6000&lt;/p&gt; &lt;p&gt;gpu: rtx 5090&lt;/p&gt; &lt;p&gt;the rest i dont think is important for this&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Put5878"&gt; /u/Different-Put5878 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k44g1f/best_local_llm_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k44g1f/best_local_llm_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k44g1f/best_local_llm_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T03:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k46pxx</id>
    <title>Superscaling like image super resolution but for LLMs</title>
    <updated>2025-04-21T06:17:38+00:00</updated>
    <author>
      <name>/u/pharrowking</name>
      <uri>https://old.reddit.com/user/pharrowking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/pharrow/DeepSeek-R1-Distill-Llama-8B-abliterated-SuperScaled"&gt;https://huggingface.co/pharrow/DeepSeek-R1-Distill-Llama-8B-abliterated-SuperScaled&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Superscaling is a technique that incrementally adjusts the scale factors of a model’s linear and RMSNorm layers and other layers—raising each by 0.005 at each step—until the output satisfies predefined thresholds for accuracy, clarity, and completeness. In benchmark tests, this approach yielded an output that a substantially larger reference model rated 10 / 10. While perfect scores are not guaranteed on every instance, most outputs receive ratings between 8 / 10 and 10 / 10.&lt;/p&gt; &lt;p&gt;tests were done with the following prompt:&lt;br /&gt; &amp;quot;who was king of france in the 1800s&amp;quot;&lt;/p&gt; &lt;p&gt;resulting output:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Okay, so I need to figure out who was the king of France during the 1800s. Hmm, let me think. I know that France had several kings before the 1800s, like Louis XIV, but that was in the 1600s. So, moving forward in time, I remember that there was a revolution in France, probably around the late 1700s. That was the French Revolution, right? So, after that, France had different forms of government, maybe a republic or an empire.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Wait, wasn't there a Napoleon Bonaparte involved? I think he was a leader after the revolution. He became Emperor of France, so maybe he was the king of France in the 1800s? Or was he a different kind of ruler? Let me get this straight. The 1800s would be from 1801 to 1900, I believe. So, if Napoleon was active in the early 1800s, maybe he was the king then.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;But wait, France had a monarchy before Napoleon. The Bourbon dynasty ruled France for a long time, right? So, after the revolution, they were overthrown, and Napoleon came to power as a military leader. So, perhaps the kingship shifted from the Bourbon kings to Napoleon. But Napoleon declared himself Emperor, so he wasn't a king anymore. That might be confusing.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;So, if Napoleon wasn't a king, then who was the king during the 1800s? Maybe there was a period when France didn't have a monarchy. The revolution led to a republic, so maybe France was a republic during that time. That would mean there wasn't a king. But I'm not entirely sure about the exact dates.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Let me break it down. The French Revolution started in 1789 and ended around 1799 with the establishment of the First Republic. So, after that, France was a republic until Napoleon Bonaparte took power in 1804, declaring himself Emperor. So, from 1792 to 1804, France was a republic, so there wasn't a king. Then, Napoleon was the Emperor from 1804 until his defeat in 1815, when he was exiled. After that, France had different rulers, like the Bourbon Restoration, where the monarchy was reinstated.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Wait, so after Napoleon was exiled, the Bourbon kings came back. The 1800s would include both the republican period and the periods under different Bourbon monarchs. So, maybe Louis XVIII, Charles X, and Louis-Philippe were the kings during parts of the 1800s.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Let me check the timeline. The Bourbon Restoration happened in 1815, so Louis XVIII became king. He ruled until 1824. Then his brother Charles X took over until 1830. After that, Louis-Philippe, the cousin of Charles X, became king until 1848, when there was another revolution, leading to the Second Republic. So, from 1848 onwards, France was a republic again.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;So, during the 1800s, France had three different kings: Louis XVIII from 1815 to 1824, Charles X from 1824 to 1830, and Louis-Philippe from 1830 to 1848. After that, there was no king because France became a republic again.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Therefore, the kings of France in the 1800s were Louis XVIII, Charles X, and Louis-Philippe. Napoleon, although a significant figure, was an Emperor, not a king.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I think that's it. So, depending on the specific time in the 1800s, different Bourbon kings were on the throne. There wasn't a single king throughout the entire century because of the revolutions and changes in government.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;In the 1800s, France experienced several changes in its governance, including periods of republican rule and the return of the Bourbon monarchy. The kings of France during this period were:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1. **Louis XVIII** - Ruled from 1815 to 1824.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2. **Charles X** - Ruled from 1824 to 1830.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;3. **Louis-Philippe** - Ruled from 1830 to 1848.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;After 1848, France transitioned to a republic, ending the monarchy. Napoleon Bonaparte, though a significant figure, was declared Emperor of France rather than a king.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;scored result: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Criterion&lt;/th&gt; &lt;th align="left"&gt;Score (0–4 / 0–3)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;4/4&lt;/td&gt; &lt;td align="left"&gt;Correctly identifies that there were no kings during the republic or empire periods and lists the three Bourbon kings with their correct reigns.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Clarity&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3/3&lt;/td&gt; &lt;td align="left"&gt;Very clear structure: outlines the revolutionary context, defines the century, then lists the monarchs.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Completeness&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3/3&lt;/td&gt; &lt;td align="left"&gt;Addresses the gaps (republic, empire), names all kings in the 19th century, and notes the end of monarchy in 1848.Criterion Score (0–4 / 0–3) NotesAccuracy 4/4 Correctly identifies that there were no kings during the republic or empire periods and lists the three Bourbon kings with their correct reigns.Clarity 3/3 Very clear structure: outlines the revolutionary context, defines the century, then lists the monarchs.Completeness 3/3 Addresses the gaps (republic, empire), names all kings in the 19th century, and notes the end of monarchy in 1848.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pharrowking"&gt; /u/pharrowking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k46pxx/superscaling_like_image_super_resolution_but_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k46pxx/superscaling_like_image_super_resolution_but_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k46pxx/superscaling_like_image_super_resolution_but_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T06:17:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3pw8n</id>
    <title>Intel releases AI Playground software for generative AI as open source</title>
    <updated>2025-04-20T16:05:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt; &lt;img alt="Intel releases AI Playground software for generative AI as open source" src="https://external-preview.redd.it/UgxZ6n5LChFWd0HyYltaWavgJZl1YXoXz0YJ03N3rv0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb487f61c95f7de558ca1ddf5ec1b62010b36b5e" title="Intel releases AI Playground software for generative AI as open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Announcement video: &lt;a href="https://www.youtube.com/watch?v=dlNvZu-vzxU"&gt;https://www.youtube.com/watch?v=dlNvZu-vzxU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt; AI Playground open source project and AI PC starter app for doing AI image creation, image stylizing, and chatbot on a PC powered by an Intel® Arc™ GPU. AI Playground leverages libraries from GitHub and Huggingface which may not be available in all countries world-wide. AI Playground supports many Gen AI libraries and models including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image Diffusion: Stable Diffusion 1.5, SDXL, Flux.1-Schnell, LTX-Video&lt;/li&gt; &lt;li&gt;LLM: Safetensor PyTorch LLMs - DeepSeek R1 models, Phi3, Qwen2, Mistral, GGUF LLMs - Llama 3.1, Llama 3.2: OpenVINO - TinyLlama, Mistral 7B, Phi3 mini, Phi3.5 mini&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/intel/AI-Playground"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3pw8n/intel_releases_ai_playground_software_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T16:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k45plp</id>
    <title>A collection of benchmarks for LLM inference engines: SGLang vs vLLM</title>
    <updated>2025-04-21T05:08:31+00:00</updated>
    <author>
      <name>/u/Michaelvll</name>
      <uri>https://old.reddit.com/user/Michaelvll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt; &lt;img alt="A collection of benchmarks for LLM inference engines: SGLang vs vLLM" src="https://external-preview.redd.it/GB15PFHcTZYqNqz5LwH26yT5wqSIL5PcEgrLsjCjETM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb815c9e568ec4387033a7258b7b6f6ec5f87982" title="A collection of benchmarks for LLM inference engines: SGLang vs vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Competition in open source could advance the technology rapidly. &lt;/p&gt; &lt;p&gt;Both vLLM and SGLang teams are amazing, speeding up the LLM inference, but the recent arguments for the different benchmark numbers confused me quite a bit. &lt;/p&gt; &lt;p&gt;I deeply respect both teams and trust their results, so I created a collection of benchmarks from both systems to learn more: &lt;a href="https://github.com/Michaelvll/llm-ie-benchmarks"&gt;https://github.com/Michaelvll/llm-ie-benchmarks&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I created a few &lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt; YAMLs for those benchmarks, so they can be easily run with a single command, ensuring consistent and reproducible infrastructure deployment across benchmarks. &lt;/p&gt; &lt;p&gt;Thanks to the high availability of H200 on Nebius cloud, I ran those benchmarks on 8 H200 GPUs. &lt;/p&gt; &lt;p&gt;Some findings are quite surprising:&lt;br /&gt; 1. Even though the two benchmark scripts are similar: derived from the same source, they generate contradictory results. That makes me wonder if the benchmarks reflect the performance, or whether the implementation of the benchmarks matters more.&lt;br /&gt; 2. The benchmarks are fragile: simply changing the number of prompts can flip the conclusion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/whh6rmwtv2we1.png?width=1263&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=449371de63393db56bcd6688a71185ea6fef8768"&gt;Reproducing benchmark by vLLM team&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tytcqsvyv2we1.png?width=1178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=959da8b6727bbdbdd0fcee55c91c51efb64efc6e"&gt;Reproducing benchmark by SGLang team&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Later, SGLang maintainer submitted a &lt;a href="https://github.com/Michaelvll/llm-ie-benchmarks/pull/1"&gt;PR&lt;/a&gt; to our GitHub repo to update the optimal flags to be used for the benchmark: using &lt;code&gt;0.4.5.post2&lt;/code&gt; release, removing the &lt;code&gt;--enable-dp-attention&lt;/code&gt;, and adding three retries for warmup:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwyoxmrud4we1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7727ea1dcf5da0dcd78e013c70b68f95bc0904d2"&gt;Benchmark from SGLang team with optimal flags&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, if we change the number of prompts to 200 (vs 50 from the official benchmark), the performance conclusion flips. &lt;/p&gt; &lt;p&gt;That said, these benchmarks may be quite fragile, not reflecting the serving performance in a real application -- the input/output lengths could vary.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrx0u8s7e4we1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=092a61ecce69ee5f0dd2a33055388c1743af0cbd"&gt;Benchmark from SGLang team with optimal flags and 200 prompts in total&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Michaelvll"&gt; /u/Michaelvll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k45plp/a_collection_of_benchmarks_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T05:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49p6y</id>
    <title>Still no contestant to NeMo in the 12B range for RP?</title>
    <updated>2025-04-21T09:51:20+00:00</updated>
    <author>
      <name>/u/Xhatz</name>
      <uri>https://old.reddit.com/user/Xhatz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering what are y'all using for roleplay or ERP in that range. I've tested more than a hundred models and also fine-tunes of NeMo but not a single one has beaten Mag-Mell, a 1 yo fine-tune, for me, in storytelling, instruction following...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhatz"&gt; /u/Xhatz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49p6y/still_no_contestant_to_nemo_in_the_12b_range_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1k423l6</id>
    <title>Which drawing do you think is better? What does your LLM output?</title>
    <updated>2025-04-21T01:43:36+00:00</updated>
    <author>
      <name>/u/BlaiseLabs</name>
      <uri>https://old.reddit.com/user/BlaiseLabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"&gt; &lt;img alt="Which drawing do you think is better? What does your LLM output?" src="https://preview.redd.it/vmsd8uf2f3we1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a81ff88fa2df5f09153edaca6969a11f10e9caa1" title="Which drawing do you think is better? What does your LLM output?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What output do you get when asking an LLM to draw a face with matplotlib? Any tips or techniques you’d recommend for better results?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlaiseLabs"&gt; /u/BlaiseLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vmsd8uf2f3we1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k423l6/which_drawing_do_you_think_is_better_what_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T01:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k48u73</id>
    <title>Is Google’s Titans architecture doomed by its short context size?</title>
    <updated>2025-04-21T08:50:53+00:00</updated>
    <author>
      <name>/u/eesahe</name>
      <uri>https://old.reddit.com/user/eesahe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.00663"&gt;Paper link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Titans is hyped for its &amp;quot;learn‑at‑inference&amp;quot; long‑term memory, but the tradeoff is that it only has a tiny context window - in the paper they train their experiment models with a 4 K context size.&lt;/p&gt; &lt;p&gt;That context size cannot be easily scaled up because keeping the long-term memory updated becomes unfeasibly expensive with a longer context window, as I understand it.&lt;/p&gt; &lt;p&gt;Titans performs very well in some benchmarks with &amp;gt; 2 M‑token sequences, but I wonder if splitting the input into tiny windows and then compressing that into long-term memory vectors could end in some big tradeoffs outside of the test cases shown, due to losing direct access to the original sequence?&lt;/p&gt; &lt;p&gt;I wonder could that be part of why we haven't seen any models trained with this architecture yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eesahe"&gt; /u/eesahe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k48u73/is_googles_titans_architecture_doomed_by_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T08:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k47wlc</id>
    <title>🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!</title>
    <updated>2025-04-21T07:44:24+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt; &lt;img alt="🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!" src="https://external-preview.redd.it/NXEwbDIyYmU3NXdlMdvd-QuaL2Iymjf8AR2toHyHT4xxu-3H8nMusFAc2zhu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baa192645e7ad1e9a4b256373e06dac1ec7f2235" title="🚀 Dive v0.8.0 is Here — Major Architecture Overhaul and Feature Upgrades!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hgg9u2be75we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T07:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43htm</id>
    <title>Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image</title>
    <updated>2025-04-21T02:58:47+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt; &lt;img alt="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" src="https://b.thumbs.redditmedia.com/CBmqcJTwYbnQt5krSlEFj-sxPkmBsS_zYzMxkxvM3-Y.jpg" title="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InstantCharacter is an innovative, tuning-free method designed to achieve character-preserving generation from a single image&lt;/p&gt; &lt;p&gt;One image + text → custom poses, styles &amp;amp; scenes 1️⃣ First framework to balance character consistency, image quality, &amp;amp; open-domain flexibility/generalization 2️⃣ Compatible with Flux, delivering high-fidelity, text-controllable results 3️⃣ Comparable to industry leaders like GPT-4o in precision &amp;amp; adaptability&lt;/p&gt; &lt;p&gt;Try it yourself on： 🔗Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/InstantX/InstantCharacter"&gt;https://huggingface.co/spaces/InstantX/InstantCharacter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dive Deep into InstantCharacter: 🔗Project Page: &lt;a href="https://instantcharacter.github.io/"&gt;https://instantcharacter.github.io/&lt;/a&gt; 🔗Code: &lt;a href="https://github.com/Tencent/InstantCharacter"&gt;https://github.com/Tencent/InstantCharacter&lt;/a&gt; 🔗Paper：&lt;a href="https://arxiv.org/abs/2504.12395"&gt;https://arxiv.org/abs/2504.12395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k43htm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3wuud</id>
    <title>nsfw orpheus early v1</title>
    <updated>2025-04-20T21:21:27+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview"&gt;https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;can do the common sounds / generalises pretty well - preview has only 1 voice but good enough to get an idea of where we are heading &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T21:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43x1h</id>
    <title>Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)</title>
    <updated>2025-04-21T03:21:32+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt; &lt;img alt="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" src="https://external-preview.redd.it/ZHd1MzQzZGp3M3dlMYSg_wSm3961EYqonF0X5c18rpErhfTomdHPrQd5DrBK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c49081fab5d934bbcd3802ccff3f83e4f2270f76" title="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8hsjp4q1w3we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T03:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43g7a</id>
    <title>Why are so many companies putting so much investment into free open source AI?</title>
    <updated>2025-04-21T02:56:18+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand alot of the big pictures for these companies, but considering how many open source options we have and how they will continue to get better. How will these companies like OpenAI or Google ever make back their investment?&lt;/p&gt; &lt;p&gt;Personally i have never had to stay subscribed to a company because there's so many free alternatives. Not to mention all these companies have really good free options of the best models.&lt;/p&gt; &lt;p&gt;Unless one starts screaming ahead of the rest in terms of performance what is their end goal?&lt;/p&gt; &lt;p&gt;Not that I'm complaining, just want to know.&lt;/p&gt; &lt;p&gt;EDIT: I should probably say i know OpenAI isn't open source yet from what i know but they also offer a very high quality free plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h0n</id>
    <title>24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?</title>
    <updated>2025-04-21T09:35:34+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt; &lt;img alt="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" src="https://external-preview.redd.it/WdUGpP3unGKFZihZrELR3GH6ZUOa768rHIdn2YSXrsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=461a9ba85d2877f5c00bb8c11f93f1ceac11d893" title="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:35:34+00:00</published>
  </entry>
</feed>
