<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-05T09:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j3v3c4</id>
    <title>Recommend LLMs to run locally for Coding related tasks</title>
    <updated>2025-03-05T04:39:17+00:00</updated>
    <author>
      <name>/u/binarySolo0h1</name>
      <uri>https://old.reddit.com/user/binarySolo0h1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run something locally that can reliably help with one or more aspects of everyday development tasks without braking my machine. Maybe integrate with Cline or RooCode to improve development workflows and lower the token usage for the paid models, even by a little.&lt;/p&gt; &lt;p&gt;System Configuration:&lt;/p&gt; &lt;p&gt;----------------------&lt;/p&gt; &lt;p&gt;Processor AMD Ryzen 7 7840HS w/ Radeon 780M Graphics 3.80 GHz&lt;/p&gt; &lt;p&gt;Installed RAM 16.0 GB (15.3 GB usable) Speed: 5600 MT/s&lt;/p&gt; &lt;p&gt;System type 64-bit operating system, x64-based processor&lt;/p&gt; &lt;p&gt;NPU NPU Compute Accelerator Device&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Dedicated GPU memory 0.2/6.0 GB Shared GPU memory 0.0/7.6 GB GPU Memory 0.0/7.6 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GPU 0 AMD Radeon(TM) Graphics&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Dedicated GPU memory 399/448 MB Shared GPU memory 0.9/7.6 GB GPU Memory 1.3/8.1 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GPU 1 NVIDIA GeForce RTX 3050 6GB Laptop GPU&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Dedicated GPU memory 0.2/6.0 GB Shared GPU memory 0.1/7.6 GB GPU Memory 0.3/13.6 GB &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/binarySolo0h1"&gt; /u/binarySolo0h1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3v3c4/recommend_llms_to_run_locally_for_coding_related/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3v3c4/recommend_llms_to_run_locally_for_coding_related/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3v3c4/recommend_llms_to_run_locally_for_coding_related/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T04:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3929v</id>
    <title>When will Meta AI get a Llama upgrade already?</title>
    <updated>2025-03-04T11:52:51+00:00</updated>
    <author>
      <name>/u/Cheetah3051</name>
      <uri>https://old.reddit.com/user/Cheetah3051</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been stuck at 3.2 for months&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheetah3051"&gt; /u/Cheetah3051 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3929v/when_will_meta_ai_get_a_llama_upgrade_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T11:52:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3lh4x</id>
    <title>Used the deepseek to start my v h s collection. Happy with the results.</title>
    <updated>2025-03-04T21:02:35+00:00</updated>
    <author>
      <name>/u/WingsOfNth</name>
      <uri>https://old.reddit.com/user/WingsOfNth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"&gt; &lt;img alt="Used the deepseek to start my v h s collection. Happy with the results." src="https://b.thumbs.redditmedia.com/b_NGrrSOxyA3TCuGoUhpwPGjc1Mc5kqu5z1MeKH4tGA.jpg" title="Used the deepseek to start my v h s collection. Happy with the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WingsOfNth"&gt; /u/WingsOfNth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j3lh4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lh4x/used_the_deepseek_to_start_my_v_h_s_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T21:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3wje9</id>
    <title>Can a Rtx 4060 16GB run Codestral 22b?</title>
    <updated>2025-03-05T06:05:33+00:00</updated>
    <author>
      <name>/u/Stormili</name>
      <uri>https://old.reddit.com/user/Stormili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My planned setup would be something like this:&lt;br /&gt; Rtx 4060 16GB&lt;br /&gt; 32GB Ram&lt;br /&gt; 1TB SSD&lt;/p&gt; &lt;p&gt;Could I expect this to run? Could I go even lower?&lt;/p&gt; &lt;p&gt;Also if you know about a site that gives a good overview of models an their realistic min specs, that would be immensely helpfull.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stormili"&gt; /u/Stormili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wje9/can_a_rtx_4060_16gb_run_codestral_22b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wje9/can_a_rtx_4060_16gb_run_codestral_22b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wje9/can_a_rtx_4060_16gb_run_codestral_22b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T06:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j37yhi</id>
    <title>Just 2B R1 like model achieved "aha" moment in vision task!!!</title>
    <updated>2025-03-04T10:38:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt; &lt;img alt="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" src="https://external-preview.redd.it/FrHOpKWMdqORPFSxLwpGvRx_uoQMNXcCJsvQiFYTNWw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2604aa322cfdf8200f3a6d6f9f972bb244dac34" title="Just 2B R1 like model achieved &amp;quot;aha&amp;quot; moment in vision task!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j37yhi/just_2b_r1_like_model_achieved_aha_moment_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:38:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3ussu</id>
    <title>Ollama is using up all my CPU and not GPU</title>
    <updated>2025-03-05T04:22:34+00:00</updated>
    <author>
      <name>/u/butterenergy</name>
      <uri>https://old.reddit.com/user/butterenergy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3ussu/ollama_is_using_up_all_my_cpu_and_not_gpu/"&gt; &lt;img alt="Ollama is using up all my CPU and not GPU" src="https://b.thumbs.redditmedia.com/Kp5xBXGURMkDxn4obU8qu5C1yIRwLIViIra9mSmDXvA.jpg" title="Ollama is using up all my CPU and not GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zbg3q7scssme1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9dd84f4d63bff9defd1640e2df4f219527043ac"&gt;https://preview.redd.it/zbg3q7scssme1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9dd84f4d63bff9defd1640e2df4f219527043ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello, I'm trying to get something to run a version of Llama on my local system, but the problem is that it only seems to use my CPU, leaving my GPU completely unutilized. Do you guys have any idea what might be causing this? Completely new to any kind of LLM, first time I've ever tried hosting locally.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/butterenergy"&gt; /u/butterenergy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3ussu/ollama_is_using_up_all_my_cpu_and_not_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3ussu/ollama_is_using_up_all_my_cpu_and_not_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3ussu/ollama_is_using_up_all_my_cpu_and_not_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T04:22:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3wgy7</id>
    <title>Training LLMs with MXFP4</title>
    <updated>2025-03-05T06:01:09+00:00</updated>
    <author>
      <name>/u/tsengalb99</name>
      <uri>https://old.reddit.com/user/tsengalb99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to announce our latest paper on training LLMs with MXFP4 (what the B200 supports as &amp;quot;FP4&amp;quot;) matrix multiplications. We use stochastic rounding and random Hadamard transforms to get bounded variance, unbiased gradient estimates. Our method is an estimated 30% faster than FP8 during backprop and has almost no perplexity gap vs BF16 training on billion parameter scale GPT models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.20586"&gt;https://arxiv.org/abs/2502.20586&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tsengalb99"&gt; /u/tsengalb99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wgy7/training_llms_with_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wgy7/training_llms_with_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3wgy7/training_llms_with_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T06:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3elmz</id>
    <title>Chain-of-Experts: Unlocking the Communication Power of MoEs</title>
    <updated>2025-03-04T16:23:21+00:00</updated>
    <author>
      <name>/u/finallyifoundvalidUN</name>
      <uri>https://old.reddit.com/user/finallyifoundvalidUN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We propose Chain-of-Experts (CoE), which fundamentally changes sparse Large Language Model (LLM) processing by implementing sequential communication between intra-layer experts within Mixture-of-Experts (MoE) models.&lt;/p&gt; &lt;p&gt;Mixture-of-Experts (MoE) models process information independently in parallel between experts and have high memory requirements. CoE introduces an iterative mechanism enabling experts to &amp;quot;communicate&amp;quot; by processing tokens on top of outputs from other experts.&lt;/p&gt; &lt;p&gt;Experiments show that CoE significantly outperforms previous MoE models in multiple aspects:&lt;/p&gt; &lt;p&gt;Performance: CoE with 2x iterations reduces Math validation loss from 1.20 to 1.12 Scaling: 2x iterations matches performance of 3x expert selections, outperforming layer scaling Efficiency: 17.6% lower memory usage with equivalent performance Flexibility: 823x increase in expert combinations, improving utilization, communication, and specialization&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZihanWang314/coe"&gt;https://github.com/ZihanWang314/coe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finallyifoundvalidUN"&gt; /u/finallyifoundvalidUN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3elmz/chainofexperts_unlocking_the_communication_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T16:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3thkl</id>
    <title>PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)</title>
    <updated>2025-03-05T03:10:50+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt; &lt;img alt="PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)" src="https://external-preview.redd.it/GpXpL5-MZLw335WIDJMcyqdgdCC-2-IxUj5GgZ4bS2s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=765db037dfefdef59d32d12b9a1ca0123e55f05b" title="PromptRose 🌹 is your AI prompt companion, blooming at your fingertips. (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/x66ccff/PromptRose"&gt;https://github.com/x66ccff/PromptRose&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Prompt Rose&amp;quot; is a sleek productivity tool that, with just a press and hold of the Alt key (customizable), brings up a radial menu similar to those found in the Battlefield game series. Instead of game commands, this menu is filled with quick prompt instructions designed for Large Language Models (LLMs). This eliminates the need for users to search through their notebooks for prompts.&lt;/p&gt; &lt;p&gt;Designed for Windows platforms, &amp;quot;Prompt Rose&amp;quot; also allows for prompt management directly from the right-click system tray.&lt;/p&gt; &lt;p&gt;✨ Features&lt;/p&gt; &lt;p&gt;- **Intuitive Radial Menu**: Press and hold your hotkey to bring up a wheel of prompts&lt;/p&gt; &lt;p&gt;- **Quick Selection**: Simply move your mouse in the direction of the desired prompt and release&lt;/p&gt; &lt;p&gt;- **Customizable Prompts**: Add, edit, or remove prompts through an easy-to-use interface&lt;/p&gt; &lt;p&gt;- **Customizable Hotkey**: Choose your preferred activation key combination&lt;/p&gt; &lt;p&gt;- **Direct Pasting**: Automatically pastes your selected prompt into the active application&lt;/p&gt; &lt;p&gt;- **System Tray Integration**: Minimal footprint with easy access to all functions&lt;/p&gt; &lt;p&gt;- **Persistent Settings**: Your configurations are saved and loaded between sessions &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bi66v08nfsme1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa396622380ccf9948f8a4c0dab7f9643d6ff370"&gt;https://preview.redd.it/bi66v08nfsme1.png?width=1138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa396622380ccf9948f8a4c0dab7f9643d6ff370&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3thkl/promptrose_is_your_ai_prompt_companion_blooming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T03:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bhzb</id>
    <title>Survival Specialist Fine-tune (Llama 3.1- 8B)</title>
    <updated>2025-03-04T14:05:03+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt; &lt;img alt="Survival Specialist Fine-tune (Llama 3.1- 8B)" src="https://external-preview.redd.it/85ZgfWZxijLN2bSUGD1P0g3arLnrKn-sjIsrvAEBZ2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65ea7c4f77726004c20e1a7fce60c95196efde1f" title="Survival Specialist Fine-tune (Llama 3.1- 8B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lolzinventor/Meta-Llama-3.1-8B-SurviveV3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bhzb/survival_specialist_finetune_llama_31_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3yc4l</id>
    <title>Apple Notes + Ollama</title>
    <updated>2025-03-05T08:14:06+00:00</updated>
    <author>
      <name>/u/arne226</name>
      <uri>https://old.reddit.com/user/arne226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a desktop app to converse with Apple Notes using Ollama. Could be interesting to add other Notes sources like Obsidian for example. &lt;/p&gt; &lt;p&gt;Theres a version that only supports local processing &lt;a href="https://github.com/arnestrickmann/Notechat"&gt;https://github.com/arnestrickmann/Notechat&lt;/a&gt; but also one that offers processing with Gemini. &lt;/p&gt; &lt;p&gt;Would be happy about your feedback.&lt;br /&gt; Arne &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arne226"&gt; /u/arne226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T08:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3byj5</id>
    <title>ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models</title>
    <updated>2025-03-04T14:27:00+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt; &lt;img alt="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" src="https://external-preview.redd.it/zs_VjubpLgixBxipH26-eVw1VDZXtfTYy2chbOoXyyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2bad2c795210312fd91bc4ff5801de93d52e3cad" title="ByteDance Unveils SuperGPQA: A New Benchmark for Evaluating Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance’s Doubao Large Model Team, in collaboration with the M-A-P open-source community, has announced the release of SuperGPQA, a comprehensive benchmark designed to evaluate the knowledge and reasoning capabilities of large language models (LLMs) across 285 graduate-level disciplines. This dataset encompasses 26,529 multiple-choice questions, offering a rigorous assessment of LLM performance.&lt;br /&gt; &lt;a href="https://github.com/SuperGPQA/SuperGPQA"&gt;Github&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/m-a-p/SuperGPQA"&gt;HuggingFace&lt;/a&gt; &lt;a href="https://www.arxiv.org/abs/2502.14739"&gt;Paper&lt;/a&gt; &lt;a href="https://supergpqa.github.io"&gt;Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed"&gt;https://preview.redd.it/fljufnevlome1.png?width=4895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fd8837a9f4b68167e4b3d12a0a3f97ecc356ed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fq3ne63lmome1.png?width=1190&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e93f86a69d8ef6992f76c640ccbafb72504b045"&gt;Performance on SuperGPQA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x3dqea5umome1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=66a03b81db1827e82a1db111db6581188a2f4f06"&gt;LLM Performance Across Different Categories&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3byj5/bytedance_unveils_supergpqa_a_new_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j38499</id>
    <title>DiffRhythm - ASLP-lab: generate full songs (4 min) with vocals</title>
    <updated>2025-03-04T10:49:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/ASLP-lab/DiffRhythm"&gt;https://huggingface.co/spaces/ASLP-lab/DiffRhythm&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894"&gt;https://huggingface.co/collections/ASLP-lab/diffrhythm-67bc10cdf9641a9ff15b5894&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/ASLP-lab"&gt;https://github.com/ASLP-lab&lt;/a&gt;&lt;br /&gt; Paper: DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion: &lt;a href="https://arxiv.org/abs/2503.01183"&gt;https://arxiv.org/abs/2503.01183&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j38499/diffrhythm_aslplab_generate_full_songs_4_min_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T10:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j32p97</id>
    <title>Qwen 32b coder instruct can now drive a coding agent fairly well</title>
    <updated>2025-03-04T04:29:49+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt; &lt;img alt="Qwen 32b coder instruct can now drive a coding agent fairly well" src="https://external-preview.redd.it/aDJ4N25hdXlvbG1lMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b763684453c2eb0539d13912eebe98f2d438296" title="Qwen 32b coder instruct can now drive a coding agent fairly well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2000d3tolme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j32p97/qwen_32b_coder_instruct_can_now_drive_a_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T04:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3k8o8</id>
    <title>There is a space on HF where you can convert models to MLX without downloading them</title>
    <updated>2025-03-04T20:10:48+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been downloading models and converting them to MLX with the convert script from the MLX library. This takes lots of time and I'm limited on how big models I can convert by my 16gb ram.&lt;/p&gt; &lt;p&gt;Well I just learned that there is a space on HF where you can convert models, you just enter the model name and the quant and it uploads it straight to your HF profile! No need to download or do anything at all. This also means I'm not limited by my ram, I can convert any model now and so should you :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;https://huggingface.co/spaces/mlx-community/mlx-my-repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:10:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3xn8k</id>
    <title>Making vision language models point to objects in image, introducing new modality to a language model</title>
    <updated>2025-03-05T07:21:28+00:00</updated>
    <author>
      <name>/u/SmallTimeCSGuy</name>
      <uri>https://old.reddit.com/user/SmallTimeCSGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying something similar as MoonDream, and Molmo. i.e make the language model capable of producing normalized coordinates of objects asked about. &amp;quot;Point: Dog&amp;quot; e.g.&lt;/p&gt; &lt;p&gt;I am trying to make smolvlm do this as a fun project to get better understanding. I am trying on a subset(1mil) of pixmo-points dataset.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;tried plain SFT, both full and PEFT, obviously that did not work, as the model does not have notion of points being output.&lt;/li&gt; &lt;li&gt;tried GRPO, that too, did not work, as the model evidently did not have latent capabilities as such for this to emerge.&lt;/li&gt; &lt;li&gt;taking some inspiration from moondream, I introduced a new modality for points altogether. i.e. points are encoded, same embedding dimension as accepted by the autoregressive part of the model, then after autoregressive, have another decoder decode the points. Keeping the other parts frozen. I tried SFT with cross entropy, though am a bit skeptical of it being used for a pointing task, where MSE loss seems more suitable. But this too, failed though showing a nice loss characteristics during training. The model just produces random points.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Has anyone tried something similar? Any suggestions on what else I can try? Any pointer on how to make some progress would be good, as clearly this is feasible. What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmallTimeCSGuy"&gt; /u/SmallTimeCSGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T07:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fkax</id>
    <title>LLM Quantization Comparison</title>
    <updated>2025-03-04T17:02:00+00:00</updated>
    <author>
      <name>/u/dat1-co</name>
      <uri>https://old.reddit.com/user/dat1-co</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt; &lt;img alt="LLM Quantization Comparison" src="https://external-preview.redd.it/CGbzq4JDmMgH-DT2hVt-MPGAGrs7Io3E0dHabckY9J8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbd8d1effbe5db86d79260e6b8463c84c31b3a11" title="LLM Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat1-co"&gt; /u/dat1-co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dat1.co/blog/llm-quantization-comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bldn</id>
    <title>C4AI Aya Vision</title>
    <updated>2025-03-04T14:09:38+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt; &lt;img alt="C4AI Aya Vision" src="https://external-preview.redd.it/2FtgBIdrUTjZVqld2wWXrJgxCyutz4lA4knvupaJc-g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cffd187923d62691d6d92d4dd9ba2db2a4f098" title="C4AI Aya Vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/CohereForAI/c4ai-aya-vision-67c4ccd395ca064308ee1484"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3m8v5</id>
    <title>Cohere Blog: Aya Vision — Expanding the Worlds AI Can See</title>
    <updated>2025-03-04T21:34:39+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt; &lt;img alt="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" src="https://external-preview.redd.it/N2GDJUmAUpd9T37b2tIt2DUV5G6cVnF0cK_wUy2iAqI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1371c07fff0fcedfb4dc146d3550992b34c14bbd" title="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cohere.com/blog/aya-vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T21:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3lbck</id>
    <title>SCANN: A Self-Organizing Coherent Attention Neural Network</title>
    <updated>2025-03-04T20:56:00+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt; &lt;img alt="SCANN: A Self-Organizing Coherent Attention Neural Network" src="https://external-preview.redd.it/JLOcLL_amzNZJjQB5eq2ns36cBxnwXJySZseItLLLD8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58af66c266b498548139bfda71fa96ad039e54f4" title="SCANN: A Self-Organizing Coherent Attention Neural Network" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few weeks ago, in my latest deep dive into random thought experiments, I went the furthest I've gone in terms of research/depth/experiments and was able to come out of the other side with an interesting information based field equation which I've now been able to apply to a new ML and neural network for learning. It's in the early stages, but the results so far are incredible with the clear path to scaling into a full LLM architecture.&lt;/p&gt; &lt;p&gt;So then what am I talking about?&lt;/p&gt; &lt;p&gt;Instead of conventional gradient-based optimization used in machine learning models such as logistic regression, random forests, and deep learning, or the attention-based token weighting in LLMs, SCANN employs a fundamentally different approach.&lt;/p&gt; &lt;p&gt;What Makes SCANN Different?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Self-Organization Instead of Training&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Unlike traditional models that explicitly train weights via backpropagation, SCANN allows features to evolve dynamically over time.&lt;/p&gt; &lt;p&gt;The transformation follows a mathematically governed Partial Differential Equation (PDE):&lt;/p&gt; &lt;p&gt;SCANN Equation =&lt;/p&gt; &lt;p&gt;D[ψ[t, x], t] == -γ ψ[t, x] - ∇ ⋅ (D[ψ[t, x]] ∇ ψ[t, x]) +&lt;/p&gt; &lt;p&gt;λnl Sum[ψ[t, xi], {xi, Neighbors}] + β Tanh[ψ[t, x]^2]&lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;p&gt;- Diffusion spreads feature information naturally:&lt;/p&gt; &lt;p&gt;D(ψ) = D0 (1 + α ψ^2)&lt;/p&gt; &lt;p&gt;- Nonlocal interactions allow features to learn from global structures.&lt;/p&gt; &lt;p&gt;- Resonance amplifies meaningful patterns.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN Generalizes Without Dataset-Specific Tuning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;SCANN has been evaluated across multiple datasets (Digits, Wine Classification, Breast Cancer, etc.) and has consistently performed well without dataset-specific retraining.&lt;/p&gt; &lt;p&gt;Increasing the number of time steps improves representation learning, allowing SCANN to refine feature structures dynamically over time.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN vs. LLMs and Traditional Machine Learning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Traditional Machine Learning models (e.g., SVMs, Neural Networks) require explicit parameter training to fit a loss function.&lt;/p&gt; &lt;p&gt;LLMs use layered token attention to interpret complex relationships in text.&lt;/p&gt; &lt;p&gt;SCANN, however, does not rely on pre-set parameters or static learning mechanisms. Instead, it evolves feature representations dynamically, resembling a physical system seeking equilibrium.&lt;/p&gt; &lt;p&gt;Why This is Exciting&lt;/p&gt; &lt;p&gt;SCANN represents a new perspective on representation learning—one that does not depend on large datasets or brute-force optimization. It offers a self-organizing mechanism for feature discovery, potentially revealing patterns in ways that traditional ML approaches cannot.&lt;/p&gt; &lt;p&gt;Further refinements and formalization are ongoing, but these early results highlight SCANN’s potential for a fundamentally different kind of machine learning.&lt;/p&gt; &lt;p&gt;I'll be open sourcing all the code and releasing a paper once I get some more tests done and hopefully a small LLM built from it as well.&lt;/p&gt; &lt;p&gt;In the meantime, if you're interested in the core information-based field equation I built and then integrated into this ML model you can check out all the details and experiments here: &lt;a href="https://github.com/severian42/Informational-Relative-Evolution"&gt;https://github.com/severian42/Informational-Relative-Evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And a more longform paper here: &lt;a href="https://huggingface.co/blog/Severian/informational-relative-evolution"&gt;https://huggingface.co/blog/Severian/informational-relative-evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Test Results:&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88"&gt;https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143"&gt;https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3w4zs</id>
    <title>Why I say ollama is great for PoC but bad for production. (yet another rabbit hole) They finally support setting the default context size but I don't think beginners would notice this.</title>
    <updated>2025-03-05T05:40:32+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How time flies. The latest ollama release finally support the setting of default context length. I'm sure that 2k default context length have killed tons of beginners. Why is my model so dummy when I give it 10 pages PDF? Because you haven't set the context length, surprise!&lt;/p&gt; &lt;p&gt;Okay, now they support setting the default context length. You may wonder, how?&lt;br /&gt; Well it's written in a release note. But other than that, there's zero documentation about it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.5.13"&gt;https://github.com/ollama/ollama/releases/tag/v0.5.13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You could see there's an environment variable called &lt;code&gt;OLLAMA_CONTEXT_LENGTH&lt;/code&gt; . But if you search through their poorly organized documents, you would find nothing. If you went on searching the GitHub code, you would find ... &lt;a href="https://github.com/search?q=repo%3Aollama%2Follama+OLLAMA_CONTEXT_LENGTH+&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Aollama%2Follama+OLLAMA_CONTEXT_LENGTH+&amp;amp;type=code&lt;/a&gt; There is no markdown document that mentions this newly introduced environment variable. &lt;/p&gt; &lt;p&gt;Actually if you are smart enough, you could sense that this is not ready for production when you find out there's serval different markdown file you need to go through to set this thing up correctly. &lt;a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;https://github.com/ollama/ollama/blob/main/docs/modelfile.md&lt;/a&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/api.md"&gt;https://github.com/ollama/ollama/blob/main/docs/api.md&lt;/a&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md"&gt;https://github.com/ollama/ollama/blob/main/docs/faq.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. ollama was not my first framework that brings me to LLM, however I did use it for quite a while before I finally switch to llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T05:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3hjxb</id>
    <title>Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark</title>
    <updated>2025-03-04T18:21:37+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt; &lt;img alt="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" src="https://preview.redd.it/6jg8ae9drpme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85bce8c13b47a201032eadf727ee61bfb00869ed" title="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6jg8ae9drpme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T18:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3r8ls</id>
    <title>Deepseek V2.5 Becomes No.1 on Copilot Arena</title>
    <updated>2025-03-05T01:17:48+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt; &lt;img alt="Deepseek V2.5 Becomes No.1 on Copilot Arena" src="https://external-preview.redd.it/qz0mj3UiKjc9f5igcmafTaZjnzBMMP1WUULpUFnNC8Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904655aa1b01986e22fef932164110cccbc659a5" title="Deepseek V2.5 Becomes No.1 on Copilot Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the latest Copilot Arena rankings, &lt;strong&gt;Deepseek V2.5 (FIM)&lt;/strong&gt; has reached the &lt;strong&gt;top position&lt;/strong&gt; with an &lt;strong&gt;Arena Score of 1028&lt;/strong&gt;, outperforming strong competitors like &lt;strong&gt;Claude 3.5 Sonnet&lt;/strong&gt; and &lt;strong&gt;Codetral&lt;/strong&gt; to become the highest-ranked AI coding assistant! 🚀&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0q3v0xiurme1.png?width=1740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=333d8e307f646075cbe1f4cbd5da55fe0b31bafe"&gt;Rank&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The leaderboard differs from existing evaluations. In particular, smaller models over perform in static benchmarks compared to real development workflows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db"&gt;https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared to previous benchmarks, Copilot Arena observes more programming languages (PL), natural languages (NL), longer context lengths, multiple task types, and various code structures.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2276cjwuurme1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b61eb97054808650e4dd23bb287a01af99c4a0"&gt;Data Distribution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/iamwaynechi/status/1896996806481109377"&gt;X&lt;/a&gt; &lt;a href="https://lmarena.ai/?leaderboard"&gt;Leaderboard&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.09328"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T01:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gahy</id>
    <title>NVIDIA’s GeForce RTX 4090 With 96GB VRAM Reportedly Exists; The GPU May Enter Mass Production Soon, Targeting AI Workloads.</title>
    <updated>2025-03-04T17:31:10+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/"&gt;https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly highly interested. If this will be true.&lt;/p&gt; &lt;p&gt;Price around 6k. &lt;/p&gt; &lt;p&gt;Source; &amp;quot;The user did confirm that the one with a 96 GB VRAM won't guarantee stability and that its cost, due to a higher VRAM, will be twice the amount you would pay on the 48 GB edition. As per the user, this is one of the reasons why the factories are considering making only the 48 GB edition but may prepare the 96 GB in about 3-4 months.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3vbfh</id>
    <title>Ollama v0.5.13 has been released</title>
    <updated>2025-03-05T04:52:19+00:00</updated>
    <author>
      <name>/u/Inevitable-Rub8969</name>
      <uri>https://old.reddit.com/user/Inevitable-Rub8969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt; &lt;img alt="Ollama v0.5.13 has been released" src="https://b.thumbs.redditmedia.com/ZjpiHa9AL7vA9lJSktHNr9Xm0TdCOx4eLZRQfB67ZNA.jpg" title="Ollama v0.5.13 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f"&gt;https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Rub8969"&gt; /u/Inevitable-Rub8969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T04:52:19+00:00</published>
  </entry>
</feed>
