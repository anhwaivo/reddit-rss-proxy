<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-24T02:11:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i8b8mw</id>
    <title>DeepSeek R1 Distill Qwen 2.5 32B ablated (uncensored)</title>
    <updated>2025-01-23T19:09:21+00:00</updated>
    <author>
      <name>/u/ro5ssss</name>
      <uri>https://old.reddit.com/user/ro5ssss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share this release with the community of an &lt;a href="https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction"&gt;ablated&lt;/a&gt; (or &amp;quot;abliterated&amp;quot;) version of DeepSeek R1 Distill Qwen 2.5 (32B) instruct. In this way the assistant will refuse requests less often, for a more uncensored experience. We landed on layer 16 as the candidate. But wanted to explore other attempts and learnings. The release on hf: &lt;a href="https://huggingface.co/NaniDAO/deepseek-r1-qwen-2.5-32B-ablated"&gt;deepseek-r1-qwen-2.5-32B-ablated&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ro5ssss"&gt; /u/ro5ssss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b8mw/deepseek_r1_distill_qwen_25_32b_ablated_uncensored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b8mw/deepseek_r1_distill_qwen_25_32b_ablated_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b8mw/deepseek_r1_distill_qwen_25_32b_ablated_uncensored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T19:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87boz</id>
    <title>SmolVLM goes even smaller, running on your toaster ðŸ¥µðŸ”¥ Today Hugging Face release newer SmolVLMs: 256M and 500M</title>
    <updated>2025-01-23T16:29:43+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"&gt; &lt;img alt="SmolVLM goes even smaller, running on your toaster ðŸ¥µðŸ”¥ Today Hugging Face release newer SmolVLMs: 256M and 500M" src="https://external-preview.redd.it/ZWkxc3lkdXRzcmVlMeOQ6gi0MASgTgbv3hzqEbPqGdFSlYbcrJRLih99qg-l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=617c7797fb5d38c550d1817560f4884cecce1540" title="SmolVLM goes even smaller, running on your toaster ðŸ¥µðŸ”¥ Today Hugging Face release newer SmolVLMs: 256M and 500M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6d3hweutsree1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87boz/smolvlm_goes_even_smaller_running_on_your_toaster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:29:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x5nd</id>
    <title>The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)</title>
    <updated>2025-01-23T06:19:53+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt; &lt;img alt="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" src="https://preview.redd.it/o28q2pl6roee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbcc9bcc3ec8f998f0c0b948df4117b2f7a6ae30" title="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o28q2pl6roee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81ev6</id>
    <title>DeepSeek added recommandations for R1 local use to model card</title>
    <updated>2025-01-23T11:36:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#usage-recommendations&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;1, Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Avoid adding a system prompt; all instructions should be contained within the user prompt.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;For mathematical problems, it is advisable to include a directive in your prompt such as: &amp;quot;Please reason step by step, and put your final answer within \boxed{}.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When evaluating model performance, it is recommended to conduct multiple tests and average the results.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81ev6/deepseek_added_recommandations_for_r1_local_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8b750</id>
    <title>It's not free; you pay with your data, and it is used for training.</title>
    <updated>2025-01-23T19:07:41+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just something to think about when you use &amp;quot;free&amp;quot; ChatGPT, or others... is never free. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b750/its_not_free_you_pay_with_your_data_and_it_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b750/its_not_free_you_pay_with_your_data_and_it_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8b750/its_not_free_you_pay_with_your_data_and_it_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T19:07:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8hqp0</id>
    <title>DeepSeek R1 (reasoner) can use internet there o1 still can't</title>
    <updated>2025-01-23T23:47:38+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"&gt; &lt;img alt="DeepSeek R1 (reasoner) can use internet there o1 still can't" src="https://b.thumbs.redditmedia.com/NWZ7upEkaFaUDhq-5d7CT7VuzH9PhcF4ym1RrfuBo1M.jpg" title="DeepSeek R1 (reasoner) can use internet there o1 still can't" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Funny ... DeepSeek doing more for free than paid o1...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i8hqp0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8hqp0/deepseek_r1_reasoner_can_use_internet_there_o1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T23:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zeqz</id>
    <title>Nivida just open sourced their long context goodies - 128k context for 50% less memory</title>
    <updated>2025-01-23T09:09:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;edited : &lt;del&gt;50%&lt;/del&gt; 35% less memory&lt;/p&gt; &lt;p&gt;If you need long context for RAG, tool use, agents, or just because, Nvidia released a new library to make it super simple.&lt;/p&gt; &lt;p&gt;TLDR: You can get 128k context at 35% less memory&lt;/p&gt; &lt;p&gt;Here's a blog post on everything: &lt;a href="https://huggingface.co/blog/nvidia/kvpress"&gt;https://huggingface.co/blog/nvidia/kvpress&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81znv</id>
    <title>SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!</title>
    <updated>2025-01-23T12:14:25+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt; &lt;img alt="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" src="https://external-preview.redd.it/GQobgTxgKDrhJhG-UAnJyEaSqhi-5vBwrdogSL5wNmU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b8e0c51d51b02acc52b56c533ba8ea0c229040" title="SmolVLM 256 and 500M. Beating our 80B VLM from 17 months ago!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! &lt;/p&gt; &lt;p&gt;It's Andi from multimodal team at Hugging Face Today we released new SmolVLM: tiny vision language models that come in 256M and 500M parameters!&lt;/p&gt; &lt;p&gt;How did we compress SmolVLM 2.2B even further while making it generalize?&lt;br /&gt; &amp;gt; Smoller image encoder (SigLIP-base 93M)&lt;br /&gt; &amp;gt; More diverse data mixture&lt;br /&gt; &amp;gt; Better efficiency in tokenization for images&lt;/p&gt; &lt;p&gt;We have released base and instruction fine-tuned variants, they work out of the box with transformers and TRL, and we also release a WebGPU demo. &lt;/p&gt; &lt;p&gt;All of them are in this collection: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0"&gt;https://huggingface.co/collections/HuggingFaceTB/smolvlm-256m-and-500m-6791fafc5bb0ab8acc960fb0&lt;/a&gt;&lt;br /&gt; Here's a demo: &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo"&gt;https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo&lt;/a&gt;&lt;br /&gt; Also here's the blog for more in-depth documentation:&lt;a href="https://huggingface.co/blog/smolervlm"&gt;https://huggingface.co/blog/smolervlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to see what you think!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066"&gt;https://preview.redd.it/5viklrh4jqee1.png?width=1866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=464a14c3795f760c96a584570cb454f31b3db066&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81znv/smolvlm_256_and_500m_beating_our_80b_vlm_from_17/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T12:14:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81pbk</id>
    <title>This merge is amazing: FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-23T11:56:19+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model is a Fusion(merge?) of DeepSeekR1 &amp;amp; QwQ &amp;amp; SkyT1 32B models.&lt;/p&gt; &lt;p&gt;I usually donâ€™t use merged models because, most of the time, I canâ€™t find any noticeable differences. However, this one is truly different. Here are some examples:&lt;/p&gt; &lt;p&gt;My test setup is ollama + IQ4_XS + 32k context + Q8_0 KV Cache, questions are from: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1i65599/r1_32b_is_be_worse_than_qwq_32b_tests_included/&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Hello! I have multiple different files with different sizes, I want to move files from disk 1 to disk 2, which has only 688 space available. Without yapping, and being as concise as possible. What combination of files gets me closer to that number? The file sizes are: 36, 36, 49, 53, 54, 54, 63, 94, 94, 107, 164, 201, 361, 478&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/W2QFACDM"&gt;https://pastebin.com/W2QFACDM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To maximize the use of disk 2's 688-unit capacity, here are the optimal file combinations: 1. **478 + 53 + 54 + 54 + 49 = 688** 2. **361 + 201 + 36 + 36 + 54 = 688** 3. **478 + 107 + 49 + 54 = 688** 4. **361 + 107 + 49 + 54 + 54 + 63 = 688** Each combination precisely fills the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;This is first time I saw a local model give me 4 correct answers for this question, it's amazing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/2VJrjVmJ"&gt;https://pastebin.com/2VJrjVmJ&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The combination of files that gets you closest to the 688 space limit is: 361 + 164 + 107 + 54 + 54 = 686 This totals 686 units, which is just 2 units less than the available space on disk 2. &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How many days are between 12-12-1971 and 18-4-2024?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/PQGr1DDq"&gt;https://pastebin.com/PQGr1DDq&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The number of days between December 12, 1971, and April 18, 2024, is 19,121 days . This calculation includes the full years, accounts for leap years, and considers the days in the partial years, providing the difference between the two dates. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is correct answer, 19121 days&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/T0iNuYHf"&gt;https://pastebin.com/T0iNuYHf&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### **Final Answer** \[ \boxed{19,486} \] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 3 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;FuseO1-DeekSeekR1-QwQ-SkyT1-32B-Preview:&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/MZZ5nDiM"&gt;https://pastebin.com/MZZ5nDiM&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;To transport the man, sheep, and cat across the river, the solution depends on the boat's capacity. Given that the boat has &amp;quot;3 free spaces,&amp;quot; it can carry three items in total, including the man. Therefore, the man can take all three (himself, the sheep, and the cat) across the river in one trip. Here's the step-by-step solution: 1. **First Trip**: The man boards the boat with the sheep and the cat. 2. **Crossing the River**: The man rows the boat across the river to the other side. 3. **Arrival**: Upon reaching the other side, the man, sheep, and cat all disembark from the boat. This completes the task as all three are now on the opposite side of the river in a single trip. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is the correct answer, they only need to cross the river once&lt;/p&gt; &lt;p&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/p&gt; &lt;p&gt;full answer with thoughts: &lt;a href="https://pastebin.com/XHQHwQth"&gt;https://pastebin.com/XHQHwQth&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*fell into an infinite loop, I had to terminate it* &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8i41v</id>
    <title>Openai is ahead only till china reverse engineers...</title>
    <updated>2025-01-24T00:04:50+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"&gt; &lt;img alt="Openai is ahead only till china reverse engineers..." src="https://preview.redd.it/zy8ljay42uee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e8626d46d75e083e4343a6f229defc70f8055d8" title="Openai is ahead only till china reverse engineers..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zy8ljay42uee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8i41v/openai_is_ahead_only_till_china_reverse_engineers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T00:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87826</id>
    <title>The R1 Distillation you want is FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview</title>
    <updated>2025-01-23T16:25:31+00:00</updated>
    <author>
      <name>/u/TheActualStudy</name>
      <uri>https://old.reddit.com/user/TheActualStudy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an exl2 4.25 BPW quantization of FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview, and it functions how I was expecting DeepSeek-R1-Distill-Qwen-32B to have. It does not degrade on multi-turn performance, its instruction following is superior, and the writing results were more closely in line with R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview"&gt;HF Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I know people said this late on Monday already, but it took me until now to get it and test it, so I figured that others may still be struggling with DeepSeek-R1-Distill-Qwen-32B. I, personally, believe it's the new SOTA you were probably expecting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheActualStudy"&gt; /u/TheActualStudy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87826/the_r1_distillation_you_want_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i887aa</id>
    <title>Facebook's Coconut: Training Large Language Model to Reason in a Continuous Latent Space has been open-sourced</title>
    <updated>2025-01-23T17:05:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i887aa/facebooks_coconut_training_large_language_model/"&gt; &lt;img alt="Facebook's Coconut: Training Large Language Model to Reason in a Continuous Latent Space has been open-sourced" src="https://external-preview.redd.it/I9BvV7l8sFZkcQHfse_ouglW3Xd01DaryGObNbw54ZQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122bc0d4722c551cb2b629c373210ddafb6f6f92" title="Facebook's Coconut: Training Large Language Model to Reason in a Continuous Latent Space has been open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/facebookresearch/coconut"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i887aa/facebooks_coconut_training_large_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i887aa/facebooks_coconut_training_large_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T17:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i867k8</id>
    <title>First 5090 LLM results, compared to 4090 and 6000 ada</title>
    <updated>2025-01-23T15:42:34+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt; &lt;img alt="First 5090 LLM results, compared to 4090 and 6000 ada" src="https://external-preview.redd.it/PnzUTHeUQDah3Madq3JF5tCDBdEWLySpwcBRwh4t1-o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5868e7a86ac26107edab60570277f4095ed053" title="First 5090 LLM results, compared to 4090 and 6000 ada" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source:&lt;br /&gt; &lt;a href="https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration"&gt;https://www.storagereview.com/review/nvidia-geforce-rtx-5090-review-pushing-boundaries-with-ai-acceleration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4"&gt;https://preview.redd.it/viyw7xmqkree1.png?width=1554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=337b84f0d6c0110a49bec512c785fb7d3bc61bb4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073"&gt;https://preview.redd.it/mumd1wmqkree1.png?width=1608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=318bb83f4539fe28fd903026a2ad07ba033f9073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i867k8/first_5090_llm_results_compared_to_4090_and_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8fpza</id>
    <title>SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU.</title>
    <updated>2025-01-23T22:17:19+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"&gt; &lt;img alt="SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU." src="https://external-preview.redd.it/NTYzZXAwOXdpdGVlMeNP1riRHGftFiyraDTq8M0dXNR_Xk41nSkLrV2F0EOo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aefeccdf988fce29d96f197804b176f76684cb54" title="SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qikrzy8witee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8fpza/smolvlm_256m_the_worlds_smallest_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T22:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i81iim</id>
    <title>Been ages since google released an open model</title>
    <updated>2025-01-23T11:43:36+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt; &lt;img alt="Been ages since google released an open model" src="https://preview.redd.it/fa91scqqdqee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f78c10935c8984f8f9d17834c7720f182fed482" title="Been ages since google released an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fa91scqqdqee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i81iim/been_ages_since_google_released_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T11:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i87fkl</id>
    <title>Deepseek R1 is the only one that nails this new viral benchmark</title>
    <updated>2025-01-23T16:34:21+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"&gt; &lt;img alt="Deepseek R1 is the only one that nails this new viral benchmark" src="https://external-preview.redd.it/dTNsOXYwcnJ0cmVlMQcGL6cDuoI_ROA8VT0SlOGuG2iHRRkQmxqkRS_k8D6O.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f916bf51cb83dcc443906e66c7cbbd04f4cf9cc" title="Deepseek R1 is the only one that nails this new viral benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4skrezsntree1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i87fkl/deepseek_r1_is_the_only_one_that_nails_this_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T16:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8chpr</id>
    <title>Deepseek-r1-Qwen 1.5B's overthinking is adorable</title>
    <updated>2025-01-23T20:01:44+00:00</updated>
    <author>
      <name>/u/Ill-Still-6859</name>
      <uri>https://old.reddit.com/user/Ill-Still-6859</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"&gt; &lt;img alt="Deepseek-r1-Qwen 1.5B's overthinking is adorable" src="https://external-preview.redd.it/azZ1d2EzZ2x1c2VlMWwcsRUdCKlecN3EYDmX-jmw1aKFL7Ec90KkMpgcpWxW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ffd8792fef56427e2ddfd654527bbf426c6fbf8" title="Deepseek-r1-Qwen 1.5B's overthinking is adorable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Still-6859"&gt; /u/Ill-Still-6859 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b5coo5glusee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8chpr/deepseekr1qwen_15bs_overthinking_is_adorable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T20:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8a9qb</id>
    <title>Deepmind learning from Deepseek. Power of open source!</title>
    <updated>2025-01-23T18:30:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"&gt; &lt;img alt="Deepmind learning from Deepseek. Power of open source!" src="https://preview.redd.it/xouhskggesee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2d75d3fe9869f9aa59bf1661a57a8050b9bde4" title="Deepmind learning from Deepseek. Power of open source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xouhskggesee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8a9qb/deepmind_learning_from_deepseek_power_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T18:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i856wr</id>
    <title>Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !</title>
    <updated>2025-01-23T14:57:59+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt; &lt;img alt="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" src="https://preview.redd.it/lxwhx4eicree1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6a7b23129dc6a37671b0f77472359990567d0e4" title="Open-source Deepseek beat not so OpenAI in 'humanity's last exam' !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lxwhx4eicree1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i856wr/opensource_deepseek_beat_not_so_openai_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T14:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i86e4y</id>
    <title>Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model</title>
    <updated>2025-01-23T15:50:30+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt; &lt;img alt="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" src="https://external-preview.redd.it/QaGEWAoaN73yKpJcRFLASUVmy5TY0ehTzhGZuFAVhPY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05b9252fa9a2aa815d8f1c4c41bc8b680d1e4628" title="Scale AI CEO says China has quickly caught the U.S. with the DeepSeek open-source model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/01/23/scale-ai-ceo-says-china-has-quickly-caught-the-us-with-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i86e4y/scale_ai_ceo_says_china_has_quickly_caught_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8ifxd</id>
    <title>Ollama is confusing people by pretending that the little distillation models are "R1"</title>
    <updated>2025-01-24T00:20:07+00:00</updated>
    <author>
      <name>/u/blahblahsnahdah</name>
      <uri>https://old.reddit.com/user/blahblahsnahdah</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was baffled at the number of people who seem to think they're using &amp;quot;R1&amp;quot; when they're actually running a Qwen or Llama finetune, until I saw a screenshot of the Ollama interface earlier. Ollama is misleadingly pretending in their UI and command line that &amp;quot;R1&amp;quot; is a series of differently-sized models and that distillations are just smaller sizes of &amp;quot;R1&amp;quot;. Rather than what they actually are which is some quasi-related experimental finetunes of other models that Deepseek happened to release at the same time.&lt;/p&gt; &lt;p&gt;It's not just annoying, it seems to be doing reputational damage to Deepseek as well, because a lot of low information Ollama users are using a shitty 1.5B model, noticing that it sucks (because it's 1.5B), and saying &amp;quot;wow I don't see why people are saying R1 is so good, this is terrible&amp;quot;. Plus there's misleading social media influencer content like &amp;quot;I got R1 running on my phone!&amp;quot; (no, you got a Qwen-1.5B finetune running on your phone).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blahblahsnahdah"&gt; /u/blahblahsnahdah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-24T00:20:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80cwf</id>
    <title>deepseek is a side project</title>
    <updated>2025-01-23T10:22:48+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt; &lt;img alt="deepseek is a side project" src="https://preview.redd.it/zdvrlxahzpee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df808bd8bc2e4ba90db2fdb005eaae092d5d8206" title="deepseek is a side project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zdvrlxahzpee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i8996r</id>
    <title>I think it's forced. DeepSeek did its best...</title>
    <updated>2025-01-23T17:49:34+00:00</updated>
    <author>
      <name>/u/Alexs1200AD</name>
      <uri>https://old.reddit.com/user/Alexs1200AD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"&gt; &lt;img alt="I think it's forced. DeepSeek did its best..." src="https://preview.redd.it/b3n1jpj17see1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55a5ea362bf2cb802996106f2fc698c1f579cfff" title="I think it's forced. DeepSeek did its best..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alexs1200AD"&gt; /u/Alexs1200AD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b3n1jpj17see1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i8996r/i_think_its_forced_deepseek_did_its_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T17:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i88g4y</id>
    <title>Meta panicked by Deepseek</title>
    <updated>2025-01-23T17:15:55+00:00</updated>
    <author>
      <name>/u/Optimal_Hamster5789</name>
      <uri>https://old.reddit.com/user/Optimal_Hamster5789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt; &lt;img alt="Meta panicked by Deepseek" src="https://preview.redd.it/ek65oz361see1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd236f1570226e841c54a41cd8f2a2e7c6328a8c" title="Meta panicked by Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimal_Hamster5789"&gt; /u/Optimal_Hamster5789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek65oz361see1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T17:15:55+00:00</published>
  </entry>
</feed>
