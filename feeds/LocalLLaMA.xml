<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-23T13:24:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iw4mof</id>
    <title>A book on foundational LLMs</title>
    <updated>2025-02-23T07:21:30+00:00</updated>
    <author>
      <name>/u/s1lv3rj1nx</name>
      <uri>https://old.reddit.com/user/s1lv3rj1nx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I work as an AI consultant. Currently, I am writing a book on foundational LLMs where you will be taught transformers from scratch with intuition, examples, maths and code. Every chapter will be a llm building project in itself. So far, I have completed two chapters where I solve an indic translation problem (vanilla transformer), and local pre training (gpt2). Currently, I am 80% completed on 3rd chapter (llama 3.2).&lt;/p&gt; &lt;p&gt;You will learn everything from: Embedding, positional encodings, different types of attention mechanisms, training strategies, etc. Going ahead, this book will also teach u cuda, flash attention, MoE, MLA, etc. &lt;/p&gt; &lt;p&gt;Does this book sound interesting to you? This was my new year resolution and I feel happy to get the ball rolling. If there are any helping hands as initial set of reviewers, do let me know, either via dm or comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s1lv3rj1nx"&gt; /u/s1lv3rj1nx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4mof/a_book_on_foundational_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4mof/a_book_on_foundational_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4mof/a_book_on_foundational_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:21:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivfwta</id>
    <title>Finally stable</title>
    <updated>2025-02-22T10:30:39+00:00</updated>
    <author>
      <name>/u/StandardLovers</name>
      <uri>https://old.reddit.com/user/StandardLovers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt; &lt;img alt="Finally stable" src="https://preview.redd.it/67mcka284oke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d58089abaf3cb6614bb83158b3a7bf5ad67876d7" title="Finally stable" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Lazarus – Dual RTX 3090 Build&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;GPUs: 2x RTX 3090 @ 70% TDP&lt;/p&gt; &lt;p&gt;CPU: Ryzen 9 9950X&lt;/p&gt; &lt;p&gt;RAM: 64GB DDR5 @ 5600MHz&lt;/p&gt; &lt;p&gt;Total Power Draw (100% Load): ~700watts&lt;/p&gt; &lt;p&gt;GPU temps are stable at 60-70c at max load.&lt;/p&gt; &lt;p&gt;These RTX 3090s were bought used with water damage, and I’ve spent the last month troubleshooting and working on stability. After extensive cleaning, diagnostics, and BIOS troubleshooting, today I finally managed to fit a full 70B model entirely in GPU memory.&lt;/p&gt; &lt;p&gt;Since both GPUs are running at 70% TDP, I’ve temporarily allowed one PCIe power cable to feed two PCIe inputs, though it's still not optimal for long-term stability.&lt;/p&gt; &lt;p&gt;Currently monitoring temps and perfmance—so far, so good!&lt;/p&gt; &lt;p&gt;Let me know if you have any questions or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandardLovers"&gt; /u/StandardLovers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67mcka284oke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivfwta/finally_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T10:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw7b9s</id>
    <title>What's a good model for complex military conflict or nuclear conflict scenarios?</title>
    <updated>2025-02-23T10:31:53+00:00</updated>
    <author>
      <name>/u/BelleHades</name>
      <uri>https://old.reddit.com/user/BelleHades</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially when between real nations, or between fictional nations, or a mix of the two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BelleHades"&gt; /u/BelleHades &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw7b9s/whats_a_good_model_for_complex_military_conflict/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw7b9s/whats_a_good_model_for_complex_military_conflict/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw7b9s/whats_a_good_model_for_complex_military_conflict/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T10:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9lls</id>
    <title>Qwen2.5 1M context works on llama.cpp?</title>
    <updated>2025-02-23T12:59:42+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are these models, but according to model card, &amp;quot;Accuracy degradation may occur for sequences exceeding 262,144 tokens until improved support is added.&amp;quot;&lt;/p&gt; &lt;p&gt;Qwen's blog post talks about &amp;quot;Dual Chunk Attention&amp;quot; that allows this. (&lt;a href="https://qwenlm.github.io/blog/qwen2.5-1m/"&gt;https://qwenlm.github.io/blog/qwen2.5-1m/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The question is - was this already implemented in llama.cpp, and things like LM Studio? &lt;/p&gt; &lt;p&gt;If not - what is a strategy of using these models? Just setting context for 256k and thats it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9lls/qwen25_1m_context_works_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T12:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrtrq</id>
    <title>open source, local AI companion that learns about you and handles tasks for you</title>
    <updated>2025-02-22T20:10:28+00:00</updated>
    <author>
      <name>/u/therealkabeer</name>
      <uri>https://old.reddit.com/user/therealkabeer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/existence-master/sentient"&gt;https://github.com/existence-master/sentient&lt;/a&gt; &lt;/p&gt; &lt;p&gt;my team and I have been building this for a while and we just open-sourced it!&lt;/p&gt; &lt;p&gt;its a personal AI companion that learns facts about you and saves them in a knowledge graph. it can use these &amp;quot;memories&amp;quot; to respond to queries and perform actions like sending emails, preparing presentations and docs, adding calendar events, etc with personal context&lt;/p&gt; &lt;p&gt;it runs fully locally, powered by Ollama and can even search the web, if required. (all user data also stays local) &lt;/p&gt; &lt;p&gt;an initial base graph is prepared from your responses to a personality test and by pulling data from your linkedin, reddit and twitter profiles - this gives the companion some initial context about you.&lt;/p&gt; &lt;p&gt;knowledge graphs are maintained in a neo4j database using a GraphRAG pipeline we built from scratch to retrieve and update knowledge efficiently&lt;/p&gt; &lt;p&gt;future plans include voice mode, browser-use capabilities, the ability to perform actions autonomously, better UI/UX and more!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealkabeer"&gt; /u/therealkabeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtrq/open_source_local_ai_companion_that_learns_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivpa6r</id>
    <title>Abusing WebUI Artifacts (Again)</title>
    <updated>2025-02-22T18:21:42+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt; &lt;img alt="Abusing WebUI Artifacts (Again)" src="https://external-preview.redd.it/cDhodWx2cjZncWtlMQu5ROv8uFTKESGnRAtwEPoYjV9P5uC6sL5S6dTjkckO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96024f10c2840d152729af0e38edf6258ccf9cd5" title="Abusing WebUI Artifacts (Again)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1eav6zr6gqke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivpa6r/abusing_webui_artifacts_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T18:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw4cot</id>
    <title>How did we all miss the release of AutoGen Studio 0.4.1.11? (incorporates new visual drag-and-drop interface for building agent workflows).</title>
    <updated>2025-02-23T07:02:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one that COMPLETELY MISSED THIS MAJOR RELEASE?? I had been waiting for the new Autogen Studio drag-and-drop interface for like 6 months, and apparently it was released like a month ago with a major patch arriving last week. This got pretty much zero press and seems lost in the shuffle due to all the DeepSeek news most likely. AutoGen Studio’s 0,4’s interface is way better than 0.2. They’ve incorporated a ton of stuff, the biggest addition being the drag-and-drop visual workflow interface. I think they also added Magentic One agents. Magentic One was pretty great on its own, but kind of a pain in the ass to get running. Now it’s integrated into AgentChat I believe. This seems like a huge step forward and makes it very compelling and on par with Crew AI in my opinion. &lt;/p&gt; &lt;p&gt;Here is the release page with all the details:&lt;/p&gt; &lt;p&gt;&lt;a href="https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html"&gt;https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the Pypi download page &lt;/p&gt; &lt;p&gt;&lt;a href="https://pypi.org/project/autogenstudio/"&gt;https://pypi.org/project/autogenstudio/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4cot/how_did_we_all_miss_the_release_of_autogen_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvxep</id>
    <title>L2E llama2.c on Commodore C-64</title>
    <updated>2025-02-22T23:16:37+00:00</updated>
    <author>
      <name>/u/AMICABoard</name>
      <uri>https://old.reddit.com/user/AMICABoard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt; &lt;img alt="L2E llama2.c on Commodore C-64" src="https://b.thumbs.redditmedia.com/_5JXsW38zuiDL6biPVP13Lmb8qwW0s56fYU2rf1xacY.jpg" title="L2E llama2.c on Commodore C-64" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you ever wanted to inference tiny stories on a C64 while going about your daily life and then return after many years to read a story? No? Well, as luck would have it, now YOU CAN!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044"&gt;https://preview.redd.it/w8yc07k7wrke1.png?width=725&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f9ae9426ef9ae4cea7c8acd33772becebfcf0044&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/trholding/semu-c64"&gt;https://github.com/trholding/semu-c64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/VulcanIgnis/status/1893420241310335329"&gt;VulcanIgnis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AMICABoard"&gt; /u/AMICABoard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvxep/l2e_llama2c_on_commodore_c64/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtr02</id>
    <title>Local TTS document reader web app (EPUB/PDF)</title>
    <updated>2025-02-22T21:35:57+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt; &lt;img alt="Local TTS document reader web app (EPUB/PDF)" src="https://external-preview.redd.it/N3NpNmJkYnFlcmtlMZSfHsyHsWneNrgFt80RlQiAMZETD9pSG3kMthVQacWZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee91a64c4cffd32d5265d584881203470826fdc5" title="Local TTS document reader web app (EPUB/PDF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/olajvdbqerke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtr02/local_tts_document_reader_web_app_epubpdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:35:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivvoto</id>
    <title>Qwen2.5 VL 7B Instruct GGUF + Benchmarks</title>
    <updated>2025-02-22T23:05:11+00:00</updated>
    <author>
      <name>/u/Ragecommie</name>
      <uri>https://old.reddit.com/user/Ragecommie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;We were able to get Qwen2.5 VL working on llama.cpp!&lt;br /&gt; It is not official yet, but it's pretty easy to get going with a custom build.&lt;br /&gt; Instructions &lt;a href="https://github.com/ggml-org/llama.cpp/issues/11483#issuecomment-2676422772"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Over the next couple of days, we'll upload quants, along with tests / performance evals here:&lt;br /&gt; &lt;a href="https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main"&gt;https://huggingface.co/IAILabs/Qwen2.5-VL-7b-Instruct-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original 16-bit and Q8_0 are up along with the mmproj model.&lt;/p&gt; &lt;p&gt;First impressions are pretty good, not only in terms of quality, but speed as well.&lt;/p&gt; &lt;p&gt;Will post updates and more info as we go!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ragecommie"&gt; /u/Ragecommie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivvoto/qwen25_vl_7b_instruct_gguf_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw0zgo</id>
    <title>It's not that mistral 24b is dry, it's parsable and it rocks!</title>
    <updated>2025-02-23T03:36:00+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to say that, what are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw0zgo/its_not_that_mistral_24b_is_dry_its_parsable_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T03:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivtten</id>
    <title>Perplexity R1 Llama 70B Uncensored GGUFs &amp; Dynamic 4bit quant</title>
    <updated>2025-02-22T21:39:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity I think quietly released uncensored versions of DeepSeek R1 Llama 70B Distilled versions - I actually totally missed this - did anyone see an announcement or know about this?&lt;/p&gt; &lt;p&gt;I uploaded 2bit all the way until 16bit GGUFs for the model: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also uploaded dynamic 4bit quants for finetuning and vLLM serving: &lt;a href="https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few days ago I uploaded dynamic 2bit, 3bit and 4bit quants for the full R1 Uncensored 671B MoE version, which dramatically increase accuracy by not quantizing certain modules. This is similar to the 1.58bit quant of DeepSeek R1 we did! &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivtten/perplexity_r1_llama_70b_uncensored_ggufs_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T21:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw2z5w</id>
    <title>Surprising Performance on CPU-only Ryzen 9 9950x | 64 GB DDR5 Build</title>
    <updated>2025-02-23T05:31:50+00:00</updated>
    <author>
      <name>/u/gmdtrn</name>
      <uri>https://old.reddit.com/user/gmdtrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I wait for my GPU to arrive, I decided to give my CPU-only system a run. I just purchased a bundle from Microcenter for a MSI X870E MAG Tomahawk WiFi motherboard, Ryzen 9 9950x CPU (16 cores, 32 threads), and G.Skill Flare X5 DDR5 RAM (though I upgraded to 64 GB). The OS I'm running is PopOS (Ubuntu derivative).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm getting ~12 tokens/sec on `deepseek-r1:8b` (which is build on Llama3.1:8b) running off the CPU alone&lt;/strong&gt;. I was quite impressed by this as it's out-performing my RTX 2060 mobile by about 30-35%. Thus, it may make for a solid LLM budget build. So, I wanted to share it here.&lt;/p&gt; &lt;p&gt;I hope some of you find this useful. And, I apologize for not performing a more thorough analysis and presenting it here. However, I am up against the clock on a quiz I must take tomorrow that I need to study for.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gmdtrn"&gt; /u/gmdtrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw2z5w/surprising_performance_on_cpuonly_ryzen_9_9950x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw5p9d</id>
    <title>GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!</title>
    <updated>2025-02-23T08:37:31+00:00</updated>
    <author>
      <name>/u/zero_proof_fork</name>
      <uri>https://old.reddit.com/user/zero_proof_fork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"&gt; &lt;img alt="GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!" src="https://external-preview.redd.it/GnlFZWyrYECCoMkLp0lVi1tNxIZHtjUtX7RTE1eOK5M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc7815aa88eb6cdca7dd6f6e74b11f741a4bf50f" title="GitHub - stacklok/mockllm: MockLLM, when you want it to do what you tell it to do!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero_proof_fork"&gt; /u/zero_proof_fork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/stacklok/mockllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw5p9d/github_stacklokmockllm_mockllm_when_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T08:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivyc62</id>
    <title>Chirp 3b | Ozone AI</title>
    <updated>2025-02-23T01:15:13+00:00</updated>
    <author>
      <name>/u/Perfect-Bowl-1601</name>
      <uri>https://old.reddit.com/user/Perfect-Bowl-1601</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;From the same creators of Reverb 7b, we present, &lt;strong&gt;CHIRP 3b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We’re excited to introduce our latest model: &lt;strong&gt;Chirp-3b!&lt;/strong&gt; The Ozone AI team has been pouring effort into this one, and we think it’s a big step up for 3B performance. Chirp-3b was trained on over 50 million tokens of distilled data from GPT-4o, fine-tuned from a solid base model to bring some serious capability to the table.&lt;/p&gt; &lt;p&gt;The benchmarks are in, and Chirp-3b is shining! It’s delivering standout results on both MMLU Pro and IFEval, exceeding what we’d expect from a model this size. Check out the details:&lt;/p&gt; &lt;h3&gt;MMLU Pro&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Subject&lt;/th&gt; &lt;th&gt;Average Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Biology&lt;/td&gt; &lt;td&gt;0.6234&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Business&lt;/td&gt; &lt;td&gt;0.5032&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chemistry&lt;/td&gt; &lt;td&gt;0.3701&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Computer Science&lt;/td&gt; &lt;td&gt;0.4268&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economics&lt;/td&gt; &lt;td&gt;0.5284&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Engineering&lt;/td&gt; &lt;td&gt;0.3013&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Health&lt;/td&gt; &lt;td&gt;0.3900&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;History&lt;/td&gt; &lt;td&gt;0.3885&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Law&lt;/td&gt; &lt;td&gt;0.2252&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Math&lt;/td&gt; &lt;td&gt;0.5736&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Other&lt;/td&gt; &lt;td&gt;0.4145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Philosophy&lt;/td&gt; &lt;td&gt;0.3687&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Physics&lt;/td&gt; &lt;td&gt;0.3995&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Psychology&lt;/td&gt; &lt;td&gt;0.5589&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.4320&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That’s a 9-point boost over the base model—pretty remarkable!&lt;/p&gt; &lt;h3&gt;IFEval&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;72%&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;These gains make Chirp-3b a compelling option for its class. (More benchmarks are on the way!)&lt;/p&gt; &lt;p&gt;Model Card &amp;amp; Download: &lt;a href="https://huggingface.co/ozone-research/Chirp-01"&gt;https://huggingface.co/ozone-research/Chirp-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re passionate about advancing open-source LLMs, and Chirp-3b is a proud part of that journey. We’ve got more models cooking, including 2B and bigger versions, so watch this space!&lt;/p&gt; &lt;p&gt;We’re pumped to get your feedback! Download Chirp-3b, give it a spin, and let us know how it performs for you. Your input helps us keep improving.&lt;/p&gt; &lt;p&gt;Thanks for the support—we’re eager to see what you create with Chirp-3b!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Bowl-1601"&gt; /u/Perfect-Bowl-1601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivyc62/chirp_3b_ozone_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T01:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw3mjz</id>
    <title>Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?</title>
    <updated>2025-02-23T06:13:14+00:00</updated>
    <author>
      <name>/u/grey-seagull</name>
      <uri>https://old.reddit.com/user/grey-seagull</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt; &lt;img alt="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" src="https://preview.redd.it/67czpmm7ztke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebb21e546af6ea899f35bd1b3facbe9552d08b76" title="Why don’t LLMs use alibi? Were these result found be non-reproducible? I’ve only read of the failed Bloom model. Anyone else?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grey-seagull"&gt; /u/grey-seagull &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/67czpmm7ztke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw3mjz/why_dont_llms_use_alibi_were_these_result_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T06:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrprb</id>
    <title>Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer.</title>
    <updated>2025-02-22T20:05:32+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt; &lt;img alt="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." src="https://external-preview.redd.it/8-2Sl3ne20MUsYCwIhDQN3Ob-UIeeembj6eG4654s7k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41c166131e1d55a90ce452e5b495385744c6917d" title="Kimi.ai released Moonlight a 3B/16B MoE model trained with their improved Muon optimizer." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonlight beats other similar SOTA models in most of the benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/Moonlight?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrprb/kimiai_released_moonlight_a_3b16b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9rt1</id>
    <title>DeepSeek crushing it in long context</title>
    <updated>2025-02-23T13:09:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt; &lt;img alt="DeepSeek crushing it in long context" src="https://preview.redd.it/kqree46b1wke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b4f371c949241bcfdbd78e6160ae872ceb045b" title="DeepSeek crushing it in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqree46b1wke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9rt1/deepseek_crushing_it_in_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivrtqk</id>
    <title>DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask</title>
    <updated>2025-02-22T20:10:26+00:00</updated>
    <author>
      <name>/u/cramdev</name>
      <uri>https://old.reddit.com/user/cramdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt; &lt;img alt="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" src="https://external-preview.redd.it/Y2j22dshKg69yVQTELClk4zSnJfoKi77KX2nOwS6buo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba4da483ee892a27f534028e7c20f82a3a3b889f" title="DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cramdev"&gt; /u/cramdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivrtqk/deepseek_founders_are_worth_1_billion_or_150/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ivua9y</id>
    <title>For the love of God, stop abusing the word "multi"</title>
    <updated>2025-02-22T22:00:30+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We trained a SOTA multimodal LLM&amp;quot; and then you dig deep and find it only supports text and vision. These are only two modalities. You trained a SOTA BI-MODAL LLM. &lt;/p&gt; &lt;p&gt;&amp;quot;Our model shows significant improvement in multilingual applications.... The model supports English and Chinese text&amp;quot; yeah... This is a BILINGUAL model. &lt;/p&gt; &lt;p&gt;The word &amp;quot;multi&amp;quot; means &amp;quot;many&amp;quot;. While two is technically &amp;quot;many&amp;quot;, there's a better prefix for that and it is &amp;quot;bi&amp;quot;.&lt;/p&gt; &lt;p&gt;I can't count the number of times people claim they trained a SOTA open model that &amp;quot;beats gpt-4o in multimodal tasks&amp;quot; only to find out the model only supports image and text and not audio (which was the whole point behind gpt-4o anyway) &lt;/p&gt; &lt;p&gt;TLDR: Use &amp;quot;bi&amp;quot; when talking about 2 modalities and languages, use &amp;quot;multi&amp;quot; when talking about 3 or mode.&lt;/p&gt; &lt;p&gt;P.S. I am not downplaying the importance and significance of these open models, but it's better to avoid hyping and deceiving the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ivua9y/for_the_love_of_god_stop_abusing_the_word_multi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-22T22:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw9m8r</id>
    <title>AMD inference using AMDVLK driver is 40% faster than RADV on pp, ~15% faster than ROCm inference performance*</title>
    <updated>2025-02-23T13:00:35+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using 7900 XTX and decided to do some testing after getting intrigued by &lt;a href="/u/fallingdowndizzyvr"&gt;/u/fallingdowndizzyvr&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: AMDVLK is 45% faster than RADV (default Vulkan driver supplied by mesa) on PP (Prompt Processing), but still slower than ROCm. BUT faster than ROCM at TG (Text Generation) by 12-20% (*- though slower on IQ2_XS by 15%). To use, I just installed amdvlk and ran &lt;code&gt;VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/amd_icd64.json ./build/bin/llama-bench ...&lt;/code&gt; (Arch Linux, might be different on other OSes)&lt;/p&gt; &lt;p&gt;Here are some results done on AMD RX 7900 XTX, arch linux, llama.cpp commit &lt;code&gt;51f311e0&lt;/code&gt;, using bartowski GGUFs. I wanted to test different quants and after testing it all it seems like AMDVLK is a much better option for Q4-Q8 quants for tg speed. ROCm still wins on more exotic quants.&lt;/p&gt; &lt;h3&gt;on ROCm, linux&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1414.84 ± 3.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;36.33 ± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;672.70 ± 1.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;22.80 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1407.50 ± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.88 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;671.31 ± 1.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.65 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, default mesa driver, RADV&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;798.98 ± 3.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.72 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;279.68 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;28.96 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;779.84 ± 2.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;41.42 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;331.11 ± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.74 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Vulkan, AMDVLK open source&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1239.63 ± 4.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;14.62 GiB&lt;/td&gt; &lt;td align="right"&gt;14.77 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;43.73 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;394.89 ± 0.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;td align="right"&gt;18.48 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;25.60 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1110.21 ± 10.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi3 14B Q8_0&lt;/td&gt; &lt;td align="right"&gt;13.82 GiB&lt;/td&gt; &lt;td align="right"&gt;13.96 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;46.16 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;463.22 ± 1.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen2 32B IQ2_XS - 2.3125 bpw&lt;/td&gt; &lt;td align="right"&gt;9.27 GiB&lt;/td&gt; &lt;td align="right"&gt;32.76 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;24.38 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T13:00:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw6y8f</id>
    <title>"dry_goods" on LMArena. Llama 4?</title>
    <updated>2025-02-23T10:06:25+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"&gt; &lt;img alt="&amp;quot;dry_goods&amp;quot; on LMArena. Llama 4?" src="https://preview.redd.it/8o9c09js4vke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=007847da1dc8f67a41fffc1eb59a5074a9df3647" title="&amp;quot;dry_goods&amp;quot; on LMArena. Llama 4?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8o9c09js4vke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw6y8f/dry_goods_on_lmarena_llama_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T10:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw1xn7</id>
    <title>The Paradox of Open Weights, but Closed Source</title>
    <updated>2025-02-23T04:29:18+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- An open-weight model has public weights, which you can download from sites like Hugging Face.&lt;/p&gt; &lt;p&gt;- An open-source model has public training code and training dataset, allowing full reproduction. (I didn't come up with that definition, personally I think the dataset requirement is too strict, because then nearly every major model is closed-source.)&lt;/p&gt; &lt;p&gt;- A permissive model has a permissive license, like MIT or Apache 2.0, which means you can do many things with the weights, like serve them over a commercialized inference endpoint. A license like CC-BY-NC is often considered &amp;quot;non-permissive&amp;quot; since the NC means non-commercial.&lt;/p&gt; &lt;p&gt;Kokoro-82M is an Apache 2.0 model that I trained and uploaded to HF &lt;em&gt;without also uploading the accompanying training code or dataset&lt;/em&gt;, thus making it permissive and open-weight, yet also closed-source under the above definitions.&lt;/p&gt; &lt;p&gt;As I've said in the past, there is already MIT-licensed training code at &lt;a href="https://github.com/yl4579/StyleTTS2"&gt;https://github.com/yl4579/StyleTTS2&lt;/a&gt; which others have already used/modified to produce models comparable to, or in some cases better than, Kokoro. But nobody seems to care about that that, they want &lt;em&gt;my&lt;/em&gt; specific training code. Many have speculated why I have not (yet) done this. I'll offer two very practical reasons here—there may be others, but these ones are critical &amp;amp; sufficient.&lt;/p&gt; &lt;p&gt;First, commercial. Obviously, there is commercial value (to me &amp;amp; others) in the code I write, including the training code. Many of those calling for me to release my training code would, undoubtedly, turn around and commercialize that code. On the inference side, I have understood and accepted this reality, and that does not deter me from releasing and improving inference code, especially for other languages. I cannot promise that I'll get there on training.&lt;/p&gt; &lt;p&gt;Second, surge pricing, or basic supply and demand. I have no local NVIDIA GPU and therefore rely on A100 80GB cloud rentals. My training code is specifically configured (in some places hardcoded) for A100 80GB, since these training runs are often vRAM intensive. Unless (or even if) I refactor, open sourcing the training code would probably lead to increased rental demand for the same machines I want, making current and future training runs more expensive. The lowest five A100 80GB prices I see on Vast.ai are $1.1, $1.35, $1.35, $1.41, $1.47, which is typical pricing depth (or lack thereof). Even a handful of people scooping up the cheapest A100s moves the needle quite a lot.&lt;/p&gt; &lt;p&gt;Despite my own training code currently not being released:&lt;/p&gt; &lt;p&gt;- You can train StyleTTS2 models today using the aforementioned MIT training code. I have not gatekept or obfuscated the StyleTTS2 roots of Kokoro—it has been in the README since day 0. Sure, I picked a new model name, but in line with industry standards, it is generally acceptable to name a model when it has substantially new weights.&lt;/p&gt; &lt;p&gt;- Others have/will publish their own training code, for StyleTTS2 models and others.&lt;/p&gt; &lt;p&gt;- There will simply be better open models, in the Kokoro series, in TTS at large, and all modalities in general.&lt;/p&gt; &lt;p&gt;This particular post was motivated by a back-and-forth I had with &lt;a href="/u/Fold-Plastic"&gt;u/Fold-Plastic&lt;/a&gt;. To those who think I am The Enemy for not releasing the training code: I think you are directing way too much animosity towards a permissive-open-weight solo dev operating in a field of non-permissive and closed-weight orgs. It's that sort of animosity that makes open source exhausting rather than rewarding, and pushes devs to leave for the warm embrace of money-printing closed source.&lt;/p&gt; &lt;p&gt;Some other notes:&lt;/p&gt; &lt;p&gt;- I have not yet made a decision on voice cloning, although unlike training code, an encoder release won't spike my A100 costs by +50%, so it is more likely than a training code release.&lt;/p&gt; &lt;p&gt;- For Kokoro, take your voice cloning performance expectations and divide them by 10, since the volume of audio seen during training remains OOMs lower than other TTS models.&lt;/p&gt; &lt;p&gt;- In the meantime, for voice cloning you should be looking at larger TTS models trained on more audio, like XTTS Fish Zonos etc.&lt;/p&gt; &lt;p&gt;- Voice cloning Trump TSwift or Obama may be less &amp;quot;dark magic&amp;quot; and more &amp;quot;retrieval&amp;quot;, assuming those celebrities are in the training dataset (not currently the case for Kokoro).&lt;/p&gt; &lt;p&gt;- Future Kokoro models (i.e. above v1.0) will likely follow a naming scheme like `hexgrad/Kokoro-82M-vX.Y`.&lt;/p&gt; &lt;p&gt;- If voice cloning were to be released, it would change the model naming to `hexgrad/Kokoro-vX.Y`. This is because the encoder is ~25M params, and summing the params across the encoder and the 82M decoder does not feel appropriate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T04:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw4q2e</id>
    <title>Where is Llama 4? I expected that in January.</title>
    <updated>2025-02-23T07:28:16+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all the new release from all the labs, Meta has been quiet. They have the talent and resources. They need to compete. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw4q2e/where_is_llama_4_i_expected_that_in_january/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T07:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iw35xy</id>
    <title>SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity</title>
    <updated>2025-02-23T05:43:36+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt; &lt;img alt="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" src="https://external-preview.redd.it/TCljVIqB29jZGbvnEemLaBHNh4_np29Eo1N9f7IuxMc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dbdd3cfb8dd25b151a368faf5c7855efb0390fd" title="SanDisk's new High Bandwidth Flash memory enables 4TB of VRAM on GPUs, matches HBM bandwidth at higher capacity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iw35xy/sandisks_new_high_bandwidth_flash_memory_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-23T05:43:36+00:00</published>
  </entry>
</feed>
