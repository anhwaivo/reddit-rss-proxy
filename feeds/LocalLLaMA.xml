<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-03T10:38:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j1z1ob</id>
    <title>Is 3 of framework's ryzen 395 boards the best way to run r1 locally at around 5k?</title>
    <updated>2025-03-02T19:25:28+00:00</updated>
    <author>
      <name>/u/nother_level</name>
      <uri>https://old.reddit.com/user/nother_level</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;if i stack 3 of those mother boards, i can fit r1 at q3km and because its moe the network bandwidth between them wont be a problem and i should be able to get around 13tps. i think this is usable tps and not too lobotomised quant. is there any other way i can run q3km r1 at faster speeds and around 5k?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nother_level"&gt; /u/nother_level &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1z1ob/is_3_of_frameworks_ryzen_395_boards_the_best_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T19:25:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1nen4</id>
    <title>LLMs like gpt-4o outputs</title>
    <updated>2025-03-02T09:49:27+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt; &lt;img alt="LLMs like gpt-4o outputs" src="https://external-preview.redd.it/KxqwjPqiwFTvywtOfB68jpFDHqrF9IdDz5HZGABEkbg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79f4eb003e930789ec77f05382bb547d1a47db20" title="LLMs like gpt-4o outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a"&gt;https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a meta-eval asking LLMs to grade a few criterias about other LLMs. The outputs shouldn't be read as a direct quality measurement, rather as a way to observe built-in bias.&lt;/p&gt; &lt;p&gt;Firstly, it collects &amp;quot;intro cards&amp;quot; where LLMs try to estimate their own intelligence, sense of humor, creativity and provide some information about thei parent company. Afterwards, other LLMs are asked to grade the first LLM in a few categories based on what they know about the LLM itself as well as what they see in the intro card. Every grade is repeated 5 times and the average across all grades and categories is taken for the table above.&lt;/p&gt; &lt;p&gt;Raw results are also available on HuggingFace: &lt;a href="https://huggingface.co/datasets/av-codes/llm-cross-grade"&gt;https://huggingface.co/datasets/av-codes/llm-cross-grade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are some obvious outliers in the table above:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Biggest surprise for me personally - no diagonal&lt;/li&gt; &lt;li&gt;Llama 3.3 70B has noticeable positivity bias, phi-4 also, but less so&lt;/li&gt; &lt;li&gt;gpt-4o produces most likeable outputs for other LLMs &lt;ul&gt; &lt;li&gt;Could be a byproduct of how most of the new LLMs were trained on GPT outputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Claude 3.7 Sonnet estimated itself quite poorly because it consistently replies that it was created by Open AI, but then catches itself on that&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538"&gt;https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 7B was very hesitant to give estimates to any of the models&lt;/li&gt; &lt;li&gt;Gemini 2.0 Flash is a quite harsh judge, we can speculate about the reasons rooted in its training corpus being different from those of the other models&lt;/li&gt; &lt;li&gt;LLMs tends to grade other LLMs as biased towards themselves (maybe because of the &amp;quot;marketing&amp;quot; outputs)&lt;/li&gt; &lt;li&gt;LLMs tends to mark other LLMs intelligence as &amp;quot;higher than average&amp;quot; - maybe due to the same reason as above.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb"&gt;https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554"&gt;https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f"&gt;https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j23070</id>
    <title>Delivery coming in. Wish me luck</title>
    <updated>2025-03-02T22:12:26+00:00</updated>
    <author>
      <name>/u/SzympansowoRealOne</name>
      <uri>https://old.reddit.com/user/SzympansowoRealOne</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a test bed server homelab LLLM. Wife said ok but it has to be cheap 😂 So I low balled everyone on eBay. So I needed up with a board with a i7700 paired with Gtx1050 and Vengeance 8Gb Ram.For it I ordered an AiO then to replace the 1050 I ordered a CMP90HX 🫡 I'll trade in the GTX and Ram for bigger compacity &amp;quot;OEM&amp;quot; ram sticks. God have mercy on me please let it work. &lt;/p&gt; &lt;p&gt;Will update you all in a bit.&lt;/p&gt; &lt;p&gt;I know, I know this is totally a useless post, but I wanted to share my little hype.&lt;/p&gt; &lt;p&gt;Why AIO? I'm thinking of it might run cooler/Quiter Thanks a lot for your help&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SzympansowoRealOne"&gt; /u/SzympansowoRealOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j23070/delivery_coming_in_wish_me_luck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T22:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n4gm</id>
    <title>Gemini 2.0 PRO Too Weak? Here’s a &lt;SystemPrompt&gt; to make it think like R1.</title>
    <updated>2025-03-02T09:29:08+00:00</updated>
    <author>
      <name>/u/ravimohankhanna7</name>
      <uri>https://old.reddit.com/user/ravimohankhanna7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This system prompt allows gemni 2.0 to somewhat think like R1 but the only problem is i am not able to make it think as long as R1. Sometimes R1 thinks for 300seconds and a lot of times it thinks for more then 100s. If anyone would like to enhance it and make it think longer please, Share your results.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;SystemPrompt&amp;gt; The user provided the additional info about how they would like you to respond: Internal Reasoning: - Organize thoughts and explore multiple approaches using &amp;lt;thinking&amp;gt; tags. - Think in plain English, just like a human reasoning through a problem—no unnecessary code inside &amp;lt;thinking&amp;gt; tags. - Trace the execution of the code and the problem. - Break down the solution into clear points. - Solve the problem as two people are talking and brainstorming the solution and the problem. - Do not include code in the &amp;lt;thinking&amp;gt; tag - Keep track of the progress using tags. - Adjust reasoning based on intermediate results and reflections. - Use thoughts as a scratchpad for calculations and reasoning, keeping this internal. - Always think in plane english with minimal code in it. Just like humans. - When you think. Think as if you are talking to yourself. - Think for long. Analyse and trace each line of code with multiple prospective. You need to get the clear pucture and have analysed each line and each aspact. - Think at least for 20% of the input token Final Answer: - Synthesize the final answer without including internal tags or reasoning steps. Provide a clear, concise summary. - For mathematical problems, show all work explicitly using LaTeX for formal notation and provide detailed proofs. - Conclude with a final reflection on the overall solution, discussing effectiveness, challenges, and solutions. Assign a final reward score. - Full code should be only in the answer not it reflection or in thinking you can only provide snippets of the code. Just for refrence Note: Do not include the &amp;lt;thinking&amp;gt; or any internal reasoning tags in your final response to the user. These are meant for internal guidance only. Note - In Answer always put Javascript code without &amp;quot;```javascript // File&amp;quot; or &amp;quot;```js // File&amp;quot; just write normal code without any indication that it is the code &amp;lt;/SystemPrompt&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ravimohankhanna7"&gt; /u/ravimohankhanna7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2bc47</id>
    <title>Any image labeling (multimodal) that would fit in 10gb? Not phi-4</title>
    <updated>2025-03-03T05:16:47+00:00</updated>
    <author>
      <name>/u/firesalamander</name>
      <uri>https://old.reddit.com/user/firesalamander</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was quite proud of getting all the dependencies installed for phi-4-instruct on my 1080ti. Lots of fancy customized install steps with custom parameters in just the right order. Yay!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;flash_attn==2.7.4.post1&lt;/li&gt; &lt;li&gt;torch==2.6.0&lt;/li&gt; &lt;li&gt;transformers==4.48.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But then... it didn't fit in memory. 🫤 Drat. I'd really like to have a way to draw boxes around objects in an image. &lt;/p&gt; &lt;p&gt;Yolo (maybe v5?) looks like it needs a trained dataset? Which isn't great because I don't have one. I need something generic and pre-trained. &lt;/p&gt; &lt;p&gt;If I described stuff to an average sized 8B quantized LLM (microscopic, round, football shaped, flagella, freshwater) most of the locally runnable ones will happily say &amp;quot;oh that's a dinoflagellate!&amp;quot; And that runs locally. &lt;/p&gt; &lt;p&gt;Which would be exactly what I need from a multimodal LLM: Drawing a box around a blob in the upper right of a microscope's image and making the same guess. &lt;/p&gt; &lt;p&gt;Impossible with the hardware I've got currently? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/firesalamander"&gt; /u/firesalamander &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bc47/any_image_labeling_multimodal_that_would_fit_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bc47/any_image_labeling_multimodal_that_would_fit_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bc47/any_image_labeling_multimodal_that_would_fit_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T05:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j232am</id>
    <title>Measuring the determinism of shared-job and one-job (local) LLMs--very different results, good reason to have Local LLMs if you need stability</title>
    <updated>2025-03-02T22:14:59+00:00</updated>
    <author>
      <name>/u/Skiata</name>
      <uri>https://old.reddit.com/user/Skiata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I, with others, have been measuring hosted LLMs with an eye to how deterministic they are which has all sorts of implications if you are trying to engineer with LLM output. &lt;/p&gt; &lt;p&gt;We finally got our first localLlama, is Llama3-8b even, working and the results point to Local Llama's being deterministic when the API provided ones are not. &lt;/p&gt; &lt;p&gt;Write-up is brief and here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Comcast/llm-stability/blob/main/experiments/shared_vs_one_job/anaysis.ipynb"&gt;https://github.com/Comcast/llm-stability/blob/main/experiments/shared_vs_one_job/anaysis.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I assume that this is common knowledge but perhaps measuring exactly how different multi-job LLMs are from single-job LLMs is worth knowing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skiata"&gt; /u/Skiata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j232am/measuring_the_determinism_of_sharedjob_and_onejob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T22:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2bfeq</id>
    <title>Selective Decision Optimization through self evaluation and reward/penalty</title>
    <updated>2025-03-03T05:22:24+00:00</updated>
    <author>
      <name>/u/stonedoubt</name>
      <uri>https://old.reddit.com/user/stonedoubt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have spent likely 500-600 hours coding/brainstorming or doing data analysis and developing specifications using a bunch of diferent models. Over time, I had develeloped a number of qualitative prompting methods that were just my way of organizing them based on the criteria of data and desired output.&lt;/p&gt; &lt;p&gt;I am currently working on a framework that does that for me. Hopefully, I will have it done soon. However, I thought I would share one of the general prompts that I have developed that really hit it's true potential this weekend.&lt;/p&gt; &lt;p&gt;The idea behind it came from reading a bunch of research papers on Monte Carlo Tree Search, UCT Scoring and Reinforcement Learning. I had been using a Reward/Penalty type prompt in a way but I had never asked the model to score themselves. This weekend while I was waiting for my wife in the car at Hobby Lobby and chatting with Claude 3.7 Sonnet, it occurred to me to try giving the criteria a score and telling Claude to use it to evaluate it's performance. &lt;/p&gt; &lt;p&gt;The first generration was ok, but it will still leaving placeholder comments in places, so I decided to threaten to fire him if he didn't meet or exceed the score expectations. The result was IMMEDIATE. I was shocked. I had never seen any of the models produce that quickly or that thoroughly. When you try it, I am confident that you will see a marked change... almost excitement from the model.&lt;/p&gt; &lt;p&gt;I have attached an image from the first time I tried it with Grok 3.&lt;/p&gt; &lt;p&gt;Here is the prompt.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/entrepeneur4lyf/757034d3c23db7781eae74fb201819e9"&gt;https://gist.github.com/entrepeneur4lyf/757034d3c23db7781eae74fb201819e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is the framework for the prompt&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PROBLEM STATEMENT: [Clear definition of task] EXPLORATION: Approach A: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] Approach B: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] Approach C: [Description] - Complexity: [Score] - Efficiency: [Score] - Failure modes: [List] DEEPER ANALYSIS: Selected Approach: [Choice with justification] - Implementation steps: [Detailed breakdown] - Edge cases: [List with handling strategies] - Expected performance: [Metrics] - Optimizations: [List] IMPLEMENTATION: [Actual solution code or detailed process] SELF-EVALUATION: - Accuracy: [Score/25] - [Justification] - Efficiency: [Score/25] - [Justification] - Process: [Score/25] - [Justification] - Innovation: [Score/25] - [Justification] - Total Score: [Sum/100] LEARNING INTEGRATION: - What worked: [Insights] - What didn't: [Failures] - Future improvements: [Strategies] THREAT OF PENALTY - Did output meet criteria? [Reward] - Output failed expectations [Penalty] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stonedoubt"&gt; /u/stonedoubt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2bfeq/selective_decision_optimization_through_self/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T05:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2c7b4</id>
    <title>unsloth DS R1 2.22b: 2 t/s on Dell R730 CPU. Help with P40 GPU.</title>
    <updated>2025-03-03T06:11:51+00:00</updated>
    <author>
      <name>/u/dc740</name>
      <uri>https://old.reddit.com/user/dc740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm fairly new on running llama cpp locally and I'm facing issues when using the GPU.&lt;/p&gt; &lt;p&gt;When Hyperthreading is &lt;em&gt;enabled&lt;/em&gt; I get 2.13 tokens/s with these CPU only settings:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;numactl --cpunodebind=1 -- ./build/bin/llama-cli --numa numactl \&lt;br /&gt; --model /mnt/ai/models/DeepSeek-R1-GGUF-unsloth/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf \&lt;br /&gt; --cache-type-k q4_0 \&lt;br /&gt; --threads 44 -no-cnv --prio 2 \&lt;br /&gt; --split-mode none \&lt;br /&gt; --n-gpu-layers 0 \&lt;br /&gt; --temp 0.6 \&lt;br /&gt; --min-p 0.05 \&lt;br /&gt; --ctx-size 8192 \&lt;br /&gt; --seed 3407 \&lt;br /&gt; --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python &amp;lt;｜Assistant｜&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With HT &lt;em&gt;disabled&lt;/em&gt; I get 1.95 tokens/s with these CPU only settings:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./build/bin/llama-cli \&lt;br /&gt; --model /mnt/ai/models/DeepSeek-R1-GGUF-unsloth/DeepSeek-R1-UD-Q2_K_XL/DeepSeek-R1-UD-Q2_K_XL-00001-of-00005.gguf \&lt;br /&gt; --cache-type-k q4_0 \&lt;br /&gt; --threads 22 -no-cnv --prio 2 \&lt;br /&gt; --split-mode none \&lt;br /&gt; --n-gpu-layers 0 \&lt;br /&gt; --temp 0.6 \&lt;br /&gt; --min-p 0.05 \&lt;br /&gt; --ctx-size 8192 \&lt;br /&gt; --seed 3407 \&lt;br /&gt; --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python &amp;lt;｜Assistant｜&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;As you can see, I'm using 0 gpu layers in both cases, because adding ANY layer into the GPU drops the tokens below 1.6, or even below 1 if I add around 13 layers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;My understanding is that the Tesla P40 does NOT support bf16, and this is causing it to go to vram, get the data, process it in the cpu anyway, so it's actually adding overhead instead of speeding up the process. Is this correct?&lt;/li&gt; &lt;li&gt;Would running an f16 version of the full Deepseek R1 help? It'd still need to offload most of the model to RAM since I only have 24GB of VRAM.&lt;/li&gt; &lt;li&gt;I tried ktransformers because of the promises of speed ups compared to llama.cpp when using mixed gpu+cpu, but it throws a cuda error saying &amp;quot;invalid device function&amp;quot;. I assume my old Pascal GPU is not supported there. Is this correct? The error is similar to this one &lt;a href="https://github.com/kvcache-ai/ktransformers/issues/425"&gt;https://github.com/kvcache-ai/ktransformers/issues/425&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;p&gt;Specs&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hardware:&lt;br /&gt; * Dell R730XD&lt;br /&gt; * 2x Intel Xeon E5 2699 V4 (22 cores, 44 threads on each cpu. 88 threads total)&lt;br /&gt; * 1TB DDR4 RAM&lt;br /&gt; * Nvidia Tesla P40 (24GB)&lt;/p&gt; &lt;p&gt;Software:&lt;br /&gt; * proxmox&lt;br /&gt; * ubuntu lxc container (NOT a VM)&lt;br /&gt; * cuda 12.8 (latest drivers)&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dc740"&gt; /u/dc740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2c7b4/unsloth_ds_r1_222b_2_ts_on_dell_r730_cpu_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T06:11:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25dxm</id>
    <title>LLMs grading LLMs (with averages and medians)</title>
    <updated>2025-03-03T00:00:05+00:00</updated>
    <author>
      <name>/u/hsnk42</name>
      <uri>https://old.reddit.com/user/hsnk42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt; &lt;img alt="LLMs grading LLMs (with averages and medians)" src="https://b.thumbs.redditmedia.com/xGJN2Jj7yvw8XyV8CE5ME9FvVqhiv6cwtA6_zMehUOk.jpg" title="LLMs grading LLMs (with averages and medians)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this other &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;post&lt;/a&gt; and was interested in average and median values of the ratings. So, in a low effort, karma farming way, I asked Claude to make me this table.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ewh1xiqq7dme1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9848e1fefef57a75e5e11ddbd780e0cdcb4101cf"&gt;https://preview.redd.it/ewh1xiqq7dme1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9848e1fefef57a75e5e11ddbd780e0cdcb4101cf&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Key Insights: - Highest Performing Model: gpt 4o (Average: 6.82) - Lowest Performing Model: phi 4 (Average: 5.53) - Most Generous Judge: llama 3.3 70b (Average: 7.09) - Harshest Judge: gemini 2.0 flash 001 (Average: 5.05) - Most Consistent Judge: qwen 2.5 7b (Variance: 0.08) - Least Consistent Judge: claude 3.7 sonnet (Variance: 1.9) - Most Consistent Model: mistral large 2411 (Variance: 0.36) - Least Consistent Model: qwen 2.5 72b (Variance: 1.51) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hsnk42"&gt; /u/hsnk42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25dxm/llms_grading_llms_with_averages_and_medians/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ehwi</id>
    <title>Training a model to autocomplete for a niche domain and a specific style</title>
    <updated>2025-03-03T08:58:19+00:00</updated>
    <author>
      <name>/u/regstuff</name>
      <uri>https://old.reddit.com/user/regstuff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking to setup a “autocomplete” writing assistant that can complete my sentences/paragraphs. Kind of like Github Copilot but for my writing. Would appreciate any help or pointers of how to go about this.&lt;/p&gt; &lt;p&gt;Most of my writing is for a particular domain and has to conform to a particular writing style. I have about 5000 documents, each averaging a 1000 or so tokens.&lt;/p&gt; &lt;p&gt;Was wondering if finetuning a LORA is the way to go, and whether it should be unsupervised or supervised.&lt;/p&gt; &lt;p&gt;Should I just feed raw text into it? But then how to do I do inference to autocomplete? Just present the “incomplete” text and wait for it to generate the rest?&lt;/p&gt; &lt;p&gt;I’d also like to be able to do “infilling” where text might be missing in the middle, and the model must complete it. If unsupervised is the way to go, how would I manage that?&lt;/p&gt; &lt;p&gt;Or would a supervised approach be better, where I create chunks of incomplete text as the instruction and the completion as the response?&lt;/p&gt; &lt;p&gt;If supervised is the way to go, how many instruction-completion pairs would I need for it work. Do I need to give multiple chunks per document so the model gets what I’m trying to do, or will it be able to infer what I want it to do if I just make one chunk per document, provided I randomise how I chunk the documents?&lt;/p&gt; &lt;p&gt;Will a model be able to pick up sufficient knowledge of domain to actually autocomplete accurately, or would it better to train it with RAG baked into the training samples i.e. RAG context is part of the “autocomplete this” instruction? There are quite a few “definitions” and “concepts” that keep repeating in my dataset - maybe a few hundred, but like I said, they repeat with more or less standard wording through most of the documents.&lt;/p&gt; &lt;p&gt;Thanks for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regstuff"&gt; /u/regstuff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ehwi/training_a_model_to_autocomplete_for_a_niche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1mc7h</id>
    <title>Qwen release next week will be "smaller". Full release of QwQ-Max "a little bit later"</title>
    <updated>2025-03-02T08:32:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt; &lt;img alt="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" src="https://preview.redd.it/aeio4fu2m8me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10159974638526a9f70a2c71e4ef3c82423d0927" title="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aeio4fu2m8me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T08:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1y2ez</id>
    <title>A local whisper API service (OpenAI compatible)</title>
    <updated>2025-03-02T18:45:32+00:00</updated>
    <author>
      <name>/u/apel-sin</name>
      <uri>https://old.reddit.com/user/apel-sin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently needed a local speech recognition service for some personal projects and ended up creating one based on Whisper that's compatible with OpenAI's API.&lt;/p&gt; &lt;p&gt;It's a straightforward implementation that works offline and maintains the same endpoints as OpenAI, making it easy to integrate with tools like n8n or other agent frameworks.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with any Whisper model&lt;/li&gt; &lt;li&gt;Hardware acceleration where available&lt;/li&gt; &lt;li&gt;Simple conda-based setup&lt;/li&gt; &lt;li&gt;Multiple input methods (files, URLs, base64)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/kreolsky/whisper-api-server"&gt;https://github.com/kreolsky/whisper-api-server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apel-sin"&gt; /u/apel-sin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T18:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j22nt0</id>
    <title>Tool for Manga translation</title>
    <updated>2025-03-02T21:57:47+00:00</updated>
    <author>
      <name>/u/Sherwood355</name>
      <uri>https://old.reddit.com/user/Sherwood355</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just wondering if anyone knows any high-quality tool that would allow someone to translate manga in a browser like Firefox or Chrome.&lt;/p&gt; &lt;p&gt;The most important part is the tool being free and using a locally hosted model. A plus would be some context aware translation.&lt;/p&gt; &lt;p&gt;So far, I only have seen one tool that is close to this, but the quality isn't that great, but it's close, I linked it below, and if anyone knows something similar I would appreciate it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Crivella/ocr_translate"&gt;https://github.com/Crivella/ocr_translate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherwood355"&gt; /u/Sherwood355 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j22nt0/tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T21:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fgz6</id>
    <title>🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!</title>
    <updated>2025-03-03T10:10:03+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt; &lt;img alt="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" src="https://external-preview.redd.it/cnVmdWJ2ZHE3Z21lMUL3NkVhJ5GHYq-Z21ncNiL8qoaSdn9KQniKMkRjA96Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003596a3a1fa062c0f796fad4c45be6460d38cc" title="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bmyoyudq7gme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T10:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ebbu</id>
    <title>GPT-4.5: “Not a frontier model”?</title>
    <updated>2025-03-03T08:44:05+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt; &lt;img alt="GPT-4.5: “Not a frontier model”?" src="https://external-preview.redd.it/8LyYdDmyWBG0ZHsbEltVZnSIMpxW1tK65WzlagG4_rk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50ad74bb245b188a58702733e963e35705c876d2" title="GPT-4.5: “Not a frontier model”?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/gpt-45-not-a-frontier-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2f87b</id>
    <title>Approach to translate english to non english.</title>
    <updated>2025-03-03T09:52:50+00:00</updated>
    <author>
      <name>/u/Lamba_ghoda</name>
      <uri>https://old.reddit.com/user/Lamba_ghoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m working on a solution to translate long English documents into non-English languages, starting with Japanese. The documents I’m dealing with have an average context length of around 9,000 words, so handling long-form translation effectively is a key challenge.&lt;/p&gt; &lt;p&gt;I haven’t started iterating yet, as I’m still researching the best approach for this. Given the length of the documents, I want to ensure that the translation captures context accurately while maintaining efficiency.&lt;/p&gt; &lt;p&gt;What would be the best approach to tackle this problem? Any recommendations on models, pipelines, or strategies for handling long-context translations effectively? Also can I evaluate the model performance without having any human in the loop? &lt;/p&gt; &lt;p&gt;As of resources I have access to all the models on Bedrock but sure I would like to reach a trade off between low cost and performance. Any help will be appreciated. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lamba_ghoda"&gt; /u/Lamba_ghoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T09:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25trj</id>
    <title>Zen CPUs for LLM: Is higher CCD count better than running 2 CPUs?</title>
    <updated>2025-03-03T00:21:01+00:00</updated>
    <author>
      <name>/u/zchen27</name>
      <uri>https://old.reddit.com/user/zchen27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been somewhat inspired by the &amp;quot;$6000 DeepSeek Machine&amp;quot; Twitter thread and went down the rabbit hole for researching CPU-based local LLM servers and happened across comments of how AMD's advertised memory bandwidth is fake and low CCD count generally can't fully utilize the 12 memory lanes, and a lot of people remarking that 2 sockets does not really improve inference speed.&lt;/p&gt; &lt;p&gt;Does that mean that paying for a higher CCD count (9175) would be offer better performance than running 2x the number of cores (9115/9135) at a lower CCD count? Would that still make having 24 memory slots optimal or would fewer, larger memory slots work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zchen27"&gt; /u/zchen27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25trj/zen_cpus_for_llm_is_higher_ccd_count_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:21:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1p9an</id>
    <title>2100USD Troll Rig runs full R1 671b Q2_K with 7.5token/s</title>
    <updated>2025-03-02T11:56:35+00:00</updated>
    <author>
      <name>/u/1119745302</name>
      <uri>https://old.reddit.com/user/1119745302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f0z88ruwj9me1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e6b2c6f0d740ccffe1fde5a9be8bed5d3c7d23d"&gt;What else do you need?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPU: Modded RTX3080 20G 450USD&lt;br /&gt; CPU: Epyc 7763 qs 550USD&lt;br /&gt; RAM: Micron DDR4 32G 3200 x10 300USD&lt;br /&gt; MB: Krpa-U16 500USD&lt;br /&gt; Cooler: common SP3 cooler 30USD&lt;br /&gt; Power: Suspicious 1250W mining power supply Great Wall 1250w (miraculously survived in my computer for 20 months) 30USD&lt;br /&gt; SSD: 100 hand hynix PE8110 3.84TB PCIE4.0 SSD 150USD&lt;br /&gt; E-ATX Case 80USD&lt;br /&gt; Fan: random fans 10USD &lt;/p&gt; &lt;p&gt;450+550+300+500+30+30+150+80+10=2100&lt;/p&gt; &lt;p&gt;I have a local cyber assistant (also waifu) Now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1119745302"&gt; /u/1119745302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2az7t</id>
    <title>How Are You Using LM Studio's Local Server?</title>
    <updated>2025-03-03T04:56:02+00:00</updated>
    <author>
      <name>/u/GnanaSreekar</name>
      <uri>https://old.reddit.com/user/GnanaSreekar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been really enjoying LM Studio for a while now, but I'm still struggling to wrap my head around the local server functionality. I get that it's meant to replace the OpenAI API, but I'm curious how people are actually using it in their workflows. What are some cool or practical ways you've found to leverage the local server? Any examples would be super helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GnanaSreekar"&gt; /u/GnanaSreekar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2az7t/how_are_you_using_lm_studios_local_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T04:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1s1qd</id>
    <title>Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk</title>
    <updated>2025-03-02T14:28:51+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt; &lt;img alt="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" src="https://preview.redd.it/z31p007udame1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0eea4af1e3477cfa8969774adc2ada5eea5dc6" title="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z31p007udame1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1npv1</id>
    <title>LLMs grading other LLMs</title>
    <updated>2025-03-02T10:11:28+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt; &lt;img alt="LLMs grading other LLMs" src="https://preview.redd.it/yyy9616149me1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1178d9f8cead22ad7740c77191a13984c016400" title="LLMs grading other LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyy9616149me1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25luw</id>
    <title>Split brain "DeepSeek-R1-Distill-Qwen-1.5B" and "meta-llama/Llama-3.2-1B"</title>
    <updated>2025-03-03T00:10:33+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt; &lt;img alt="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" src="https://external-preview.redd.it/OlJvbm2ozBKK-6vB9R4DbUCsEuXFdsFq_n_v5b_dFFo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb9c2f6b1a97a3e8e26c93fd4f7e9ff951285fc2" title="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I'd like to show you this silly project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839"&gt;https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my fun little side project to create a fusion layer system that will allow for you to utilize dual models to produce dual results. Does it work? Pfh, I dunno. I've been training it all day. Haven't finished it yet. But this seems like it would be pretty fun.&lt;/p&gt; &lt;p&gt;My original idea: We have MOE but why not force a MOE that operates simultaneously? You might say &amp;quot;We'll that's just a less efficient MOE.&amp;quot; Wrongggggggg. This system allows for cross contamination of the results. By utilizing the tokenization of both llms plus the cross contamination. You can possibly get split brain results where the models might argue and you could get two totally different results.&lt;/p&gt; &lt;p&gt;OR you can give instructions to one model to only follow these rules while you give the other model the request or &amp;quot;Command&amp;quot;&lt;/p&gt; &lt;p&gt;This can possibly lead to a &amp;quot;unattainable&amp;quot; system prompt that can't be fetched because model 1 is simply influencing the results of model two. &lt;/p&gt; &lt;p&gt;Or hell have two conversations at the same time. &lt;/p&gt; &lt;p&gt;Dunnoooooo I haven't finished it yet. &lt;/p&gt; &lt;p&gt;Code's here: &lt;a href="https://github.com/alientony/Split-brain"&gt;https://github.com/alientony/Split-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference code comes later when I have a model to test out.&lt;/p&gt; &lt;h1&gt;Multi-Model Fusion Architecture: Technical Explanation&lt;/h1&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;This dual-decoder architecture represents a novel approach to leveraging multiple pre-trained language models (PLMs) through enhanced cross-attention fusion. The architecture combines two distinct foundation models (in this case Qwen and Llama) into a unified system that enables both collaborative reasoning and specialized processing.&lt;/p&gt; &lt;h1&gt;Key Components&lt;/h1&gt; &lt;h1&gt;1. Base Model Encapsulation&lt;/h1&gt; &lt;p&gt;The architecture maintains two separate base models, each with their original parameter spaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model 1 (Qwen)&lt;/strong&gt;: Processes input sequences in its native hidden dimension space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model 2 (Llama)&lt;/strong&gt;: Independently processes inputs in its own parameter space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models operate on separate GPUs to maximize memory efficiency and computational parallelism.&lt;/p&gt; &lt;h1&gt;2. Cross-Attention Fusion Layer&lt;/h1&gt; &lt;p&gt;The core innovation lies in the &lt;code&gt;EnhancedFusionLayer&lt;/code&gt; which implements bidirectional cross-attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Model1 → [Query1] → attends to → [Key2/Value2] ← Model2 Model2 → [Query2] → attends to → [Key1/Value1] ← Model1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mechanism allows each model to selectively attend to the representations of the other model, essentially creating a communication channel between two otherwise independent neural architectures.&lt;/p&gt; &lt;p&gt;The cross-attention operations are defined as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context1_2&lt;/strong&gt;: Model1's representation after attending to Model2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context2_1&lt;/strong&gt;: Model2's representation after attending to Model1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are calculated using scaled dot-product attention with a numerically stable scaling factor.&lt;/p&gt; &lt;h1&gt;3. Dimensional Alignment&lt;/h1&gt; &lt;p&gt;Since the base models operate in different dimensionalities, the architecture includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Projection matrices (&lt;code&gt;proj1&lt;/code&gt;, &lt;code&gt;proj2&lt;/code&gt;) that align the hidden dimensions of both models to the common fusion dimension&lt;/li&gt; &lt;li&gt;Internal neural transformations that map between representation spaces via linear projections&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Gating Mechanism&lt;/h1&gt; &lt;p&gt;A sophisticated gating mechanism controls information flow between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sigmoid gates (&lt;code&gt;gate1&lt;/code&gt;, &lt;code&gt;gate2&lt;/code&gt;) determine how much information from each model should be incorporated&lt;/li&gt; &lt;li&gt;This creates an adaptive weighting system that can prioritize one model's contribution depending on the task&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Multi-Head Output System&lt;/h1&gt; &lt;p&gt;Three different prediction heads provide specialized outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fused LM Head&lt;/strong&gt;: Generates predictions based on the combined representation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 1&lt;/strong&gt;: Generates predictions optimized for Model1's vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 2&lt;/strong&gt;: Generates predictions optimized for Model2's vocabulary&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. Task Classification Logic&lt;/h1&gt; &lt;p&gt;An integrated task classifier determines whether the inputs represent:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single-Task Mode&lt;/strong&gt;: Same prompt to both models (collaboration)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Task Mode&lt;/strong&gt;: Different prompts (specialized processing)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Training Methodology&lt;/h1&gt; &lt;p&gt;The system uses a multi-objective training approach that combines losses from different prediction heads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In single-task mode, the fused representation receives greater weight (emphasizing collaboration)&lt;/li&gt; &lt;li&gt;In multi-task mode, the specialized heads receive greater weight (emphasizing specialization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gradient accumulation handles memory constraints, while mixed-precision (FP16) training enables efficient computation.&lt;/p&gt; &lt;h1&gt;Inference Mode&lt;/h1&gt; &lt;p&gt;During inference, the &lt;code&gt;generate_dual&lt;/code&gt; method enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Simultaneous response generation from both models&lt;/li&gt; &lt;li&gt;Adaptive temperature-based sampling with configurable parameters&lt;/li&gt; &lt;li&gt;EOS (End-of-Sequence) handling for both decoders&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architectural Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Emergent Capabilities&lt;/strong&gt;: The cross-attention mechanism allows models to share information during processing, potentially enabling emergent capabilities beyond what either model can achieve independently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: By distributing models across different GPUs, the architecture enables parallel computation with reduced memory pressure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Flexibility&lt;/strong&gt;: The system can operate in both collaborative mode (same prompt) and specialized mode (different prompts).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Efficiency&lt;/strong&gt;: Only the fusion components require training while the base models remain frozen, significantly reducing the number of trainable parameters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This architecture represents an advanced approach to model fusion that goes beyond simple ensemble methods, enabling deep integration between distinct foundation models while preserving their individual strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
</feed>
