<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-30T23:06:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kypm3g</id>
    <title>Noticed Deepseek-R1-0528 mirrors user language in reasoning tokensâ€”interesting!</title>
    <updated>2025-05-29T23:35:28+00:00</updated>
    <author>
      <name>/u/Sparkyu222</name>
      <uri>https://old.reddit.com/user/Sparkyu222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt; &lt;img alt="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokensâ€”interesting!" src="https://b.thumbs.redditmedia.com/WRHf27QKCY7p3CNCAxsLaRHX273gAWeS2cN-dG5ubhk.jpg" title="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokensâ€”interesting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally, Deepseek-R1's reasoning tokens were only in English by default. Now it adapts to the user's languageâ€”pretty cool!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sparkyu222"&gt; /u/Sparkyu222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kypm3g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T23:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf6hu</id>
    <title>Where can I use medgemma 27B (medical LLM) for free online? Can't inference it</title>
    <updated>2025-05-30T20:53:15+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf6hu/where_can_i_use_medgemma_27b_medical_llm_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf6hu/where_can_i_use_medgemma_27b_medical_llm_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf6hu/where_can_i_use_medgemma_27b_medical_llm_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz1l5i</id>
    <title>Setup for DeepSeek-R1-0528 (just curious)?</title>
    <updated>2025-05-30T11:14:37+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, just out of curiosity, I really wonder if a suitable setup for the DeepSeek-R1-0528 exists, I mean with &amp;quot;decent&amp;quot; total speed (pp+t/s), context size (let's say 32k) and without needing to rely on a niche backend (like ktransformers)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz1l5i/setup_for_deepseekr10528_just_curious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz1l5i/setup_for_deepseekr10528_just_curious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz1l5i/setup_for_deepseekr10528_just_curious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T11:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyuwkv</id>
    <title>deepseek r1 0528 qwen 8b on android MNN chat</title>
    <updated>2025-05-30T04:02:26+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"&gt; &lt;img alt="deepseek r1 0528 qwen 8b on android MNN chat" src="https://external-preview.redd.it/MHF5ZWNxbGRmdTNmMX8IQ7wMputh-guPLEhiv4RqFz7Hc1SxI_2yIws75pQ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de49549bd457449030d7aeedc287412c7520ad63" title="deepseek r1 0528 qwen 8b on android MNN chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;seems very good for its size&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/81j2f2ldfu3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyuwkv/deepseek_r1_0528_qwen_8b_on_android_mnn_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T04:02:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kykez2</id>
    <title>PSA: Don't waste electricity when running vllm. Use this patch</title>
    <updated>2025-05-29T19:53:38+00:00</updated>
    <author>
      <name>/u/pmur12</name>
      <uri>https://old.reddit.com/user/pmur12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was annoyed by vllm using 100% CPU on as many cores as there are connected GPUs even when there's no activity. I have 8 GPUs connected connected to a single machine, so this is 8 CPU cores running at full utilization. Due to turbo boost idle power usage was almost double compared to optimal arrangement.&lt;/p&gt; &lt;p&gt;I went forward and fixed this: &lt;a href="https://github.com/vllm-project/vllm/pull/16226"&gt;https://github.com/vllm-project/vllm/pull/16226&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The PR to vllm is getting ages to be merged, so if you want to reduce your power cost today, you can use instructions outlined here &lt;a href="https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179"&gt;https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179&lt;/a&gt; to apply fix. This only works when deploying vllm in a container.&lt;/p&gt; &lt;p&gt;There's similar patch to sglang as well: &lt;a href="https://github.com/sgl-project/sglang/pull/6026"&gt;https://github.com/sgl-project/sglang/pull/6026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the way, thumbsup reactions is a relatively good way to make it known that the issue affects lots of people and thus the fix is more important. Maybe the maintainers will merge the PRs sooner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmur12"&gt; /u/pmur12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz3m3f</id>
    <title>gvtop: ðŸŽ® Material You TUI for monitoring NVIDIA GPUs</title>
    <updated>2025-05-30T13:00:32+00:00</updated>
    <author>
      <name>/u/Intelligent_Carry_14</name>
      <uri>https://old.reddit.com/user/Intelligent_Carry_14</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz3m3f/gvtop_material_you_tui_for_monitoring_nvidia_gpus/"&gt; &lt;img alt="gvtop: ðŸŽ® Material You TUI for monitoring NVIDIA GPUs" src="https://b.thumbs.redditmedia.com/mVvxvNY_GZHSRShEWaIsI8TiCQ_K4sX24Oq7nSLDD_A.jpg" title="gvtop: ðŸŽ® Material You TUI for monitoring NVIDIA GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/aw69nhba3x3f1.png?width=1719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d96599c727ef2572967583f8f98c86d935d77e6d"&gt;https://preview.redd.it/aw69nhba3x3f1.png?width=1719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d96599c727ef2572967583f8f98c86d935d77e6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m7me3eba3x3f1.png?width=1719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d8a5aeaa77c4f7d4779bcca4ad976565ab0263"&gt;https://preview.redd.it/m7me3eba3x3f1.png?width=1719&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47d8a5aeaa77c4f7d4779bcca4ad976565ab0263&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello guys!&lt;/p&gt; &lt;p&gt;I hate how nvidia-smi looks, so I made my own TUI, using Material You palettes.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/gvlassis/gvtop"&gt;https://github.com/gvlassis/gvtop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent_Carry_14"&gt; /u/Intelligent_Carry_14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz3m3f/gvtop_material_you_tui_for_monitoring_nvidia_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz3m3f/gvtop_material_you_tui_for_monitoring_nvidia_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz3m3f/gvtop_material_you_tui_for_monitoring_nvidia_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T13:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyt71a</id>
    <title>Deepseek-r1-0528-qwen3-8b is much better than expected.</title>
    <updated>2025-05-30T02:31:33+00:00</updated>
    <author>
      <name>/u/EasyDev_</name>
      <uri>https://old.reddit.com/user/EasyDev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"&gt; &lt;img alt="Deepseek-r1-0528-qwen3-8b is much better than expected." src="https://b.thumbs.redditmedia.com/A3yXEkyGGDloUPterq4_gTTrR5cIudfb3AcKe-EU6vc.jpg" title="Deepseek-r1-0528-qwen3-8b is much better than expected." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past, I tried creating agents with models smaller than 32B, but they often gave completely off-the-mark answers to commands or failed to generate the specified JSON structures correctly. However, this model has exceeded my expectations. I used to think of small models like the 8B ones as just tech demos, but it seems the situation is starting to change little by little.&lt;/p&gt; &lt;p&gt;First image â€“ Structured question request&lt;br /&gt; Second image â€“ Answer&lt;/p&gt; &lt;p&gt;Tested : LMstudio, Q8, Temp 0.6, Top_k 0.95&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyDev_"&gt; /u/EasyDev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kyt71a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyt71a/deepseekr10528qwen38b_is_much_better_than_expected/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T02:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzegpe</id>
    <title>qSpeak - Superwhisper cross-platform alternative now with MCP support</title>
    <updated>2025-05-30T20:23:35+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we've released a new version of qSpeak with advanced support for MCP. Now you can access whatever platform tools wherever you would want in your system using voice. &lt;/p&gt; &lt;p&gt;We've spent a great amount of time to make the experience of steering your system with voice a pleasure. We would love to get some feedback. The app is still completely free so hope you'll like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qspeak.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzegpe/qspeak_superwhisper_crossplatform_alternative_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzegpe/qspeak_superwhisper_crossplatform_alternative_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kysms8</id>
    <title>DeepSeek-R1-0528 Unsloth Dynamic 1-bit GGUFs</title>
    <updated>2025-05-30T02:03:17+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! I made some &lt;strong&gt;dynamic GGUFs for the large R1&lt;/strong&gt; at &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently there is a &lt;strong&gt;IQ1_S (185GB)&lt;/strong&gt; Q2_K_XL (251GB), Q3_K_XL, Q4_K_XL, Q4_K_M versions and other ones, and also full BF16 and Q8_0 versions.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;R1-0528&lt;/th&gt; &lt;th align="left"&gt;R1 Qwen Distil 8B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;GGUFs IQ1_S&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF"&gt;Dynamic GGUFs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-BF16"&gt;Full BF16 version&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit"&gt;Dynamic Bitsandbytes 4bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528"&gt;Original FP8 version&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit"&gt;Bitsandbytes 4bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;Remember to use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; which offloads all MoE layers to disk / RAM. This means &lt;strong&gt;Q2_K_XL needs ~ 17GB of VRAM (RTX 4090, 3090&lt;/strong&gt;) using 4bit KV cache. You'll get ~4 to 12 tokens / s generation or so. 12 on H100.&lt;/li&gt; &lt;li&gt;If you have more VRAM, try &lt;code&gt;-ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot;&lt;/code&gt; instead, which offloads the up and down, and leaves the gate in VRAM. This uses ~70GB or so of VRAM.&lt;/li&gt; &lt;li&gt;And if you have even more VRAM try &lt;code&gt;-ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot;&lt;/code&gt; which offloads only the up MoE matrix.&lt;/li&gt; &lt;li&gt;You can change layer numbers as well if necessary ie &lt;code&gt;-ot &amp;quot;(0|2|3).ffn_(up)_exps.=CPU&amp;quot;&lt;/code&gt; which offloads layers 0, 2 and 3 of up.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;temperature = 0.6, top_p = 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No &lt;code&gt;&amp;lt;think&amp;gt;\n&lt;/code&gt; necessary, but suggested&lt;/li&gt; &lt;li&gt;I'm still doing other quants! &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Also would y'all like a 140GB sized quant? (50 ish GB smaller)?&lt;/strong&gt; The accuracy might be worse, so I decided to leave it at 185GB.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details here:&lt;/strong&gt; &lt;a href="https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally"&gt;&lt;strong&gt;https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are have &lt;strong&gt;XET&lt;/strong&gt; issues, please upgrade it. &lt;code&gt;pip install --upgrade --force-reinstall hf_xet&lt;/code&gt; If you find XET to cause issues, try &lt;code&gt;os.environ[&amp;quot;HF_XET_CHUNK_CACHE_SIZE_BYTES&amp;quot;] = &amp;quot;0&amp;quot;&lt;/code&gt; for Python or &lt;code&gt;export HF_XET_CHUNK_CACHE_SIZE_BYTES=0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Also GPU / CPU offloading for llama.cpp MLA MoEs has been finally fixed - please update llama.cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kysms8/deepseekr10528_unsloth_dynamic_1bit_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T02:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymbcn</id>
    <title>DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro</title>
    <updated>2025-05-29T21:10:08+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt; &lt;img alt="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" src="https://external-preview.redd.it/NXIzbTE5bXRkczNmMTPgNQxrmyDrsqQqm5XEPHINTq7pqExK0opX4bhpHRYD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62607f45a99cf2231166bccc6235669ff6c4e8dc" title="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the updated DeepSeek-R1-0528-Qwen3-8B with 4bit quant in my app to test it on iPhone. It's running with MLX.&lt;/p&gt; &lt;p&gt;It runs which is impressive but too slow to be usable, the model is thinking for too long and the phone get really hot. I wonder if 8B models will be usable when the iPhone 17 drops.&lt;/p&gt; &lt;p&gt;That said, I will add the model on iPad with M series chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mb6zoiqtds3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz01fo</id>
    <title>DeepSeek-R1-0528-Qwen3-8B</title>
    <updated>2025-05-30T09:39:43+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"&gt; &lt;img alt="DeepSeek-R1-0528-Qwen3-8B" src="https://preview.redd.it/grc43exi3w3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3aa78855c2c46d5947ddfd09811953f40904470e" title="DeepSeek-R1-0528-Qwen3-8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/grc43exi3w3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T09:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzfces</id>
    <title>ResembleAI provides safetensors for Chatterbox TTS</title>
    <updated>2025-05-30T21:00:05+00:00</updated>
    <author>
      <name>/u/WackyConundrum</name>
      <uri>https://old.reddit.com/user/WackyConundrum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Safetensors files are now uploaded on Hugging Face:&lt;br /&gt; &lt;a href="https://huggingface.co/ResembleAI/chatterbox/tree/main"&gt;https://huggingface.co/ResembleAI/chatterbox/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And a PR is that adds support to use them to the example code is ready and will be merged in a couple of days:&lt;br /&gt; &lt;a href="https://github.com/resemble-ai/chatterbox/pull/82/files"&gt;https://github.com/resemble-ai/chatterbox/pull/82/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nice!&lt;/p&gt; &lt;p&gt;An examples from the model are here:&lt;br /&gt; &lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;https://resemble-ai.github.io/chatterbox_demopage/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WackyConundrum"&gt; /u/WackyConundrum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzcc3f</id>
    <title>Noob question: Why did Deepseek distill Qwen3?</title>
    <updated>2025-05-30T18:56:24+00:00</updated>
    <author>
      <name>/u/Turbulent-Week1136</name>
      <uri>https://old.reddit.com/user/Turbulent-Week1136</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In unsloth's &lt;a href="https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally"&gt;documentation&lt;/a&gt;, it says &amp;quot;DeepSeek also released a R1-0528 distilled version by fine-tuning Qwen3 (8B).&amp;quot;&lt;/p&gt; &lt;p&gt;Being a noob, I don't understand why they would use Qwen3 as the base and then distill from there and then call it Deepseek-R1-0528. Isn't it mostly Qwen3 and they are taking Qwen3's work and then doing a little bit extra and then calling it DeepSeek? What advantage is there to using Qwen3's as the base? Are they allowed to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Week1136"&gt; /u/Turbulent-Week1136 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz6cbp</id>
    <title>Fiance-Llama-8B: Specialized LLM for Financial QA, Reasoning and Dialogue</title>
    <updated>2025-05-30T14:57:14+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, Just sharing a model release that might be useful for those working on financial NLP or building domain-specific assistants.&lt;/p&gt; &lt;p&gt;Model on Hugging Face: &lt;a href="https://huggingface.co/tarun7r/Finance-Llama-8B"&gt;https://huggingface.co/tarun7r/Finance-Llama-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finance-Llama-8B is a fine-tuned version of Meta-Llama-3.1-8B, trained on the Finance-Instruct-500k dataset, which includes over 500,000 examples from high-quality financial datasets.&lt;/p&gt; &lt;p&gt;Key capabilities:&lt;/p&gt; &lt;p&gt;â€¢ Financial question answering and reasoning&lt;/p&gt; &lt;p&gt;â€¢ Multi-turn conversations with contextual depth&lt;/p&gt; &lt;p&gt;â€¢ Sentiment analysis, topic classification, and NER&lt;/p&gt; &lt;p&gt;â€¢ Multilingual financial NLP tasks&lt;/p&gt; &lt;p&gt;Data sources include: Cinder, Sujet-Finance, Phinance, BAAI/IndustryInstruction_Finance-Economics, and others&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzansb</id>
    <title>Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)</title>
    <updated>2025-05-30T17:48:04+00:00</updated>
    <author>
      <name>/u/dehydratedbruv</name>
      <uri>https://old.reddit.com/user/dehydratedbruv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt; &lt;img alt="Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)" src="https://b.thumbs.redditmedia.com/jGFinxlbFv2rq8NpZZCIWMRhFd5eFKx_XUVhWmNNIUY.jpg" title="Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yappus is a terminal-native LLM interface written in Rust, focused on being local-first, fast, and scriptable. &lt;/p&gt; &lt;p&gt;No GUI, no HTTP wrapper. Just a CLI tool that integrates with your filesystem and shell. I am planning to turn into a little shell inside shell kinda stuff. Integrating with Ollama soon!.&lt;/p&gt; &lt;p&gt;Check out system-specific installation scripts:&lt;br /&gt; &lt;a href="https://yappus-term.vercel.app"&gt;&lt;strong&gt;https://yappus-term.vercel.app&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Still early, but stable enough to use daily. Would love feedback from people using local models in real workflows.&lt;/p&gt; &lt;p&gt;I personally use it to just bash script and google , kinda a better alternative to tldr because it's faster and understand errors quickly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fo8wb12niy3f1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2baa841806904135cc39744a3e6e91d19efd615"&gt;https://preview.redd.it/fo8wb12niy3f1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2baa841806904135cc39744a3e6e91d19efd615&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dehydratedbruv"&gt; /u/dehydratedbruv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T17:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kynytt</id>
    <title>DeepSeek is THE REAL OPEN AI</title>
    <updated>2025-05-29T22:19:53+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every release is great. I am only dreaming to run the 671B beast locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyr9gd</id>
    <title>"Open source AI is catching up!"</title>
    <updated>2025-05-30T00:55:07+00:00</updated>
    <author>
      <name>/u/Overflow_al</name>
      <uri>https://old.reddit.com/user/Overflow_al</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's kinda funny that everyone says that when Deepseek released R1-0528.&lt;/p&gt; &lt;p&gt;Deepseek seems to be the only one really competing in frontier model competition. The other players always have something to hold back, like Qwen not open-sourcing their biggest model (qwen-max).I don't blame them,it's business,I know.&lt;/p&gt; &lt;p&gt;Closed-source AI company always says that open source models can't catch up with them. &lt;/p&gt; &lt;p&gt;Without Deepseek, they might be right.&lt;/p&gt; &lt;p&gt;Thanks Deepseek for being an outlier!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overflow_al"&gt; /u/Overflow_al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T00:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz2o1w</id>
    <title>Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size</title>
    <updated>2025-05-30T12:13:32+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"&gt; &lt;img alt="Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size" src="https://b.thumbs.redditmedia.com/CbbUFJ6UKWwshy_dhxYKGDQcMJhn6z1ajVWlwzo7OzM.jpg" title="Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Xiaomi released an update to its 7B reasoning model, which performs very well on benchmarks, and claims SOTA for its size.&lt;/p&gt; &lt;p&gt;Also, Xiaomi released a reasoning VLM version, which again performs excellent in benchmarks.&lt;/p&gt; &lt;p&gt;Compatible w/ Qwen VL arch so works across vLLM, Transformers, SGLang and Llama.cpp &lt;/p&gt; &lt;p&gt;Bonus: it can reason and is MIT licensed ðŸ”¥&lt;/p&gt; &lt;p&gt;LLM: &lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-RL-0530"&gt;https://huggingface.co/XiaomiMiMo/MiMo-7B-RL-0530&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VLM: &lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL"&gt;https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kz2o1w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzfrdt</id>
    <title>ubergarm/DeepSeek-R1-0528-GGUF</title>
    <updated>2025-05-30T21:17:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt; &lt;img alt="ubergarm/DeepSeek-R1-0528-GGUF" src="https://external-preview.redd.it/_ie2E-L6KKHmnErLSoR3DbJuxwXvA6bw-mpTR5JchI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c275cc69bd7ce1a4060b1155a9689d94d05bdc" title="ubergarm/DeepSeek-R1-0528-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all just cooked up some ik_llama.cpp exclusive quants for the recently updated DeepSeek-R1-0528 671B. New recipes are looking pretty good (lower perplexity is &amp;quot;better&amp;quot;):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-Q8_0&lt;/code&gt; 666GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I didn't upload this, it is for baseline reference only.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ3_K_R4&lt;/code&gt; 301GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2730 +/- 0.01738&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 24GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ2_K_R4&lt;/code&gt; 220GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.5069 +/- 0.01893&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 16GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still might release one or two more e.g. one bigger and one smaller if there is enough interest.&lt;/p&gt; &lt;p&gt;As usual big thanks to Wendell and the whole Level1Techs crew for providing hardware expertise and access to release these quants!&lt;/p&gt; &lt;p&gt;Cheers and happy weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf9nl</id>
    <title>Deepseek is cool, but is there an alternative to Claude Code I can use with it?</title>
    <updated>2025-05-30T20:56:55+00:00</updated>
    <author>
      <name>/u/BITE_AU_CHOCOLAT</name>
      <uri>https://old.reddit.com/user/BITE_AU_CHOCOLAT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for an AI coding framework that can help me with training diffusion models. Take existing quasi-abandonned spaguetti codebases and update them to latest packages, implement papers, add features like inpainting, autonomously experiment using different architectures, do hyperparameter searches, preprocess my data and train for me etc... It wouldn't even require THAT much intelligence I think. Sonnet could probably do it. But after trying the API I found its tendency to deceive and take shortcuts a bit frustrating so I'm still on the fence for the â‚¬110 subscription (although the auto-compact feature is pretty neat). Is there an open-source version that would get me more for my money?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BITE_AU_CHOCOLAT"&gt; /u/BITE_AU_CHOCOLAT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:56:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz5hev</id>
    <title>Why are LLM releases still hyping "intelligence" when solid instruction-following is what actually matters (and they're not that smart anyway)?</title>
    <updated>2025-05-30T14:22:13+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the (somewhat) click bait title, but really, mew LLMs drop, and all of their benchmarks are AIME, GPQA or the nonsense Aider Polyglot. Who cares about these? For actual work like information extraction (even typical QA given a context is pretty much information extraction), summarization, text formatting/paraphrasing, I just need them to FOLLOW MY INSTRUCTION, especially with longer input. These aren't &amp;quot;smart&amp;quot; tasks. And if people still want LLMs to be their personal assistant, there should be more attention to intruction following ability. Assistant doesn't need to be super intellegent, but they need to reliability do the dirty work. &lt;/p&gt; &lt;p&gt;This is even MORE crucial for smaller LLMs. We need those cheap and fast models for bulk data processing or many repeated, day-to-day tasks, and for that, pinpoint instruction-following is everything needed. If they can't follow basic directions reliably, their speed and cheap hardware requirements mean pretty much nothing, however intelligent they are.&lt;/p&gt; &lt;p&gt;Apart from instruction following, tool calling might be the next most important thing. &lt;/p&gt; &lt;p&gt;Let's be real, current LLM &amp;quot;intelligence&amp;quot; is massively overrated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T14:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz0kqi</id>
    <title>Ollama continues tradition of misnaming models</title>
    <updated>2025-05-30T10:13:30+00:00</updated>
    <author>
      <name>/u/profcuck</name>
      <uri>https://old.reddit.com/user/profcuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't really get the hate that Ollama gets around here sometimes, because much of it strikes me as unfair. Yes, they rely on llama.cpp, and have made a great wrapper around it and a very useful setup.&lt;/p&gt; &lt;p&gt;However, their propensity to misname models is very aggravating.&lt;/p&gt; &lt;p&gt;I'm very excited about DeepSeek-R1-Distill-Qwen-32B. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But to run it from Ollama, it's: ollama run deepseek-r1:32b&lt;/p&gt; &lt;p&gt;This is nonsense. It confuses newbies all the time, who think they are running Deepseek and have no idea that it's a distillation of Qwen. It's inconsistent with HuggingFace for absolutely no valid reason.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/profcuck"&gt; /u/profcuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T10:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz48qx</id>
    <title>Even DeepSeek switched from OpenAI to Google</title>
    <updated>2025-05-30T13:29:07+00:00</updated>
    <author>
      <name>/u/Utoko</name>
      <uri>https://old.reddit.com/user/Utoko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt; &lt;img alt="Even DeepSeek switched from OpenAI to Google" src="https://preview.redd.it/uy7wbaj17x3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e5d766354c0e76341191c5702f69924996f4b0e" title="Even DeepSeek switched from OpenAI to Google" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Similar in text Style analyses from &lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt; shows that R1 is now much closer to Google. &lt;/p&gt; &lt;p&gt;So they probably used more synthetic gemini outputs for training. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Utoko"&gt; /u/Utoko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uy7wbaj17x3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T13:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzcalh</id>
    <title>llama-server is cooking! gemma3 27b, 100K context, vision on one 24GB GPU.</title>
    <updated>2025-05-30T18:54:42+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama-server has really improved a lot recently. With vision support, SWA (sliding window attention) and performance improvements I've got 35tok/sec on a 3090. P40 gets 11.8 tok/sec. Multi-gpu performance has improved. Dual 3090s performance goes up to 38.6 tok/sec (600W power limit). Dual P40 gets 15.8 tok/sec (320W power max)! Rejoice P40 crew. &lt;/p&gt; &lt;p&gt;I've been writing more guides for the llama-swap wiki and was very surprised with the results. Especially how usable the P40 still are!&lt;/p&gt; &lt;p&gt;llama-swap config (&lt;a href="https://github.com/mostlygeek/llama-swap/wiki/gemma3-27b-100k-context"&gt;source wiki page&lt;/a&gt;): &lt;/p&gt; &lt;p&gt;```yaml macros: &amp;quot;server-latest&amp;quot;: /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap&lt;/p&gt; &lt;p&gt;# quantize KV cache to Q8, increases context but # has a small effect on perplexity # &lt;a href="https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt; &amp;quot;q8-kv&amp;quot;: &amp;quot;--cache-type-k q8_0 --cache-type-v q8_0&amp;quot;&lt;/p&gt; &lt;p&gt;models: # fits on a single 24GB GPU w/ 100K context # requires Q8 KV quantization &amp;quot;gemma&amp;quot;: env: # 3090 - 35 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 11.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1&amp;quot; cmd: | ${server-latest} ${q8-kv} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# Requires 30GB VRAM # - Dual 3090s, 38.6 tok/sec # - Dual P40s, 15.8 tok/sec &amp;quot;gemma-full&amp;quot;: env: # 3090s - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40s # - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-ea4&amp;quot; cmd: | ${server-latest} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 # uncomment if using P40s # -sm row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kze1r6</id>
    <title>Ollama run bob</title>
    <updated>2025-05-30T20:06:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt; &lt;img alt="Ollama run bob" src="https://preview.redd.it/v4krpd9g7z3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2201e590a1b08cca19a9ca4d26c56ddf0e869e85" title="Ollama run bob" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4krpd9g7z3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:06:52+00:00</published>
  </entry>
</feed>
