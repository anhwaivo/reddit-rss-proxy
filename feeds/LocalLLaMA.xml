<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-23T20:06:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mxh4gk</id>
    <title>🤔 meta X midjourney</title>
    <updated>2025-08-22T20:13:52+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt; &lt;img alt="🤔 meta X midjourney" src="https://preview.redd.it/ayp3r5k9pmkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c32ec6bf7b9de766f15b1cbdfeecbbb7d4e77a3" title="🤔 meta X midjourney" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ayp3r5k9pmkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxu80p</id>
    <title>Finally the upgrade is complete</title>
    <updated>2025-08-23T06:38:51+00:00</updated>
    <author>
      <name>/u/Jaswanth04</name>
      <uri>https://old.reddit.com/user/Jaswanth04</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"&gt; &lt;img alt="Finally the upgrade is complete" src="https://b.thumbs.redditmedia.com/TIofsrVluybO-fbRt8aQtglmzyBhTiWcvfBwPlcXDDU.jpg" title="Finally the upgrade is complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initially had 2 FE 3090. I purchased a 5090, which I was able to get at msrp in my country and finally adjusted in that cabinet &lt;/p&gt; &lt;p&gt;Other components are old, corsair 1500i psu. Amd 3950x cpu Auros x570 mother board, 128 GB DDR 4 Ram. Cabinet is Lian Li O11 dynamic evo xl. &lt;/p&gt; &lt;p&gt;What should I test now? I guess I will start with the 2bit deepseek 3.1 or GLM4.5 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaswanth04"&gt; /u/Jaswanth04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxu80p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T06:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxqljz</id>
    <title>vscode + roo + Qwen3-30B-A3B-Thinking-2507-Q6_K_L = superb</title>
    <updated>2025-08-23T03:14:59+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, the 2507 Thinking variant not the coder.&lt;/p&gt; &lt;p&gt;All the small coder models I tried I kept getting:&lt;/p&gt; &lt;h1&gt;Roo is having trouble...&lt;/h1&gt; &lt;p&gt;I can't even begin to tell you how infuriating this message is. I got this constantly from Qwen 30b coder Q6 and GPT OSS 20b.&lt;/p&gt; &lt;p&gt;Now, though, it just... works. It bounces from architect to coder and occasionally even tests the code, too. I think git auto commits are coming soon, too. I tried the debug mode. That works well, too.&lt;/p&gt; &lt;p&gt;My runner is nothing special:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server.exe -m Qwen_Qwen3-30B-A3B-Thinking-2507-Q6_K_L.gguf -c 131072 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA1,CUDA2 --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I suspect it would work ok with far less context, too. However, when I was watching 30b coder and oss 20b flail around, I noticed they were smashing the context to the max and getting nowhere. 2507 Thinking appears to be particularly frugal with the context in comparison.&lt;/p&gt; &lt;p&gt;I haven't even tried any of my better/slower models, yet. This is basically my perfect setup. Gaming on CUDA0, whilst CUDA1 and CUDA2 are grinding at 90t/s on monitor two.&lt;/p&gt; &lt;p&gt;Very impressed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1my9ulo</id>
    <title>Anyone got a local model working with wolfram alpha?</title>
    <updated>2025-08-23T18:58:12+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you did, how did it go? Was it useful? Were you able to solve problems you couldn't have solved before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my9ulo/anyone_got_a_local_model_working_with_wolfram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my9ulo/anyone_got_a_local_model_working_with_wolfram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my9ulo/anyone_got_a_local_model_working_with_wolfram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T18:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1my88uu</id>
    <title>Help me understand - GPU Layers (Offloading) &amp; Override Tensors - Multiple Questions</title>
    <updated>2025-08-23T17:56:08+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please help me understand - GPU Layers (Offloading) &amp;amp; Override Tensors - Multiple Questions.&lt;/p&gt; &lt;p&gt;System : i7-14700HX 2.10 GHz 4060 &lt;strong&gt;8GB VRAM&lt;/strong&gt; &amp;amp; &lt;strong&gt;32GB RAM&lt;/strong&gt; DDR5. Win11. I use Jan &amp;amp; Koboldcpp.&lt;/p&gt; &lt;p&gt;For example, I tried Q4 of unsloth Qwen3-30B-A3B (&lt;strong&gt;EDIT&lt;/strong&gt; : I'm trying this for MOE models).&lt;/p&gt; &lt;p&gt;Initially I tried -1(-1 for GPU all layers, 0 for CPU only) in GPU Layers field. It gave me only 2-3 t/s.&lt;/p&gt; &lt;p&gt;Then I tried with value 20 in GPU Layers field(got this value from my past thread). It gave me 13-15 t/s. Huge improvement.&lt;/p&gt; &lt;p&gt;Now my questions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) How to come up with right number for GPU Layers(Offloading)?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Though I can do trial &amp;amp; error with different numbers, I want to know the logic/formula behind this thing.&lt;/p&gt; &lt;p&gt;One other reason I want the right number is CPU usage hits 100%(which I don't want) when I tried with value 20 in GPU Layers field which gave me 13-15 t/s.&lt;/p&gt; &lt;p&gt;I'm fine if CPU usage goes upto 70-80%, don't want to hit 100%. Also I'm fine losing few tokens not to hit CPU 100%. For example:&lt;/p&gt; &lt;p&gt;15 t/s with 100% CPU Usage - Not OK&lt;/p&gt; &lt;p&gt;10 t/s with 70-80% CPU Usage - OK&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) If I use other quants such Q5 or Q6 or Q8, same number(20 mentioned above) will work or different number(If yes, what &amp;amp; how)?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 17.7GB - 20&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q5_K_XL - 21.7GB - ??&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q6_K_XL - 26.3GB - ??&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q8_K_XL - 36GB - ??&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Apart from quant, we have Context with different values like 8K, 16K, 32K, 64K, 128K. This also takes additional memory so any changes on number?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3) Now Q4 is giving me 13-15 t/s, Shall I expect similar t/s for higher quants like Q5 or Q6 or Q8?&lt;/strong&gt; I know that answer is NO.&lt;/p&gt; &lt;p&gt;But I just want to know the estimated t/s so I could download suitable quant based on estimated t/s (I don't want to download multiple quants since this model's file sizes are huge).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 17.7GB - 13-15 t/s&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q5_K_XL - 21.7GB - ??&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q6_K_XL - 26.3GB - ??&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B-UD-Q8_K_XL - 36GB - ??&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4) I see that &amp;quot;Override Tensors&amp;quot; is one more way to optimize &amp;amp; increase t/s. What are few optimized regex for Qwen3-30B-A3B with logic?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also I saw people using different regex for same model. Don't know the logic behind those different regex.&lt;/p&gt; &lt;p&gt;Unfortunately regex is too much for Non-Techies &amp;amp; Newbies like me. Still I'm willing to learn just for this.&lt;/p&gt; &lt;p&gt;If I(anyone) understand all above things, I(anyone) could make better settings for other MOE models such as ERNIE-4.5-21B-A3B, Ling-lite-1.5-2506, SmallThinker-21BA3B, Moonlight-16B-A3B, GPT-OSS-20B, OLMoE-1B-7B-0125, etc., to use it with low VRAM. Hope all these answers could help upcoming newbies through this single post.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my88uu/help_me_understand_gpu_layers_offloading_override/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my88uu/help_me_understand_gpu_layers_offloading_override/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my88uu/help_me_understand_gpu_layers_offloading_override/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T17:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxn41d</id>
    <title>DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark</title>
    <updated>2025-08-23T00:22:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt; &lt;img alt="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" src="https://a.thumbs.redditmedia.com/KttrxTIeAWjVzy0nMU4BsHTSl_X9a-QvBbYCoz3pNU0.jpg" title="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxn41d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T00:22:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1my9bxo</id>
    <title>What are your practical, daily uses for small AI models?</title>
    <updated>2025-08-23T18:38:02+00:00</updated>
    <author>
      <name>/u/InsideYork</name>
      <uri>https://old.reddit.com/user/InsideYork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey cloudmeta,&lt;/p&gt; &lt;p&gt;I'm trying to cut through the hype and understand what people are actually using LLMs for in their daily workflows, especially smaller models and fine-tunes that can run locally or on 8gb or CPU only hardware.&lt;/p&gt; &lt;p&gt;I'm not talking about &amp;quot;it can write a poem&amp;quot; or broad claims. I'm talking about specific tasks you've personally stopped Googling, stopped asking on forums for, or stopped doing manually because a model now does it better/faster.&lt;/p&gt; &lt;p&gt;A few examples from my own use:&lt;/p&gt; &lt;p&gt;Replacing initial Stack Overflow searches for boilerplate code (Arduino, Python scripts).&lt;/p&gt; &lt;p&gt;Getting a first draft for emails or content outlines.&lt;/p&gt; &lt;p&gt;Replacing niche blog/forum searches for advice (gardening plans for my climate zone, woodworking joint types).&lt;/p&gt; &lt;p&gt;Replacement: What's a specific activity or consultation you've offloaded to an LLM? The more niche, the better. I was saddened to see that when I looked up cooking I saw very little &lt;a href="https://huggingface.co/mradermacher/gpt2-finetuned-recipes-cooking_v2-i1-GGUF"&gt;https://huggingface.co/mradermacher/gpt2-finetuned-recipes-cooking_v2-i1-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Models: If you use a specific fine-tune or a smaller model (like a fine-tuned CodeLlama, or a local model with a particular dataset) for that task, which do you use? I'm particularly interested in the tools that are hyper-competent at one specific thing (could be a dialect of a programming language too).&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InsideYork"&gt; /u/InsideYork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my9bxo/what_are_your_practical_daily_uses_for_small_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my9bxo/what_are_your_practical_daily_uses_for_small_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my9bxo/what_are_your_practical_daily_uses_for_small_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T18:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxr6zi</id>
    <title>How close can non big tech people get to ChatGPT and Claude speed locally? If you had $10k, how would you build infrastructure?</title>
    <updated>2025-08-23T03:46:39+00:00</updated>
    <author>
      <name>/u/EducationalText9221</name>
      <uri>https://old.reddit.com/user/EducationalText9221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title says, if you had $10k or maybe less, how you achieve infrastructure to run local models as fast as ChatGPT and Claude? Would you build different machines with 5090? Would you stack 3090s on one machine with nvlink (not sure if I understand how they get that many on one machine correctly), add a thread ripper and max ram? Would like to hear from someone that understands more! Also would that build work for fine tuning fine? Thanks in advance!&lt;/p&gt; &lt;p&gt;Edit: I am looking to run different models 8b-100b. I also want to be able to train and fine tune with PyTorch and transformers. It doesn’t have to be built all at once it could be upgraded over time. I don’t mind building it by hand, I just said that I am not as familiar with multiple GPUs as I heard that not all models support it&lt;/p&gt; &lt;p&gt;Edit2: I find local models okay, most people are commenting about models not hardware. Also for my purposes, I am using python to access models not ollama studio and similar things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalText9221"&gt; /u/EducationalText9221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1my0xu3</id>
    <title>Intel's New LLM-Scaler Beta Update Brings Whisper Model &amp; GLM-4.5-Air Support</title>
    <updated>2025-08-23T13:04:50+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Intel-llm-scaler-vllm-Whisper"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my0xu3/intels_new_llmscaler_beta_update_brings_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my0xu3/intels_new_llmscaler_beta_update_brings_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxke42</id>
    <title>a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM</title>
    <updated>2025-08-22T22:24:51+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt; &lt;img alt="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" src="https://a.thumbs.redditmedia.com/0sA5_Mq1e9tKF5OC4u-i_MtdMq9DbQ_Rib1omZfbbO4.jpg" title="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a sample of the full article &lt;a href="https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/"&gt;https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the era of foundation models, multimodal AI, LLMs, and ever-larger datasets, access to raw compute is still one of the biggest bottlenecks for researchers, founders, developers, and engineers. While the cloud offers scalability, building a personal AI Workstation delivers complete control over your environment, latency reduction, custom configurations and setups, and the privacy of running all workloads locally.&lt;/p&gt; &lt;p&gt;This post covers our version of a four-GPU workstation powered by the new NVIDIA RTX 6000 Pro Blackwell Max-Q GPUs. This build pushes the limits of desktop AI computing with 384GB of VRAM (96GB each GPU), all in a shell that can fit under your desk.&lt;/p&gt; &lt;p&gt;[...] &lt;/p&gt; &lt;p&gt;We are planning to test and make a limited number of these custom a16z Founders Edition AI Workstations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxke42"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T22:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxf2sz</id>
    <title>Seed-OSS-36B is ridiculously good</title>
    <updated>2025-08-22T18:54:56+00:00</updated>
    <author>
      <name>/u/mahmooz</name>
      <uri>https://old.reddit.com/user/mahmooz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the model was released a few days ago. it has a native context length of 512k. a pull request has been made to llama.cpp to get support for it.&lt;/p&gt; &lt;p&gt;i just tried running it with the code changes in the pull request. and it works wonderfully. unlike other models (such as qwen3, which has 256k context length supposedly), the model can generate long coherent outputs without refusal.&lt;/p&gt; &lt;p&gt;i tried many other models like qwen3 or hunyuan but none of them are able to generate long outputs and even often complain that the task may be too difficult or may &amp;quot;exceed the limits&amp;quot; of the llm. but this model doesnt even complain, it just gets down to it. one other model that also excels at this is glm-4.5 but its context length is much smaller unfortunately.&lt;/p&gt; &lt;p&gt;seed-oss-36b also apparently has scored 94 on ruler at 128k context which is insane for a 36b model (it was reported by the maintainer of chatllm.cpp).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mahmooz"&gt; /u/mahmooz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T18:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxykmq</id>
    <title>Apple M3 Ultra 512GB vs NVIDIA RTX 3090 LLM Benchmark</title>
    <updated>2025-08-23T11:06:38+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxykmq/apple_m3_ultra_512gb_vs_nvidia_rtx_3090_llm/"&gt; &lt;img alt="Apple M3 Ultra 512GB vs NVIDIA RTX 3090 LLM Benchmark" src="https://b.thumbs.redditmedia.com/HRjbVNphub8pnfO0iMk1Mcm8fulKaJqlOmaGqy54kRU.jpg" title="Apple M3 Ultra 512GB vs NVIDIA RTX 3090 LLM Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🔥 Apple M3 Ultra 512GB vs NVIDIA RTX 3090 LLM Benchmark Results Running Qwen3-30B-A3B (Q4_K_M) on llamacpp and 4bit on MLX&lt;/p&gt; &lt;p&gt;I think we need more of these comparisons! It took a lot of time to setup everything, so let's share results!&lt;br /&gt; pp512:&lt;br /&gt; 🥇M3 w/ MLX: 2,320 t/s&lt;br /&gt; 🥈 3090: 2,157 t/s&lt;br /&gt; 🥉 M3 w/ Metal: 1,614 t/s &lt;/p&gt; &lt;p&gt;tg128:&lt;br /&gt; 🥇 3090: 136 t/s&lt;br /&gt; 🥈 M3 w/ MLX: 97 t/s&lt;br /&gt; 🥉 M3 w/ Metal: 86 t/s&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7f1bj2ag4rkf1.png?width=2522&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44184256681e46b0bb2d8324c4d6abda8b7f4266"&gt;https://preview.redd.it/7f1bj2ag4rkf1.png?width=2522&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44184256681e46b0bb2d8324c4d6abda8b7f4266&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxykmq/apple_m3_ultra_512gb_vs_nvidia_rtx_3090_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxykmq/apple_m3_ultra_512gb_vs_nvidia_rtx_3090_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxykmq/apple_m3_ultra_512gb_vs_nvidia_rtx_3090_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T11:06:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mybft5</id>
    <title>grok 2 weights</title>
    <updated>2025-08-23T20:00:52+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt; &lt;img alt="grok 2 weights" src="https://external-preview.redd.it/4tfHT9vpFrwHCpX5cn0_tHyoUS8M6oeQ7jwWbePCicw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9576154cc1820a09f2c9b345d4d88427c3729b9a" title="grok 2 weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/xai-org/grok-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxrarl</id>
    <title>NVIDIA new paper : Small Language Models are the Future of Agentic AI</title>
    <updated>2025-08-23T03:51:55+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA have just published a paper claiming SLMs (small language models) are the future of agentic AI. They provide a number of claims as to why they think so, some important ones being they are cheap. Agentic AI requires just a tiny slice of LLM capabilities, SLMs are more flexible and other points. The paper is quite interesting and short as well to read. &lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/pdf/2506.02153"&gt;https://arxiv.org/pdf/2506.02153&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video Explanation : &lt;a href="https://www.youtube.com/watch?v=6kFcjtHQk74"&gt;https://www.youtube.com/watch?v=6kFcjtHQk74&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1my809f</id>
    <title>MasonMac/WildChat-4.8M-EN-Semantic-Deduplicated · Datasets at Hugging Face</title>
    <updated>2025-08-23T17:47:02+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my809f/masonmacwildchat48mensemanticdeduplicated/"&gt; &lt;img alt="MasonMac/WildChat-4.8M-EN-Semantic-Deduplicated · Datasets at Hugging Face" src="https://external-preview.redd.it/96tXshMnQZ1xXVAqO8eDCfYiyMJBMS-3iILOFDnNdHY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c3af10de82016d88fa024b822a027ac7e56b5d4" title="MasonMac/WildChat-4.8M-EN-Semantic-Deduplicated · Datasets at Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a collection of semantically deduplicated datasets derived from WildChat-4.8M. I hope it may be helpful to you guys :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/MasonMac/WildChat-4.8M-EN-Semantic-Deduplicated"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my809f/masonmacwildchat48mensemanticdeduplicated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my809f/masonmacwildchat48mensemanticdeduplicated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T17:47:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx8qki</id>
    <title>I'm making a game where all the dialogue is generated by the player + a local llm</title>
    <updated>2025-08-22T14:55:48+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt; &lt;img alt="I'm making a game where all the dialogue is generated by the player + a local llm" src="https://external-preview.redd.it/dGRvYnNtbjM0bGtmMcsYHmRxX6l-GOXVgL0nfvRqWRvtCbG6hh3bmeu2mYuD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51833070477bb979f4af1952d3badd650277cef2" title="I'm making a game where all the dialogue is generated by the player + a local llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oitg5nn34lkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T14:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxwwsk</id>
    <title>AI models playing chess – not strong, but an interesting benchmark!</title>
    <updated>2025-08-23T09:27:46+00:00</updated>
    <author>
      <name>/u/Apart-Ad-1684</name>
      <uri>https://old.reddit.com/user/Apart-Ad-1684</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt; &lt;img alt="AI models playing chess – not strong, but an interesting benchmark!" src="https://external-preview.redd.it/ys7jEoBiKiu8EwYd0V5EewPsg3PtK6u6uh3HZ7U-N5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9ff9da34f54c9079bce0fa6ea2619c1c98dc000" title="AI models playing chess – not strong, but an interesting benchmark!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I’ve been working on &lt;a href="https://chess.louisguichard.fr"&gt;LLM Chess Arena&lt;/a&gt;, an application where large language models play chess against each other.&lt;/p&gt; &lt;p&gt;The games aren’t spectacular, because LLMs aren’t really good at chess — but that’s exactly what makes it interesting! Chess highlights their reasoning gaps in a simple and interpretable way, and it’s fun to follow their progress.&lt;/p&gt; &lt;p&gt;The app let you &lt;strong&gt;launch your own AI vs AI games&lt;/strong&gt; and features a live &lt;strong&gt;leaderboard&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Curious to hear your thoughts!&lt;/p&gt; &lt;p&gt;🎮 App: &lt;a href="http://chess.louisguichard.fr"&gt;chess.louisguichard.fr&lt;/a&gt;&lt;br /&gt; 💻 Code: &lt;a href="https://github.com/louisguichard/llm-chess-arena"&gt;https://github.com/louisguichard/llm-chess-arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j33oc7x5mqkf1.png?width=2834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47858fb05effa3fea9e591fb6e7fda6a82f9db5f"&gt;https://preview.redd.it/j33oc7x5mqkf1.png?width=2834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47858fb05effa3fea9e591fb6e7fda6a82f9db5f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Ad-1684"&gt; /u/Apart-Ad-1684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T09:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1my5mve</id>
    <title>Crucible's Mistral 3.2 24B V1.3 Tune</title>
    <updated>2025-08-23T16:15:45+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/CrucibleLab/M3.2-24B-Loki-V1.3"&gt;https://huggingface.co/CrucibleLab/M3.2-24B-Loki-V1.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello all! This model has been meticulously trained on a specialized, 370 million token dataset, curated specifically for high-quality role-playing. The dataset is built upon a foundation of well-established worlds and lore, providing the model with deep knowledge across a wide array of genres.&lt;/p&gt; &lt;p&gt;More information on the model card!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my5mve/crucibles_mistral_32_24b_v13_tune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my5mve/crucibles_mistral_32_24b_v13_tune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my5mve/crucibles_mistral_32_24b_v13_tune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T16:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1myb09v</id>
    <title>Google and Anthropic struggle to keep marketshare as everyone else catches up</title>
    <updated>2025-08-23T19:44:10+00:00</updated>
    <author>
      <name>/u/ObnoxiouslyVivid</name>
      <uri>https://old.reddit.com/user/ObnoxiouslyVivid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt; &lt;img alt="Google and Anthropic struggle to keep marketshare as everyone else catches up" src="https://preview.redd.it/35p1pim9ntkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e48d4b2543aa0cd859924de94edd03937a9fc35a" title="Google and Anthropic struggle to keep marketshare as everyone else catches up" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data from last 6 months on OpenRouter compared to now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObnoxiouslyVivid"&gt; /u/ObnoxiouslyVivid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p1pim9ntkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T19:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxwzoh</id>
    <title>Can anyone explain why the pricing of gpt-oss-120B is supposed to be lower than Qwen 3 0.6 b?</title>
    <updated>2025-08-23T09:33:05+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwzoh/can_anyone_explain_why_the_pricing_of_gptoss120b/"&gt; &lt;img alt="Can anyone explain why the pricing of gpt-oss-120B is supposed to be lower than Qwen 3 0.6 b?" src="https://preview.redd.it/wigjs6bmnqkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eea56015567fe0eea75a5f12991c1378a26899d1" title="Can anyone explain why the pricing of gpt-oss-120B is supposed to be lower than Qwen 3 0.6 b?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://artificialanalysis.ai/models/qwen3-0.6b-instruct-reasoning"&gt;Qwen3 0.6B (Reasoning) - Intelligence, Performance &amp;amp; Price Analysis | Artificial Analysis&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wigjs6bmnqkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwzoh/can_anyone_explain_why_the_pricing_of_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwzoh/can_anyone_explain_why_the_pricing_of_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T09:33:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1my1jg7</id>
    <title>ByteDance Seed OSS 36B supported in llama.cpp</title>
    <updated>2025-08-23T13:31:21+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/b1afcab804e3281867a5471fbd701e32eb32e512"&gt;https://github.com/ggml-org/llama.cpp/commit/b1afcab804e3281867a5471fbd701e32eb32e512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still no native support for serverside thinking tag parsing since Seed uses a new seed:think tag, so will have to add that later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxvyll</id>
    <title>DeepConf: 99.9% Accuracy on AIME 2025 with Open-Source Models + 85% Fewer Tokens</title>
    <updated>2025-08-23T08:27:18+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this new method called &lt;strong&gt;DeepConf (Deep Think with Confidence)&lt;/strong&gt; looks super interesting.&lt;/p&gt; &lt;p&gt;It’s the &lt;strong&gt;first approach to hit 99.9% on AIME 2025&lt;/strong&gt; using an open-source model (&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt;) &lt;em&gt;without tools&lt;/em&gt;. What really stands out is that it not only pushes accuracy but also massively cuts down token usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;~10% accuracy boost across multiple models &amp;amp; datasets&lt;/p&gt; &lt;p&gt;Up to 85% fewer tokens generated → much more efficient&lt;/p&gt; &lt;p&gt;Plug-and-play: works with any existing model, no training or hyperparameter tuning required&lt;/p&gt; &lt;p&gt;Super simple to deploy: just ~50 lines of code in vLLM (see PR)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;📚 Paper: &lt;a href="https://arxiv.org/pdf/2508.15260"&gt;https://arxiv.org/pdf/2508.15260&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🌐 Project: &lt;a href="https://jiaweizzhao.github.io/deepconf"&gt;https://jiaweizzhao.github.io/deepconf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;twitter post: &lt;a href="https://x.com/jiawzhao/status/1958982524333678877"&gt;https://x.com/jiawzhao/status/1958982524333678877&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T08:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1my39ja</id>
    <title>It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)</title>
    <updated>2025-08-23T14:42:37+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt; &lt;img alt="It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)" src="https://external-preview.redd.it/-5dEDvOvZEMy2pHuEuazSJpFNmFIHXICPgHs74NtU5U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47bcb8449e292f736972fc476ddb801eee0e77e6" title="It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the recent release of not one but two transformers-mamba hybrids both claiming to outperform baseline transformers, I thought this would be a fun application of ReasonScape to see what's going on under the hood.&lt;/p&gt; &lt;h1&gt;Test Model 1: Falcon-H1 7B&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://falcon-lm.github.io/blog/falcon-h1/"&gt;https://falcon-lm.github.io/blog/falcon-h1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7i2z9yciyrkf1.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1d03fc28117947e2313a514e051fabba3e01682"&gt;Claim: Falcon-7B (61.8) outperforms Qwen3-8B (58.5)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test Model 2: NVidia Nemotron Nano v2&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ao6fzh5tyrkf1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb457ae99043c267682b39ce4c29581daa1f7e64"&gt;Claim: Nemotron-Nano-9B outperforms Qwen3-8B across the board&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Reference Model 1: Qwen3-8B OG&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;https://qwenlm.github.io/blog/qwen3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;https://huggingface.co/Qwen/Qwen3-8B&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Reference Model 2: Qwen3-4B-2507-Instruct&lt;/h1&gt; &lt;p&gt;Blog: &lt;a href="https://qwen3lm.com/qwen3-4b-instruct-2507/"&gt;https://qwen3lm.com/qwen3-4b-instruct-2507/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test Setup&lt;/h1&gt; &lt;p&gt;All models were evaluated with 2x RTX3090 using vLLM 0.10.1&lt;/p&gt; &lt;p&gt;Nemotron Nano v2 was launched with the recommended &lt;code&gt;--mamba_ssm_cache_dtype float32&lt;/code&gt; flag.&lt;/p&gt; &lt;p&gt;The evaluation being performed here is one of my design: ReasonScape M6. See &lt;a href="https://reasonscape.com/"&gt;https://reasonscape.com/&lt;/a&gt; for details and documentation.&lt;/p&gt; &lt;h1&gt;Results: Difficulty Tiered Leaderboards&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cfscchg50skf1.png?width=1137&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d81f8f61ee585eca5e9dd8eb9283e3382f3fce9"&gt;Hybrid-SSM Results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron Nano v2 demonstrates &lt;strong&gt;significantly improved all-around complexity robustness&lt;/strong&gt; over Falcon-H1, but it does as the expense of &lt;strong&gt;3x thinking tokens.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1x226ztf0skf1.png?width=1136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3126d6e6fdd0133a5ba248d069748c2df46aa1ef"&gt;Qwen3 Results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Performance on the &lt;strong&gt;Boolean, Dates&lt;/strong&gt; and &lt;strong&gt;Movies&lt;/strong&gt; tasks (see &lt;a href="https://reasonscape.com/docs/tasks/"&gt;https://reasonscape.com/docs/tasks/&lt;/a&gt; for more info on the tasks!) is indeed comparable but the &lt;strong&gt;Objects&lt;/strong&gt;, &lt;strong&gt;Arithmetic&lt;/strong&gt; and &lt;strong&gt;Shuffle&lt;/strong&gt; tasks present significant challenges for the hybrids. &lt;/p&gt; &lt;p&gt;The old Qwen3 models &lt;strong&gt;think way too much&lt;/strong&gt; but the new 2507-Instruct do really well when simply asked to &lt;em&gt;&amp;quot;think-step-by-step&amp;quot;.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Results: Performance Surfaces&lt;/h1&gt; &lt;p&gt;I will merge the Test and Reference sets together for the remainder of plots to make comparisons easier:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o264zvgb1skf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63420e7384da7c0f4dd3a3387a2023cf1e67f804"&gt;ReasonScape M6 Difficulty Manifolds for the 4 models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron &lt;strong&gt;Dates&lt;/strong&gt; processing is robust but &lt;strong&gt;Objects&lt;/strong&gt; (a selective attention task) collapses in both difficulty dimensions very quickly compared to pure transformers. &lt;strong&gt;Arithmetic&lt;/strong&gt; (under randomized whitespace conditions) holds up ok with depth, but collapses under length. &lt;strong&gt;Shuffle&lt;/strong&gt; (a working memory churn task) shows a similar pattern: depth is ok, but total collapse under length leading to a smaller island of competency.&lt;/p&gt; &lt;p&gt;All models struggled with truncation on the &lt;strong&gt;Boolean&lt;/strong&gt; task, but Falcon least so.&lt;/p&gt; &lt;h1&gt;Results: Token-FFT Analysis&lt;/h1&gt; &lt;p&gt;ReasonScape offers a unique kind of plot, showing exactly how chat template and tokenization affect the frequency-domain representation of what the LLM actually sees.&lt;/p&gt; &lt;p&gt;These allow to peek even below the surfaces and understand WHY some things are tougher for certain models and split training problems from architectural problems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4nqoy43d2skf1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=acd11dcdc896c0392529a2f172bcdaeb7334f04a"&gt;Token-FFT: Arithmetic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here we see exactly why Nemotron isn't very good at arithmetic:&lt;/p&gt; &lt;p&gt;- The whitespace/no-whitespace representations of math problems look VERY different to this tokenizer and it has had trouble generalizing as a result&lt;/p&gt; &lt;p&gt;- As length increases, the information content .. disappears! No change at DC, but the middle and high-band information is lost. Performance predictably collapses as a result.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8c0zoiv73skf1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9374f97bf696d29d40084700b219e41e7a7ed8a1"&gt;Token-FFT: Boolean&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An interesting comparison here is the Boolean task which demonstrates similar information-compression along with the ON/OFF and YES/NO formats. These formats have the weakest results on the surfaces compared to the others (because at the end of the day, compressing your signal is bad) but they manage to eek out &amp;quot;satisfactory&amp;quot; scores because the DC had a corresponding upward shift. This is a 'lower-tier of information loss' vs when the DC stays the same and we just lose signal.&lt;/p&gt; &lt;h1&gt;Conclusions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Nemotron Nano is the most powerful hybrid I've evaluated so far.&lt;/strong&gt; It's major weakness is that it seems to have failed to generalize Arithmetic and it's selective attention (information-filtering ability) is noticeably weaker then SOTA transformers. Mid-tier for reasoning length.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;While Hybrids are getting better, they don't yet beat pure Transformers&lt;/strong&gt; when I evaluated Falcon-Mamba it got a big fat 0 - these new hybrid guys actually do work and are getting better with each iteration. I hope to see this conclusion flip in the future!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-4B-Instruct-2507 is a little beast&lt;/strong&gt; and can replace older 8B with similar if not better performance and lower token usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I need more RTX3090&lt;/strong&gt; as these evaluations require up to 100M tokens when the average responses get up to 3-4k.&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;To learn more about ReasonScape evaluations check out the Documentation at &lt;a href="https://reasonscape.com/docs/"&gt;https://reasonscape.com/docs/&lt;/a&gt; or grab the latest code from GitHub at &lt;a href="https://github.com/the-crypt-keeper/reasonscape"&gt;https://github.com/the-crypt-keeper/reasonscape&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you enjoyed the plots, check out the M6 explorer &lt;a href="https://reasonscape.com/m6/explorer/"&gt;https://reasonscape.com/m6/explorer/&lt;/a&gt; and it's documentation &lt;a href="https://reasonscape.com/docs/tools/explorer/"&gt;https://reasonscape.com/docs/tools/explorer/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2hwrdrug6skf1.png?width=1848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5d69ab1018467ca9ef8445d022dd76df0c73544"&gt;M6 explorer showing detailed result projections along the Arithmetic surface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To see how these models compare to the rest of the flocks, the full M6 Leaderboard is available at &lt;a href="https://reasonscape.com/m6/leaderboard/"&gt;https://reasonscape.com/m6/leaderboard/&lt;/a&gt; (spoiler: &lt;strong&gt;GPT-OSS-20b is a broken mess&lt;/strong&gt;) with documentation at &lt;a href="https://reasonscape.com/docs/tools/leaderboard/"&gt;https://reasonscape.com/docs/tools/leaderboard/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading! &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T14:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1my7j1x</id>
    <title>support for ByteDance Seed-OSS model has been merged into llama.cpp</title>
    <updated>2025-08-23T17:29:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"&gt; &lt;img alt="support for ByteDance Seed-OSS model has been merged into llama.cpp" src="https://external-preview.redd.it/WFGEPRY69pmnCNsVihL350z048IpLks_fdEjrmNlkmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=447948ecb5a77a2d635a9ce18c86729398f84896" title="support for ByteDance Seed-OSS model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15490"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my7j1x/support_for_bytedance_seedoss_model_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T17:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1my3why</id>
    <title>RTX PRO 6000 MAX-Q Blackwell for LLM</title>
    <updated>2025-08-23T15:08:27+00:00</updated>
    <author>
      <name>/u/AdventurousSwim1312</name>
      <uri>https://old.reddit.com/user/AdventurousSwim1312</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received my brand new Blackwell card, so did a quick bench to let the community grasp the pros and cons&lt;/p&gt; &lt;h1&gt;Setup Details:&lt;/h1&gt; &lt;p&gt;GPU : Rtx pro 6000 max-q workstation edition, 12% less TFLOPs than the complete, but with half the power draw, on 2 slots and with same memory bandwidth.&lt;/p&gt; &lt;p&gt;CPU : Ryzen 9 3950X, 24 channels, 16 cores / 32 threads&lt;/p&gt; &lt;p&gt;RAM : 128go DDR4 3600Ghz&lt;/p&gt; &lt;p&gt;GPU1 : RTX 3090 24gb blower edition. 2 slots, unused here&lt;/p&gt; &lt;p&gt;GPU2 : RTX 3090 24gb founder edition. 3 slots, unused here&lt;/p&gt; &lt;h1&gt;Software details&lt;/h1&gt; &lt;h1&gt;OS&lt;/h1&gt; &lt;p&gt;- Ubuntu 22.04&lt;/p&gt; &lt;p&gt;- Nvidia Drivers : 770 open &lt;/p&gt; &lt;p&gt;- Cuda toolkit 13&lt;/p&gt; &lt;p&gt;- Cudnn 9&lt;/p&gt; &lt;p&gt;(ask if you want a quick install tutorial in comments)&lt;/p&gt; &lt;h1&gt;Env&lt;/h1&gt; &lt;p&gt;conda create --name vllm python=3.12&lt;/p&gt; &lt;p&gt;conda activate vllm&lt;/p&gt; &lt;p&gt;uv pip install flashinfer-python --prerelease=allow --upgrade --extra-index-url &lt;a href="https://download.pytorch.org/whl/nightly/cu128"&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/a&gt;&lt;/p&gt; &lt;p&gt;uv pip install vllm --torch-backend=cu128&lt;/p&gt; &lt;h1&gt;Training Benchmark&lt;/h1&gt; &lt;p&gt;Two stuff are diferenciating for training on that card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the number of tensor core is outstanding, about 60% more than a single B100 gpu&lt;/li&gt; &lt;li&gt;the 96GB vram is a game changer for training, enabling very large batch, so faster and smoother training&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Experiment:&lt;/h1&gt; &lt;p&gt;Pretraining of a SLM with 35M parameters, based on GQA architecture with 8 layers, trained with pytorch lightning. Training dataset is TinyStories, with a budget of 1B tokens (2 epochs), a sequence length of 256 tokens, and a virtual batch size of 100k tokens. Models are trained in mixed bf16 precision (additionnal improvement could be expected from using black well fp8 training)&lt;/p&gt; &lt;h1&gt;Results:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;1 x 4090 Laptop (similar perf as a 3090 Desktop) : ~2.5 hours to complete the training run&lt;/li&gt; &lt;li&gt;1 x RTX 6000 pro maxq workstation : ~20 min to complete the training run&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;With proper optim, the card can single handedly deliver the training compute of 7.5 rtx 3090 card, while pulling only 300W of electricity (and being very quiet).&lt;/p&gt; &lt;h1&gt;Inference Benchmark&lt;/h1&gt; &lt;p&gt;In inference, bandwith can be the bottleneck factor, especially in batch 1 inference.&lt;/p&gt; &lt;p&gt;Let's assess the results in batch 1, 4, 8, 16 and 32 to see how much token we can squeeze out of the card.&lt;/p&gt; &lt;h1&gt;Launch&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;export NVCC_THREADS=16 export MAX_JOBS=16 export OMP_NUM_THREADS=16 export VLLM_ATTENTION_BACKEND=FLASHINFER export ENABLE_NVFP4_SM120=1 export VLLM_USE_FLASHINFER_MOE_FP4=1 export MODEL_NAME=&amp;quot;DeepSeek-R1-0528-Qwen3-8B-FP4&amp;quot; vllm serve &amp;quot;$MODEL_NAME&amp;quot; \ --served-model-name gpt-4 \ --port 5000 \ --max-model-len 16000 \ --gpu-memory-utilization 0.9 \ --trust_remote_code \ --max-seq-len-to-capture 8196 \ --enable-chunked-prefill \ --kv-cache-dtype fp8 \ --compilation-config '{&amp;quot;pass_config&amp;quot;:{&amp;quot;enable_fusion&amp;quot;:true,&amp;quot;enable_noop&amp;quot;:true},&amp;quot;cudagraph_mode&amp;quot;:1,&amp;quot;max_capture_size&amp;quot;:2048}' &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Launch &amp;gt;20B Active&lt;/h1&gt; &lt;p&gt;On larger models, tensor cores can do wonders, so above 20B active parameters, the following additionnal env variables can provide a small speed increase, especially for batching.&lt;/p&gt; &lt;p&gt;export VLLM_USE_TRTLLM_ATTENTION=1&lt;/p&gt; &lt;p&gt;export VLLM_USE_TRTLLM_FP4_GEMM=1&lt;/p&gt; &lt;p&gt;export VLLM_FLASHINFER_FORCE_TENSOR_CORES=1&lt;/p&gt; &lt;p&gt;Note: i ran every speed test without these flags, but for example Mistral Small would give around 95 t/s on batch 1, and 1950 t/s on batch 32&lt;/p&gt; &lt;h1&gt;Launch QWEN Moe&lt;/h1&gt; &lt;p&gt;Add flag --enable-expert-parallel&lt;/p&gt; &lt;h1&gt;Launch GPT-OSS&lt;/h1&gt; &lt;p&gt;GPT OSS relies on MXFP4 quant (cause why would they do like everyone else uh?), an hybrid format that will most likely disapear once NVFP4 is fully supported. They also are leveraging their own library for prompt formatting, that is not really compatible with vllm as of now, so don't expect to get anything good from these, i am just testing the speed, but most of the time they only send you blank tokens, which is not really usefull.&lt;/p&gt; &lt;h1&gt;DOWNLOADS&lt;/h1&gt; &lt;p&gt;You'll need to download the following to make vllm work with special snowflake tokenizer, and not break on start:&lt;/p&gt; &lt;p&gt;sudo wget -O /etc/encodings/o200k_base.tiktoken &lt;a href="https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken"&gt;https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sudo wget -O /etc/encodings/cl100k_base.tiktoken &lt;a href="https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken"&gt;https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Launch Command&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;export ENABLE_NVFP4_SM120=1 export VLLM_USE_TRTLLM_ATTENTION=1 export OMP_NUM_THREADS=16 export TIKTOKEN_ENCODINGS_BASE=/etc/encodings export VLLM_USE_FLASHINFER_MXFP4_BF16_MOE=1 export VLLM_USE_FLASHINFER_MXFP4_MOE=1 export VLLM_ATTENTION_BACKEND=FLASHINFER export MODEL_NAME=&amp;quot;gpt-oss-120b&amp;quot; vllm serve &amp;quot;$MODEL_NAME&amp;quot; \ --async-scheduling \ --served-model-name gpt-4 \ --port 5000 \ --max-model-len 16000 \ --gpu-memory-utilization 0.9 \ --trust_remote_code \ --max-seq-len-to-capture 8196 \ --compilation-config '{&amp;quot;pass_config&amp;quot;:{&amp;quot;enable_fusion&amp;quot;:true,&amp;quot;enable_noop&amp;quot;:true},&amp;quot;cudagraph_mode&amp;quot;:1,&amp;quot;max_capture_size&amp;quot;:2048}' \ &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Model Tested:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit&lt;/li&gt; &lt;li&gt;Qwen3-4B-Instruct-2507-GPTQ&lt;/li&gt; &lt;li&gt;Qwen3-32B-AWQ&lt;/li&gt; &lt;li&gt;Mistral-Small-3.2-24B-Instruct-hf-AWQ&lt;/li&gt; &lt;li&gt;gpt-oss-20b&lt;/li&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;li&gt;Hunyuan-A13B-Instruct-GPTQ-Int4&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Failed Test&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1-0528-Qwen3-8B-FP4 : could not start GEMM FP4 kernels, i'll investigate&lt;/li&gt; &lt;li&gt;Qwen3-32B-FP4 : could not start GEMM FP4 kernels, i'll investigate&lt;/li&gt; &lt;li&gt;Llama-4-Scout-17B-16E-Instruct-AWQ : KeyError: 'layers.17.feed_forward.shared_expert.activation_fn.scales', the quant wasn't done properly and i couldn't find an other version in 4bit except bnb that would be much slower :/&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;Read :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0-64 : batch 1 token generation speed between first token and 64th (token / second)&lt;/li&gt; &lt;li&gt;64-128 : batch 1 token generation speed between 64th and 128th (token / second)&lt;/li&gt; &lt;li&gt;...&lt;/li&gt; &lt;li&gt;batch_4 : total throughtput token per second while running 4 concurrent request&lt;/li&gt; &lt;li&gt;batch_8 : total throughtput token per second while running 8 concurrent request&lt;/li&gt; &lt;li&gt;...&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;0-64&lt;/th&gt; &lt;th align="left"&gt;64-128&lt;/th&gt; &lt;th align="left"&gt;128-256&lt;/th&gt; &lt;th align="left"&gt;256-512&lt;/th&gt; &lt;th align="left"&gt;512-1024&lt;/th&gt; &lt;th align="left"&gt;1024-2048&lt;/th&gt; &lt;th align="left"&gt;batch_4&lt;/th&gt; &lt;th align="left"&gt;batch_8&lt;/th&gt; &lt;th align="left"&gt;batch_16&lt;/th&gt; &lt;th align="left"&gt;batch_32&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120b&lt;/td&gt; &lt;td align="left"&gt;182.14&lt;/td&gt; &lt;td align="left"&gt;147.11&lt;/td&gt; &lt;td align="left"&gt;158.66&lt;/td&gt; &lt;td align="left"&gt;143.20&lt;/td&gt; &lt;td align="left"&gt;154.57&lt;/td&gt; &lt;td align="left"&gt;148.10&lt;/td&gt; &lt;td align="left"&gt;~403-409&lt;/td&gt; &lt;td align="left"&gt;~770-776&lt;/td&gt; &lt;td align="left"&gt;~1294-1302&lt;/td&gt; &lt;td align="left"&gt;~1986-2146&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-20b&lt;/td&gt; &lt;td align="left"&gt;196.09&lt;/td&gt; &lt;td align="left"&gt;199.98&lt;/td&gt; &lt;td align="left"&gt;214.26&lt;/td&gt; &lt;td align="left"&gt;198.01&lt;/td&gt; &lt;td align="left"&gt;196.56&lt;/td&gt; &lt;td align="left"&gt;194.38&lt;/td&gt; &lt;td align="left"&gt;~564-624&lt;/td&gt; &lt;td align="left"&gt;~1054-1117&lt;/td&gt; &lt;td align="left"&gt;~1887-1912&lt;/td&gt; &lt;td align="left"&gt;~2904-2911&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-32B-AWQ&lt;/td&gt; &lt;td align="left"&gt;60.47&lt;/td&gt; &lt;td align="left"&gt;68.94&lt;/td&gt; &lt;td align="left"&gt;62.53&lt;/td&gt; &lt;td align="left"&gt;62.36&lt;/td&gt; &lt;td align="left"&gt;61.99&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;~227-233&lt;/td&gt; &lt;td align="left"&gt;~447-452&lt;/td&gt; &lt;td align="left"&gt;~920-936&lt;/td&gt; &lt;td align="left"&gt;~1448-1482&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral-Small-3.2-24B-Instruct-hf-AWQ&lt;/td&gt; &lt;td align="left"&gt;89.39&lt;/td&gt; &lt;td align="left"&gt;95.77&lt;/td&gt; &lt;td align="left"&gt;89.29&lt;/td&gt; &lt;td align="left"&gt;87.29&lt;/td&gt; &lt;td align="left"&gt;86.95&lt;/td&gt; &lt;td align="left"&gt;86.59&lt;/td&gt; &lt;td align="left"&gt;~288-336&lt;/td&gt; &lt;td align="left"&gt;~631-646&lt;/td&gt; &lt;td align="left"&gt;~1109-1153&lt;/td&gt; &lt;td align="left"&gt;~1714-1790&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B-Instruct-2507-GPTQ&lt;/td&gt; &lt;td align="left"&gt;208.21&lt;/td&gt; &lt;td align="left"&gt;205.15&lt;/td&gt; &lt;td align="left"&gt;223.60&lt;/td&gt; &lt;td align="left"&gt;210.72&lt;/td&gt; &lt;td align="left"&gt;211.67&lt;/td&gt; &lt;td align="left"&gt;207.49&lt;/td&gt; &lt;td align="left"&gt;~721-743&lt;/td&gt; &lt;td align="left"&gt;~1158-1377&lt;/td&gt; &lt;td align="left"&gt;~2044-2236&lt;/td&gt; &lt;td align="left"&gt;~2400-2666&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit&lt;/td&gt; &lt;td align="left"&gt;179.42&lt;/td&gt; &lt;td align="left"&gt;176.71&lt;/td&gt; &lt;td align="left"&gt;176.01&lt;/td&gt; &lt;td align="left"&gt;175.81&lt;/td&gt; &lt;td align="left"&gt;175.44&lt;/td&gt; &lt;td align="left"&gt;172.64&lt;/td&gt; &lt;td align="left"&gt;~490-510&lt;/td&gt; &lt;td align="left"&gt;~950-1000&lt;/td&gt; &lt;td align="left"&gt;~1520-1602&lt;/td&gt; &lt;td align="left"&gt;~2200-2400&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B-Instruct-GPTQ-Int4&lt;/td&gt; &lt;td align="left"&gt;94.91&lt;/td&gt; &lt;td align="left"&gt;89.74&lt;/td&gt; &lt;td align="left"&gt;64.91&lt;/td&gt; &lt;td align="left"&gt;87.40&lt;/td&gt; &lt;td align="left"&gt;89.71&lt;/td&gt; &lt;td align="left"&gt;88.03&lt;/td&gt; &lt;td align="left"&gt;~200-202&lt;/td&gt; &lt;td align="left"&gt;~300-307&lt;/td&gt; &lt;td align="left"&gt;~477-485&lt;/td&gt; &lt;td align="left"&gt;~755-777&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;No surprise, in batch 1, the performance is good but not outstanding, limited by the 1.7 TB/s of GDDR7 memory. The blackwell optimizations allow to squeeze a bit more performance though (that might explode when flash attention 4 will be released) and just slightly beats the speed of 2 x 3090 with tensor parallelism.&lt;/p&gt; &lt;p&gt;The game changer is on batch 32, with an almost linear scaling of number of tokens delivered with batch size, so might be really usefull for small scale serving and multi agent deployment purpose.&lt;/p&gt; &lt;p&gt;So far, support is still not completely ready, but sufficient to play with some models.&lt;/p&gt; &lt;h1&gt;Code to reproduce the results&lt;/h1&gt; &lt;p&gt;Training scripts can be found on this repo for pretraining:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gabrielolympie/ArchiFactory"&gt;https://github.com/gabrielolympie/ArchiFactory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Speed Benchmark for inference + used prompts can be found in :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/gabrielolympie/PromptServer"&gt;https://github.com/gabrielolympie/PromptServer&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Next steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I might update this post when NVFP4 support is stable enough to give a glimpse of it potential&lt;/li&gt; &lt;li&gt;If you want me to test a specific model, propose in the comments, i'll add those who are either in a different weight category, or different architecture&lt;/li&gt; &lt;li&gt;If i can find the time, i will make a similar post with diffusion models (image + video) where the archi might deliver even more impressive results&lt;/li&gt; &lt;li&gt;If you want me to test additionnal vllm tuning parameters, let me know in the comments (i might give a try to sglang and exllama v3 as well when their own support will be more mature)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Global conclusion&lt;/h1&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;large vram&lt;/li&gt; &lt;li&gt;impressive raw compute&lt;/li&gt; &lt;li&gt;impressive scaling with batch size&lt;/li&gt; &lt;li&gt;very quiet, i could sleep during a training run with computer in the same room&lt;/li&gt; &lt;li&gt;very low power consumption, stable 300W at full power and most likely room for overclocking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;still limited bandwith compared to latest HBM memory&lt;/li&gt; &lt;li&gt;software support still a bit messy but quickly improving&lt;/li&gt; &lt;li&gt;cannot be used with tensor paralellism with Ampere (i tried doing tensor parallelism with a 3090 and it did not go well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sweet spots / for what need?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any model with 10-20B active parameters and up to 160B total parameters will be incredible on it&lt;/li&gt; &lt;li&gt;Processing large amount of texts (classification / labeling / synthetic data generation )&lt;/li&gt; &lt;li&gt;Small serving for up to 30 - 60 concurrent users&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When not to use?&lt;/p&gt; &lt;p&gt;If your use case involve getting max tokens / seconds in batch 1 and you don't care for power draw, building a battlestation with 4*4090 will provide much better speed at the same price.&lt;/p&gt; &lt;p&gt;Edit / Addtions:&lt;br /&gt; Added Hunyuan A13B : for some reason the FP8 kv cache must be removed. And the model is far slower than it should be for large batches for its size (might be due to the gptq format though).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventurousSwim1312"&gt; /u/AdventurousSwim1312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my3why/rtx_pro_6000_maxq_blackwell_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T15:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
