<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-01T14:05:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j0zeht</id>
    <title>Is This AI PC a Good Deal for $1500? (i7-13700 + RTX 4070 Ti + 32GB DDR5)</title>
    <updated>2025-03-01T13:25:21+00:00</updated>
    <author>
      <name>/u/ate50eggs</name>
      <uri>https://old.reddit.com/user/ate50eggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m considering buying this AI workstation for $1500, and I’d love some feedback on whether it’s a good deal. I’ll mainly use it for AI model training, machine learning, and local LLM inference.&lt;/p&gt; &lt;p&gt;Specs: • CPU: Intel i7-13700 (16C/24T) • GPU: ASUS ROG Strix RTX 4070 Ti (12GB VRAM) • RAM: 32GB DDR5 6000MHz • Storage: 2TB NVMe SSD • Cooling: NZXT Kraken liquid cooler • PSU: 1000W Gold-rated • Case: ROG Strix White • OS: Windows 11&lt;/p&gt; &lt;p&gt;Use Case: • Training AI/ML models (PyTorch, TensorFlow, Stable Diffusion) • Running local LLMs (LLaMA, DeepSeek, etc.) • Data preprocessing &amp;amp; AI development&lt;/p&gt; &lt;p&gt;I know the RTX 5090 is out now, but they’re still very expensive and hard to get. Would this RTX 4070 Ti build be good enough for AI work in 2025, or will I end up needing an upgrade too soon?&lt;/p&gt; &lt;p&gt;I’m also thinking about buying an NVIDIA DIGITS system when they release, so I’m wondering if this PC would still be useful alongside it or if I should just wait.&lt;/p&gt; &lt;p&gt;Would love to hear any advice—should I buy now or wait for better deals? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ate50eggs"&gt; /u/ate50eggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zeht/is_this_ai_pc_a_good_deal_for_1500_i713700_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zeht/is_this_ai_pc_a_good_deal_for_1500_i713700_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zeht/is_this_ai_pc_a_good_deal_for_1500_i713700_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:25:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00v4y</id>
    <title>"Crossing the uncanny valley of conversational voice" post by Sesame - realtime conversation audio model rivalling OpenAI</title>
    <updated>2025-02-28T05:52:31+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So this is one of the craziest voice demos I've heard so far, and they apparently want to release their models under an Apache-2.0 license in the future: I've never heard of Sesame, they seem to be very new.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Our models will be available under an Apache 2.0 license&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Your thoughts? Check the demo first: &lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No public weights yet, we can only dream and hope, but this easily matches or beats OpenAI's Advanced Voice Mode. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0nli0</id>
    <title>DeepSeek R1: distilled &amp; quantized models explained simply for beginners</title>
    <updated>2025-03-01T01:10:21+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"&gt; &lt;img alt="DeepSeek R1: distilled &amp;amp; quantized models explained simply for beginners" src="https://external-preview.redd.it/gk92HmQ5dLETjf3xIkaITfkJIkYIq_lnYJClNB1FntQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3f5dc2938e5bd646952339cc0f2189330ec1bca" title="DeepSeek R1: distilled &amp;amp; quantized models explained simply for beginners" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/AxAj16ZmanY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j088yg</id>
    <title>RX 9070 XT Potential performance discussion</title>
    <updated>2025-02-28T13:57:36+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As some of you might have seen, AMD just revealed the new RDNA 4 GPUS. RX 9070 XT for $599 and RX 9070 for $549&lt;/p&gt; &lt;p&gt;Looking at the numbers, 9070 XT offers &amp;quot;2x&amp;quot; in FP16 per compute unit compared to 7900 XTX [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;], so at 64U vs 96U that means RX 9070 XT would have 33% compute uplift.&lt;/p&gt; &lt;p&gt;The issue is the bandwitdh - at 256bit GDDR6 we get ~630GB/s compared to 960GB/s on a 7900 XTX.&lt;/p&gt; &lt;p&gt;BUT! According to the same presentation [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;] they mention they've added INT8 and INT8 &lt;em&gt;with sparsity&lt;/em&gt; computations to RDNA 4, which make it 4x and 8x faster than RDNA 3 &lt;em&gt;per unit&lt;/em&gt;, which would make it 2.67x and 5.33x times faster than RX 7900 XTX.&lt;/p&gt; &lt;p&gt;I wonder if newer model architectures that are less limited by memory bandwidth could use these computations and make new AMD GPUs great inference cards. What are your thoughts?&lt;/p&gt; &lt;p&gt;EDIT: Updated links after they cut the video. Both are now the same, originallly I quoted two different parts of the video.&lt;/p&gt; &lt;p&gt;EDIT2: I missed it, but hey also mention 4-bit tensor types!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0mxyu</id>
    <title>The Fastest Way to Fine-Tune LLMs Locally</title>
    <updated>2025-03-01T00:37:15+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"&gt; &lt;img alt="The Fastest Way to Fine-Tune LLMs Locally" src="https://external-preview.redd.it/6UOl6M06GBFlN2fyjDaRlq770836HUnVCb2siqyuTdg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d807111a3b15fb2be2e302d215faacb851bd7a6" title="The Fastest Way to Fine-Tune LLMs Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MaxHastings/Kolo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0l2kp</id>
    <title>OpenArc upcoming olmoOCR, Qwen2-VL support</title>
    <updated>2025-02-28T23:08:30+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I have been talking about &lt;a href="https://github.com/SearchSavior"&gt;OpenArc&lt;/a&gt; a lot this week. Even have a release planned this weekend.&lt;/p&gt; &lt;p&gt;Right now I am excited because I was just able to convert &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview"&gt;olmOCR-7B-0225-preview&lt;/a&gt; to OpenVINO format!!! Weights will be posted this weekend and OpenArc will support serving eventually with the full gamut of Qwen2-VL input/output. For those who don't know, this is a pretty big deal; olmoOCR seems very powerful for document analysis and Qwen2-VL can be tooled to be a very effect edge compute vision agent on top of it's own image vision capabilities. Pair that with omniparser or paddle... we'll have capable local intel vsion agents very soon. Not sooner than text only however.&lt;/p&gt; &lt;p&gt;Holy crap this is very exciting do a lot of work with CPU only OCR and am very familair with the literature in this area- Qwen2-VL was my first major llm project. For reference, on my 2x xeon 6242 server I was getting ~7t/s for 100 dpi images with Qwen2-VL-7B which was 7x faster than full precision; I replicated their accuracy int4 results from the qwen 2 vl paper with openvino int4 in my work on dense table analysis.&lt;/p&gt; &lt;p&gt;Anyway, stay tuned for text only, a gradio dashboard with baked in documentation and openai endpoints this weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T23:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j09mfx</id>
    <title>is 9070xt any good for localAI on windows ?</title>
    <updated>2025-02-28T15:00:48+00:00</updated>
    <author>
      <name>/u/gfy_expert</name>
      <uri>https://old.reddit.com/user/gfy_expert</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt; &lt;img alt="is 9070xt any good for localAI on windows ?" src="https://a.thumbs.redditmedia.com/IHRW4iuZw3RS3QXGe1CwjVtw2Kt1kBPYjq-WbucEht0.jpg" title="is 9070xt any good for localAI on windows ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfy_expert"&gt; /u/gfy_expert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j09mfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T15:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0gs1g</id>
    <title>AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play</title>
    <updated>2025-02-28T20:00:12+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt; &lt;img alt="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" src="https://external-preview.redd.it/GK3DdqrCYGvJGL-l88rd-LZtTxw8isk9OD7o602ntRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20e1333c4dbdf7f381d26b7f87d3f296cf306559" title="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-Vulkan-SPIR-V-Wide-AI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T20:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j045xn</id>
    <title>I trained a reasoning model that speaks French—for just $20! 🤯🇫🇷</title>
    <updated>2025-02-28T09:51:24+00:00</updated>
    <author>
      <name>/u/TheREXincoming</name>
      <uri>https://old.reddit.com/user/TheREXincoming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j045xn/video/mvudzukrpule1/player"&gt;https://reddit.com/link/1j045xn/video/mvudzukrpule1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheREXincoming"&gt; /u/TheREXincoming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T09:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0zvwj</id>
    <title>Retrieval augmented generation - pdf extraction is all you need ?</title>
    <updated>2025-03-01T13:50:21+00:00</updated>
    <author>
      <name>/u/kleenex007</name>
      <uri>https://old.reddit.com/user/kleenex007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community!&lt;/p&gt; &lt;p&gt;I have checking with the community how to best approach rag in 2025 for niche domain, in particular for sensitive pdfs you can’t send to a closed vendor.&lt;/p&gt; &lt;p&gt;I have observed over the last year - tons of framework all claiming to be the best, but no leaderboard to sort out. It’s about learning curve and abstraction level - most tools out there are not really modular, so you have to commit to one. Or just write your own with api call directly - llms seem to plateau, hard to see big difference - context size increasing , why bother with chunking, embedding etc.. too much hassle - rag, cag, kag, etc, to improve context retrieval, but now you have more parameters to tweak through vibe testing - pdf extraction creates a lot of gibberish, especially on books, business reports, presentations where you have lots of tables, figures, special content structure&lt;/p&gt; &lt;p&gt;where are the most effective area to focus in terms of ROI? How would you start over given what you know today ?&lt;/p&gt; &lt;p&gt;Personally I would invest my entire effort on two fronts:&lt;/p&gt; &lt;p&gt;1) make pdf extraction bullet proof, possibly vision as well (graph, figures, infographic, etc). 2) create eval dataset + llm-as-a-judge&lt;/p&gt; &lt;p&gt;Thanks. Looking forward to a vibrant discussion &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kleenex007"&gt; /u/kleenex007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zvwj/retrieval_augmented_generation_pdf_extraction_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zvwj/retrieval_augmented_generation_pdf_extraction_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0zvwj/retrieval_augmented_generation_pdf_extraction_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0wurx</id>
    <title>UPDATE: Tool Calling for DeepSeek-R1 with LangChain and LangGraph: Now in TypeScript!</title>
    <updated>2025-03-01T10:46:24+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted here a Github repo Python package I created on tool calling for DeepSeek-R1 671B with LangChain and LangGraph, or more generally for any LLMs available in LangChain's ChatOpenAl class (particularly useful for newly released LLMs which isn't supported for tool calling yet by LangChain and LangGraph):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By community request, I'm thrilled to announce a TypeScript version of this package is now live!&lt;/p&gt; &lt;p&gt;Introducing &amp;quot;taot-ts&amp;quot; - The npm package that brings tool calling capabilities to DeepSeek-R1 671B in TypeScript:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/leockl/tool-ahead-of-time-ts"&gt;https://github.com/leockl/tool-ahead-of-time-ts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kindly give me a star on my repo if this is helpful. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0wurx/update_tool_calling_for_deepseekr1_with_langchain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T10:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0cbvs</id>
    <title>🗣️ Free &amp; Open-Source AI TTS: Kokoro Web v0.1.0</title>
    <updated>2025-02-28T16:53:25+00:00</updated>
    <author>
      <name>/u/EduardoDevop</name>
      <uri>https://old.reddit.com/user/EduardoDevop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Excited to share &lt;strong&gt;Kokoro Web&lt;/strong&gt;, a fully open-source AI text-to-speech tool that you can use for free. No paywalls, no restrictions—just high-quality, local-friendly TTS. &lt;/p&gt; &lt;h2&gt;🔥 Why It Matters:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Open-Source&lt;/strong&gt;: No locked features, no subscriptions.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hostable&lt;/strong&gt;: Run it locally or on your own server.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI API Compatible&lt;/strong&gt;: Drop-in replacement for AI projects.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Generate speech in different accents.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built on Kokoro v1.0&lt;/strong&gt;: One of the top-ranked models in &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;TTS Arena&lt;/a&gt;, just behind ElevenLabs.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;🚀 Try It Out:&lt;/h2&gt; &lt;p&gt;Live demo: &lt;a href="https://voice-generator.pages.dev"&gt;https://voice-generator.pages.dev&lt;/a&gt; &lt;/p&gt; &lt;h2&gt;🔧 Self-Hosting:&lt;/h2&gt; &lt;p&gt;Spin it up with Docker in minutes: &lt;a href="https://github.com/eduardolat/kokoro-web"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts—feedback, contributions, and ideas are always welcome! 🖤 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EduardoDevop"&gt; /u/EduardoDevop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0c53c</id>
    <title>Inference speed comparisons between M1 Pro and maxed-out M4 Max</title>
    <updated>2025-02-28T16:45:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Won’t Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I don't think these models are actually utilizing the CPU. But I'm not definitive on this.&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I added a github repo in case anyone wants to contribute their own speed tests. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0kgyn</id>
    <title>99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900</title>
    <updated>2025-02-28T22:41:24+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt; &lt;img alt="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" src="https://external-preview.redd.it/HFws3DDkcEP1xVBovP3WDu-ptb9oIschwqz-M_5LaEc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1b52e8ca906335c7d16f3c20d0abb029388892" title="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ZWBQPKc.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T22:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0sxsv</id>
    <title>What do the deepseek papers mean for local inference?</title>
    <updated>2025-03-01T06:11:13+00:00</updated>
    <author>
      <name>/u/oldschooldaw</name>
      <uri>https://old.reddit.com/user/oldschooldaw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Upfront, I can’t understand the papers. I don’t know enough to read them. But the snippets I’m seeing about them on X suggest to me a lot of the improvements are for VERY VERY VERY large players, not those with a single 4090. &lt;/p&gt; &lt;p&gt;Is there any developments in the drops I’ve missed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oldschooldaw"&gt; /u/oldschooldaw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d30g</id>
    <title>There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day</title>
    <updated>2025-02-28T17:23:46+00:00</updated>
    <author>
      <name>/u/unixmachine</name>
      <uri>https://old.reddit.com/user/unixmachine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt; &lt;img alt="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" src="https://external-preview.redd.it/VGXH8e8_7pJ5Oyuiy8alPcJzC5slCQHySpimzMU8-QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7786514b391f52128b399c1e789d18e7f6f10a" title="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixmachine"&gt; /u/unixmachine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-ROCm-RX-9070-Launch-Day"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0muz1</id>
    <title>Phi-4-mini Bug Fixes + GGUFs</title>
    <updated>2025-03-01T00:33:13+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! llama.cpp added supported for Phi-4 mini today - we also found and fixed 4 tokenization related problems in Phi-4 mini!&lt;/p&gt; &lt;p&gt;The biggest problem with the chat template is the EOS token was set to &amp;lt;|endoftext|&amp;gt;, but it should be &amp;lt;|end|&amp;gt;!&lt;/p&gt; &lt;p&gt;GGUFs are at: &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The rest of the versions including 16-bit are&lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt; &lt;/a&gt;also on &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And the dynamic 4bit bitsandbytes version is at &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There were also tokenization problems for the larger Phi-4 14B as well, which we fixed a while back for those who missed it and Microsoft used our fixes 2 weeks ago.&lt;/p&gt; &lt;p&gt;Thank you! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yyg1</id>
    <title>AMD Ryzen AI Max+ Pro 395 "Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%</title>
    <updated>2025-03-01T13:01:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt; &lt;img alt="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" src="https://external-preview.redd.it/BkZkseIe-TX7LG_20ukN9CeYcn2i-ty5QOZ4F8-FqfE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d471c1208a7a116839a4c4a45eef1d42f16e7d2c" title="AMD Ryzen AI Max+ Pro 395 &amp;quot;Strix Halo Benchmarked In CPU Mark, Outperforms Core i9 14900HX By 9%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-benchmarked-in-cpu-mark-outperforms-core-i9-14900hx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yyg1/amd_ryzen_ai_max_pro_395_strix_halo_benchmarked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0o8mt</id>
    <title>The first real open source DeepResearch attempt I've seen</title>
    <updated>2025-03-01T01:43:56+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Search-R1&lt;/strong&gt; is a reproduction of &lt;strong&gt;DeepSeek-R1(-Zero)&lt;/strong&gt; methods for &lt;em&gt;training reasoning and searching (tool-call) interleaved LLMs&lt;/em&gt;. Built upon &lt;a href="https://github.com/volcengine/verl"&gt;veRL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Through RL (rule-based outcome reward), the 3B &lt;strong&gt;base&lt;/strong&gt; LLM (both Qwen2.5-3b-base and Llama3.2-3b-base) develops reasoning and search engine calling abilities all on its own.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PeterGriffinJin/Search-R1/tree/main"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0ync3</id>
    <title>TinyR1-32B-Preview: SuperDistillation Achieves Near-R1 Performance with Just 5% of Parameters.</title>
    <updated>2025-03-01T12:43:48+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/qihoo360/TinyR1-32B-Preview"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We applied supervised fine-tuning (SFT) to Deepseek-R1-Distill-Qwen-32B across three target domains—Mathematics, Code, and Science — using the 360-LLaMA-Factory training framework to produce three domain-specific models. We used questions from open-source data as seeds. Meanwhile, responses for mathematics, coding, and science tasks were generated by R1, creating specialized models for each domain. Building on this, we leveraged the Mergekit tool from the Arcee team to combine multiple models, creating Tiny-R1-32B-Preview, which demonstrates strong overall performance.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0ync3/tinyr132bpreview_superdistillation_achieves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T12:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0yxm1</id>
    <title>AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well</title>
    <updated>2025-03-01T13:00:07+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt; &lt;img alt="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" src="https://external-preview.redd.it/kjFdjz1K9W0AjIQXXFkEbN2PFu7C3Ga1YSe9tdCduuw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dfacaf6bed989273560dbf7b34460efac41ca6a" title="AMD's RX 9070 Series GPUs Will Feature Support For ROCm; Team Red Shows A Running Sample As Well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-rx-9070-series-gpus-will-feature-support-for-rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0yxm1/amds_rx_9070_series_gpus_will_feature_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T13:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0uoht</id>
    <title>Chain of Draft: Thinking Faster by Writing Less</title>
    <updated>2025-03-01T08:10:14+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt; &lt;img alt="Chain of Draft: Thinking Faster by Writing Less" src="https://b.thumbs.redditmedia.com/fwljouG_I7UUcaN7hDPRHRGsfe6zHb3U7bt9TnwQ_OA.jpg" title="Chain of Draft: Thinking Faster by Writing Less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.18600"&gt;https://arxiv.org/abs/2502.18600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CoD System prompt:&lt;/p&gt; &lt;p&gt;Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j0uoht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0r3go</id>
    <title>Day 6: One More Thing, DeepSeek-V3/R1 Inference System Overview</title>
    <updated>2025-03-01T04:19:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md"&gt;https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T04:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0n56h</id>
    <title>Finally, a real-time low-latency voice chat model</title>
    <updated>2025-03-01T00:47:24+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't seen it yet, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tried it fow a few minutes earlier today and another 15 minutes now. I tested and it remembered our chat earlier. It is the first time that I treated AI as a person and felt that I needed to mind my manners and say &amp;quot;thank you&amp;quot; and &amp;quot;good bye&amp;quot; at the end of the conversation.&lt;/p&gt; &lt;p&gt;Honestly, I had more fun chatting with this than some of my ex-girlfriends!&lt;/p&gt; &lt;p&gt;Github here (code not yet dropped):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;``` Model Sizes: We trained three model sizes, delineated by the backbone and decoder sizes:&lt;/p&gt; &lt;p&gt;Tiny: 1B backbone, 100M decoder Small: 3B backbone, 250M decoder Medium: 8B backbone, 300M decoder Each model was trained with a 2048 sequence length (~2 minutes of audio) over five epochs. ```&lt;/p&gt; &lt;p&gt;The model sizes look friendly to local deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0tnsr</id>
    <title>We're still waiting Sam...</title>
    <updated>2025-03-01T06:59:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt; &lt;img alt="We're still waiting Sam..." src="https://preview.redd.it/31jfuybv01me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128f969cd722b072d73b4d77393ee7c0bc1b057b" title="We're still waiting Sam..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/31jfuybv01me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:59:17+00:00</published>
  </entry>
</feed>
