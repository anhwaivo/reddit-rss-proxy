<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-19T07:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k2o007</id>
    <title>Is there a formula or rule of thumb about the effect of increasing context size on tok/sec speed? Does it *linearly* slow down, or *exponentially* or ...?</title>
    <updated>2025-04-19T04:20:08+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, is there a way to estimate how much VRAM is needed to run a model with P parameters, quantized at Q bits per parameter, with context length C?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2o007/is_there_a_formula_or_rule_of_thumb_about_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T04:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22e41</id>
    <title>Good news: 5090s now in stock in my local market. Bad news: cheapest is $3,550</title>
    <updated>2025-04-18T11:13:40+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I wonder if I should have just bought the 2nd hand 3090s that were on sale for $700.&lt;/p&gt; &lt;p&gt;Can someone tell me what the typical 'street price' for 5090s in the US?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2j3pm</id>
    <title>Super Excited, Epyc 9354 Build</title>
    <updated>2025-04-18T23:48:42+00:00</updated>
    <author>
      <name>/u/joelasmussen</name>
      <uri>https://old.reddit.com/user/joelasmussen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am really excited to be joining you guys soon. I've read a lot of your posts and am an older guy looking to have a local llm. I'm starting from scratch in the tech world (I am a Nurse and former Elementary school teacher) so please forgive my naivete in a lot of the technical stuff. I want my own 70b model someday. Starting with a formidible foundation to grow into has been my goal.&lt;/p&gt; &lt;p&gt;I have a 9354 chip I'm getting used and for a good price. Going with a C8 case and H13SSL-N supermicro Mobo (rev 2.01) intel optane 905p for a boot drive for now just because I have it, and I got an optane 5801 for a llm cache drive. 1300w psu. 1 3090 but soon to be two. Gotta save and take my time. I got 6 2Rx8 32 gb rdimms coming (also used so I'll need to check them). I think my set up os overkill but there's a hell of a lot of room to grow. Please let me know what cpu aircooler you folks use. Also any thoughts on other equipment. I read about this stuff on here,Medium,Github and other places. Penny for your thoughts. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joelasmussen"&gt; /u/joelasmussen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2j3pm/super_excited_epyc_9354_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2j3pm/super_excited_epyc_9354_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2j3pm/super_excited_epyc_9354_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T23:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2fb67</id>
    <title>Save 13W of idle power on your 3090?</title>
    <updated>2025-04-18T20:51:07+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A comment on my other post (see: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k22e41/comment/mnr7mk5/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k22e41/comment/mnr7mk5/&lt;/a&gt; ) led me to do some testing.&lt;/p&gt; &lt;p&gt;With my old drivers:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 39C P8 21W / 255W | 15967MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 35C P8 26W / 255W | 15977MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;After updating OS/drivers/CUDA:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 32C P8 8W / 285W | 1MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 15W / 285W | 1MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Holy crap!&lt;/p&gt; &lt;p&gt;13W savings on 3090 and 11W saving on the 3090 Ti!&lt;/p&gt; &lt;p&gt;Now, I just need to check whether these are really 'at the wall' savings, or just 'nvidia-smi reporting differences'.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Old setup: Ubuntu 20.04, CUDA 12.4, 550 driver&lt;/li&gt; &lt;li&gt;New setup: Ubuntu 24.04, CUDA 12.8, 570 driver&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT: verified wall power:&lt;/p&gt; &lt;p&gt;I just rebooted to the old image to do powerwall test and found this at start-up:&lt;/p&gt; &lt;p&gt;``` +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 32C P8 8W / 255W | 2MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 34C P8 18W / 255W | 2MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;So also same low idle power (before models are loaded).&lt;/p&gt; &lt;p&gt;And after models are loaded:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 54% 49C P8 22W / 255W | 15967MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 37C P8 25W / 255W | 15979MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Aftermodels are unloaded, the idle power is not recovered:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.144.03 Driver Version: 550.144.03 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 0% 43C P8 22W / 255W | 2MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 26W / 255W | 2MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt; Wall power: 105W +/- 3W&lt;/p&gt; &lt;p&gt;New setup before model loads:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 53% 44C P8 8W / 355W | 1MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 41C P8 19W / 355W | 1MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Wall power: 73W +/- 1W&lt;/p&gt; &lt;p&gt;Now tried loading a model:&lt;/p&gt; &lt;p&gt;&lt;code&gt; +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 On | 00000000:00:10.0 Off | N/A | | 53% 45C P8 8W / 275W | 22759MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 3090 Ti On | 00000000:00:11.0 Off | Off | | 0% 37C P8 19W / 275W | 22769MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Wall power: 75W +/- 2W&lt;/p&gt; &lt;p&gt;OK. It looks like these are real power savings!&lt;/p&gt; &lt;p&gt;I think more work needs to be done:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the saving permanent or does it degrade after time&lt;/li&gt; &lt;li&gt;What causes the saving? The original comment said saving was triggered by an OS update - but it could be an interaction of different elements perhaps kernel + driver?&lt;/li&gt; &lt;li&gt;Does this also fix the P40 idle power issue? (which can currently be worked around with pstated)&lt;/li&gt; &lt;li&gt;Dare I dream that it could help with P100 idle power?&lt;/li&gt; &lt;li&gt;What about other cards e.g. 2080 Ti?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2fb67/save_13w_of_idle_power_on_your_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T20:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22lyx</id>
    <title>FULL LEAKED Replit Agent System Prompts and Tools</title>
    <updated>2025-04-18T11:26:40+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 18/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get full official Replit Agent system prompts, including its tools (JSON). Over 400 lines.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28j02</id>
    <title>Llama 4 Maverick MLX performance on M3 Ultra</title>
    <updated>2025-04-18T16:03:30+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM studio released an MLX update today so we can run Maverick in MLX format.&lt;/p&gt; &lt;p&gt;Q4 version numbers:&lt;/p&gt; &lt;p&gt;Prompt size: 12405&lt;br /&gt; Prompt eval rate: 332 t/s&lt;br /&gt; Token gen rate: 47.42&lt;/p&gt; &lt;p&gt;Right now for me there is a bug where it's not using prompt caching. Promising initial results though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1xvvr</id>
    <title>Where is the promised open Grok 2?</title>
    <updated>2025-04-18T06:01:19+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As far as I know, Grok 2 was supposed to be open-sourced some time after Grok 3's release. But I'm afraid that by the time they decide to open-source Grok 2, it will already be completely obsolete. This is because even now, it significantly lags behind in performance compared to the likes of DeepSeek V3, and we also have Qwen 3 and Llama 4 Reasoning on the horizon (not to mention a potential open model from OpenAI). I believe that when they eventually decide to release it to the community, it will be of no use to anyone anymore, much like what happened with Grok 1. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T06:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ex99</id>
    <title>Gemma3-4b-qat-int4 for OpenVINO is up</title>
    <updated>2025-04-18T20:34:13+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov"&gt;https://huggingface.co/Echo9Zulu/gemma-3-4b-it-qat-int4_asym-ov&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ex99/gemma34bqatint4_for_openvino_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T20:34:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2asly</id>
    <title>I wrote a memory system with GUI for Gemma3 using the Kobold.cpp API</title>
    <updated>2025-04-18T17:38:29+00:00</updated>
    <author>
      <name>/u/PSInvader</name>
      <uri>https://old.reddit.com/user/PSInvader</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PSInvader"&gt; /u/PSInvader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Asagix/RecallWeaver"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2asly/i_wrote_a_memory_system_with_gui_for_gemma3_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ahak</id>
    <title>Built a Chrome extension to organize chats on DeepSeek</title>
    <updated>2025-04-18T17:25:12+00:00</updated>
    <author>
      <name>/u/cedparadis</name>
      <uri>https://old.reddit.com/user/cedparadis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt; &lt;img alt="Built a Chrome extension to organize chats on DeepSeek" src="https://external-preview.redd.it/ZGkzaHZmajlvbXZlMTULimii-_X8OCS81GF8X6Vu1vt1qbza3aRbGy-kyjt6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bff768996433c8136b0e96997b920b2ed78b6ed" title="Built a Chrome extension to organize chats on DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using DeepSeek a lot recently as a faster, free alternative to ChatGPT.&lt;/p&gt; &lt;p&gt;After a while your chat history gets messy and pretty long.&lt;/p&gt; &lt;p&gt;So I tried a couple of Chrome extensions to have folders or pin my important conversations but either they were broken or felt out of place with the DeepSeek UI.&lt;/p&gt; &lt;p&gt;I kind of scratch my own itch by building my own. I made it super integrated in the UI so it feels its part of the native Deepseek interface.&lt;/p&gt; &lt;p&gt;It's pretty simple: you can have folders and subfolders for your convos, pin chats as favorite and even resize the sidebar.&lt;/p&gt; &lt;p&gt;Just pushed it live on the Chrome Store: &lt;a href="https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac"&gt;https://chromewebstore.google.com/detail/deepseek-folders-chat-org/mlfbmcmkefmdhnnkecdoegomcikmbaac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now I am working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clipping specific parts of chats&lt;/li&gt; &lt;li&gt;Secret section with PIN access&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prompt Genie - one click prompt enhancement&lt;/p&gt; &lt;p&gt;Happy to hear feedback or questions — first real project I’ve built and shipped solo.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cedparadis"&gt; /u/cedparadis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lbx60gj9omve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ahak/built_a_chrome_extension_to_organize_chats_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2b75l</id>
    <title>Anyone having voice conversations? What’s your setup?</title>
    <updated>2025-04-18T17:55:39+00:00</updated>
    <author>
      <name>/u/markosolo</name>
      <uri>https://old.reddit.com/user/markosolo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apologies to anyone who’s already seen this posted - I thought this might be a better place to ask.&lt;/p&gt; &lt;p&gt;I want something similar to Googles AI Studio where I can call a model and chat with it. Ideally I'd like that to look something like voice conversation where I can brainstorm and do planning sessions with my &amp;quot;AI&amp;quot;. &lt;/p&gt; &lt;p&gt;Is anyone doing anything like this? What's your setup? Would love to hear from anyone having regular voice conversations with AI as part of their daily workflow.&lt;/p&gt; &lt;p&gt;In terms of resources I have plenty of compute, 20GB of GPU I can use. I prefer local if there’s are viable local options I can cobble together even if it’s a bit of work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/markosolo"&gt; /u/markosolo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2b75l/anyone_having_voice_conversations_whats_your_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T17:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2lxzr</id>
    <title>Everything about AI Function Calling and MCP, the keyword to Agentic AI</title>
    <updated>2025-04-19T02:20:12+00:00</updated>
    <author>
      <name>/u/Wrtnlabs</name>
      <uri>https://old.reddit.com/user/Wrtnlabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"&gt; &lt;img alt="Everything about AI Function Calling and MCP, the keyword to Agentic AI" src="https://external-preview.redd.it/Ie5EgDsUWjA9acmsHaaxGDexZ4okrsyLqI9mnKmi-PE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca27c42e80039606642a16608b907a305ab65b" title="Everything about AI Function Calling and MCP, the keyword to Agentic AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrtnlabs"&gt; /u/Wrtnlabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://wrtnlabs.io/agentica/articles/everything-about-ai-function-calling-and-mcp-the-keyword-to-agentic-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2lxzr/everything_about_ai_function_calling_and_mcp_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T02:20:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2paa5</id>
    <title>Is Gemma3-12B-QAT bad?</title>
    <updated>2025-04-19T05:43:16+00:00</updated>
    <author>
      <name>/u/FbF_</name>
      <uri>https://old.reddit.com/user/FbF_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying it out compared to the Bartowski's Q4_K_M version and it seems noticeably worse. It just tends to be more repetitive and summarize the prompt uncritically. It's not clear to me if they compared the final QAT model with the non-quantized BF16 version in their proclamation of having a better quantization. Has anyone else had the same experience or done more in-depth analyses on the difference in output with the non-quantized model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FbF_"&gt; /u/FbF_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2paa5/is_gemma312bqat_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2qrqq</id>
    <title>Amoral Gemma 3 - QAT</title>
    <updated>2025-04-19T07:25:15+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt; &lt;img alt="Amoral Gemma 3 - QAT" src="https://preview.redd.it/zvrccxdusqve1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2786d393d1943e56077477a8167c9ea8a34db8e1" title="Amoral Gemma 3 - QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The same old Amoral Gemma 3, just with the QAT at q4. Refer to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;my first post&lt;/a&gt; for more info.&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-1B-v2-qat"&gt;[1B] &lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-4B-v2-qat"&gt;[4B]&lt;/a&gt; &lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2-qat"&gt;[12B]&lt;/a&gt; [27B - coming soon]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvrccxdusqve1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2qrqq/amoral_gemma_3_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T07:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250fu</id>
    <title>Gemma 3 QAT launch with MLX, llama.cpp, Ollama, LM Studio, and Hugging Face</title>
    <updated>2025-04-18T13:31:34+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Some weeks ago we released GGUFs corresponding to the QAT checkpoints of Gemma 3. Thanks to QAT, the model is able to preserve similar quality as &lt;code&gt;bfloat16&lt;/code&gt; while significantly reducing the memory requirements to load the model. That is, QAT is an additional fine-tuning that makes the model more rigorous to quantization.&lt;/p&gt; &lt;p&gt;As we only released the GGUFs, we got feedback that it would be great to have the unquantized QAT-based checkpoints to allow people to quantize for their own tools. So...we did it! Today we're releasing the unquantized QAT-based checkpoints. The models preserve quality better than naive quantization. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also collaborated with Prince (from MLX), llama.cpp, Ollama, LM Studio, and Hugging Face to make sure you can use the models in all your favorite tools!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post : &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unquantized checkpoints: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; (try ollama run gemma3:12b-it-qat)&lt;/li&gt; &lt;li&gt;LM Studio: &lt;a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"&gt;https://lmstudio.ai/model/gemma-3-12b-it-qat&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX: &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"&gt;https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k27fz2</id>
    <title>I created an interactive tool to visualize *every* attention weight matrix within GPT-2!</title>
    <updated>2025-04-18T15:18:17+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt; &lt;img alt="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" src="https://external-preview.redd.it/YW45M2FibXYwbXZlMWaepLM_4Oin4KjR_zAxiUwp5NOaLzCHkxa3urw0ZqL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60742f26e3a407482898d8e82f2a5d6e8f6ee5f" title="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgo9qamv0mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k29oe2</id>
    <title>QAT is slowly becoming mainstream now?</title>
    <updated>2025-04-18T16:52:07+00:00</updated>
    <author>
      <name>/u/__amberluz__</name>
      <uri>https://old.reddit.com/user/__amberluz__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released a QAT optimized Gemma 3 - 27 billion parameter model. The quantization aware training claims to recover close to 97% of the accuracy loss that happens during the quantization. Do you think this is slowly becoming the norm? Will non-quantized safetensors slowly become obsolete?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__amberluz__"&gt; /u/__amberluz__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2chcw</id>
    <title>Gemma 27B QAT works surprisingly well at Q2_K</title>
    <updated>2025-04-18T18:49:11+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test how well QAT models do at a lower quant size so I grabbed the smallest quant currently out for it, Q2_K at 10.5 GB. &lt;a href="https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF"&gt;https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I use my models mostly for my Japanese indie game, so following instructions, custom formatting and if it can roleplay or not is what I look for in models. My tests were all done in Japanese, which many models already have issues with at Q4 so I mostly use Q5. In my testing there were no grammatical errors, no random English or Chinese characters. It was able to roleplay in a custom format where I split the spoken words, the actions and the thoughts of the character into different brackets like ()&amp;lt;&amp;gt;「」without any issues. I also asked it basic questions about celebrities, and historical events, it got names and basic information right but dates were all wrong. My tests were done in Ollama with the standard Gemma3 settings.&lt;/p&gt; &lt;p&gt;Overall I am really impressed by the performance of the model especially for being a 27B at Q2. In theory running a 70B model at Q2 would fit into a single 24GB GPU so this technology is very interesting and could allow us to fit even larger models into our cards. After testing it I am really excited for more QAT models to come out in the future.&lt;/p&gt; &lt;p&gt;Have you guys tried running them at smaller quants?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2chcw/gemma_27b_qat_works_surprisingly_well_at_q2_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T18:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250r6</id>
    <title>New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality.</title>
    <updated>2025-04-18T13:32:01+00:00</updated>
    <author>
      <name>/u/Sea_Sympathy_495</name>
      <uri>https://old.reddit.com/user/Sea_Sympathy_495</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt; &lt;img alt="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." src="https://external-preview.redd.it/5lq32BTIzHqmPYcHvNrCp8JMhag9gsSSkR3cQgoYZBU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed6a861b423ef5ef481e863b5c6947b3cef14c0c" title="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Sympathy_495"&gt; /u/Sea_Sympathy_495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/?linkId=14034718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2li9f</id>
    <title>Speed testing Llama 4 Maverick with various hardware configs</title>
    <updated>2025-04-19T01:56:23+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured I would share some speed tests of Llama 4 Maverick with my various hardware setups.&lt;br /&gt; Wish we had VLLM quants, guessing the 3090's would be 2x faster vs llama.cpp.&lt;/p&gt; &lt;p&gt;llama.cpp 10x P40's - Q3.5 full offload&lt;br /&gt; &lt;strong&gt;15 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;162 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama.cpp on 16x 3090's - Q4.5 full offload&lt;br /&gt; &lt;strong&gt;36 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;781 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers on 1x 3090 + 16 core DDR4 Epyc - Q4.5&lt;br /&gt; &lt;strong&gt;29 T/s&lt;/strong&gt; at 3k context&lt;br /&gt; Prompt &lt;strong&gt;129 T/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ktransformers really shines with these tiny active param MOE's.&lt;/p&gt; &lt;p&gt;EDIT:&lt;br /&gt; Not my numbers but the M3 ultra can do:&lt;br /&gt; 47 T/s gen&lt;br /&gt; 332 T/s prompt&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2li9f/speed_testing_llama_4_maverick_with_various/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2ov6b</id>
    <title>How are NSFW LLMs trained/fine-tuned?</title>
    <updated>2025-04-19T05:15:43+00:00</updated>
    <author>
      <name>/u/GeneTangerine</name>
      <uri>https://old.reddit.com/user/GeneTangerine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does someone know? Generally LLMs are censored, do you guys have any resources?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeneTangerine"&gt; /u/GeneTangerine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2ov6b/how_are_nsfw_llms_trainedfinetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T05:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28ulo</id>
    <title>Time to step up the /local reasoning game</title>
    <updated>2025-04-18T16:17:11+00:00</updated>
    <author>
      <name>/u/vornamemitd</name>
      <uri>https://old.reddit.com/user/vornamemitd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt; &lt;img alt="Time to step up the /local reasoning game" src="https://preview.redd.it/wtibm8c3cmve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f80a0bad3e3f79619d29663e49d519eaa7898d" title="Time to step up the /local reasoning game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest OAI models tucked away behind intrusive &amp;quot;ID verification&amp;quot;....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vornamemitd"&gt; /u/vornamemitd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wtibm8c3cmve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k2kl84</id>
    <title>gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params).</title>
    <updated>2025-04-19T01:06:33+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt; &lt;img alt="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." src="https://preview.redd.it/2mx3qffqxove1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fccae6bd7191507d797de642e31420fffe50ff03" title="gemma 3 27b is underrated af. it's at #11 at lmarena right now and it matches the performance of o1(apparently 200b params)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mx3qffqxove1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-19T01:06:33+00:00</published>
  </entry>
</feed>
