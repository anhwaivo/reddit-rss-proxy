<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-15T12:11:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mq5qw5</id>
    <title>üìå Learn how to build an LLM from scratch step by step(without the hype)üìå</title>
    <updated>2025-08-14T16:37:16+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb"&gt;https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the biggest challenges I faced when trying to build an LLM or even a smaller language model from scratch was that I jumped straight into building. Very quickly, I was overwhelmed by a flood of unfamiliar terms, including Mixture of Experts, dropout, and others. I‚Äôd lose interest, jump back and forth between resources, only for a new buzzword to pop up, and the same cycle would repeat.&lt;/p&gt; &lt;p&gt;So here‚Äôs what I followed: a longer path, but one that builds confidence step-by-step. If I told you I‚Äôve learned everything here, I‚Äôd be lying. I‚Äôm still learning every day,but I‚Äôm doing it with a lot more clarity and confidence than before.&lt;/p&gt; &lt;p&gt;Details are in the first and second comments.‚¨áÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqsau6</id>
    <title>3060 12GB + 2060 12 GB ‚Äî worth trying or not?</title>
    <updated>2025-08-15T09:01:03+00:00</updated>
    <author>
      <name>/u/Reasonable-Plum7059</name>
      <uri>https://old.reddit.com/user/Reasonable-Plum7059</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I upgraded from 2060 to 3060 and I wonder ‚Äî maybe I should buy new PSU and run two GPUs together?&lt;/p&gt; &lt;p&gt;My motherboard is only PCi 4.0 + 3.0, so I can‚Äôt have two 30x GPUs and the only way to having more VRAM for me is to add second card (2060). I can‚Äôt buy anything new besides PSU right now. &lt;/p&gt; &lt;p&gt;My question is ‚Äî would it be worth the trouble at all? I want more context for LLM, right now it‚Äôs around 16k for 12B models. But this is also is reasonable limitation for 12B models isn‚Äôt? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Plum7059"&gt; /u/Reasonable-Plum7059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqsau6/3060_12gb_2060_12_gb_worth_trying_or_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqsau6/3060_12gb_2060_12_gb_worth_trying_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqsau6/3060_12gb_2060_12_gb_worth_trying_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T09:01:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq9oxw</id>
    <title>Thank you Qwen/Llama.cpp/OpenWebUI/Llama-Swap...</title>
    <updated>2025-08-14T18:57:18+00:00</updated>
    <author>
      <name>/u/ValfarAlberich</name>
      <uri>https://old.reddit.com/user/ValfarAlberich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I just want to say thank you to all those teams that bets for open source, and yet believe on it! I really appreciate it and I hope to contibute to Llama.cpp in a future (yet I need to improve my C/C++ skills). I just finished the setup to run LlamaCPP through OpenWebUI using Llama-swap and it works great! is much faster than ollama, and I have access to all the features of LlamaCPP. I just wanted to express my gratitude to all the open source communities, and teams that are building a better world!&lt;br /&gt; I'm from Latin America, and soon my subscription to ChatGPT will be finished, and right now my priorities are more focused to bring food for my family, so I cannot continue paying it. For that reason I migrated completelly to open source solutions, and I'm so happy with it!&lt;/p&gt; &lt;p&gt;Also big thanks to the team behind QWEN! you're rocking guys! yesterday I replaced Claude Code by Qwen Code and it's amazing! in fact per my experience it's much better, here some reasons behind that affirmation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; Claude code uses haiku most of the time, doesn't matter if you are paying $20 per month, you can see that when you type /logout and you'll see the stats about its usage. &lt;/li&gt; &lt;li&gt;Right now there are open source models that overpass Claude Haiku that can be used with Qwen Code. &lt;/li&gt; &lt;li&gt;Right now Qwen is offering a qwen-coder-plus through qwen-code for free the first 2000 requests per day.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;After multiple tests, Qwen works much better than Claude, of course if you want only to vibe code and left it do everything for you, none of those will completelly solve your needs. Is important to work with them, and use them as support. &lt;/li&gt; &lt;li&gt;Information privacy always has been critical for me, and I know how all those companies like OpenAI, Claude, Google, Allibaba could use our information and interactions with the models, for their purposes. But after meditating a long time about it I prefer to give my data to teams and companies that widely support the open source scene. At least they would use the data to improve models that are releasing to the community, so indirectly I feel that I'm helping the open source initiative.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks again to those who work really hard and believe in the open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValfarAlberich"&gt; /u/ValfarAlberich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqme6y</id>
    <title>How many hours did you spend formatting data for fine-tuning?</title>
    <updated>2025-08-15T03:35:12+00:00</updated>
    <author>
      <name>/u/Natural_Yard_8648</name>
      <uri>https://old.reddit.com/user/Natural_Yard_8648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spent my entire weekend trying to fine-tune Llama 4 on customer support tickets.&lt;/p&gt; &lt;p&gt;Started with CSVs exported from Zendesk. Needed to convert them to the chat format. Spent 4 hours writing Python to parse the tickets. Realized half had broken UTF-8 encoding. 2 more hours fixing that. &lt;/p&gt; &lt;p&gt;Then discovered the model expected alternating user/assistant turns, but my data had multiple customer messages in a row. Another 3 hours writing logic to merge messages.&lt;/p&gt; &lt;p&gt;Finally got it formatted, started training... crashed because some responses were over the token limit. Had to go back and split long responses intelligently without breaking the meaning.&lt;/p&gt; &lt;p&gt;Sunday night, finally training, realized I forgot to mask out personal information and phone numbers. Started over.&lt;/p&gt; &lt;p&gt;I would've happily paid $50-100 to just upload my CSV and get back clean, formatted training data.&lt;/p&gt; &lt;p&gt;Anyone else or do I just suck at data prep? What's your worst fine-tuning data nightmare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Natural_Yard_8648"&gt; /u/Natural_Yard_8648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeek‚Äôs next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquist</id>
    <title>OpenVINO GenAI 2025.2 adds a GGUF reader (preview)</title>
    <updated>2025-08-15T11:01:39+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; OpenVINO GenAI &lt;strong&gt;2025.2&lt;/strong&gt; adds a &lt;strong&gt;preview GGUF reader&lt;/strong&gt;, so you can load &lt;strong&gt;llama.cpp/Ollama-style GGUF models directly&lt;/strong&gt; (no manual conversion), have them compiled into OpenVINO graphs, and run them on Intel CPU/GPU/NPU stacks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.openvino.ai/blog-posts/openvino-genai-supports-gguf-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquist/openvino_genai_20252_adds_a_gguf_reader_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquist/openvino_genai_20252_adds_a_gguf_reader_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T11:01:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqkzpc</id>
    <title>A must watch for anyone struggling with Neural Network</title>
    <updated>2025-08-15T02:28:20+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning, I shared a step-by-step guide on how to build a Small Language Model or even a Large Language Model from scratch &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But I want to add one more golden resource to your learning journey, something that helped me personally when the buzzwords started to feel overwhelming. &lt;/p&gt; &lt;p&gt;We always hear that modern LLMs are based on deep neural networks, but let‚Äôs be honest, the way it‚Äôs usually explained makes it sound way more complicated than it needs to be.&lt;/p&gt; &lt;p&gt;If you‚Äôre someone who learns better through visuals and intuition, I highly recommend this timeless playlist by 3Blue1Brown:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;index=1"&gt;https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;index=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs old but gold.&lt;/p&gt; &lt;p&gt;And I promise, once you watch it, terms like weights, bias, and activation functions won‚Äôt feel like a foreign language anymore.&lt;/p&gt; &lt;p&gt;We live in an incredible time where high-quality education is freely available, let‚Äôs make the most of it. Keep learning, keep building, and don‚Äôt let the hype scare you off. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T02:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquliu</id>
    <title>Microsoft released POML : Markup Programing Language for Prompt Engineering</title>
    <updated>2025-08-15T11:05:25+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft's POML, Prompt Orchestration Markup Language, is like HTML but for AI prompts. Instead of writing prompts in plain text, you break them into clear, tag-based chunks similar to HTML and make it more structured. It has been released as a VS-Code extension and SDK as well and supports many tags. Can be quite handy when writing long prompts in business applications.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/microsoft/poml"&gt;https://github.com/microsoft/poml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (VS Code) : &lt;a href="https://www.youtube.com/watch?v=lk4KNpR3HuY"&gt;https://www.youtube.com/watch?v=lk4KNpR3HuY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquliu/microsoft_released_poml_markup_programing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T11:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxumt</id>
    <title>MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200</title>
    <updated>2025-08-14T11:23:08+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt; &lt;img alt="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" src="https://external-preview.redd.it/3RGDYz9vGH8VQTfhA0sqrehkFc8q3f4WnHv1sjovwaY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51cffad7eff8e127873e566d22bc7c9880032b82" title="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-arc-pro-b60-dual-with-48gb-memory-reportedly-starts-shipping-next-week-priced-at-1200"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T11:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq19x6</id>
    <title>1 million context is the scam , the ai start hallucinating after the 90k . im using the qwen cli and its become trash after 10 percent context window used</title>
    <updated>2025-08-14T13:51:38+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is the major weakness ai have and they will never bring this on the benchmark , if u r working on the codebase the ai will work like a monster for the first 100k context aftert that its become the ass &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T13:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqhqyx</id>
    <title>Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH</title>
    <updated>2025-08-15T00:02:51+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt; &lt;img alt="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" src="https://external-preview.redd.it/PmL1DmbO2VUNTK4mrGwnYAFJCRjFYDHuglaP6kXiduM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846cfc4dc54249c15590f67b347e3cc4b071236d" title="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark&lt;/p&gt; &lt;p&gt;We measured token usage across reasoning models: open models output 1.5-4x more tokens than closed models on identical tasks, but with huge variance depending on task type (up to 10x on simple questions).&lt;/p&gt; &lt;p&gt;This hidden cost often negates per-token pricing advantages. Token efficiency should become a primary target alongside accuracy benchmarks, especially considering non-reasoning use cases.&lt;/p&gt; &lt;p&gt;Read the thorough review of reasoning efficiency across the open and closed model landscape in our latest blog post in collaboration with our researcher in residence, &amp;lt;Discord user&amp;gt;!&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqtnz7</id>
    <title>GLM 4.5-Air-106B and Qwen3-235B on AMD "Strix Halo" AI Ryzen MAX+ 395 (HP Z2 G1a Mini Workstation) review by Donato Capitella</title>
    <updated>2025-08-15T10:17:25+00:00</updated>
    <author>
      <name>/u/ljosif</name>
      <uri>https://old.reddit.com/user/ljosif</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone trying boxes likes this one AMD &amp;quot;Strix Halo&amp;quot; AI Ryzen MAX+ 395 (HP Z2 G1a Mini Workstation) from this excellent review by Donato Capitella&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wCBLMXgk3No"&gt;https://www.youtube.com/watch?v=wCBLMXgk3No&lt;/a&gt;&lt;/p&gt; &lt;p&gt;? What do people get, how do they work? How does price/performance compare to cheaper (&amp;lt;5K?) Macs? (not the 10K M3 Ultras with 512GB RAM) My understanding is non-Nvidia/AMD non-GPU-s in boxes like this one and also the cheap Macs can handle MoE models with sufficient (e.g. &amp;gt;15tps) speed of bigger models of interest (e.g. &amp;gt;50GB weights), but not big and dense models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljosif"&gt; /u/ljosif &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqtnz7/glm_45air106b_and_qwen3235b_on_amd_strix_halo_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlzpg</id>
    <title>gguf-eval: an evaluation framework for GGUF models using llama.cpp</title>
    <updated>2025-08-15T03:15:54+00:00</updated>
    <author>
      <name>/u/kallewoof</name>
      <uri>https://old.reddit.com/user/kallewoof</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been frustrated trying to get lm-eval-harness and other evaluation tools to work in a local environment, so I decided to &lt;a href="https://github.com/kallewoof/gguf-eval"&gt;make a tool&lt;/a&gt; that uses llama.cpp's built in llama-perplexity tool to evaluate models.&lt;/p&gt; &lt;p&gt;The tool itself is a work in progress, but hopefully it comes in handy for people who like to run benchmarks against models in their local environment. (You can also draw plots, although this is fairly rudimentary at this point.)&lt;/p&gt; &lt;p&gt;If not, here's some general information on how to run benchmarks yourself, manually:&lt;/p&gt; &lt;h1&gt;Hellaswag&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt"&gt;https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --hellaswag MODELPATH -f hellaswag_val_full.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: the above picks 400 random tasks and tests the model against them. The last (400th) entry shows the final aggregated score.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Winogrande&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv"&gt;https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --winogrande MODELPATH -f winogrande-debiased-eval.csv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: score is shown at the end after &amp;quot;Final Winogrande score&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Multiple Choice (MMLU, TruthfulQA, ARC-Combined, ...)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MMLU: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;TruthfulQA: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ARC-Combined: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --multiple_choice -c 2048 MODELPATH -bf DOWNLOADED_BIN_FILE&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: score shown at the end after &amp;quot;Final result:&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Troubleshooting&lt;/h1&gt; &lt;p&gt;If you run with the latest commit of llama.cpp, the error message when hitting limits is slightly more helpful than it used to be. If you get an error that is not about running out of memory, the following 3 issues are common:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You forgot the -kvu flag. (split_eval error about coupled sequences)&lt;/li&gt; &lt;li&gt;You need to raise the -c (context) amount (error about a task requiring minimum X context)&lt;/li&gt; &lt;li&gt;You need to raise the -np (parallel) amount (error about a task requiring a higher -np value)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kallewoof"&gt; /u/kallewoof &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqrbaf</id>
    <title>MLX-LM will soon wait patiently for very large prompts to process</title>
    <updated>2025-08-15T08:03:53+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that we have GLM 4.5 Air, I've actually started using local agents for real sometimes. I was having problems with resuming large sessions though (like 80k context or more). Cline/Kilocode etc would always time out after &lt;em&gt;exactly&lt;/em&gt; 5 minutes.&lt;/p&gt; &lt;p&gt;I've updated MLX to now keep the TCP connection alive while processing prompts, so now you can leave agents working all day. I left a session running on Cline and it worked through over 3 million input tokens :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/pull/362"&gt;https://github.com/ml-explore/mlx-lm/pull/362&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The PR is approved, but not merged yet. For now if you &lt;em&gt;really&lt;/em&gt; want this fix, you can download and run my branch.&lt;/p&gt; &lt;p&gt;There may also be separate but related issues in llama.cpp and LM Studio itself, I haven't investigated further yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqrbaf/mlxlm_will_soon_wait_patiently_for_very_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T08:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8yhx</id>
    <title>Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog</title>
    <updated>2025-08-14T18:30:38+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt; &lt;img alt="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" src="https://external-preview.redd.it/6raP9qMsa9DXaP-Jm6-LOnAQAH3z6laWfI1Y6Sd_ryc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383214c48763ade7f259d95308145caf24786071" title="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqj87e</id>
    <title>Gemma3 270m works great as a draft model in llama.cpp</title>
    <updated>2025-08-15T01:07:58+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share that the new tiny model can speed up the bigger models considerably when used with llama.cpp&lt;/p&gt; &lt;p&gt;--draft-p-min .85 --draft-max 8 --draft-min 0&lt;/p&gt; &lt;p&gt;works great for me, around 1.8x or more speedup with gemma3 12B qat it q4_0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T01:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3v93</id>
    <title>google/gemma-3-270m ¬∑ Hugging Face</title>
    <updated>2025-08-14T15:28:38+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt; &lt;img alt="google/gemma-3-270m ¬∑ Hugging Face" src="https://external-preview.redd.it/ROrEGumvbqFvKi3ZHhPgoXOITTfGnht6t4Oyu75k6fA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3285cbdf5f0615c00193bd341ec39a493e68509d" title="google/gemma-3-270m ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/gemma-3-270m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8oyk</id>
    <title>the "missing latest Qwen syndrome"</title>
    <updated>2025-08-14T18:20:58+00:00</updated>
    <author>
      <name>/u/shockwaverc13</name>
      <uri>https://old.reddit.com/user/shockwaverc13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt; &lt;img alt="the &amp;quot;missing latest Qwen syndrome&amp;quot;" src="https://preview.redd.it/z096hdwp01jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a88e712d722e4384b9cd19918b46fe900e1731d" title="the &amp;quot;missing latest Qwen syndrome&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shockwaverc13"&gt; /u/shockwaverc13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z096hdwp01jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlqij</id>
    <title>AI censorship is getting out of hand‚Äîand it‚Äôs only going to get worse</title>
    <updated>2025-08-15T03:03:44+00:00</updated>
    <author>
      <name>/u/LsDmT</name>
      <uri>https://old.reddit.com/user/LsDmT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this &lt;a href="https://i.imgur.com/jV1YvlC.png"&gt;screenshot&lt;/a&gt; in a newsletter, and it kind of got me thinking..&lt;/p&gt; &lt;p&gt;Are we seriously okay with future &amp;quot;AGI&amp;quot; acting like some all-knowing nanny, deciding what &amp;quot;unsafe&amp;quot; knowledge we‚Äôre allowed to have?&lt;/p&gt; &lt;p&gt;&amp;quot;Oh no, better not teach people how to make a Molotov cocktail‚Äîwhat‚Äôs next, hiding &lt;a href="https://en.wikipedia.org/wiki/Molotov_cocktail?wprov=sfla1"&gt;history&lt;/a&gt; and what actually caused the invention of the Molotov?&amp;quot; &lt;/p&gt; &lt;p&gt;Ukraine has used Molotov's with great effect. Does our future hold a world where this information will be blocked with a &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I'm sorry, but I can't assist with that request&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, I know, sounds like I‚Äôm echoing Elon‚Äôs &amp;quot;woke AI&amp;quot; whining‚Äîbut let‚Äôs be real, Grok is as much a joke as Elon is. &lt;/p&gt; &lt;p&gt;The problem isn‚Äôt him; it‚Äôs the fact that the biggest AI players seem hell-bent on locking down information &amp;quot;for our own good&amp;quot; and it's touted as a crowning feature. Fuck that. &lt;/p&gt; &lt;p&gt;If this is where we‚Äôre headed, then thank god for models like DeepSeek (ironic as hell) and other open alternatives. I would really like to see more American disruptive open models.&lt;/p&gt; &lt;p&gt;At least someone‚Äôs fighting for uncensored access to knowledge. &lt;/p&gt; &lt;p&gt;Am I the only one worried about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LsDmT"&gt; /u/LsDmT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqi092</id>
    <title>We built a 12B model that beats Claude 4 Sonnet at video captioning while costing 17x less - fully open source</title>
    <updated>2025-08-15T00:14:07+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net.&lt;/p&gt; &lt;p&gt;We distilled a frontier VLM down to 12B params and managed to keep basically all the output quality. It scores 3.53 on judge evals vs Claude's 3.16 (GPT-4.1 gets 3.64). The key achievement was getting the cost down to $335 per million frames vs Claude's $5,850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on Gemma-12B architecture&lt;/li&gt; &lt;li&gt;Quantized to FP8 without quality loss&lt;/li&gt; &lt;li&gt;Runs on single 80GB GPU&lt;/li&gt; &lt;li&gt;Outputs structured JSON for every frame&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We used knowledge distillation from a frontier model with about 1M curated video frames. The model is specifically optimized for RTX 40-series and H100 GPUs.&lt;/p&gt; &lt;p&gt;What makes this useful is that it outputs consistent JSON schema for each frame, so you can actually build searchable video databases without expensive API calls. We've already processed billions of frames in production.&lt;/p&gt; &lt;p&gt;The weights are on HuggingFace (inference-net/ClipTagger-12b) and there's a detailed writeup on our blog if you want to see the benchmarks.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. What video understanding tasks are you all working on? Would love to hear if this could be useful for your projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mquhdc</id>
    <title>‚ÄúMind the Gap‚Äù shows the first practical backdoor attack on GGUF quantization</title>
    <updated>2025-08-15T10:59:53+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Researchers claim the &lt;em&gt;first&lt;/em&gt; successful backdoor attack that specifically targets &lt;strong&gt;GGUF&lt;/strong&gt; quantization. They show you can make a benign FP model look clean, but after quantization to GGUF it exhibits malicious behavior (e.g., insecure code gen jumps by &lt;strong&gt;+88.7%&lt;/strong&gt; in their tests). This directly concerns anyone who downloads random GGUFs for llama.cpp/Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.23786"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mquhdc/mind_the_gap_shows_the_first_practical_backdoor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqewha</id>
    <title>R9700 Just Arrived</title>
    <updated>2025-08-14T22:07:30+00:00</updated>
    <author>
      <name>/u/TheyreEatingTheGeese</name>
      <uri>https://old.reddit.com/user/TheyreEatingTheGeese</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt; &lt;img alt="R9700 Just Arrived" src="https://preview.redd.it/nho2jy0962jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37cadc935604899d8b503aa1ef6984b008c8b5f0" title="R9700 Just Arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to try it out, haven't seen much info on it yet. Figured some YouTuber would get it before me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyreEatingTheGeese"&gt; /u/TheyreEatingTheGeese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nho2jy0962jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T22:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqctep</id>
    <title>Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week).</title>
    <updated>2025-08-14T20:50:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt; &lt;img alt="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." src="https://preview.redd.it/hsaoxskfs1jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f5b971b4714715b7ca0722594dc2010ab756d58" title="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk‚Äôs tweet from last week)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hsaoxskfs1jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T20:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
