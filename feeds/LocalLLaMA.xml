<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-14T22:23:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jbfdr2</id>
    <title>More about SOCAMM on GTC</title>
    <updated>2025-03-14T21:50:56+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbfdr2/more_about_socamm_on_gtc/"&gt; &lt;img alt="More about SOCAMM on GTC" src="https://preview.redd.it/srgq2bct7qoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dae0db874380c97c71b35fa953da2e8a5377c87b" title="More about SOCAMM on GTC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's pretty early, if they are not coming out until the end of the year. But I guess why not, the cat is already out of the bag. They might aswell drum up some hype.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/SKhynix/status/1900517279667757363?t=ZbMX81TTikd8Nag5t4YNOQ&amp;amp;s=19"&gt;https://x.com/SKhynix/status/1900517279667757363?t=ZbMX81TTikd8Nag5t4YNOQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/srgq2bct7qoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbfdr2/more_about_socamm_on_gtc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbfdr2/more_about_socamm_on_gtc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T21:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoc8n</id>
    <title>QwQ on LiveBench (update) - is better than DeepSeek R1!</title>
    <updated>2025-03-13T22:14:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt; &lt;img alt="QwQ on LiveBench (update) - is better than DeepSeek R1!" src="https://preview.redd.it/sb78tt607joe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22464da14ee7a94ffe6b7ad76b52c34bab00a921" title="QwQ on LiveBench (update) - is better than DeepSeek R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb78tt607joe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7a7w</id>
    <title>Sesame's CSM is good actually.</title>
    <updated>2025-03-14T15:58:53+00:00</updated>
    <author>
      <name>/u/CognitiveSourceress</name>
      <uri>https://old.reddit.com/user/CognitiveSourceress</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1jb7a7w/video/qwjbtau6cooe1/player"&gt;https://reddit.com/link/1jb7a7w/video/qwjbtau6cooe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, I understand that a lot of people are disappointed that Sesame's model isn't what we thought it was. I certainly was.&lt;/p&gt; &lt;p&gt;But I think a lot of people don't realize how much of the heart of their demo this model actually is. It's just going to take some elbow grease to make it work and make it work quickly, locally.&lt;/p&gt; &lt;p&gt;The video above contains dialogue generated with Sesame's CSM. It demonstrates, to an extent, why this isn't just TTS. It &lt;em&gt;is&lt;/em&gt; TTS but not &lt;em&gt;just&lt;/em&gt; TTS.&lt;/p&gt; &lt;p&gt;Sure we've seen TTS get expressive before, but this TTS gets expressive &lt;em&gt;in context&lt;/em&gt;. You feed it the audio of the whole conversation leading up to the needed line (or, at least enough of it) all divided up by speaker, in order. The CSM then considers that context when deciding how to express the line.&lt;/p&gt; &lt;p&gt;This is cool for an example like the one above, but what about Maya (and whatever his name is, I guess, we all know what people wanted)?&lt;/p&gt; &lt;p&gt;Well, what their model does (probably, educated guess) is record you, break up your speech into utterances and add them to the stack of audio context, do speech recognition for transcription, send the text to an LLM, then use the CSM to generate the response.&lt;/p&gt; &lt;p&gt;Rinse repeat.&lt;/p&gt; &lt;p&gt;All of that with normal TTS isn't novel. This has been possible for... years honestly. It's the CSM and it's ability to express itself in context that makes this all click into something wonderful. Maya is just proof of how well it works.&lt;/p&gt; &lt;p&gt;I understand people are disappointed not to have a model they can download and run for full speech to speech expressiveness all in one place. I hoped that was what this was too.&lt;/p&gt; &lt;p&gt;But honestly, in some ways this is better. This can be used for so much more. Your local NotebookLM clones just got WAY better. The video above shows the potential for production. And it does it all with voice cloning so it can be anyone.&lt;/p&gt; &lt;p&gt;Now, Maya was running an 8B model, 8x larger than what we have, and she was fine tuned. Probably on an actress specifically asked to deliver the &amp;quot;girlfriend experience&amp;quot; if we're being honest. But this is far from nothing.&lt;/p&gt; &lt;p&gt;This CSM is good actually.&lt;/p&gt; &lt;p&gt;On a final note, the vitriol about this is a bad look. This is the kind of response that makes good people not wanna open source stuff. They released something really cool and people are calling them scammers and rug-pullers over it. I can understand &amp;quot;liar&amp;quot; to an extent, but honestly? The research explaining what this was was right under the demo all this time.&lt;/p&gt; &lt;p&gt;And if you don't care about other people, you should care that this response may make this CSM, which is genuinely good, get a bad reputation and be dismissed by people making the end user open source tools you so obviously want.&lt;/p&gt; &lt;p&gt;So, please, try to reign in the bad vibes.&lt;/p&gt; &lt;p&gt;Technical:&lt;/p&gt; &lt;p&gt;NVIDIA RTX3060 12GB&lt;/p&gt; &lt;p&gt;Reference audio generated by Hailuo's remarkable and free limited use TTS. The script for both the reference audio and this demo was written by ChatGPT 4.5.&lt;/p&gt; &lt;p&gt;I divided the reference audio into sentences, fed them in with speaker ID and transcription, then ran the second script through the CSM. I did three takes and took the best complete take for each line, no editing. I had ChatGPT gen up some images in DALL-E and put it together in DaVinci Resolve.&lt;/p&gt; &lt;p&gt;Each take took 2 min 20 seconds to generate, this includes loading the model at the start of each take.&lt;/p&gt; &lt;p&gt;Each line was generated in approximately .3 real time, meaning something 2 seconds long takes 6 seconds to generate. I stuck to utterances and generations of under 10s, as the model seemed to degrade past that, but this is nothing new for TTS and is just a matter of smart chunking for your application.&lt;/p&gt; &lt;p&gt;I plan to put together an interface for this so people can play with it more, but I'm not sure how long that may take me, so stay tuned but don't hold your breath please!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CognitiveSourceress"&gt; /u/CognitiveSourceress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7a7w/sesames_csm_is_good_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7a7w/sesames_csm_is_good_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7a7w/sesames_csm_is_good_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahs0b</id>
    <title>OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch</title>
    <updated>2025-03-13T17:38:10+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt; &lt;img alt="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" src="https://external-preview.redd.it/YuFnFIavAP98hFeGzOxLZQ1jrf6fXSzPC6RHQ4YO4ew.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2312e28fa573cb9d493e784a1275b4624e4c905" title="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/03/13/openai-calls-deepseek-state-controlled-calls-for-bans-on-prc-produced-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqylp</id>
    <title>LLM must pass a skill check to talk to me</title>
    <updated>2025-03-14T00:13:29+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt; &lt;img alt="LLM must pass a skill check to talk to me" src="https://external-preview.redd.it/Y3U2cGt3NmNzam9lMRWrFMxzjmNxpTNLvYX1gtD81VUNlzdlH0AiqtOky6_L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45045d6f36cca8e45bf1337eccb796748268735" title="LLM must pass a skill check to talk to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w7dney6csjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbcpdu</id>
    <title>AMD's Strix Halo - Under the Hood</title>
    <updated>2025-03-14T19:55:32+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcpdu/amds_strix_halo_under_the_hood/"&gt; &lt;img alt="AMD's Strix Halo - Under the Hood" src="https://external-preview.redd.it/wEu3U_UMzA19CzyyaC-damgrs2mfoqYlCEJAaUxPsJ0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40af4eabee5b182b1ed2f738034916fc7cfcca98" title="AMD's Strix Halo - Under the Hood" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chipsandcheese.com/p/amds-strix-halo-under-the-hood"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcpdu/amds_strix_halo_under_the_hood/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcpdu/amds_strix_halo_under_the_hood/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T19:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1janmn8</id>
    <title>SESAME IS HERE</title>
    <updated>2025-03-13T21:43:12+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sesame just released their 1B CSM.&lt;br /&gt; Sadly parts of the pipeline are missing.&lt;/p&gt; &lt;p&gt;Try it here:&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/sesame/csm-1b"&gt;https://huggingface.co/spaces/sesame/csm-1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation steps here:&lt;br /&gt; &lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb97u5</id>
    <title>Run SesameAILabs/csm locally with Gradio UI with CUDA or CPU (if no CUDA)</title>
    <updated>2025-03-14T17:19:38+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Choose from predefined voice prompts&lt;/li&gt; &lt;li&gt;Upload or record custom voice prompts&lt;/li&gt; &lt;li&gt;Enter multi-turn conversations&lt;/li&gt; &lt;li&gt;Generate and play audio directly in browser&lt;/li&gt; &lt;li&gt;Use your own voice as custom voice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use Cases:&lt;br /&gt; Give it text and create a discussion like NotebookLLM.&lt;br /&gt; Create podcast in your own voice.&lt;/p&gt; &lt;p&gt;Github Repo:- &lt;a href="https://github.com/akashjss/sesame-csm/tree/main"&gt;https://github.com/akashjss/sesame-csm/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbff6e</id>
    <title>Block Diffusion (hybrid autoregression/diffusion LLM)</title>
    <updated>2025-03-14T21:52:41+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"&gt; &lt;img alt="Block Diffusion (hybrid autoregression/diffusion LLM)" src="https://external-preview.redd.it/u-xTxhioI7qr36mspPK2XoVAHe5CTgB51rSZuXFnsHo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89fcd57c6397d686054b65c2b449fb7a35985494" title="Block Diffusion (hybrid autoregression/diffusion LLM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kuleshov-group/bd3lms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbff6e/block_diffusion_hybrid_autoregressiondiffusion_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T21:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb9bfb</id>
    <title>QwQ-32B seems useless on local ollama. Anyone have luck to escape from thinking hell?</title>
    <updated>2025-03-14T17:23:50+00:00</updated>
    <author>
      <name>/u/soteko</name>
      <uri>https://old.reddit.com/user/soteko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, trying new QwQ-32B from 2 days ago &lt;a href="https://huggingface.co/Qwen/QwQ-32B-GGUF"&gt;https://huggingface.co/Qwen/QwQ-32B-GGUF&lt;/a&gt; and simply I can't get any real code out from it. It is thinking and thinking and never stops and probably will hit some limit like Context or Max Tokens and will stop before getting any real result.&lt;/p&gt; &lt;p&gt;I am running it on CPU, with temperature 0.7, Top P 0.95, Max Tokens (num_predict) 12000, Context 2048 - 8192.&lt;/p&gt; &lt;p&gt;Anyone trying it for coding?&lt;/p&gt; &lt;p&gt;EDIT: Just noticed that I've made mistake it is 12 000 Max Token (num_predict)&lt;/p&gt; &lt;p&gt;EDIT: More info I am running in Docker Open Web UI and Ollama - ver 0.5.13&lt;/p&gt; &lt;p&gt;EDIT: And interesting part, in thinking process there is useful code, but it is in Thinking part and it is mess with model words.&lt;/p&gt; &lt;p&gt;EDIT: it is Q5_K_M model.&lt;/p&gt; &lt;p&gt;EDIT: Model with this settings is using 30GB memory as reported by Docker container.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soteko"&gt; /u/soteko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb4jcr</id>
    <title>Difference in Gemma 3 27b performance between ai studio and ollama</title>
    <updated>2025-03-14T13:58:51+00:00</updated>
    <author>
      <name>/u/Any-Mathematician683</name>
      <uri>https://old.reddit.com/user/Any-Mathematician683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I am building an enterprise grade RAG application and looking for a open-source LLM model for Summarisation and Question-answering purposes.&lt;/p&gt; &lt;p&gt;I really liked the Gemma 3 27B model when i tried it on ai studio. It is summarising transcripts with great precision. Infact, performance on openrouter is also great.&lt;/p&gt; &lt;p&gt;But as &lt;strong&gt;I am trying it on ollama, it is giving me subpar performance compared to aistudio.&lt;/strong&gt; I have tried &lt;a href="https://ollama.com/library/gemma3:27b-it-fp16"&gt;27b-it-fp16&lt;/a&gt; model as well as I thought performance loss might be because of quantization.&lt;/p&gt; &lt;p&gt;I also went through this &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;tutorial from Unsloth&lt;/a&gt;, and &lt;strong&gt;tried with recommended settings(temperature=1.0, top-k 64, top-p 0.95) on llama.cpp.&lt;/strong&gt; I did notice little better output but it is not as compared to output on openrouter / aistudio.&lt;/p&gt; &lt;p&gt;I noticed the &lt;strong&gt;same performance gap for command r models&lt;/strong&gt; between ollama and cohere playground.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can you please help me in identifying the root cause for this?&lt;/strong&gt; I genuinely believe there has to be some reason behind it.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Mathematician683"&gt; /u/Any-Mathematician683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jazfkf</id>
    <title>I created an OpenAI TTS compatible endpoint for Sesame CSM 1B</title>
    <updated>2025-03-14T08:48:44+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a work in progress, especially around trying to normalize the voice/voices. &lt;/p&gt; &lt;p&gt;Give it a shot and let me know what you think. PR's welcomed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/phildougherty/sesame_csm_openai"&gt;https://github.com/phildougherty/sesame_csm_openai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T08:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7u4v</id>
    <title>What is the best uncensored LLM?</title>
    <updated>2025-03-14T16:21:55+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not talking about &amp;quot;I will write you a erotic story&amp;quot; type of uncensored LLM, I'm talking about &amp;quot;I will tell you how to make a bomb&amp;quot; (I won't do that) type of uncensored LLM. It seems like everyone, when talking about &amp;quot;Uncensored&amp;quot; models, talks about erotic uncensored models and not about what I want. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T16:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxec3</id>
    <title>Sesame CSM 1B Voice Cloning</title>
    <updated>2025-03-14T06:18:15+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt; &lt;img alt="Sesame CSM 1B Voice Cloning" src="https://external-preview.redd.it/JaEGat2-q67uEqWoPpGo5Nx0tvU4ZMhHe5tLQNQxW9w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c285788573980e7289358fbf97c0847d9b60866" title="Sesame CSM 1B Voice Cloning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/csm-voice-cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoy9g</id>
    <title>Meme i made</title>
    <updated>2025-03-13T22:41:19+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt; &lt;img alt="Meme i made" src="https://external-preview.redd.it/a3h0bzNwMWxiam9lMWaOI-rE6YlXiP74zpe4ixbVM_QsxQQzHzv1tNet8B-Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aee2797aa64747b42b65d38774f6590f3d0a9e9d" title="Meme i made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzku6n1lbjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbcau6</id>
    <title>Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025</title>
    <updated>2025-03-14T19:38:17+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"&gt; &lt;img alt="Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025" src="https://external-preview.redd.it/fgsG07RfhkENrvhUxj6Tf9wZnUaizjcQ0f11neSzDnI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a885de65705817dea74206ef0fbe0b9d88413e9b" title="Race to launch most powerful AI mini PC ever heats up as GMKTec confirms Ryzen AI Max+ 395 product for May 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techradar.com/pro/race-to-launch-most-powerful-ai-mini-pc-ever-heats-up-as-gmktec-confirms-ryzen-ai-max-395-product-for-may-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbcau6/race_to_launch_most_powerful_ai_mini_pc_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T19:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb3mpe</id>
    <title>Gemma 3 Function Calling Example prompt</title>
    <updated>2025-03-14T13:14:57+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt; &lt;img alt="Gemma 3 Function Calling Example prompt" src="https://preview.redd.it/xv7wwtdmnnoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53b82d556c025c9987fe92be4363ea5c1a3d97b0" title="Gemma 3 Function Calling Example prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xv7wwtdmnnoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1sgv</id>
    <title>Conclusion: Sesame has shown us a CSM. Then Sesame announced that it would publish... something. Sesame then released a TTS, which they obviously misleadingly and falsely called a CSM. Do I see that correctly?</title>
    <updated>2025-03-14T11:34:28+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It wouldn't have been a problem at all if they had simply said that it wouldn't be open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbd55g</id>
    <title>We almost had it guys.</title>
    <updated>2025-03-14T20:13:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"&gt; &lt;img alt="We almost had it guys." src="https://preview.redd.it/fo83jzwhqpoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7f4c8a2774363ac67cc00199bad26d4f9f66019" title="We almost had it guys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fo83jzwhqpoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbd55g/we_almost_had_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T20:13:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5qvy</id>
    <title>KoboldCPP 1.86 just dropped with support of Gemma-3</title>
    <updated>2025-03-14T14:53:08+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.86"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here it is. Just tried it, thank you guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1tum</id>
    <title>HowTo: Decentralized LLM on Akash, IPFS &amp; Pocket Network, could this run LLaMA?</title>
    <updated>2025-03-14T11:36:40+00:00</updated>
    <author>
      <name>/u/era_hickle</name>
      <uri>https://old.reddit.com/user/era_hickle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt; &lt;img alt="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" src="https://external-preview.redd.it/Hrj-dOCVDQmxgV_iRUOvs_Xsy9EB5pShvqJs-R97RdI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6f0426a6f366fea7132175fe3881893d8a8b01b" title="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/era_hickle"&gt; /u/era_hickle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pocket.network/case-study-building-a-decentralized-deepseek-combining-open-data-compute-and-reasoning-with-pocket-network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:36:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb6gk7</id>
    <title>I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good</title>
    <updated>2025-03-14T15:23:51+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt; &lt;img alt="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" src="https://preview.redd.it/uc4ktdmraooe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fee6edf8a1814ec03199e5138b163065448da" title="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4ktdmraooe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jbbwc2</id>
    <title>This week did not go how I expected at all</title>
    <updated>2025-03-14T19:21:22+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"&gt; &lt;img alt="This week did not go how I expected at all" src="https://preview.redd.it/8onlrxywgpoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=029eb52afd21cbc37e07c065e010898a206b6e1e" title="This week did not go how I expected at all" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8onlrxywgpoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jbbwc2/this_week_did_not_go_how_i_expected_at_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T19:21:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jba8c1</id>
    <title>Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM</title>
    <updated>2025-03-14T18:05:43+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt; &lt;img alt="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" src="https://b.thumbs.redditmedia.com/HVn5TgokJMLlPcDoL4fVZ2_Vk4oxK6Eh7mcozu3sLRs.jpg" title="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Gemma 3 (12B) up to &lt;strong&gt;6x longer context lengths&lt;/strong&gt; with Unsloth than Hugging Face + FA2 on a 24GB GPU. 27B also fits in 24GB!&lt;/p&gt; &lt;p&gt;We also saw &lt;strong&gt;infinite exploding gradients&lt;/strong&gt; when using older GPUs (Tesla T4s, RTX 2080) with float16 for Gemma 3. Newer GPUs using float16 like A100s also have the same issue - I auto fix this in Unsloth!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are also double BOS tokens which ruin finetunes for Gemma 3 - Unsloth auto corrects for this as well!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth now supports&lt;/strong&gt; &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;everything&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; This includes &lt;strong&gt;full fine-tuning&lt;/strong&gt;, pretraining, and support for all models (like &lt;strong&gt;Mixtral&lt;/strong&gt;, MoEs, Cohere etc. models) and algorithms like DoRA&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3-4B-it&amp;quot;, load_in_4bit = True, load_in_8bit = False, # [NEW!] 8bit full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Gemma 3 (27B) fits in 22GB VRAM. You can read our in depth blog post about the new changes: &lt;a href="https://unsloth.ai/blog/gemma3"&gt;unsloth.ai/blog/gemma3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tune Gemma 3 (4B) for free using our&lt;/strong&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab notebook&lt;/strong&gt;&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;li&gt;We uploaded Dynamic 4-bit quants, and it's even more effective due to Gemma 3's multi modality. See all Gemma 3 Uploads including GGUF, 4-bit etc: &lt;a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7xnidddi3poe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75c2f0fad10c4e170d1455269118d0fff4c38baf"&gt;Gemma 3 27B quantization errors&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made a &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;Guide to run Gemma 3&lt;/a&gt; properly and fixed issues with GGUFs not working with vision - reminder the correct params according to the Gemma team are &lt;strong&gt;temperature = 1.0, top_p = 0.95, top_k = 64&lt;/strong&gt;. According to the Ollama team, you should use temp = 0.1 in Ollama for now due to some backend differences. Use temp = 1.0 in llama.cpp, Unsloth, and other backends!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gemma 3 Dynamic 4-bit instruct quants:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-unsloth-bnb-4bit"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-unsloth-bnb-4bit"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let me know if you have any questions and hope you all have a lovely Friday and weekend! :) Also to update Unsloth do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab Notebook&lt;/strong&gt;&lt;/a&gt;.ipynb) with free GPU to finetune, do inference, data prep on Gemma 3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T18:05:43+00:00</published>
  </entry>
</feed>
