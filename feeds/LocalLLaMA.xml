<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-01T13:34:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1if64ds</id>
    <title>Llama 3.1 70b Performance</title>
    <updated>2025-02-01T13:18:27+00:00</updated>
    <author>
      <name>/u/ScArL3T</name>
      <uri>https://old.reddit.com/user/ScArL3T</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I cannot find anywhere on the internet if these numbers are the expected performance or not for running Llama 3.1 70B AWQ INT4 - from my understanding the INT4 should reduce memory footprint and also increase its performance.&lt;/p&gt; &lt;p&gt;Running latest vLLM with prefix caching and chunked prefill.&lt;/p&gt; &lt;p&gt;Benched with llmperf, concurrent requests = 1.&lt;/p&gt; &lt;p&gt;- 1x H100 GPU: ~35 output tokens per second.&lt;/p&gt; &lt;p&gt;- 2x A100 GPU: ~40 output tokens per second.&lt;/p&gt; &lt;p&gt;Also speculative decoding using Llama 3.2 1B:&lt;/p&gt; &lt;p&gt;- 1x H100 GPU: ~80 output tokens per second. &lt;/p&gt; &lt;p&gt;- 2x A100 GPU: ~60 output tokens per second.&lt;/p&gt; &lt;p&gt;I feel like I got conflicting results, and I don't know what to make of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ScArL3T"&gt; /u/ScArL3T &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if64ds/llama_31_70b_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if64ds/llama_31_70b_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if64ds/llama_31_70b_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T13:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1if6azp</id>
    <title>Thoughts about phi-4?</title>
    <updated>2025-02-01T13:28:46+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a while since it was released. What do you think about it? How does it compare to qwen-2.5-14B and Mistral Nemo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if6azp/thoughts_about_phi4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if6azp/thoughts_about_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if6azp/thoughts_about_phi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T13:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iexh50</id>
    <title>Soft Prompt Tuning Modern LLMs</title>
    <updated>2025-02-01T03:34:44+00:00</updated>
    <author>
      <name>/u/xnick77x</name>
      <uri>https://old.reddit.com/user/xnick77x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"&gt; &lt;img alt="Soft Prompt Tuning Modern LLMs" src="https://external-preview.redd.it/6_3ctAqxbdvXiaHQYdKYaOoJmjsKTUr9kcigziRYZMg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4894ec1ef80e49f99a7de1c890f9b439263adb94" title="Soft Prompt Tuning Modern LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xnick77x"&gt; /u/xnick77x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://frugalgpu.substack.com/p/soft-prompt-tuning-modern-llms-442"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T03:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ielhyu</id>
    <title>Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI</title>
    <updated>2025-01-31T18:24:36+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt; &lt;img alt="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Daniel &amp;amp; I (Mike) at &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; collabed with Tim from &lt;a href="https://github.com/open-webui/open-webui"&gt;Open WebUI&lt;/a&gt; to bring you this step-by-step on how to run the non-distilled DeepSeek-R1 Dynamic 1.58-bit model locally!&lt;/p&gt; &lt;p&gt;This guide is summarized so I highly recommend you read the full guide (with pics) here: &lt;a href="https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/"&gt;https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Expect 2 tokens/s with 96GB RAM (without GPU).&lt;/p&gt; &lt;h1&gt;To Run DeepSeek-R1:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download prebuilt binaries or build from source following &lt;a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"&gt;this guide&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Download the Model (1.58-bit, 131GB) from&lt;/strong&gt; &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get the model from &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Use Python to download it programmatically:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download snapshot_download( repo_id=&amp;quot;unsloth/DeepSeek-R1-GGUF&amp;quot;, local_dir=&amp;quot;DeepSeek-R1-GGUF&amp;quot;, allow_patterns=[&amp;quot;*UD-IQ1_S*&amp;quot;] ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Once the download completes, you‚Äôll find the model files in a directory structure like this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-GGUF/ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/ ‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf ‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf ‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Ensure you know the path where the files are stored.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Install and Run Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you don‚Äôt already have it installed, no worries! It‚Äôs a simple setup. Just follow the Open WebUI docs here: &lt;a href="https://docs.openwebui.com/"&gt;https://docs.openwebui.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Once installed, start the application - we‚Äôll connect it in a later step to interact with the DeepSeek-R1 model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Start the Model Server with Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now that the model is downloaded, the next step is to run it using Llama.cpp‚Äôs server mode.&lt;/p&gt; &lt;h1&gt;üõ†Ô∏èBefore You Begin:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Locate the llama-server Binary&lt;/li&gt; &lt;li&gt;If you built Llama.cpp from source, the llama-server executable is located in:llama.cpp/build/bin Navigate to this directory using:cd [path-to-llama-cpp]/llama.cpp/build/bin Replace [path-to-llama-cpp] with your actual Llama.cpp directory. For example:cd ~/Documents/workspace/llama.cpp/build/bin&lt;/li&gt; &lt;li&gt;Point to Your Model Folder&lt;/li&gt; &lt;li&gt;Use the full path to the downloaded GGUF files.When starting the server, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üöÄStart the Server&lt;/h1&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Example (If Your Model is in /Users/tim/Documents/workspace):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;‚úÖ Once running, the server will be available at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://127.0.0.1:10000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;üñ•Ô∏è Llama.cpp Server Running&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/erjbg5v5cbge1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fff4de133562bb6f67076db17285860b7294f2ad"&gt;After running the command, you should see a message confirming the server is active and listening on port 10000.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Step 5: Connect Llama.cpp to Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open Admin Settings in Open WebUI.&lt;/li&gt; &lt;li&gt;Go to Connections &amp;gt; OpenAI Connections.&lt;/li&gt; &lt;li&gt;Add the following details:&lt;/li&gt; &lt;li&gt;URL ‚Üí &lt;a href="http://127.0.0.1:10000/v1API"&gt;http://127.0.0.1:10000/v1API&lt;/a&gt; Key ‚Üí none&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Adding Connection in Open WebUI&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2"&gt;https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You don't need a GPU to run this model but it will make it faster especially when you have at least 24GB of VRAM.&lt;/li&gt; &lt;li&gt;Try to have a sum of RAM + VRAM = 120GB+ to get decent tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have any questions please let us know and also - any suggestions are also welcome! Happy running folks! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T18:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1ahn</id>
    <title>Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful.</title>
    <updated>2025-02-01T07:36:23+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"&gt; &lt;img alt="Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful." src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/cepa/94d81ea83b0c2e97340c7c512767da18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T07:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5f3b</id>
    <title>Mistral Small 3 is awesome, but...</title>
    <updated>2025-02-01T12:37:11+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has a rather strange way of correcting itself and letting it go unnoticed.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A marble is put in a glass. The glass is then turned upside down and put on a table. Then the glass is picked up and put in a microwave. Where's the marble?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the sequence of events you described, the marble would still be in the glass. Here's why:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; The marble is put in the glass.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; The glass is turned upside down and put on a table. At this point, the marble should fall out of the glass onto the table.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; The glass is picked up and put in a microwave. Since the marble fell out in the previous step, it would not be in the glass when the glass is put in the microwave.&lt;/p&gt; &lt;p&gt;Therefore, the marble is on the table.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It initially and wrongly states that the marble is still inside the glass, and when it's telling why it came to that conclusion, it makes it correct and nonchalantly alters the outcome, lol.&lt;/p&gt; &lt;p&gt;That is the most clear example. I have noticed it do this quite a lot but more subtly for other tasks, such as creative writing. It kinda alters/improves texts on the go, which sometimes makes it a bit incoherent but not necessarily bad on the whole.&lt;/p&gt; &lt;p&gt;In my opinion, apart from this strangle little behavior, Mistral Small 3 is generally very smart and probably the most generally &amp;quot;intelligent&amp;quot; model I've used so far for its size. I've especially had much fun doing creative writing with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5f3b/mistral_small_3_is_awesome_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iesirf</id>
    <title>The new Mistral Small model is disappointing</title>
    <updated>2025-01-31T23:24:55+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was super excited to see a brand new 24B model from Mistral but after actually using it for more than single-turn interaction... I just find it to be disappointing&lt;/p&gt; &lt;p&gt;In my experience with the model it has a really hard time taking into account any information that is not crammed down its throat. It easily gets off track or confused&lt;/p&gt; &lt;p&gt;For single-turn question -&amp;gt; response it's good. For conversation, or anything that requires paying attention to context, it shits the bed. I've quadruple-checked and I'm using the right prompt format and system prompt...&lt;/p&gt; &lt;p&gt;Bonus question: Why is the rope theta value 100M? The model is not long context. I think this was a misstep in choosing the architecture &lt;/p&gt; &lt;p&gt;Am I alone on this? Have any of you gotten it to work properly on tasks that require intelligence and instruction following?&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1if0f0q</id>
    <title>kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js</title>
    <updated>2025-02-01T06:34:15+00:00</updated>
    <author>
      <name>/u/fajrmn</name>
      <uri>https://old.reddit.com/user/fajrmn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt; &lt;img alt="kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js" src="https://external-preview.redd.it/0HyIJ1l1CEno8j1KoR6iI-87ktiGCajBTpMb_ydGwDk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bee35d7add852122ecb3630117e2b2782c49c0af" title="kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1if0f0q/video/xm1kznq22hge1/player"&gt;https://reddit.com/link/1if0f0q/video/xm1kznq22hge1/player&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Browser-based text-to-speech conversion&lt;/li&gt; &lt;li&gt;Powered by Kokoro.js library by &lt;a href="https://www.reddit.com/user/xenovatech/"&gt;xenovatech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Responsive React application / dev with Vite&lt;/li&gt; &lt;li&gt;One click netlify deployment. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks to all the teams who have contributed to Kokoro.js&lt;/p&gt; &lt;p&gt;Repo : &lt;a href="https://github.com/fajrmn/kokoro-on-browser"&gt;https://github.com/fajrmn/kokoro-on-browser&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajrmn"&gt; /u/fajrmn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T06:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieihjr</id>
    <title>What the hell do people expect?</title>
    <updated>2025-01-31T16:20:03+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release of R1 I saw so many &amp;quot;But it can't talk about tank man!&amp;quot;, &amp;quot;But it's censored!&amp;quot;, &amp;quot;But it's from the chinese!&amp;quot; posts. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are all censored. And for R1 in particular... I don't want to discuss chinese politics (or politics at all) with my LLM. That's not my use-case and I don't think I'm in a minority here.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would happen if it was not censored the way it is? The guy behind it would probably have disappeared by now.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They all give a fuck about data privacy as much as they can. Else we wouldn't have ever read about samsung engineers not being allowed to use GPT for processor development anymore.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The model itself is much less censored than the web chat&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;IMHO it's not worse or better than the rest (non self-hosted) and the negative media reports are 1:1 the same like back in the days when Zen was released by AMD and all Intel could do was cry like &amp;quot;But it's just cores they glued together!&amp;quot;&lt;/p&gt; &lt;p&gt;Edit: Added clarification that the web chat is more censored than the model itself (self-hosted)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iettv1</id>
    <title>Deepseek bitnet</title>
    <updated>2025-02-01T00:25:36+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"&gt; &lt;img alt="Deepseek bitnet" src="https://preview.redd.it/cm74ybjy8fge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf896e28b8d16473f9f4db98d30877291e849edd" title="Deepseek bitnet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cm74ybjy8fge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T00:25:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3xpg</id>
    <title>Virtuoso-Small-v2 - Distilled from Deepseek-v3, 128k context</title>
    <updated>2025-02-01T10:56:34+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Virtuoso-Small-v2 (14B)&lt;/strong&gt; is our next-generation, 14-billion-parameter language model that builds upon the original Virtuoso-Small architecture. This version is distilled from Deepseek-v3, leveraging an expanded dataset of 5B+ tokens worth of logits.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;arcee-ai/Virtuoso-Small-v2:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Virtuoso-Small-v2"&gt;https://huggingface.co/arcee-ai/Virtuoso-Small-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF"&gt;https://huggingface.co/bartowski/arcee-ai_Virtuoso-Small-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3xpg/virtuososmallv2_distilled_from_deepseekv3_128k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:56:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iejazu</id>
    <title>DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it</title>
    <updated>2025-01-31T16:54:33+00:00</updated>
    <author>
      <name>/u/Fusseldieb</name>
      <uri>https://old.reddit.com/user/Fusseldieb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt; &lt;img alt="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" src="https://preview.redd.it/oemawg4i0dge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c75d540a6d15cd68cdeabc673be92b5e657f0e0" title="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fusseldieb"&gt; /u/Fusseldieb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oemawg4i0dge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefan2</id>
    <title>Idea: "Can I Run This LLM?" Website</title>
    <updated>2025-01-31T13:55:03+00:00</updated>
    <author>
      <name>/u/Dangerous_Bunch_3669</name>
      <uri>https://old.reddit.com/user/Dangerous_Bunch_3669</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt; &lt;img alt="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" src="https://preview.redd.it/l344q42n4cge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d54b955e3b1b2cf6f6d117e19782d25f8f4603c8" title="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have and idea. You know how websites like Can You Run It let you check if a game can run on your PC, showing FPS estimates and hardware requirements?&lt;/p&gt; &lt;p&gt;What if there was a similar website for LLMs? A place where you could enter your hardware specs and see:&lt;/p&gt; &lt;p&gt;Tokens per second, VRAM &amp;amp; RAM requirements etc.&lt;/p&gt; &lt;p&gt;It would save so much time instead of digging through forums or testing models manually. &lt;/p&gt; &lt;p&gt;Does something like this exist already? ü§î&lt;/p&gt; &lt;p&gt;I would pay for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Bunch_3669"&gt; /u/Dangerous_Bunch_3669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l344q42n4cge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T13:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewnwe</id>
    <title>I was able to 1-shot prompt the unlsoth "python flappy bird game" test with Deepseek R1 distilled 70B. The distilled models deserve more credit.</title>
    <updated>2025-02-01T02:50:40+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the unlsoth models and how they were getting excellent performance using dynamic quantization of the main model down to 2 bits and shrinking the model 80% in size. See article here - &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still, even at 80% the size, when you consider kv cache &amp;amp; context, you still need a minimum of around 200GB of high bandwidth memory to run those models at any sort of respectable speed. And even then, you're getting 5-6 t/s tops, unless you can splurge on a 24-channel dual EPYC system. &lt;/p&gt; &lt;p&gt;I am lucky enough to have a 4x 3090 system, so I have 96GB of VRAM to play with. To me 96GB of VRAM is the sweet spot for running 70B models where you can run bigger quants (6bit+) with plenty left over for context window. And with all that still get above 15 t/s (and much much faster using parallelism &amp;amp; draft models/etc).&lt;/p&gt; &lt;p&gt;Anyway, I played around with 70B distilled model. I am running it with ollama, but I had to customize it to run at it's intended temperature of .6, and increased the context window to 16k (ollama stupidly defaults every model to 2k context). &lt;/p&gt; &lt;p&gt;Then I ran the unsloth flappy bird test prompt to see how close I could come to replicate it's performance - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Prompt: Create a Flappy Bird game in Python. You must include these things: You must use pygame. The background color should be randomly chosen and is a light shade. Start with a light blue color. Pressing SPACE multiple times will accelerate the bird. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color. Place on the bottom some land colored as dark brown or yellow chosen randomly. Make a score shown on the top right side. Increment if you pass pipes and don't hit them. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here's the full response the model produced - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Okay, I need to create a Flappy Bird game using Pygame based on the user's requirements. Let me go through each requirement step by step.&lt;/p&gt; &lt;p&gt;First, the background color must be light and randomly chosen, starting with light blue. So I'll set a default light blue but have it change randomly each time the game restarts.&lt;/p&gt; &lt;p&gt;Next, pressing SPACE should make the bird accelerate. That means each press will add to the bird's velocity, allowing it to jump higher. I'll need to handle multiple key presses, so maybe track if the spacebar is pressed and apply force each frame until released.&lt;/p&gt; &lt;p&gt;The bird's shape should be random‚Äîsquare, circle, or triangle. Its color must be dark. So I'll create functions to generate a random dark color and another to choose the shape type randomly each game.&lt;/p&gt; &lt;p&gt;There's land at the bottom, colored either dark brown or yellow, chosen randomly. I'll add a ground rectangle with a randomly selected color from those options.&lt;/p&gt; &lt;p&gt;The score is displayed on the top right and increases when passing pipes without hitting them. So I'll need to track when the bird passes a pipe and increment the score accordingly. The best score should be saved and shown when the game ends.&lt;/p&gt; &lt;p&gt;Pipes must be randomly spaced with enough space between them for the bird to pass. They can be dark green, light brown, or dark gray. I'll create a list of pipes, each with random heights and gaps, moving from right to left.&lt;/p&gt; &lt;p&gt;When the bird hits a pipe or the ground, the game ends. Then, display the best score on the screen. The user can press Q or Esc to quit and SPACE to restart.&lt;/p&gt; &lt;p&gt;I'll structure the code with functions for generating colors, creating pipes, handling collisions, drawing elements, and updating the game state. I'll also set up event loops for handling key presses and quitting.&lt;/p&gt; &lt;p&gt;Testing each part as I go is important. For example, ensuring the bird's velocity increases correctly when pressing SPACE multiple times, checking that pipes generate without overlapping, and verifying collision detection works as expected.&lt;/p&gt; &lt;p&gt;Finally, I'll make sure all elements are styled according to the color requirements and that the game restarts properly with new randomizations. &lt;/p&gt; &lt;p&gt;Here's a complete implementation of Flappy Bird with all the specified requirements:&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;import pygame import sys import random pygame.init() # Constants WIDTH, HEIGHT = 800, 600 GRAVITY = 0.5 JUMP_FORCE = -8 PIPE_WIDTH = 80 PIPE_GAP = 150 GROUND_HEIGHT = 50 SCORE_FONT = pygame.font.SysFont('Arial', 30) BEST_SCORE_FONT = pygame.font.SysFont('Arial', 40) class Bird: def __init__(self): self.shape = random.choice(['square', 'circle', 'triangle']) self.color = (random.randint(0,100), random.randint(0,100), random.randint(0,100)) self.x = WIDTH // 4 self.y = HEIGHT // 2 self.velocity = 0 self.size = 20 def draw(self, screen): if self.shape == 'square': pygame.draw.rect(screen, self.color, (self.x, self.y, self.size, self.size)) elif self.shape == 'circle': pygame.draw.circle(screen, self.color, (int(self.x + self.size//2), int(self.y + self.size//2)), self.size//2) elif self.shape == 'triangle': points = [ (self.x, self.y), (self.x + self.size, self.y), (self.x + self.size//2, self.y + self.size) ] pygame.draw.polygon(screen, self.color, points) class Pipe: def __init__(self): self.height = random.randint(50, HEIGHT - PIPE_GAP - 100) self.x = WIDTH self.color = (random.choice([0, 128, 64]), random.choice([128, 64, 0]), random.choice([0, 64, 32])) def draw(self, screen): pygame.draw.rect(screen, self.color, (self.x, 0, PIPE_WIDTH, self.height)) pygame.draw.rect(screen, self.color, (self.x, self.height + PIPE_GAP, PIPE_WIDTH, HEIGHT - (self.height + PIPE_GAP))) def get_light_color(): return (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255)) def get_dark_color(): return (random.randint(0, 128), random.randint(0, 128), random.randint(0, 128)) def main(): screen = pygame.display.set_mode((WIDTH, HEIGHT)) clock = pygame.time.Clock() background_color = (173, 216, 230) # Light blue ground_color = (101, 67, 33) # Dark brown bird = Bird() pipes = [Pipe()] score = 0 best_score = 0 game_active = True jump_force = JUMP_FORCE while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_SPACE and game_active: jump_force = JUMP_FORCE bird.velocity = jump_force elif (event.key == pygame.K_q or event.key == pygame.K_ESCAPE): pygame.quit() sys.exit() keys = pygame.key.get_pressed() if keys[pygame.K_SPACE] and game_active: bird.velocity += 0.5 screen.fill(background_color) # Draw ground ground_rect = pygame.Rect(0, HEIGHT - GROUND_HEIGHT, WIDTH, GROUND_HEIGHT) pygame.draw.rect(screen, ground_color, ground_rect) if game_active: # Bird physics bird.velocity += GRAVITY bird.y += bird.velocity # Update pipes for pipe in pipes: pipe.x -= 2 if pipe.x &amp;lt; -PIPE_WIDTH: pipes.remove(pipe) score += 1 # Add new pipes if len(pipes) == 0 or pipes[-1].x &amp;lt; WIDTH // 2: pipes.append(Pipe()) # Check collisions for pipe in pipes: if (bird.x + bird.size &amp;gt; pipe.x and bird.x &amp;lt; pipe.x + PIPE_WIDTH): if bird.y &amp;lt; pipe.height or bird.y + bird.size &amp;gt; pipe.height + PIPE_GAP: game_active = False if bird.y + bird.size &amp;gt; HEIGHT - GROUND_HEIGHT: game_active = False else: # Game over screen text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) best_text = BEST_SCORE_FONT.render(f'Best Score: {best_score}', True, (0, 0, 0)) restart_text = SCORE_FONT.render('Press SPACE to restart', True, (0, 0, 0)) screen.blit(text, (WIDTH // 2 - text.get_width() // 2, HEIGHT // 2)) screen.blit(best_text, (WIDTH // 2 - best_text.get_width() // 2, HEIGHT // 2 + 40)) screen.blit(restart_text, (WIDTH // 2 - restart_text.get_width() // 2, HEIGHT // 2 + 80)) # Draw pipes for pipe in pipes: pipe.draw(screen) bird.draw(screen) score_text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) screen.blit(score_text, (WIDTH - 100, 10)) if not game_active: best_score = max(best_score, score) if keys[pygame.K_SPACE]: # Reset game background_color = get_light_color() bird = Bird() pipes.clear() score = 0 game_active = True pygame.display.flip() clock.tick(60) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;This implementation includes all the requested features:&lt;/p&gt; &lt;p&gt;Random light background color (starting with light blue) Bird shape can be square, circle, or triangle with random dark colors Ground with random dark brown or yellow color Randomly spaced pipes with different colors Score tracking and best score display Space to jump, Q/Esc to quit, SPACE to restart Physics-based bird movement Collision detection The game runs smoothly at 60 FPS and includes all the requested gameplay mechanics. You can run it by saving it as a Python file and running it with Pygame Installed (pip install pygame).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The results - &lt;a href="https://imgur.com/Sv0micW"&gt;Imgur&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This response nailed every requirement. And I know for a fact this model wasn't pre-trained on this prompt. Because I tried it w/ the default 2k context and it did not produce a fully working game first try. &lt;/p&gt; &lt;p&gt;Deepseek R1 is a reasoning model, so it needs enough context to go through it's thought process + produce a output. So bumping the context window to 16k is what allowed the model to run at it's potential. The base R1 model was actually designed to run at 128k context, so I'll play with how high I can go before I run out of GPU memory. But going from 2k to 16k already yielded much better results.&lt;/p&gt; &lt;p&gt;So, color me impressed. I wonder if the people that have had poor results w/ the distilled models are because they didn't give the model enough context, or ran 4 bpw or below quants. I think with these models you're better off trying to get 6 bpw or higher as they're already distilled and unoptimized out of the gate. And to repeat, reasoning models need enough context to perform their CoT and produce an output.&lt;/p&gt; &lt;p&gt;All I know is even the distilled models are incredible with just a little tuning. And I believe in the near future someone will actually optimize these distilled models and we'll have near frontier model performance at reasonable sizes. &lt;/p&gt; &lt;p&gt;This is such an exciting time to be a local llm enthusiast!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T02:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iep1i4</id>
    <title>DeepSeek AI blocked by Italian authorities</title>
    <updated>2025-01-31T20:54:04+00:00</updated>
    <author>
      <name>/u/ApprehensiveCook2236</name>
      <uri>https://old.reddit.com/user/ApprehensiveCook2236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt; &lt;img alt="DeepSeek AI blocked by Italian authorities" src="https://external-preview.redd.it/cwVbdtxL_MOCraBvILhveGZjsoXBHPHOS4Ik8eBEAT4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74e97f5b3d89f2b950da588950bdaa1d7f71e9d" title="DeepSeek AI blocked by Italian authorities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveCook2236"&gt; /u/ApprehensiveCook2236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/01/31/deepseek-ai-blocked-by-italian-authorities-as-others-member-states-open-probes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieooqe</id>
    <title>DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark</title>
    <updated>2025-01-31T20:38:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt; &lt;img alt="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" src="https://preview.redd.it/i2p0m8em4ege1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4859ed3af650610750eb873e1231f2d526388ec" title="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i2p0m8em4ege1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1if4m53</id>
    <title>How can I force DeepSeek to think much more? And how much the longest i can force it?</title>
    <updated>2025-02-01T11:44:16+00:00</updated>
    <author>
      <name>/u/Western_Soil_4613</name>
      <uri>https://old.reddit.com/user/Western_Soil_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any benchmark/best practice for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Soil_4613"&gt; /u/Western_Soil_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if4m53/how_can_i_force_deepseek_to_think_much_more_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iehstw</id>
    <title>GPU pricing is spiking as people rush to self-host deepseek</title>
    <updated>2025-01-31T15:50:54+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt; &lt;img alt="GPU pricing is spiking as people rush to self-host deepseek" src="https://preview.redd.it/599a10y9pcge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60ac26bd7a2d3395eefcaee8fbf07a28102792f" title="GPU pricing is spiking as people rush to self-host deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/599a10y9pcge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ienetu</id>
    <title>Deepseek R1 is now hosted by Nvidia</title>
    <updated>2025-01-31T19:44:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt; &lt;img alt="Deepseek R1 is now hosted by Nvidia" src="https://preview.redd.it/1zufl131vdge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c70d8c80da395577b63301493ed66fac0dc6c408" title="Deepseek R1 is now hosted by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA just brought DeepSeek-R1 671-bn param model to NVIDIA NIM microservice on build.nvidia .com&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The DeepSeek-R1 NIM microservice can deliver up to 3,872 tokens per second on a single NVIDIA HGX H200 system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using NVIDIA Hopper architecture, DeepSeek-R1 can deliver high-speed inference by leveraging FP8 Transformer Engines and 900 GB/s NVLink bandwidth for expert communication.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;As usual with NVIDIA's NIM, its a enterprise-scale setu to securely experiment, and deploy AI agents with industry-standard APIs. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zufl131vdge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T19:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1if5q97</id>
    <title>Just canceled my ChatGPT Plus subscription</title>
    <updated>2025-02-01T12:56:16+00:00</updated>
    <author>
      <name>/u/Anxietrap</name>
      <uri>https://old.reddit.com/user/Anxietrap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I initially subscribed when they introduced uploading documents when it was limited to the plus plan. I kept holding onto it for o1 since it really was a game changer for me. But since R1 is free right now (when it‚Äôs available at least lol) and the quantized distilled models finally fit onto a GPU I can afford, I cancelled my plan and am going to get a GPU with more VRAM instead. I love the direction that open source machine learning is taking right now. It‚Äôs crazy to me that distillation of a reasoning model to something like Llama 8B can boost the performance by this much. I hope we soon will get more advancements in more efficient large context windows and projects like Open WebUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anxietrap"&gt; /u/Anxietrap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if5q97/just_canceled_my_chatgpt_plus_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T12:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ies630</id>
    <title>openai can be opening again</title>
    <updated>2025-01-31T23:09:00+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt; &lt;img alt="openai can be opening again" src="https://preview.redd.it/1oovs3vgvege1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acc945b69ba442d4e66865f5e83ab96ac9b83b7b" title="openai can be opening again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1oovs3vgvege1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieurv8</id>
    <title>My PC 10 seconds after I typed ‚Äúollama run deepseek-r1:671b‚Äù:</title>
    <updated>2025-02-01T01:11:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt; &lt;img alt="My PC 10 seconds after I typed ‚Äúollama run deepseek-r1:671b‚Äù:" src="https://preview.redd.it/jixqkaabhfge1.gif?width=216&amp;amp;crop=smart&amp;amp;s=c67a878b6f732544b4693cf47d6dc14a8220e551" title="My PC 10 seconds after I typed ‚Äúollama run deepseek-r1:671b‚Äù:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jixqkaabhfge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T01:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1rls</id>
    <title>We've been incredibly fortunate with how things have developed over the past year</title>
    <updated>2025-02-01T08:11:39+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I still remember how in late 2023, people were speculating that Mixtral-8x7b was the best open-weights model that the community would get &amp;quot;for a long time&amp;quot;, and possibly ever. Shortly afterwards, Mistral published a controversial blog post that appeared to indicate that they were moving away from open weights ‚Äì an ominous sign at a time when there were very few open-weights models available, and Anthropic and OpenAI seemed as far out of reach as the stars.&lt;/p&gt; &lt;p&gt;But since then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta released the excellent Llama 3 series as open weights (though not entirely free software).&lt;/li&gt; &lt;li&gt;Contrary to what many had feared, Mistral continued to publish open-weights models, even releasing the weights for Mistral Large, which was previously API-only, and now publishing their latest Mistral Small under the Apache License, when the previous version was still under their proprietary MRL.&lt;/li&gt; &lt;li&gt;Yi-34b transitioned from a proprietary license to Apache.&lt;/li&gt; &lt;li&gt;Microsoft has been publishing a number of excellent small models under permissive licenses.&lt;/li&gt; &lt;li&gt;Qwen came out of nowhere, and released the best models that can be run on consumer hardware, almost all of them under permissive licenses.&lt;/li&gt; &lt;li&gt;DeepSeek upended the entire industry, and &lt;strong&gt;an MIT-licensed model is now ranked joint #1 on style-controlled LMSYS,&lt;/strong&gt; on par with cutting-edge, proprietary, API-only models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was completely unforeseeable a year ago. Reality has outpaced the wildest dreams of the most naive optimists. Some doomsayers even predicted that open-weights models would soon be outlawed. The exact opposite has happened, and continues to happen.&lt;/p&gt; &lt;p&gt;To get an idea for what could easily have been, just look at the world of image generation models. In 15 months, there have only been two significant open-weights releases: SD3, and Flux.1D. SD3 was mired in controversy due to Stability's behavior and has been all but ignored by the community, and Flux is crippled by distillation. Both models are censored to a degree that has become the stuff of memes, and their licenses essentially make them unusable for anything except horsing around.&lt;/p&gt; &lt;p&gt;That is how the LLM world could have turned out. Instead, we have a world where I don't even download every new model anymore, because there are multiple exciting releases every week and I simply lack the time to take all of them for a spin. I now regularly delete models from my hard drive that I would have given my right hand for not too long ago. It's just incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T08:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1if43uf</id>
    <title>Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source</title>
    <updated>2025-02-01T11:08:19+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt; &lt;img alt="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" src="https://preview.redd.it/iewy2sxsfige1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad808e1a916c32f2181d2dc562c2065ba8cb4c99" title="Sam Altman: OpenAI has been on the 'wrong side of history' concerning open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iewy2sxsfige1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if43uf/sam_altman_openai_has_been_on_the_wrong_side_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T11:08:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1if3lq1</id>
    <title>Sam Altman acknowledges R1</title>
    <updated>2025-02-01T10:31:35+00:00</updated>
    <author>
      <name>/u/ybdave</name>
      <uri>https://old.reddit.com/user/ybdave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt; &lt;img alt="Sam Altman acknowledges R1" src="https://preview.redd.it/ot5nsk399ige1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67ca17a8d86fa20881ff4876577c465ae2c733d9" title="Sam Altman acknowledges R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Straight from the horses mouth. Without R1, or bigger picture open source competitive models, we wouldn‚Äôt be seeing this level of acknowledgement from OpenAI. &lt;/p&gt; &lt;p&gt;This highlights the importance of having open models, not only that, but open models that actively compete and put pressure on closed models. &lt;/p&gt; &lt;p&gt;R1 for me feels like a real &lt;em&gt;hard takeoff&lt;/em&gt; moment. &lt;/p&gt; &lt;p&gt;No longer can OpenAI or other closed companies dictate the rate of release. &lt;/p&gt; &lt;p&gt;No longer do we have to get the scraps of what they decide to give us. &lt;/p&gt; &lt;p&gt;Now they have to actively compete in an open market.&lt;/p&gt; &lt;p&gt;No moat. &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Source: &lt;a href="https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC"&gt;https://www.reddit.com/r/OpenAI/s/nfmI5x9UXC&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ybdave"&gt; /u/ybdave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ot5nsk399ige1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if3lq1/sam_altman_acknowledges_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T10:31:35+00:00</published>
  </entry>
</feed>
