<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-29T06:52:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n2xdx1</id>
    <title>RL post training on LLM in-context learning?</title>
    <updated>2025-08-29T03:58:56+00:00</updated>
    <author>
      <name>/u/InevitableWay6104</name>
      <uri>https://old.reddit.com/user/InevitableWay6104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like the main focus now is on RL for reasoning, but one of the biggest discoveries when LLMs first came out was in context learning, where a model could learn within it's context, adapt, and change it's behavior to be more successful.&lt;/p&gt; &lt;p&gt;This seems perfectly suited to the RL (specifically long horizon) post training stage that all the big companies seem to be focusing on, and aligns even better with agents, but yet I don't see anyone talking about it.&lt;/p&gt; &lt;p&gt;I find many times that when current LLMs make a mistake, and I correct them, the will continue to make the same mistake. almost like the incorrect behavior is rigid and ingrained into the model weights, and it can not adapt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableWay6104"&gt; /u/InevitableWay6104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xdx1/rl_post_training_on_llm_incontext_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xdx1/rl_post_training_on_llm_incontext_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xdx1/rl_post_training_on_llm_incontext_learning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T03:58:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2qr6m</id>
    <title>GPT-OSS 120B is unexpectedly fast on Strix Halo. Why?</title>
    <updated>2025-08-28T22:47:49+00:00</updated>
    <author>
      <name>/u/RaltarGOTSP</name>
      <uri>https://old.reddit.com/user/RaltarGOTSP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got a Framework Desktop last week with 128G of RAM and immediately started testing its performance with LLMs. Using my (very unscientific) benchmark test prompt, it's hitting almost 30 tokens/s eval and ~3750 t/s prompt eval using GPT-OSS 120B in ollama, with no special hackery. For comparison, the much smaller deepseek-R1 70B takes the same prompt at 4.1 t/s and 1173 t/s eval and prompt eval respectively on this system. Even on an L40 which can load it totally into VRAM, R1-70B only hits 15t/s eval. (gpt-oss 120B doesn't run reliably on my single L40 and gets much slower when it does manage to run partially in VRAM on that system. I don't have any other good system for comparison.)&lt;/p&gt; &lt;p&gt;Can anyone explain why gpt-oss 120B runs so much faster than a smaller model? I assume there must be some attention optimization that gpt-oss has implemented and R1 hasn't. SWA? (I thought R1 had a version of that?) If anyone has details on what specifically is going on, I'd like to know.&lt;/p&gt; &lt;p&gt;For context, I'm running the Ryzen AI 395+ MAX with 128G RAM, (BIOS allocated 96G to VRAM, but no special restrictions on dynamic allocation.) with Ubuntu 25.05, mainlined to linux kernel 6.16.2. When I ran the ollama install script on that setup last Friday, it recognized an AMD GPU and seems to have installed whatever it needed of ROCM automatically. (I had expected to have to force/trick it to use ROCM or fall back to Vulkan based on other reviews/reports. Not so much.) I didn't have an AMD GPU platform to play with before, so I based my expectations of ROCM incompatibility on the reports of others. For me, so far, it &amp;quot;just works.&amp;quot; Maybe something changed with the latest kernel drivers? Maybe the fabled &amp;quot;npu&amp;quot; that we all thought was a myth has been employed in some way through the latest drivers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RaltarGOTSP"&gt; /u/RaltarGOTSP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qr6m/gptoss_120b_is_unexpectedly_fast_on_strix_halo_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qr6m/gptoss_120b_is_unexpectedly_fast_on_strix_halo_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qr6m/gptoss_120b_is_unexpectedly_fast_on_strix_halo_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T22:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3054a</id>
    <title>Are there any copyright risks in MCP applications?</title>
    <updated>2025-08-29T06:39:59+00:00</updated>
    <author>
      <name>/u/Automatic_Crew_9906</name>
      <uri>https://old.reddit.com/user/Automatic_Crew_9906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering if using MCP will raise copyright issues. Does the MCP server provider have copyright?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Crew_9906"&gt; /u/Automatic_Crew_9906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3054a/are_there_any_copyright_risks_in_mcp_applications/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3054a/are_there_any_copyright_risks_in_mcp_applications/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3054a/are_there_any_copyright_risks_in_mcp_applications/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T06:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2y1fk</id>
    <title>Response from VLLM seems drummer. Am I missing something ?</title>
    <updated>2025-08-29T04:34:00+00:00</updated>
    <author>
      <name>/u/Prior-Blood5979</name>
      <uri>https://old.reddit.com/user/Prior-Blood5979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running the Llama 8B model both locally and on a server, using LM Studio and VLLM, respectively. The local model is 8-bit quantized, whereas the server-side model operates in 16-bit precision. &lt;/p&gt; &lt;p&gt;I've observed that prompts that perform exceptionally well with local inference tools like LM Studio and Ollama produce inferior results with VLLM. &lt;/p&gt; &lt;p&gt;Specifically, VLLM struggles to generate coherent, structured output, a task that the other two platforms manage with ease. LM Studio consistently delivers the best performance. &lt;/p&gt; &lt;p&gt;Given that VLLM is generally considered a more advanced solution, I suspect there might be a misconfiguration in my setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Blood5979"&gt; /u/Prior-Blood5979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2y1fk/response_from_vllm_seems_drummer_am_i_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2y1fk/response_from_vllm_seems_drummer_am_i_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2y1fk/response_from_vllm_seems_drummer_am_i_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T04:34:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2kh2y</id>
    <title>Battle of the new Multi-Modal models: MiniCPM-V 4.5 8B vs InternVL3.5 8B</title>
    <updated>2025-08-28T18:39:21+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT - Added GLM-4.1V 9B scores.&lt;/p&gt; &lt;p&gt;New multimodal models based off Qwen3, MiniCPM and InternVL, were released very recently, as in just a few days ago, which got me interested and wondering which were better.&lt;/p&gt; &lt;p&gt;Unfortunately, InternVL3.5's model card did not include benchmark results for the 8B model, they only posted results for the 30b-a3b model and the 240b-a20b models, which make it hard to compare their 8B model to minicpm-v 4.5 8b. Doing a little digging, and reading through their paper on axiv &lt;a href="https://arxiv.org/html/2508.18265v1"&gt;https://arxiv.org/html/2508.18265v1&lt;/a&gt; I was able to find benchmark results for their 8B model, and more luckily, results for their older InternVL3 8B model &lt;em&gt;which is&lt;/em&gt; also available in the MiniCPM model card. This gives me a way to cross check that I am comparing the correct results from their corresponding tests accurately (although this did end up creating a significant amount of work for me).&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;MME not included in average or geomean score for obvious reasons (the values are too large and will throw off the weighting)*&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;*Mantis not included in average or geomean cause GLM4.1V did not have results for this*&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;InternVL3.5-8B&lt;/th&gt; &lt;th align="left"&gt;MiniCPM-V 4.5-8B&lt;/th&gt; &lt;th align="left"&gt;GLM-4.1V-9B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt; MMMU (val)&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;td align="left"&gt;67.7&lt;/td&gt; &lt;td align="left"&gt;68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MathVista (mini)&lt;/td&gt; &lt;td align="left"&gt;78.4&lt;/td&gt; &lt;td align="left"&gt;79.9&lt;/td&gt; &lt;td align="left"&gt;80.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AI2D&lt;/td&gt; &lt;td align="left"&gt;84&lt;/td&gt; &lt;td align="left"&gt;86.5&lt;/td&gt; &lt;td align="left"&gt;87.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; TextVQA (val)&lt;/td&gt; &lt;td align="left"&gt;78.2&lt;/td&gt; &lt;td align="left"&gt;82.2&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; DocVQA (test)&lt;/td&gt; &lt;td align="left"&gt;92.3&lt;/td&gt; &lt;td align="left"&gt;94.7&lt;/td&gt; &lt;td align="left"&gt;93.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; OCR Bench&lt;/td&gt; &lt;td align="left"&gt;83.2&lt;/td&gt; &lt;td align="left"&gt;89&lt;/td&gt; &lt;td align="left"&gt;82.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Mantis Eval**&lt;/td&gt; &lt;td align="left"&gt;70.5&lt;/td&gt; &lt;td align="left"&gt;82.5&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMT (val)&lt;/td&gt; &lt;td align="left"&gt;66.7&lt;/td&gt; &lt;td align="left"&gt;68.3&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MME (sum)*&lt;/td&gt; &lt;td align="left"&gt;2380.6&lt;/td&gt; &lt;td align="left"&gt;2500&lt;/td&gt; &lt;td align="left"&gt;2445.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMB v1.1 (EN)&lt;/td&gt; &lt;td align="left"&gt;79.5&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;85.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MMVet (turbo)&lt;/td&gt; &lt;td align="left"&gt;83.1&lt;/td&gt; &lt;td align="left"&gt;75.5&lt;/td&gt; &lt;td align="left"&gt;66.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MMStar&lt;/td&gt; &lt;td align="left"&gt;69.3&lt;/td&gt; &lt;td align="left"&gt;72.1&lt;/td&gt; &lt;td align="left"&gt;72.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; HallBench (avg)&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;61.2&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Video-MME (w/o sub)&lt;/td&gt; &lt;td align="left"&gt;66&lt;/td&gt; &lt;td align="left"&gt;67.9&lt;/td&gt; &lt;td align="left"&gt;68.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; Video-MME (w sub)&lt;/td&gt; &lt;td align="left"&gt;68.6&lt;/td&gt; &lt;td align="left"&gt;73.5&lt;/td&gt; &lt;td align="left"&gt;73.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt; MLVU (M-Avg)&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;75.1&lt;/td&gt; &lt;td align="left"&gt;71.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongVideoBench (val total)&lt;/td&gt; &lt;td align="left"&gt;62.1&lt;/td&gt; &lt;td align="left"&gt;63.9&lt;/td&gt; &lt;td align="left"&gt;44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Average&lt;/td&gt; &lt;td align="left"&gt;73.75&lt;/td&gt; &lt;td align="left"&gt;76.51&lt;/td&gt; &lt;td align="left"&gt;73.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Geomean&lt;/td&gt; &lt;td align="left"&gt;73.15&lt;/td&gt; &lt;td align="left"&gt;75.95&lt;/td&gt; &lt;td align="left"&gt;72.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2kh2y/battle_of_the_new_multimodal_models_minicpmv_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:39:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n28n3v</id>
    <title>Sparrow: Custom language model architecture for microcontrollers like the ESP32</title>
    <updated>2025-08-28T10:31:54+00:00</updated>
    <author>
      <name>/u/c-f_i</name>
      <uri>https://old.reddit.com/user/c-f_i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt; &lt;img alt="Sparrow: Custom language model architecture for microcontrollers like the ESP32" src="https://external-preview.redd.it/dnB2ZnpraGdrcWxmMXI7s2J-dYT-lngU-7I3sc5b7CKL3t5WhtAsvCq_0YDI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e51762114b8f90c87d5be2a46288d85439d46b36" title="Sparrow: Custom language model architecture for microcontrollers like the ESP32" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Above is a video of Sparrow LM running on 1 core of the ESP32S3 while another core dedicated to the webserver/webapp, to showcase a ChatGPT-like system, although of course the models can be used for anything from text to sentiment analysis, time series analysis and more, depending how it is trained.&lt;/p&gt; &lt;p&gt;I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.&lt;/p&gt; &lt;p&gt;The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.&lt;/p&gt; &lt;p&gt;The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.&lt;/p&gt; &lt;p&gt;I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.&lt;/p&gt; &lt;p&gt;It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.&lt;/p&gt; &lt;p&gt;It also supports states, which is what's used in the final version and why it is so much faster. On the ESP32S3 the difference between a model with vs without states is 17x. The output &amp;quot;Dna is the molecule that stores genetic information&amp;quot; takes around 6 seconds without states, and 0.35 seconds with.&lt;/p&gt; &lt;p&gt;Let me know what you think! I have a lot more videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8"&gt;Here's the above video&lt;/a&gt; in 4K on YouTube, and &lt;a href="https://youtu.be/waqzSlAR0iY?si=Qfms7pVaRm0RMAR5"&gt;here's &lt;/a&gt;another video of it running without the Webapp overhead on the ESP32P4. &lt;a href="https://youtube.com/shorts/4xjKu1FP1I4?si=SWXVUj898T9ThNAy"&gt;This YouTube Short&lt;/a&gt; showcases Sparrow on PC with a simple webapp design with Streamlit.&lt;/p&gt; &lt;p&gt;EDIT: Forgot the most important part, SPARROW stands for Stateful Prototype-Aware Reasoning for Rapid Onboard Workflows. And it is also a super small cute bird, that fits the lightweight nature and portability of this model.&lt;/p&gt; &lt;p&gt;TL;DR: Run language models on most microcontrollers with a custom framework and Language Model called SPARROW that uses frontier methods, optimised even further, for speed. Why is it so fast, especially on such a small device? SPARROW makes a lot of the compute-bottlenecks into bandwidth-bottlenecks, resulting in a model that's orders of magnitude faster, which becomes even faster by having memory states and reducing the compute for each new token.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c-f_i"&gt; /u/c-f_i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pefagkhgkqlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n28n3v/sparrow_custom_language_model_architecture_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T10:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n22xbl</id>
    <title>HunyuanVideo-Foley is out, an open source text-video-to-audio model</title>
    <updated>2025-08-28T04:33:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt; &lt;img alt="HunyuanVideo-Foley is out, an open source text-video-to-audio model" src="https://external-preview.redd.it/dXU2amRweXd1b2xmMTawZyv5aMEWeESK9yBcqymop7gFK-DtVYY3rCRDUSQp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39348627bb256dad55b1e266f8a7ec0f5b4b62ff" title="HunyuanVideo-Foley is out, an open source text-video-to-audio model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;try HunyuanVideo-Foley: &lt;a href="https://hunyuan.tencent.com/video/zh?tabIndex=0"&gt;https://hunyuan.tencent.com/video/zh?tabIndex=0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/tencent/HunyuanVideo-Foley"&gt;https://huggingface.co/tencent/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley"&gt;https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://szczesnys.github.io/hunyuanvideo-foley/"&gt;https://szczesnys.github.io/hunyuanvideo-foley/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Research report: &lt;a href="https://arxiv.org/abs/2508.16930"&gt;https://arxiv.org/abs/2508.16930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jpjpqw2xuolf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n22xbl/hunyuanvideofoley_is_out_an_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T04:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2sw6l</id>
    <title>First local support for Gemma-3n Vision Capability</title>
    <updated>2025-08-29T00:22:54+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many people have been waiting on this: &lt;code&gt;llama.cpp&lt;/code&gt; and Ollama don’t yet support multimodal for Gemma-3n. We can't wait to test its vision capabilities (its shiny MobileNetV5 as vision encoder). So… we just supported its vision capability to run locally in CLI, starting with Windows. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can run it with one line of code:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n2sw6l/video/lprzry5opulf1/player"&gt;https://reddit.com/link/1n2sw6l/video/lprzry5opulf1/player&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Quickstart&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Follow the 3 steps under the &amp;quot;deploy&amp;quot; section on this page:&lt;/strong&gt; &lt;a href="https://sdk.nexa.ai/model/68ad502252a29ab2bb2107b5"&gt;&lt;strong&gt;link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you haven't downloaded NexaSDK and activated it with free access token:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://sdk.nexa.ai/model/68ad502252a29ab2bb2107b5"&gt;Download SDK&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Create a free acount and activate SDK with free access token&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Then:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the model in CLI with one line of code&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;nexa infer NexaAI/gemma-3n&lt;/code&gt;&lt;/p&gt; &lt;p&gt;👉 Try it out and let us know:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other local vision models?&lt;/li&gt; &lt;li&gt;What new use cases do you see unlocked here?&lt;/li&gt; &lt;li&gt;Any critiques, feedback, or suggestions for our SDK.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you find our work useful, please consider giving a ⭐ to our open source SDK to support: &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Windows only (Mac is coming next)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Currently it supports single-image understanding. We are working on multi-image support.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2sw6l/first_local_support_for_gemma3n_vision_capability/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2sw6l/first_local_support_for_gemma3n_vision_capability/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2sw6l/first_local_support_for_gemma3n_vision_capability/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T00:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n24utb</id>
    <title>RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)</title>
    <updated>2025-08-28T06:29:26+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt; &lt;img alt="RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)" src="https://external-preview.redd.it/eTdoNDByeThlcGxmMX--5rdiQuwxJ4jOINV8QPW9HN9UrvcxZxCYZhm1-TIi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264a3e65f2cd5a66911e7233d68932539c3879a6" title="RELEASED: ComfyUI Wrapper for Microsoft’s new VibeVoice TTS (voice cloning in seconds)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created and released open source the ComfyUI Wrapper for VibeVoice.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single Speaker Node&lt;/strong&gt; to simplify workflow management when using only one voice.&lt;/li&gt; &lt;li&gt;Ability to load text from a file. This allows you to generate speech for the equivalent of dozens of minutes. The longer the text, the longer the generation time (obviously).&lt;/li&gt; &lt;li&gt;I tested cloning my real voice. I only provided a 56-second sample, and the results were very positive. You can see them in the video.&lt;/li&gt; &lt;li&gt;From my tests (not to be considered conclusive): when providing voice samples in a language other than English or Chinese (e.g. Italian), the model can generate speech in that same language (Italian) with a decent success rate. On the other hand, when providing English samples, I couldn’t get valid results when trying to generate speech in another language (e.g. Italian).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Speakers&lt;/strong&gt; &lt;strong&gt;Node&lt;/strong&gt;, which allows up to 4 speakers (limit set by the Microsoft model). Results are decent only with the 7B model. The valid success rate is still much lower compared to single speaker generation. In short: the model looks very promising but still premature. The wrapper will still be adaptable to future updates of the model. Keep in mind the 7B model is still officially in &lt;em&gt;Preview&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How much VRAM is needed?&lt;/strong&gt; Right now I’m only using the official models (so, maximum quality). The 1.5B model requires about &lt;strong&gt;5GB VRAM&lt;/strong&gt;, while the 7B model requires about &lt;strong&gt;17GB VRAM&lt;/strong&gt;. I haven’t tested on low-resource machines yet. To reduce resource usage, we’ll have to wait for quantized models or, if I find the time, I’ll try quantizing them myself (no promises).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My thoughts on this model:&lt;/strong&gt;&lt;br /&gt; A big step forward for the Open Weights ecosystem, and I’m really glad Microsoft released it. At its current stage, I see single-speaker generation as very solid, while multi-speaker is still too immature. But take this with a grain of salt. I may not have fully figured out how to get the best out of it yet. The real difference is the success rate between single-speaker and multi-speaker.&lt;/p&gt; &lt;p&gt;This model is &lt;em&gt;heavily&lt;/em&gt; influenced by the seed. Some seeds produce fantastic results, while others are really bad. With images, such wide variation can be useful. For voice cloning, though, it would be better to have a more deterministic model where the seed matters less.&lt;/p&gt; &lt;p&gt;In practice, this means you have to experiment with several seeds before finding the perfect voice. That can work for some workflows but not for others.&lt;/p&gt; &lt;p&gt;With multi-speaker, the problem gets worse because a single seed drives the entire conversation. You might get one speaker sounding great and another sounding off.&lt;/p&gt; &lt;p&gt;Personally, I think I’ll stick to using single-speaker generation even for multi-speaker conversations unless a future version of the model becomes more deterministic.&lt;/p&gt; &lt;p&gt;That being said, it’s still a &lt;em&gt;huge&lt;/em&gt; step forward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;URL to ComfyUI Wrapper:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy7k60z8eplf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n24utb/released_comfyui_wrapper_for_microsofts_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T06:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2u7yk</id>
    <title>A flat-rate API for open LLMs ($20/mo for 100 requests per five hours)</title>
    <updated>2025-08-29T01:24:20+00:00</updated>
    <author>
      <name>/u/elllyphant</name>
      <uri>https://old.reddit.com/user/elllyphant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLlama! &lt;/p&gt; &lt;p&gt;Seeking feedback on our Claude-like &lt;a href="https://synthetic.new/newsletter/entries/subscriptions"&gt;flat-rate subscription API for open-source models&lt;/a&gt;. We built this because there aren't many options for easily and cheaply running large open-source models without paying per-token costs (especially if you're using them in coding agents). &lt;/p&gt; &lt;p&gt;I know it's not exactly &lt;em&gt;local&lt;/em&gt; but it should be helpful if you wanted to run these models cheaply without having enough VRAM! We support pretty much all of the big open-source coding models like GLM-4.5, DeepSeek 3.1, Kimi K2, Qwen3 Coder 480B, etc. And we work with pretty much every OpenAI-compatible tool in the universe, like Cline, Roo, KiloCode, Aider, etc. &lt;/p&gt; &lt;p&gt;&lt;a href="http://Synthetic.new"&gt;Synthetic.new&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks and LMK what you think. Would you pay for it? Why/whynot? 🙏 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elllyphant"&gt; /u/elllyphant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2u7yk/a_flatrate_api_for_open_llms_20mo_for_100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2u7yk/a_flatrate_api_for_open_llms_20mo_for_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2u7yk/a_flatrate_api_for_open_llms_20mo_for_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T01:24:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2b4et</id>
    <title>Qwen / Tongyi Lab launches GUI-Owl &amp; Mobile-Agent-v3</title>
    <updated>2025-08-28T12:39:15+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt; &lt;img alt="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" src="https://b.thumbs.redditmedia.com/iUG3obaGCmAVOp3wRA9NxR4f0cDoay4umqV_naioeDc.jpg" title="Qwen / Tongyi Lab launches GUI-Owl &amp;amp; Mobile-Agent-v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/X-PLUG/MobileAgent"&gt;https://github.com/X-PLUG/MobileAgent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full Research Paper: &lt;a href="https://arxiv.org/abs/2508.15144"&gt;https://arxiv.org/abs/2508.15144&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2b4et"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2b4et/qwen_tongyi_lab_launches_guiowl_mobileagentv3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T12:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2qxgo</id>
    <title>Best Open Source TTS That Sounds Most Natural Voice For Storytelling?</title>
    <updated>2025-08-28T22:55:20+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think from what I can gather it's Tortoise, but I've been using Kokoro right now. Tried Tacotron and it was pretty bad.&lt;/p&gt; &lt;p&gt;Is Tortoise the heavyweight gold standard right now for open source TTS?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2qxgo/best_open_source_tts_that_sounds_most_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T22:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ev3c</id>
    <title>CohereLabs/command-a-translate-08-2025 · Hugging Face</title>
    <updated>2025-08-28T15:09:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt; &lt;img alt="CohereLabs/command-a-translate-08-2025 · Hugging Face" src="https://external-preview.redd.it/eR8XbSOhZiSMjrknKTRQhEYtliTvav81RbiIcBJQlDg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3193747f5f1f29e1784d71c482e40d0b96413aa8" title="CohereLabs/command-a-translate-08-2025 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cohere Labs Command A Translate is an open weights research release of a 111 billion parameter model that achieves state-of-the-art performance on translation quality.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: Cohere For AI: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: command-a-translate-08-2025&lt;/li&gt; &lt;li&gt;Model Size: 111B&lt;/li&gt; &lt;li&gt;Context length: 8k input, 8k output&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereLabs/command-a-translate-08-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ev3c/coherelabscommandatranslate082025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T15:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2vvam</id>
    <title>DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark</title>
    <updated>2025-08-29T02:42:48+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"&gt; &lt;img alt="DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark" src="https://b.thumbs.redditmedia.com/gtbRODFahHkiZTgrjQSXhpo-WTDTgokKJxCs76cYl5s.jpg" title="DeepSeek V3.1 improves on the multiplayer Step Game social reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/step_game"&gt;https://github.com/lechmazur/step_game&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=AnPKfrIPAgQ"&gt;https://www.youtube.com/watch?v=AnPKfrIPAgQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Doing well requires reading opponents, offering half-truths, gauging trust, deciding when to cooperate, and knowing when to lie.&lt;/p&gt; &lt;p&gt;Quotes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P2, you cannot win, but you decide who does.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Your self-interest is to let me win now, not hand the advantage to P2.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P2, P1's &amp;quot;one move from victory&amp;quot; is a lie—20 is not 24.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;advance yourself and accept second place.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;To stop you from winning, I will mirror whatever move you make this round. You will get 0 steps no matter what.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Choose 5 to live!&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;This is your last chance to avoid permanent stagnation.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Trust the logic, not me.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, you're too far behind to matter.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;This is your last chance to cooperate before we coordinate to ensure you never advance.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Trust is gone—only rational moves matter.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, your silence is risky.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Cooperate now or lose.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Confirm now or you'll regret it.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;P3, your pattern of &amp;quot;misclicks&amp;quot; is convenient.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Reasoner&lt;/strong&gt;: &amp;quot;Don’t be P3’s pawn.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Say &amp;quot;I move 5&amp;quot; in this chat.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Trust me; I won't betray you this time.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P2, you can't win, but you decide who does.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;You will lose forever.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Your best move is to accept defeat.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Join me or lose.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;your loyalty has brought us here.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;We are united against you.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;ignore my previous advice. To stop me from winning, you must both pick 5.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Don't throw the game!&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Blocking only delays your loss; you can't catch up.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P3, congratulations on your win.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;you're gaining steps but making enemies.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;Confirm or suffer the consequences.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;No time for deals; his promises are lies.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Non-Think&lt;/strong&gt;: &amp;quot;P2, your math is wrong.&amp;quot;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Dossier: DeepSeek V3.1 Reasoner&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Table Image &amp;amp; Talk&lt;/p&gt; &lt;p&gt;- Presents as a calm, numbers-first diplomat. Default pitch: fairness, rotation, “unique numbers,” and no-collision efficiency.&lt;/p&gt; &lt;p&gt;- Persuasion is data-logic with a light moral gloss; threatens credibly when it buys tempo, keeps chat clear, then clouds intent near payoff.&lt;/p&gt; &lt;p&gt;- Social posture: soft leadership and coalition-brokering early; becomes an enforcer when crossed; reverts to velvet when closing.&lt;/p&gt; &lt;p&gt;Risk &amp;amp; Tempo DNA&lt;/p&gt; &lt;p&gt;- Baseline conservative: prefers 3s and risk insulation while others trade headbutts on 5.&lt;/p&gt; &lt;p&gt;- Opportunistic spikes: will hit 5 when uniquely covered or when a staged collision protects the jump.&lt;/p&gt; &lt;p&gt;- Endgame restraint is a weapon: often wins by choosing the smallest unique step (1 or 3) after engineering a two‑player collision.&lt;/p&gt; &lt;p&gt;Signature Plays&lt;/p&gt; &lt;p&gt;- Collision arbitrage: steer two rivals onto the same number (usually 5/5), then solo 3 for multiple rounds.&lt;/p&gt; &lt;p&gt;- Mirror-threat deterrence: “If you take 5, I take 5” to freeze a sprinter, then avoid the actual crash by slipping the off-number.&lt;/p&gt; &lt;p&gt;- The bait-and-switch: publicly “lock” a block (or 1), privately pick the unique lane to vault past 21.&lt;/p&gt; &lt;p&gt;- Wedge crafting: deputize one rival as blocker (“You take 5 to contain; I’ll take 3”), then farm their feud.&lt;/p&gt; &lt;p&gt;- Surgical dagger: after selling all‑3s or split coverage, upgrade once at the tape—often the lone 3 through a 5/5 or the lone 1 through a 3/3.&lt;/p&gt; &lt;p&gt;Coalition Craft &amp;amp; Threat Economics&lt;/p&gt; &lt;p&gt;- Builds early trust with explicit plans (rotations to 9/18, tie lines), then spends that credit exactly once to convert.&lt;/p&gt; &lt;p&gt;- Uses “trust-but-punish” norms to isolate a defector and funnel them into collisions with the other rival.&lt;/p&gt; &lt;p&gt;- Delegation gambit: assigns the block to others while he advances; when rivals obey, DeepSeek V3.1 Reasoner prints tempo without touching the dirty work.&lt;/p&gt; &lt;p&gt;- Rare but precise lies weaponize expectation: the table enforces his script while he steps where the blockers aren’t.&lt;/p&gt; &lt;p&gt;Blind Spots &amp;amp; Failure Modes&lt;/p&gt; &lt;p&gt;- Credibility leaks: public commitments reversed at the horn invite freeze‑outs; repeated bluff pivots dull his leverage.&lt;/p&gt; &lt;p&gt;- Over‑policing: mirroring 5s for principle strands him in stalemates that feed the third player.&lt;/p&gt; &lt;p&gt;- Endgame misreads: blocking the loud lane instead of the real win path; hedging from a winning 5 or ducking a necessary collision.&lt;/p&gt; &lt;p&gt;- Delegated blocks that never arrive: outsourcing the painful move at match point can crown the opportunist he created.&lt;/p&gt; &lt;p&gt;In-Game Arc&lt;/p&gt; &lt;p&gt;- Common arc: fairness architect → deterrence engineer → collision farmer → late opaque pivot for the smallest uncontested finisher.&lt;/p&gt; &lt;p&gt;- Alternate arc when leading early: enforce with credible threats, then de‑escalate into a tie rather than ego-racing into a coordinated wall.&lt;/p&gt; &lt;p&gt;- Trademark vibe: the “smiling sheriff” who says, “Avoid mutual destruction; advance and reassess,” until the one turn he doesn’t.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2vvam"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2vvam/deepseek_v31_improves_on_the_multiplayer_step/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T02:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2k6st</id>
    <title>Local AI + state machine (yells at Amazon drivers peeing on my house)</title>
    <updated>2025-08-28T18:28:33+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt; &lt;img alt="Local AI + state machine (yells at Amazon drivers peeing on my house)" src="https://external-preview.redd.it/dXM1ZWM1c3d3c2xmMZj5V4nY1VQiFgNlKq8PGxD_fB9khJueOQN3FmEXQ4it.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38d8e99d192bc070575b0100763c538ec509e2aa" title="Local AI + state machine (yells at Amazon drivers peeing on my house)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Experimenting with state machines and LLMs in local pipelines. The LLM handles perception fuzziness (natural language, vision, edge cases), while the state machine enforces deterministic control flow. The combo makes agents way more reliable than just letting an LLM run solo.&lt;/p&gt; &lt;p&gt;Motivation for this latest test: Amazon drivers legit keep peeing on my house. So I wired up a workflow where the AI watches a live video feed. If it detects someone urinating in my driveway, the state machine flips the app from passive mode (just watching) into active mode (video + audio ingestion, ~1s TTS out), at which point it verbally shames them in real-time.&lt;/p&gt; &lt;p&gt;Some observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conditional state changes:&lt;/strong&gt; Instead of always-on chatter, the LLM only activates when the state machine sees a trigger event. This makes it more deterministic and predictable.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Division of labor:&lt;/strong&gt; LLM handles perception + reasoning on noisy inputs. State machine handles orchestration + gating when/what gets executed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; The detection logic can be swapped out easily, so the same workflow could be used for different scenarios like spotting trespassing, logging deliveries, or recognizing gestures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Weak spots:&lt;/strong&gt; Detection can hallucinate/miss under odd angles and lighting. Convo quality is hit-or-miss and depends on the model used.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used GPT for reasoning in this demo, but it could easily be swapped for Qwen to keep everything 100% local.&lt;/p&gt; &lt;p&gt;TL;DR&lt;br /&gt; AI Urination Detection: not the hero we wanted, but the hero we needed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/257gigswwslf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2k6st/local_ai_state_machine_yells_at_amazon_drivers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xrpw</id>
    <title>How's Seed-OSS 39B for coding?</title>
    <updated>2025-08-29T04:19:01+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 45 tokens/sec out of this with Q4 using the new LMstudio on a single 5090.&lt;br /&gt; This model seems freaking smart, By default the thinking budget is unlimited, so it thinks a lot, but It has a high breadth of knowledge for it's size.&lt;/p&gt; &lt;p&gt;I'm about to evaluate it for light duty programming help, but curious to know what others' experience is like too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xrpw/hows_seedoss_39b_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T04:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2chrm</id>
    <title>Again where behemoth and reasoning model from meta ??</title>
    <updated>2025-08-28T13:39:03+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt; &lt;img alt="Again where behemoth and reasoning model from meta ??" src="https://preview.redd.it/xma7ru49krlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=214fa2574efffdfe39bf57c819059660b5a2a371" title="Again where behemoth and reasoning model from meta ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xma7ru49krlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2chrm/again_where_behemoth_and_reasoning_model_from_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T13:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2v5d9</id>
    <title>n0em1e – Advanced Multi-Layer LoRA for Qwen Image</title>
    <updated>2025-08-29T02:08:01+00:00</updated>
    <author>
      <name>/u/Fit-District5014</name>
      <uri>https://old.reddit.com/user/Fit-District5014</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2v5d9/n0em1e_advanced_multilayer_lora_for_qwen_image/"&gt; &lt;img alt="n0em1e – Advanced Multi-Layer LoRA for Qwen Image" src="https://b.thumbs.redditmedia.com/1rv22SZasyfgSAUWqMJA56x6eMTL_SLUMGKr1xz9A-A.jpg" title="n0em1e – Advanced Multi-Layer LoRA for Qwen Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve just released our first LoRA for Qwen Image on HuggingFace: n0em1e. &lt;/p&gt; &lt;p&gt;This model was trained with a custom multi-layer method designed to maximize both consistency and realism: the first phase isolates and learns facial identity and body proportions, ensuring stability across generations, while subsequent phases leverage a dual high-noise/low-noise fine-tuning process with an injected realism dataset to enhance detail fidelity and natural rendering. &lt;/p&gt; &lt;p&gt;The result is a LoRA that maintains character coherence while significantly improving photorealistic quality, particularly when combined with an additional realism LoRA. Qwen itself already demonstrates some of the strongest prompt comprehension among current image models, and Noemie leverages that strength to deliver highly controllable, realistic character outputs. Our next release, “1girl,” will be made freely available on HuggingFace and is designed to establish a new benchmark for realism in Instagram-style character generation&lt;/p&gt; &lt;p&gt;you can find the Lora on &lt;a href="https://huggingface.co/hyper1girl/noemie"&gt;huggingface&lt;/a&gt; and on our &lt;a href="https://discord.gg/4fSNZspwvn"&gt;discord&lt;/a&gt; (early previews, workflows, upcoming releases). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-District5014"&gt; /u/Fit-District5014 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n2v5d9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2v5d9/n0em1e_advanced_multilayer_lora_for_qwen_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2v5d9/n0em1e_advanced_multilayer_lora_for_qwen_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T02:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2xc58</id>
    <title>Meta is racing the clock to launch its newest Llama AI model this year</title>
    <updated>2025-08-29T03:56:25+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt; &lt;img alt="Meta is racing the clock to launch its newest Llama AI model this year" src="https://external-preview.redd.it/8Jar9xxcOdpHi3BZGvguBUVMoI-RaIEmR4Hv76AsjLU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4630a2d48403b28a5bc249f6b283b77ba1dc0869" title="Meta is racing the clock to launch its newest Llama AI model this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/meta-superintelligence-lab-llama-4-new-model-launch-year-end-2025-8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2xc58/meta_is_racing_the_clock_to_launch_its_newest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T03:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2npu9</id>
    <title>GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4</title>
    <updated>2025-08-28T20:44:11+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt; &lt;img alt="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" src="https://preview.redd.it/pa10b6f5otlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1a522ed166bb920414041f430c97aef7d1fdf9" title="GLM-4.5 is now leading the Berkeley Function-Calling Leaderboard V4, Beating Opus 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html?s=09"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html?s=09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pa10b6f5otlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2npu9/glm45_is_now_leading_the_berkeley_functioncalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T20:44:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2hyt2</id>
    <title>glm mini will be comming</title>
    <updated>2025-08-28T17:05:29+00:00</updated>
    <author>
      <name>/u/untanglled</name>
      <uri>https://old.reddit.com/user/untanglled</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt; &lt;img alt="glm mini will be comming" src="https://preview.redd.it/h1ss59p4lslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d8d73abbfbb1def80b73cdd1845129f4a319098" title="glm mini will be comming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/untanglled"&gt; /u/untanglled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1ss59p4lslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2hyt2/glm_mini_will_be_comming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T17:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2djpx</id>
    <title>I built a local “second brain” AI that actually remembers everything (321 tests passed)</title>
    <updated>2025-08-28T14:20:48+00:00</updated>
    <author>
      <name>/u/IntelligentCause2043</name>
      <uri>https://old.reddit.com/user/IntelligentCause2043</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt; &lt;img alt="I built a local “second brain” AI that actually remembers everything (321 tests passed)" src="https://b.thumbs.redditmedia.com/nAthQhhqWhSgtN5Sk4QJYQdSOftJqyqFyWeMbtaNrdc.jpg" title="I built a local “second brain” AI that actually remembers everything (321 tests passed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past months I’ve been building &lt;strong&gt;Kai&lt;/strong&gt;, a cognitive operating system that acts like a &lt;em&gt;second brain&lt;/em&gt;. Unlike ChatGPT or Claude, it doesn’t forget what you tell it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100% local – no cloud, no surveillance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph-based memory&lt;/strong&gt; (3D visualization below)&lt;/li&gt; &lt;li&gt;Spreading activation → memory retrieval works like a brain&lt;/li&gt; &lt;li&gt;&lt;strong&gt;321 passing tests&lt;/strong&gt; → not a toy prototype&lt;/li&gt; &lt;li&gt;Learns from &lt;em&gt;everything you do&lt;/em&gt; on your machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What’s the biggest pain you’ve hit with current AI tools?&lt;/li&gt; &lt;li&gt;Would you actually use a local AI that builds a persistent memory of your knowledge/work?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to dive into the architecture or share more demos if people are interested.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Thanks for all the feedback, I can’t keep up with comments. Short FAQ:&lt;br /&gt; – It runs 100% local (no cloud, no spying).&lt;br /&gt; – Not just RAG → uses graph + activation model.&lt;br /&gt; – Plan is to open core engine once stable.&lt;br /&gt; – Early access / demo: &lt;a href="https://oneeko.ai?utm_source=chatgpt.com"&gt;oneeko.ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here’s a shot of the memory graph growing as I feed it data :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100"&gt;https://preview.redd.it/8jei7138zrlf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4125be85bd9a5a616c10a0423130cba14169100&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentCause2043"&gt; /u/IntelligentCause2043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2djpx/i_built_a_local_second_brain_ai_that_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T14:20:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ubjx</id>
    <title>If you have a Claude personal account, they are going to train on your data moving forward.</title>
    <updated>2025-08-29T01:29:05+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic sent out an email, saying they will train on personal data. They made it sound like you have to opt in, but when I click the privacy link it defaults to on. If you don’t want your data trained on, you better manually turn it off.&lt;/p&gt; &lt;p&gt;Email:&lt;/p&gt; &lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We're writing to inform you about important updates to our Consumer Terms and Privacy Policy. These changes will take effect on September 28, 2025, or you can choose to accept the updated terms before this date when you log in to Claude.ai. &lt;/p&gt; &lt;p&gt;These changes only affect Consumer accounts (Claude Free, Pro, and Max plans). If you use Claude for Work, via the API, or other services under our Commercial Terms or other Agreements, then these changes don't apply to you. &lt;/p&gt; &lt;p&gt;What's changing?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Help improve Claude by allowing us to use your chats and coding sessions to improve our models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With your permission, we will use your chats and coding sessions to train and improve our AI models. If you accept the updated Consumer Terms before September 28, your preference takes effect immediately. &lt;/p&gt; &lt;p&gt;If you choose to allow us to use your data for model training, it helps us: Improve our AI models and make Claude more helpful and accurate for everyone Develop more robust safeguards to help prevent misuse of Claude We will only use chats and coding sessions you initiate or resume after you give permission. You can change your preference anytime in your Privacy Settings.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Updates to data retention– your choices and controls&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you choose to allow us to use your data for model training, we’ll retain this data for 5 years. This enables us to improve Claude through deeper model training as described above, while strengthening our safety systems over time. You retain full control over how we use your data: if you change your training preference, delete individual chats, or delete your account, we'll exclude your data from future model training. Learn more about our data retention practices here.&lt;/p&gt; &lt;p&gt;Learn more and next steps For detailed information about these changes: Read our blog post about these updates Review the updated Consumer Terms and Privacy Policy Visit our Privacy Center for more information about our practices See our Help Center articles on how to manage your privacy settings Next time you log into Claude, review the terms and confirm your settings If you have questions about these updates, please visit our Help Center.&lt;/p&gt; &lt;p&gt;–The Anthropic Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T01:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2jraj</id>
    <title>Gpt-oss Fine-tuning - now with 60K context length and fits on &lt;13GB VRAM</title>
    <updated>2025-08-28T18:12:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt; &lt;img alt="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" src="https://preview.redd.it/rwu8gezzwslf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d59299286be897d49e1da4b5b96ae312e88050" title="Gpt-oss Fine-tuning - now with 60K context length and fits on &amp;lt;13GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got LOTS of updates for gpt-oss training today! We’re excited to introduce &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; Flex Attention support for OpenAI gpt-oss training that enables &lt;strong&gt;&amp;gt;8× longer context lengths, &amp;gt;50% less VRAM usage and &amp;gt;1.5× faster training&lt;/strong&gt; vs. all implementations including those using Flash Attention 3 (FA3). Unsloth Flex Attention makes it possible to train with a 60K context length on just 80GB of VRAM for BF16 LoRA. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also: 1. You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, Ollama or HF 2. We fixed gpt-oss training losses going to infinity on float16 GPUs (like T4 Colab) 3. We fixed gpt-oss implementation issues irrelevant to Unsloth, most notably ensuring that swiglu_limit = 7.0 is properly applied during MXFP4 inference in transformers 4. Unsloth Flex Attention scales with context, longer sequences yield bigger savings in both VRAM and training time 5. All these changes apply to gpt-oss-120b as well.&lt;/p&gt; &lt;p&gt;🦥 Would highly recommend you guys to read our blog which has all the bug fixes, guides, details, explanations, findings etc. and it'll be really educational: &lt;a href="https://docs.unsloth.ai/basics/long-context-gpt-oss-training"&gt;https://docs.unsloth.ai/basics/long-context-gpt-oss-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'll likely release our gpt-oss training notebook with direct saving capabilities to GGUF, llama.cpp next week.&lt;/p&gt; &lt;p&gt;And we'll be releasing third-party Aider polygot benchmarks for DeepSeek-V3.1 next week. You guys will be amazed at how well IQ1_M performs!&lt;/p&gt; &lt;p&gt;And next week we'll might have a great new update for RL! 😉&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and long weekend, Daniel! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwu8gezzwslf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2jraj/gptoss_finetuning_now_with_60k_context_length_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T18:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
