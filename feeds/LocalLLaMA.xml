<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-19T09:34:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1isefit</id>
    <title>218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review</title>
    <updated>2025-02-18T14:53:08+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"&gt; &lt;img alt="218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review" src="https://external-preview.redd.it/wN11nOKnxhb4X_jcsbcRwIcNxNTSj7M0aYJ70EXagdw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45e1cf5c89b225d98150e66cd20e132248282c66" title="218 GB/s real-world MBW on AMD Al Max+ 395 (Strix Halo) - The Phawx Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=yiHr8CQRZi4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isefit/218_gbs_realworld_mbw_on_amd_al_max_395_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1isku5v</id>
    <title>Perplexity: Open-sourcing R1 1776</title>
    <updated>2025-02-18T19:12:05+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isku5v/perplexity_opensourcing_r1_1776/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isku5v/perplexity_opensourcing_r1_1776/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1isyqf1</id>
    <title>Dataset for Improving LLM System Prompt Robustness</title>
    <updated>2025-02-19T06:11:56+00:00</updated>
    <author>
      <name>/u/normanmu</name>
      <uri>https://old.reddit.com/user/normanmu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, wanted to share a recent dataset that I collected for a research paper which some of you might find useful.&lt;/p&gt; &lt;p&gt;RealGuardrails is a collection of LLM training and evaluation datasets designed to study the robustness of LLM guardrails. It contains a set of 3000+ system prompts scraped from the ChatGPT store and HuggingChat, SFT/DPO training data demonstrating guardrail-following behavior in response to conflicting and non-conflicting user queries, and a suite of handwritten and procedurally generated test cases. HF link here: &lt;a href="https://huggingface.co/datasets/normster/RealGuardrails"&gt;https://huggingface.co/datasets/normster/RealGuardrails&lt;/a&gt; and our paper is on arXiv: &lt;a href="https://arxiv.org/abs/2502.12197"&gt;https://arxiv.org/abs/2502.12197&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/normanmu"&gt; /u/normanmu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isyqf1/dataset_for_improving_llm_system_prompt_robustness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isyqf1/dataset_for_improving_llm_system_prompt_robustness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isyqf1/dataset_for_improving_llm_system_prompt_robustness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T06:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1is6a0m</id>
    <title>Sama discussing the release of Phone-sized-model</title>
    <updated>2025-02-18T06:24:58+00:00</updated>
    <author>
      <name>/u/0ssamaak0</name>
      <uri>https://old.reddit.com/user/0ssamaak0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"&gt; &lt;img alt="Sama discussing the release of Phone-sized-model" src="https://preview.redd.it/lm2o9y9pcuje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9139d0f30628bce26eec5f98414cd3aa3aaad4ea" title="Sama discussing the release of Phone-sized-model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0ssamaak0"&gt; /u/0ssamaak0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lm2o9y9pcuje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is6a0m/sama_discussing_the_release_of_phonesizedmodel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T06:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1it0ith</id>
    <title>Dans-PersonalityEngine-24b</title>
    <updated>2025-02-19T08:13:32+00:00</updated>
    <author>
      <name>/u/PocketDocLabs</name>
      <uri>https://old.reddit.com/user/PocketDocLabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The largest model I've released under the personality engine name in over a year. With any luck it should retain the mold-able nature of the 12b model while lending additional nuance to your generations.&lt;/p&gt; &lt;p&gt;This version of the dataset has been packed with over 3x the literature and some unique datasets previously tested through the Sakura Kaze and Dangerous Winds series models.&lt;/p&gt; &lt;p&gt;Aside from packing more in I also went through and pruned some of the older less potent data that I feel the model no longer benefits from.&lt;/p&gt; &lt;p&gt;The composition of the training set is constantly in flux and if you have a problem, idea, or even something as intangible as a pipe dream speak up and It might end up enhancing the next release!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.2.0-24b"&gt;Model weights available on HF in this repo along with some additional information.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PocketDocLabs"&gt; /u/PocketDocLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ith/danspersonalityengine24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T08:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1is5036</id>
    <title>We're winning by just a hair...</title>
    <updated>2025-02-18T05:05:51+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"&gt; &lt;img alt="We're winning by just a hair..." src="https://preview.redd.it/v8ygdpdlytje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc10efbbcaa94e78223dfb7fe2eba928afb34b7" title="We're winning by just a hair..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v8ygdpdlytje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is5036/were_winning_by_just_a_hair/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T05:05:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskft8</id>
    <title>LlamaCon on April 29: Meta to share the latest on Open Source AI developments</title>
    <updated>2025-02-18T18:56:52+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;Following the unprecedented growth and momentum of our open source Llama collection of models and tools, we’re excited to introduce LlamaCon—a developer conference for 2025 that will take place April 29.&lt;/p&gt; &lt;p&gt;At LlamaCon, we’ll share the latest on our open source AI developments to help developers do what they do best: build amazing apps and products, whether as a start-up or at scale.&lt;/p&gt; &lt;p&gt;Mark your calendars: We’ll have more to share on LlamaCon in the coming weeks.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;Source:&lt;/em&gt; &lt;a href="https://www.meta.com/blog/connect-2025-llamacon-save-the-date/"&gt;&lt;em&gt;https://www.meta.com/blog/connect-2025-llamacon-save-the-date/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskft8/llamacon_on_april_29_meta_to_share_the_latest_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskft8/llamacon_on_april_29_meta_to_share_the_latest_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskft8/llamacon_on_april_29_meta_to_share_the_latest_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T18:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iss7k7</id>
    <title>Fastest local inference options for 2 x 3090 with NVLink</title>
    <updated>2025-02-19T00:38:17+00:00</updated>
    <author>
      <name>/u/IngeniousIdiocy</name>
      <uri>https://old.reddit.com/user/IngeniousIdiocy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iss7k7/fastest_local_inference_options_for_2_x_3090_with/"&gt; &lt;img alt="Fastest local inference options for 2 x 3090 with NVLink" src="https://preview.redd.it/vltbaz4trzje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9fdd5435addc6db417313e742e20188f2210e45" title="Fastest local inference options for 2 x 3090 with NVLink" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m currently at 15 tps with llama.cpp but both cards are at 50% utilization during inference on deepseek-r1:70b q4_k_m &lt;/p&gt; &lt;p&gt;I’m wondering if any others have experience with vLLM, exllama or TensorRT and what kind of throughput they have seen with llama 3.3 70b class models at 4 bit?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngeniousIdiocy"&gt; /u/IngeniousIdiocy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vltbaz4trzje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iss7k7/fastest_local_inference_options_for_2_x_3090_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iss7k7/fastest_local_inference_options_for_2_x_3090_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T00:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1isr944</id>
    <title>Integrated Omniparser V2, we made our agent to use Canva!</title>
    <updated>2025-02-18T23:56:07+00:00</updated>
    <author>
      <name>/u/ImpossiblePlay</name>
      <uri>https://old.reddit.com/user/ImpossiblePlay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isr944/integrated_omniparser_v2_we_made_our_agent_to_use/"&gt; &lt;img alt="Integrated Omniparser V2, we made our agent to use Canva!" src="https://external-preview.redd.it/o1KppDkDGDu0Uics73-NWTIeJ7BlV9Xzc_EMf6SWdII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d40134cdb12b70d07f379c2fec0c136d329923c" title="Integrated Omniparser V2, we made our agent to use Canva!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Canva is a hugely popular platform for graphic design, it uses iframe, making it extremely hard for DOM tree based browser use agents.&lt;/p&gt; &lt;p&gt;We integrate Omniparser V2 into our project, with some other tricks, now our agent can use Canva with ease.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1isr944/video/yl8xmaaqjzje1/player"&gt;https://reddit.com/link/1isr944/video/yl8xmaaqjzje1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also integrated with browserless remote browser, so there is no risk of using your own browser. Use a remote browser can be annoying because you have to login every time, do no worry, we added cookie management feature.&lt;/p&gt; &lt;p&gt;The best part: we open sources everything: &lt;a href="https://github.com/Aident-AI/open-cuak"&gt;https://github.com/Aident-AI/open-cuak&lt;/a&gt;, please try it and let us know your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImpossiblePlay"&gt; /u/ImpossiblePlay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isr944/integrated_omniparser_v2_we_made_our_agent_to_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isr944/integrated_omniparser_v2_we_made_our_agent_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isr944/integrated_omniparser_v2_we_made_our_agent_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T23:56:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1is1p2o</id>
    <title>The normies have failed us</title>
    <updated>2025-02-18T02:10:22+00:00</updated>
    <author>
      <name>/u/RenoHadreas</name>
      <uri>https://old.reddit.com/user/RenoHadreas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt; &lt;img alt="The normies have failed us" src="https://preview.redd.it/fosxvznb3tje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38f047fe1dccff4127d9e6709f4812f4bce14d3d" title="The normies have failed us" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenoHadreas"&gt; /u/RenoHadreas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fosxvznb3tje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is1p2o/the_normies_have_failed_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T02:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1isiyl1</id>
    <title>Stop over-engineering AI apps: just use Postgres</title>
    <updated>2025-02-18T17:59:12+00:00</updated>
    <author>
      <name>/u/Worldly_Expression43</name>
      <uri>https://old.reddit.com/user/Worldly_Expression43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isiyl1/stop_overengineering_ai_apps_just_use_postgres/"&gt; &lt;img alt="Stop over-engineering AI apps: just use Postgres" src="https://external-preview.redd.it/Wtd8xrs74fmW5Nwe-0sw5Ku9AiPwXoZUtbCsRb1plOs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82ea332b5cbdb2185db8efae5db1f596223624f1" title="Stop over-engineering AI apps: just use Postgres" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Expression43"&gt; /u/Worldly_Expression43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.timescale.com/blog/stop-over-engineering-ai-apps"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isiyl1/stop_overengineering_ai_apps_just_use_postgres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isiyl1/stop_overengineering_ai_apps_just_use_postgres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T17:59:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1isklor</id>
    <title>Perplexity open-sourcing R1 1776—a version of the DeepSeek R1 model that has been post-trained to provide uncensored, unbiased, and factual information.</title>
    <updated>2025-02-18T19:03:01+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isklor/perplexity_opensourcing_r1_1776a_version_of_the/"&gt; &lt;img alt="Perplexity open-sourcing R1 1776—a version of the DeepSeek R1 model that has been post-trained to provide uncensored, unbiased, and factual information." src="https://external-preview.redd.it/164EuGZFD9-oLigQPsgPDcYrOKgLU810xhQDdYUsGiI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=025fa40bf7a234cdc612b98149a2a5628a850f17" title="Perplexity open-sourcing R1 1776—a version of the DeepSeek R1 model that has been post-trained to provide uncensored, unbiased, and factual information." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/perplexity_ai/status/1891916573713236248"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isklor/perplexity_opensourcing_r1_1776a_version_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isklor/perplexity_opensourcing_r1_1776a_version_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1isofoj</id>
    <title>My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B.</title>
    <updated>2025-02-18T21:35:21+00:00</updated>
    <author>
      <name>/u/Salt-Frosting-7930</name>
      <uri>https://old.reddit.com/user/Salt-Frosting-7930</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"&gt; &lt;img alt="My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B." src="https://external-preview.redd.it/N3o0bzNxaHV1eWplMYLfc7KrRYLtf8MU_N3qPFBaU60Tn2QznaGhFG2NBzjF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02a7e70081720aca16adc66a96076eb71d2e83ba" title="My new game, Craft to Infinity, is an infinite craft-style RPG that runs entirely locally on your PC. Using Qwen 2.5 instruct 1.5B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt-Frosting-7930"&gt; /u/Salt-Frosting-7930 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/so8xoqhuuyje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isofoj/my_new_game_craft_to_infinity_is_an_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T21:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1isfjvc</id>
    <title>You guys made my model trending on Hugging Face—so I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)</title>
    <updated>2025-02-18T15:41:24+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"&gt; &lt;img alt="You guys made my model trending on Hugging Face—so I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)" src="https://external-preview.redd.it/am14MjN6ZnEzeGplMdXCBGqa_ZAaRyIjjsCd7dXIDjyHMhPivqJqFI-7KUQp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa567cf4c858c035a4999d41fb73458b5162ecd2" title="You guys made my model trending on Hugging Face—so I dropped a 14B and 7B upgrade with better reasoning! UIGEN-T1.1 (with gguf)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/65qy23gq3xje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isfjvc/you_guys_made_my_model_trending_on_hugging_faceso/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T15:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iserf9</id>
    <title>Deepseek R1 Distilled Models MMLU Pro Benchmarks</title>
    <updated>2025-02-18T15:07:30+00:00</updated>
    <author>
      <name>/u/RedditsBestest</name>
      <uri>https://old.reddit.com/user/RedditsBestest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt; &lt;img alt="Deepseek R1 Distilled Models MMLU Pro Benchmarks" src="https://preview.redd.it/s006z4fbnwje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68c5d9bdcbc0635cebefe1ea4e81c89993146c77" title="Deepseek R1 Distilled Models MMLU Pro Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditsBestest"&gt; /u/RedditsBestest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s006z4fbnwje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iserf9/deepseek_r1_distilled_models_mmlu_pro_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T15:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1is7yei</id>
    <title>DeepSeek is still cooking</title>
    <updated>2025-02-18T08:20:59+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt; &lt;img alt="DeepSeek is still cooking" src="https://preview.redd.it/ikhcif5gxuje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=659543f3bf6bd7985e0d2e63418ae9c0ba570196" title="DeepSeek is still cooking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Babe wake up, a new Attention just dropped&lt;/p&gt; &lt;p&gt;Sources: &lt;a href="https://x.com/deepseek_ai/status/1891745487071609327"&gt;Tweet&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.11089"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ikhcif5gxuje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1is7yei/deepseek_is_still_cooking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T08:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ise5ly</id>
    <title>Speed up downloading Hugging Face models by 100x</title>
    <updated>2025-02-18T14:40:25+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure this is common knowledge, so sharing it here.&lt;/p&gt; &lt;p&gt;You may have noticed HF downloads caps at around 10.4MB/s (at least for me).&lt;/p&gt; &lt;p&gt;But if you install hf_transfer, which is written in Rust, you get uncapped speeds! I'm getting speeds of over &amp;gt; 1GB/s, and this saves me so much time!&lt;/p&gt; &lt;p&gt;Edit: The 10.4MB limitation I’m getting is not related to Python. Probably a bandwidth limit that doesn’t exist when using hf_transfer. &lt;/p&gt; &lt;p&gt;Edit2: To clarify, I get this cap of 10.4MB/s when downloading a model with command line Python. When I download via the website I get capped at around +-40MB/s. When I enable hf_transfer I get over 1GB/s. &lt;/p&gt; &lt;p&gt;Here is the step by step process to do it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Install the HuggingFace CLI pip install -U &amp;quot;huggingface_hub[cli]&amp;quot; # Install hf_transfer for blazingly fast speeds pip install hf_transfer # Login to your HF account huggingface-cli login # Now you can download any model with uncapped speeds HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download &amp;lt;model-id&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ise5ly/speed_up_downloading_hugging_face_models_by_100x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T14:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskrsp</id>
    <title>Quantized DeepSeek R1 Distill Model With Original Model Accuracy</title>
    <updated>2025-02-18T19:09:29+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt; &lt;img alt="Quantized DeepSeek R1 Distill Model With Original Model Accuracy" src="https://external-preview.redd.it/jZumEOo5cwGIYKUmVlgM_-JZx8lE5uOoa94hgKEcVik.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30d829ba9aedf7b3112605866862b09d48061c3c" title="Quantized DeepSeek R1 Distill Model With Original Model Accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all love DeepSeek R1 Distill models. It can solve BrainTeaser Question with only 1.5B parameters, which normal 3B model cannot do. However, quantized DeepSeek-R1-Distill models often lose up to 22% accuracy, making it not as useful. &lt;strong&gt;We’ve solved the trade-off with NexaQuant, compressing DeepSeek R1 Distill models to 1/4 of their original size (4 bit) while maintaining original accuracy.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We open sourced NexaQuant DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Llama-8B on Hugging Face:&lt;/p&gt; &lt;p&gt;🤗 Llama8B &lt;a href="https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Llama-8B-NexaQuant"&gt;https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Llama-8B-NexaQuant&lt;/a&gt;&lt;br /&gt; 🤗 Qwen1.5B &lt;a href="https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Qwen-1.5B-NexaQuant"&gt;https://huggingface.co/NexaAIDev/DeepSeek-R1-Distill-Qwen-1.5B-NexaQuant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;They are compatible with your favorite llama.cpp ❤️ based projects: Ollama, LMStudio, Jan AI, AnythingLLM, Nexa-SDK, and more. Try them out now and let us know what you think!&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Benchmarks&lt;/h1&gt; &lt;p&gt;Full Blog &amp;amp; Benchmarks: &lt;a href="https://nexa.ai/blogs/deepseek-r1-nexaquant"&gt;https://nexa.ai/blogs/deepseek-r1-nexaquant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7pzzri9n4yje1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83ebd1b8b16ddb550a36daf090ccebc161473d66"&gt;https://preview.redd.it/7pzzri9n4yje1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83ebd1b8b16ddb550a36daf090ccebc161473d66&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;NexaQuant Use Case Demo&lt;/h1&gt; &lt;p&gt;Here’s a comparison of how a standard Q4_K_M and NexaQuant-4Bit handle a common investment banking brain teaser question. NexaQuant excels in accuracy while shrinking the model file size by 4 times.&lt;/p&gt; &lt;p&gt;Prompt: A Common Investment Banking BrainTeaser Question&lt;/p&gt; &lt;p&gt;There is a 6x8 rectangular chocolate bar made up of small 1x1 bits. We want to break it into the 48 bits. We can break one piece of chocolate horizontally or vertically, but cannot break two pieces together! What is the minimum number of breaks required?&lt;/p&gt; &lt;p&gt;Right Answer: 47&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mfa8ocfw4yje1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09f0fab5aaa6ecba1b33405cf80265f31ddcecad"&gt;https://preview.redd.it/mfa8ocfw4yje1.png?width=2112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09f0fab5aaa6ecba1b33405cf80265f31ddcecad&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskrsp/quantized_deepseek_r1_distill_model_with_original/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:09:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1islqfc</id>
    <title>DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report</title>
    <updated>2025-02-18T19:47:57+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"&gt; &lt;img alt="DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report" src="https://external-preview.redd.it/hJ3pkUjJF7yX73X9K7Oc8CgPe7fBFkAVbL8oSeXP3AI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62828968e35d7fbd2b70d2aa440fcdeb80a5d7ce" title="DeepSeek GPU smuggling probe shows Nvidia's Singapore GPU sales are 28% of its revenue, but only 1% are delivered to the country: Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/deepseek-gpu-smuggling-probe-shows-nvidias-singapore-gpu-sales-are-28-percent-of-its-revenue-but-only-1-percent-are-delivered-to-the-country-report"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1islqfc/deepseek_gpu_smuggling_probe_shows_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1issbzc</id>
    <title>MoonshotAI release 10m Mixture of Block Attention for Long-Context LLMs, longer than deepseek's NSA</title>
    <updated>2025-02-19T00:43:49+00:00</updated>
    <author>
      <name>/u/No_Assistance_7508</name>
      <uri>https://old.reddit.com/user/No_Assistance_7508</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/MoBA"&gt;https://github.com/MoonshotAI/MoBA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Assistance_7508"&gt; /u/No_Assistance_7508 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1issbzc/moonshotai_release_10m_mixture_of_block_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T00:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1isxyo9</id>
    <title>New LLM tech running on diffusion just dropped</title>
    <updated>2025-02-19T05:24:42+00:00</updated>
    <author>
      <name>/u/LorestForest</name>
      <uri>https://old.reddit.com/user/LorestForest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt; &lt;img alt="New LLM tech running on diffusion just dropped" src="https://external-preview.redd.it/P8PjCGtAk7UmVMdaNZLIptoXPYYcXmgBOsABEcRwX6E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fe7f123e659e819d44d4238f56ef7e2825d7a43" title="New LLM tech running on diffusion just dropped" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claims to mitigate hallucinations unless you use it as a chat application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LorestForest"&gt; /u/LorestForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://timkellogg.me/blog/2025/02/17/diffusion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T05:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1it0ocl</id>
    <title>R1-1776 Dynamic GGUFs by Unsloth</title>
    <updated>2025-02-19T08:24:48+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, we uploaded 2bit to 16bit GGUFs for R1-1776, Perplexity's new DeepSeek-R1 finetune that removes all censorship while maintaining reasoning capabilities: &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also upload Dynamic 2-bit, 3 and 4-bit versions and standard 3, 4, etc bit versions. The Dynamic 4-bit is even smaller than the medium one and achieves even higher accuracy. 1.58-bit and 1-bit will have to be done later as it relies on imatrix quants, which take more time.&lt;/p&gt; &lt;p&gt;Instructions to run the model are in the model card we provided. Do not forget about &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter. Also do not forget about &lt;code&gt;&amp;lt;think&amp;gt;\n&lt;/code&gt;! Prompt format: &lt;code&gt;&amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;lt;think&amp;gt;\n&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can also refer to our previous blog for 1.58-bit R1 GGUF for hints and results: &lt;a href="https://unsloth.ai/blog/r1-reasoning"&gt;https://unsloth.ai/blog/r1-reasoning&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;211GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;298.8GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;377.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit extra small&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;206.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;405GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/Q4_K_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And you can find the rest like 6-bit, 8-bit etc on the model card. Happy running!&lt;/p&gt; &lt;p&gt;P.S. we have a new update coming very soon which you guys will absolutely love! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T08:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskklo</id>
    <title>PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities</title>
    <updated>2025-02-18T19:01:54+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt; &lt;img alt="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" src="https://external-preview.redd.it/fBZy2Z7nV_YeGpu-AjWKc9CfcCYUjRJwHSvwj2-4VbI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0eb7d3b3713b1cc2f605e5e87e0a44f39b650f01" title="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/perplexity-ai/r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:01:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1isxhoy</id>
    <title>New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)</title>
    <updated>2025-02-19T04:58:16+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt; &lt;img alt="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" src="https://external-preview.redd.it/rA82auRK9iN2YS0cZz7VCIDRx_3W3gxSA4bgLmUO0L8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f52aaef3d5bfe41e70571f92b4c1d36cf9abcc18" title="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=IVbm2a6lVBo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T04:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1isu4un</id>
    <title>o3-mini won the poll! We did it guys!</title>
    <updated>2025-02-19T02:06:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt; &lt;img alt="o3-mini won the poll! We did it guys!" src="https://preview.redd.it/ogpvvrth70ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=441716ec57e99b756f365455cea717ed23f4f00b" title="o3-mini won the poll! We did it guys!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a lot here yesterday to vote for the o3-mini. Thank you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogpvvrth70ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T02:06:12+00:00</published>
  </entry>
</feed>
