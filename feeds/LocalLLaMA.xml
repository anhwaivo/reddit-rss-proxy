<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-31T07:23:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kzegpe</id>
    <title>qSpeak - Superwhisper cross-platform alternative now with MCP support</title>
    <updated>2025-05-30T20:23:35+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we've released a new version of qSpeak with advanced support for MCP. Now you can access whatever platform tools wherever you would want in your system using voice. &lt;/p&gt; &lt;p&gt;We've spent a great amount of time to make the experience of steering your system with voice a pleasure. We would love to get some feedback. The app is still completely free so hope you'll like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qspeak.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzegpe/qspeak_superwhisper_crossplatform_alternative_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzegpe/qspeak_superwhisper_crossplatform_alternative_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzlps2</id>
    <title>The OpenRouter-hosted Deepseek R1-0528 sometimes generate typo.</title>
    <updated>2025-05-31T01:56:03+00:00</updated>
    <author>
      <name>/u/ExcuseAccomplished97</name>
      <uri>https://old.reddit.com/user/ExcuseAccomplished97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the DS R1-0528 on Roo Code. So far, it's impressive in its ability to effectively tackle the requested tasks.&lt;br /&gt; However, it often generates code from the OpenRouter that includes some weird Chinese characters in the middle of variable or function names (e.g. 'ProjectInfo' becomes 'Project极Info'). This causes Roo to fix the code repeatedly.&lt;/p&gt; &lt;p&gt;I don't know if it's an embedding problem in OpenRouter or if it's an issue with the model itself. Has anybody experienced a similar issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcuseAccomplished97"&gt; /u/ExcuseAccomplished97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlps2/the_openrouterhosted_deepseek_r10528_sometimes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlps2/the_openrouterhosted_deepseek_r10528_sometimes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlps2/the_openrouterhosted_deepseek_r10528_sometimes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T01:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz01fo</id>
    <title>DeepSeek-R1-0528-Qwen3-8B</title>
    <updated>2025-05-30T09:39:43+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"&gt; &lt;img alt="DeepSeek-R1-0528-Qwen3-8B" src="https://preview.redd.it/grc43exi3w3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3aa78855c2c46d5947ddfd09811953f40904470e" title="DeepSeek-R1-0528-Qwen3-8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/grc43exi3w3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz01fo/deepseekr10528qwen38b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T09:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzmfum</id>
    <title>Deepseek-r1-0528-qwen3-8b rating justified?</title>
    <updated>2025-05-31T02:35:01+00:00</updated>
    <author>
      <name>/u/ready_to_fuck_yeahh</name>
      <uri>https://old.reddit.com/user/ready_to_fuck_yeahh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzmfum/deepseekr10528qwen38b_rating_justified/"&gt; &lt;img alt="Deepseek-r1-0528-qwen3-8b rating justified?" src="https://preview.redd.it/jypwbwdm414f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=253bd735de5cddb9ab907af528c245a98b5d0e27" title="Deepseek-r1-0528-qwen3-8b rating justified?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ready_to_fuck_yeahh"&gt; /u/ready_to_fuck_yeahh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jypwbwdm414f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzmfum/deepseekr10528qwen38b_rating_justified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzmfum/deepseekr10528qwen38b_rating_justified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T02:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz6cbp</id>
    <title>Fiance-Llama-8B: Specialized LLM for Financial QA, Reasoning and Dialogue</title>
    <updated>2025-05-30T14:57:14+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, Just sharing a model release that might be useful for those working on financial NLP or building domain-specific assistants.&lt;/p&gt; &lt;p&gt;Model on Hugging Face: &lt;a href="https://huggingface.co/tarun7r/Finance-Llama-8B"&gt;https://huggingface.co/tarun7r/Finance-Llama-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finance-Llama-8B is a fine-tuned version of Meta-Llama-3.1-8B, trained on the Finance-Instruct-500k dataset, which includes over 500,000 examples from high-quality financial datasets.&lt;/p&gt; &lt;p&gt;Key capabilities:&lt;/p&gt; &lt;p&gt;• Financial question answering and reasoning&lt;/p&gt; &lt;p&gt;• Multi-turn conversations with contextual depth&lt;/p&gt; &lt;p&gt;• Sentiment analysis, topic classification, and NER&lt;/p&gt; &lt;p&gt;• Multilingual financial NLP tasks&lt;/p&gt; &lt;p&gt;Data sources include: Cinder, Sujet-Finance, Phinance, BAAI/IndustryInstruction_Finance-Economics, and others&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz6cbp/fiancellama8b_specialized_llm_for_financial_qa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kznz2t</id>
    <title>How many users can an M4 Pro support?</title>
    <updated>2025-05-31T04:00:52+00:00</updated>
    <author>
      <name>/u/Cold_Sail_9727</name>
      <uri>https://old.reddit.com/user/Cold_Sail_9727</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thinking an all the bells and whistles M4 Pro unless theres a better option for the price. Not a super critical workload but they dont want it to just take a crap all the time from hardware issues either. &lt;/p&gt; &lt;p&gt;I am looking to implement some locally hosted AI workflows for a smaller company that deals with some more sensitive information. They dont need a crazy model, like gemma12b or qwen3 30b would do just fine. How many users can this support though? I mean they only have like 7-8 people but I want some background automations running plus maybe 1-2 users at a time thorought the day. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cold_Sail_9727"&gt; /u/Cold_Sail_9727 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kznz2t/how_many_users_can_an_m4_pro_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kznz2t/how_many_users_can_an_m4_pro_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kznz2t/how_many_users_can_an_m4_pro_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T04:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzansb</id>
    <title>Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)</title>
    <updated>2025-05-30T17:48:04+00:00</updated>
    <author>
      <name>/u/dehydratedbruv</name>
      <uri>https://old.reddit.com/user/dehydratedbruv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt; &lt;img alt="Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)" src="https://b.thumbs.redditmedia.com/jGFinxlbFv2rq8NpZZCIWMRhFd5eFKx_XUVhWmNNIUY.jpg" title="Yappus. Your Terminal Just Started Talking Back (The Fuck, but Better)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yappus is a terminal-native LLM interface written in Rust, focused on being local-first, fast, and scriptable. &lt;/p&gt; &lt;p&gt;No GUI, no HTTP wrapper. Just a CLI tool that integrates with your filesystem and shell. I am planning to turn into a little shell inside shell kinda stuff. Integrating with Ollama soon!.&lt;/p&gt; &lt;p&gt;Check out system-specific installation scripts:&lt;br /&gt; &lt;a href="https://yappus-term.vercel.app"&gt;&lt;strong&gt;https://yappus-term.vercel.app&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Still early, but stable enough to use daily. Would love feedback from people using local models in real workflows.&lt;/p&gt; &lt;p&gt;I personally use it to just bash script and google , kinda a better alternative to tldr because it's faster and understand errors quickly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fo8wb12niy3f1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2baa841806904135cc39744a3e6e91d19efd615"&gt;https://preview.redd.it/fo8wb12niy3f1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a2baa841806904135cc39744a3e6e91d19efd615&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dehydratedbruv"&gt; /u/dehydratedbruv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzansb/yappus_your_terminal_just_started_talking_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T17:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kynytt</id>
    <title>DeepSeek is THE REAL OPEN AI</title>
    <updated>2025-05-29T22:19:53+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every release is great. I am only dreaming to run the 671B beast locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyr9gd</id>
    <title>"Open source AI is catching up!"</title>
    <updated>2025-05-30T00:55:07+00:00</updated>
    <author>
      <name>/u/Overflow_al</name>
      <uri>https://old.reddit.com/user/Overflow_al</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's kinda funny that everyone says that when Deepseek released R1-0528.&lt;/p&gt; &lt;p&gt;Deepseek seems to be the only one really competing in frontier model competition. The other players always have something to hold back, like Qwen not open-sourcing their biggest model (qwen-max).I don't blame them,it's business,I know.&lt;/p&gt; &lt;p&gt;Closed-source AI company always says that open source models can't catch up with them. &lt;/p&gt; &lt;p&gt;Without Deepseek, they might be right.&lt;/p&gt; &lt;p&gt;Thanks Deepseek for being an outlier!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overflow_al"&gt; /u/Overflow_al &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyr9gd/open_source_ai_is_catching_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T00:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzg3yv</id>
    <title>Too Afraid to Ask: Why don't LoRAs exist for LLMs?</title>
    <updated>2025-05-30T21:31:43+00:00</updated>
    <author>
      <name>/u/Saguna_Brahman</name>
      <uri>https://old.reddit.com/user/Saguna_Brahman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Image generation models generally allow for the use of LoRAs which -- for those who may not know -- is essentially adding some weight to a model that is honed in on a certain thing (this can be art styles, objects, specific characters, etc) that make the model much better at producing images with that style/object/character in it. It may be that the base model had &lt;em&gt;some&lt;/em&gt; idea of &lt;em&gt;some&lt;/em&gt; training data on the topic already but not enough to be reliable or high quality.&lt;/p&gt; &lt;p&gt;However, this doesn't seem to exist for LLMs, it seems that LLMs require a full finetune of the entire model to accomplish this. I wanted to ask why that is, since I don't really understand the technology well enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saguna_Brahman"&gt; /u/Saguna_Brahman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzg3yv/too_afraid_to_ask_why_dont_loras_exist_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzg3yv/too_afraid_to_ask_why_dont_loras_exist_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzg3yv/too_afraid_to_ask_why_dont_loras_exist_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzjhfd</id>
    <title>Ollama 0.9.0 Supports ability to enable or disable thinking</title>
    <updated>2025-05-31T00:02:35+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"&gt; &lt;img alt="Ollama 0.9.0 Supports ability to enable or disable thinking" src="https://external-preview.redd.it/siebatzGRswDgDaN-1zNrvtsYo1Ar9xfV07jYfmXMSI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae2b9d5abfab7fd77887d83caad614ad77503d57" title="Ollama 0.9.0 Supports ability to enable or disable thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.9.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T00:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzfces</id>
    <title>ResembleAI provides safetensors for Chatterbox TTS</title>
    <updated>2025-05-30T21:00:05+00:00</updated>
    <author>
      <name>/u/WackyConundrum</name>
      <uri>https://old.reddit.com/user/WackyConundrum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Safetensors files are now uploaded on Hugging Face:&lt;br /&gt; &lt;a href="https://huggingface.co/ResembleAI/chatterbox/tree/main"&gt;https://huggingface.co/ResembleAI/chatterbox/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And a PR is that adds support to use them to the example code is ready and will be merged in a couple of days:&lt;br /&gt; &lt;a href="https://github.com/resemble-ai/chatterbox/pull/82/files"&gt;https://github.com/resemble-ai/chatterbox/pull/82/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nice!&lt;/p&gt; &lt;p&gt;An examples from the model are here:&lt;br /&gt; &lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;https://resemble-ai.github.io/chatterbox_demopage/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WackyConundrum"&gt; /u/WackyConundrum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfces/resembleai_provides_safetensors_for_chatterbox_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz2o1w</id>
    <title>Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size</title>
    <updated>2025-05-30T12:13:32+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"&gt; &lt;img alt="Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size" src="https://b.thumbs.redditmedia.com/CbbUFJ6UKWwshy_dhxYKGDQcMJhn6z1ajVWlwzo7OzM.jpg" title="Xiaomi released an updated 7B reasoning model and VLM version claiming SOTA for their size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Xiaomi released an update to its 7B reasoning model, which performs very well on benchmarks, and claims SOTA for its size.&lt;/p&gt; &lt;p&gt;Also, Xiaomi released a reasoning VLM version, which again performs excellent in benchmarks.&lt;/p&gt; &lt;p&gt;Compatible w/ Qwen VL arch so works across vLLM, Transformers, SGLang and Llama.cpp &lt;/p&gt; &lt;p&gt;Bonus: it can reason and is MIT licensed 🔥&lt;/p&gt; &lt;p&gt;LLM: &lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-RL-0530"&gt;https://huggingface.co/XiaomiMiMo/MiMo-7B-RL-0530&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VLM: &lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL"&gt;https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kz2o1w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz2o1w/xiaomi_released_an_updated_7b_reasoning_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzcc3f</id>
    <title>Noob question: Why did Deepseek distill Qwen3?</title>
    <updated>2025-05-30T18:56:24+00:00</updated>
    <author>
      <name>/u/Turbulent-Week1136</name>
      <uri>https://old.reddit.com/user/Turbulent-Week1136</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In unsloth's &lt;a href="https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally"&gt;documentation&lt;/a&gt;, it says &amp;quot;DeepSeek also released a R1-0528 distilled version by fine-tuning Qwen3 (8B).&amp;quot;&lt;/p&gt; &lt;p&gt;Being a noob, I don't understand why they would use Qwen3 as the base and then distill from there and then call it Deepseek-R1-0528. Isn't it mostly Qwen3 and they are taking Qwen3's work and then doing a little bit extra and then calling it DeepSeek? What advantage is there to using Qwen3's as the base? Are they allowed to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Week1136"&gt; /u/Turbulent-Week1136 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzq4fp</id>
    <title>M3 Ultra Binned (256GB, 60-Core) vs Unbinned (512GB, 80-Core) MLX Performance Comparison</title>
    <updated>2025-05-31T06:09:42+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I recently decided to invest in an M3 Ultra model for running LLMs, and after a &lt;em&gt;lot&lt;/em&gt; of deliberation, I wanted to share some results that might help others in the same boat.&lt;/p&gt; &lt;p&gt;One of my biggest questions was the actual performance difference between the binned and unbinned M3 Ultra models. It's pretty much impossible for a single person to own and test both machines side-by-side, so there aren't really any direct, apples-to-apples comparisons available online.&lt;/p&gt; &lt;p&gt;While there are some results out there (like on the llama.cpp GitHub, where someone compared the 8B model), they didn't really cover my use case—I'm using MLX as my backend and working with much larger models (235B and above). So the available benchmarks weren’t all that relevant for me.&lt;/p&gt; &lt;p&gt;To be clear, my main reason for getting the M3 Ultra wasn't to run Deepseek models—those are just way too large to use with long context windows, even on the Ultra. My primary goal was to run the Qwen3 235B model.&lt;/p&gt; &lt;p&gt;So I’m sharing my own benchmark results comparing 4-bit and 6-bit quantization for the Qwen3 235B model on a decently long context window (~10k tokens). Hopefully, this will help anyone else who's been stuck with the same questions I had!&lt;/p&gt; &lt;p&gt;Let me know if you have questions, or if there’s anything else you want to see tested.&lt;br /&gt; Just keep in mind that the model sizes are massive, so I might not be able to cover every possible benchmark.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Side note: In the end, I decided to return the 256GB model and stick with the 512GB one. Honestly, 256GB of memory seemed sufficient for most use cases, but since I plan to keep this machine for a while (and also want to experiment with Deepseek models), I went with 512GB. I also think it’s worth using the 80-core GPU. The pp speed difference was bigger than I expected, and for me, that’s one of the biggest weaknesses of Apple silicon. Still, thanks to the MoE architecture, the 235B models run at a pretty usable speed!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Binned (256GB, 60-Core)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 40.09&lt;br /&gt; prompt_eval_duration: 35.41&lt;br /&gt; generation_duration: 4.68&lt;br /&gt; prompt_tokens_per_second: 260.58&lt;br /&gt; generation_tokens_per_second: 22.6&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 43.23&lt;br /&gt; prompt_eval_duration: 38.9&lt;br /&gt; generation _duration: 4.33&lt;br /&gt; prompt_tokens_per_second: 237.2&lt;br /&gt; generation_tokens_per_second: 18.93&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Unbinned (512GB, 80-Core)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 31.33&lt;br /&gt; prompt_eval_duration: 26.76&lt;br /&gt; generation_duration: 4.57&lt;br /&gt; prompt_tokens_per_second: 344.84&lt;br /&gt; generation_tokens_per_second: 23.22&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 32.56&lt;br /&gt; prompt_eval_duration: 28.31&lt;br /&gt; generation _duration: 4.25&lt;br /&gt; prompt_tokens_per_second: 325.96&lt;br /&gt; generation_tokens_per_second: 19.31&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T06:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz5hev</id>
    <title>Why are LLM releases still hyping "intelligence" when solid instruction-following is what actually matters (and they're not that smart anyway)?</title>
    <updated>2025-05-30T14:22:13+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the (somewhat) click bait title, but really, mew LLMs drop, and all of their benchmarks are AIME, GPQA or the nonsense Aider Polyglot. Who cares about these? For actual work like information extraction (even typical QA given a context is pretty much information extraction), summarization, text formatting/paraphrasing, I just need them to FOLLOW MY INSTRUCTION, especially with longer input. These aren't &amp;quot;smart&amp;quot; tasks. And if people still want LLMs to be their personal assistant, there should be more attention to intruction following ability. Assistant doesn't need to be super intellegent, but they need to reliability do the dirty work. &lt;/p&gt; &lt;p&gt;This is even MORE crucial for smaller LLMs. We need those cheap and fast models for bulk data processing or many repeated, day-to-day tasks, and for that, pinpoint instruction-following is everything needed. If they can't follow basic directions reliably, their speed and cheap hardware requirements mean pretty much nothing, however intelligent they are.&lt;/p&gt; &lt;p&gt;Apart from instruction following, tool calling might be the next most important thing. &lt;/p&gt; &lt;p&gt;Let's be real, current LLM &amp;quot;intelligence&amp;quot; is massively overrated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz5hev/why_are_llm_releases_still_hyping_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T14:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzjcdf</id>
    <title>Built an open source desktop app to easily play with local LLMs and MCP</title>
    <updated>2025-05-30T23:56:00+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"&gt; &lt;img alt="Built an open source desktop app to easily play with local LLMs and MCP" src="https://preview.redd.it/i4tcl9p5c04f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80ceafd5cc7131bca4a2e6423a8fbe6fe7ed3d14" title="Built an open source desktop app to easily play with local LLMs and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tome is an open source desktop app for Windows or MacOS that lets you chat with an MCP-powered model without having to fuss with Docker, npm, uvx or json config files. Install the app, connect it to a local or remote LLM, one-click install some MCP servers and chat away.&lt;/p&gt; &lt;p&gt;GitHub link here: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on scheduled tasks and other app concepts that should be released in the coming weeks to enable new powerful ways of interacting with LLMs.&lt;/p&gt; &lt;p&gt;We created this because we wanted an easy way to play with LLMs and MCP servers. We wanted to streamline the user experience to make it easy for beginners to get started. You're not going to see a lot of power user features from the more mature projects, but we're open to any feedback and have only been around for a few weeks so there's a lot of improvements we can make. :)&lt;/p&gt; &lt;p&gt;Here's what you can do today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;connect to Ollama, Gemini, OpenAI, or any OpenAI compatible API&lt;/li&gt; &lt;li&gt;add an MCP server, you can either paste something like &amp;quot;uvx mcp-server-fetch&amp;quot; or you can use the Smithery registry integration to one-click install a local MCP server - Tome manages uv/npm and starts up/shuts down your MCP servers so you don't have to worry about it&lt;/li&gt; &lt;li&gt;chat with your model and watch it make tool calls!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you get a chance to try it out we would love any feedback (good or bad!), thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i4tcl9p5c04f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T23:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf9nl</id>
    <title>Deepseek is cool, but is there an alternative to Claude Code I can use with it?</title>
    <updated>2025-05-30T20:56:55+00:00</updated>
    <author>
      <name>/u/BITE_AU_CHOCOLAT</name>
      <uri>https://old.reddit.com/user/BITE_AU_CHOCOLAT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for an AI coding framework that can help me with training diffusion models. Take existing quasi-abandonned spaguetti codebases and update them to latest packages, implement papers, add features like inpainting, autonomously experiment using different architectures, do hyperparameter searches, preprocess my data and train for me etc... It wouldn't even require THAT much intelligence I think. Sonnet could probably do it. But after trying the API I found its tendency to deceive and take shortcuts a bit frustrating so I'm still on the fence for the €110 subscription (although the auto-compact feature is pretty neat). Is there an open-source version that would get me more for my money?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BITE_AU_CHOCOLAT"&gt; /u/BITE_AU_CHOCOLAT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:56:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz0kqi</id>
    <title>Ollama continues tradition of misnaming models</title>
    <updated>2025-05-30T10:13:30+00:00</updated>
    <author>
      <name>/u/profcuck</name>
      <uri>https://old.reddit.com/user/profcuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't really get the hate that Ollama gets around here sometimes, because much of it strikes me as unfair. Yes, they rely on llama.cpp, and have made a great wrapper around it and a very useful setup.&lt;/p&gt; &lt;p&gt;However, their propensity to misname models is very aggravating.&lt;/p&gt; &lt;p&gt;I'm very excited about DeepSeek-R1-Distill-Qwen-32B. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But to run it from Ollama, it's: ollama run deepseek-r1:32b&lt;/p&gt; &lt;p&gt;This is nonsense. It confuses newbies all the time, who think they are running Deepseek and have no idea that it's a distillation of Qwen. It's inconsistent with HuggingFace for absolutely no valid reason.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/profcuck"&gt; /u/profcuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T10:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzn4ix</id>
    <title>Running Deepseek R1 0528 q4_K_M and mlx 4-bit on a Mac Studio M3</title>
    <updated>2025-05-31T03:12:50+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Mac Model:&lt;/strong&gt; M3 Ultra Mac Studio 512GB, 80 core GPU&lt;/p&gt; &lt;p&gt;First- this model has a shockingly small KV Cache. If any of you saw my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;post about running Deepseek V3 q4_K_M&lt;/a&gt;, you'd have seen that the KV cache buffer in llama.cpp/koboldcpp was 157GB for 32k of context. I expected to see similar here.&lt;/p&gt; &lt;p&gt;Not even close.&lt;/p&gt; &lt;p&gt;64k context on this model is barely 8GB. Below is the buffer loading this model directly in llama.cpp with no special options; just specifying 65536 context, a port and a host. That's it. No MLA, no quantized cache.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama_kv_cache_unified: Metal KV buffer size = 8296.00 MiB&lt;/p&gt; &lt;p&gt;llama_kv_cache_unified: KV self size = 8296.00 MiB, K (f16): 4392.00 MiB, V (f16): 3904.00 MiB&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Speed wise- it's a fair bit on the slow side, but if this model is as good as they say it is, I really don't mind.&lt;/p&gt; &lt;p&gt;Example: ~11,000 token prompt:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp server&lt;/strong&gt; (no flash attention) &lt;strong&gt;(~9 minutes)&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 144330.20 ms / 11090 tokens (13.01 ms per token, &lt;strong&gt;76.84 tokens per second&lt;/strong&gt;)&lt;br /&gt; eval time = 390034.81 ms / 1662 tokens (234.68 ms per token, &lt;strong&gt;4.26 tokens per second&lt;/strong&gt;)&lt;br /&gt; total time = 534365.01 ms / 12752 tokens&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;MLX 4-bit&lt;/strong&gt; for the same prompt (~2.5x speed) &lt;strong&gt;(245sec or ~4 minutes)&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;2025-05-30 23:06:16,815 - DEBUG - Prompt: &lt;strong&gt;189.462 tokens-per-sec&lt;/strong&gt;&lt;br /&gt; 2025-05-30 23:06:16,815 - DEBUG - Generation: &lt;strong&gt;11.154 tokens-per-sec&lt;/strong&gt;&lt;br /&gt; 2025-05-30 23:06:16,815 - DEBUG - Peak memory: 422.248 GB&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Note- Tried flash attention in llama.cpp, and that went horribly. The prompt processing slowed to an absolute crawl. It would have taken longer to process the prompt than the non -fa run took for the whole prompt + response.&lt;/p&gt; &lt;p&gt;Another important note- &lt;strong&gt;when they say not to use System Prompts, they mean it&lt;/strong&gt;. I struggled with this model at first, until I finally completely stripped the system prompt out and jammed all my instructions into the user prompt instead. The model became far more intelligent after that. Specifically, if I passed in a system prompt, it would NEVER output the initial &amp;lt;think&amp;gt; tag no matter what I said or did. But if I don't use a system prompt, it always outputs the initial &amp;lt;think&amp;gt; tag appropriately.&lt;/p&gt; &lt;p&gt;I haven't had a chance to deep dive into this thing yet to see if running a 4bit version really harms the output quality or not, but I at least wanted to give a sneak peak into what it looks like running it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T03:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzfrdt</id>
    <title>ubergarm/DeepSeek-R1-0528-GGUF</title>
    <updated>2025-05-30T21:17:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt; &lt;img alt="ubergarm/DeepSeek-R1-0528-GGUF" src="https://external-preview.redd.it/_ie2E-L6KKHmnErLSoR3DbJuxwXvA6bw-mpTR5JchI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c275cc69bd7ce1a4060b1155a9689d94d05bdc" title="ubergarm/DeepSeek-R1-0528-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all just cooked up some ik_llama.cpp exclusive quants for the recently updated DeepSeek-R1-0528 671B. New recipes are looking pretty good (lower perplexity is &amp;quot;better&amp;quot;):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-Q8_0&lt;/code&gt; 666GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I didn't upload this, it is for baseline reference only.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ3_K_R4&lt;/code&gt; 301GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2730 +/- 0.01738&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 24GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ2_K_R4&lt;/code&gt; 220GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.5069 +/- 0.01893&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 16GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still might release one or two more e.g. one bigger and one smaller if there is enough interest.&lt;/p&gt; &lt;p&gt;As usual big thanks to Wendell and the whole Level1Techs crew for providing hardware expertise and access to release these quants!&lt;/p&gt; &lt;p&gt;Cheers and happy weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz48qx</id>
    <title>Even DeepSeek switched from OpenAI to Google</title>
    <updated>2025-05-30T13:29:07+00:00</updated>
    <author>
      <name>/u/Utoko</name>
      <uri>https://old.reddit.com/user/Utoko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt; &lt;img alt="Even DeepSeek switched from OpenAI to Google" src="https://preview.redd.it/uy7wbaj17x3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e5d766354c0e76341191c5702f69924996f4b0e" title="Even DeepSeek switched from OpenAI to Google" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Similar in text Style analyses from &lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt; shows that R1 is now much closer to Google. &lt;/p&gt; &lt;p&gt;So they probably used more synthetic gemini outputs for training. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Utoko"&gt; /u/Utoko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uy7wbaj17x3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T13:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzlb8g</id>
    <title>Unlimited Speech to Speech using Moonshine and Kokoro, 100% local, 100% open source</title>
    <updated>2025-05-31T01:34:33+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rhulha.github.io/Speech2Speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T01:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzcalh</id>
    <title>llama-server is cooking! gemma3 27b, 100K context, vision on one 24GB GPU.</title>
    <updated>2025-05-30T18:54:42+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama-server has really improved a lot recently. With vision support, SWA (sliding window attention) and performance improvements I've got 35tok/sec on a 3090. P40 gets 11.8 tok/sec. Multi-gpu performance has improved. Dual 3090s performance goes up to 38.6 tok/sec (600W power limit). Dual P40 gets 15.8 tok/sec (320W power max)! Rejoice P40 crew. &lt;/p&gt; &lt;p&gt;I've been writing more guides for the llama-swap wiki and was very surprised with the results. Especially how usable the P40 still are!&lt;/p&gt; &lt;p&gt;llama-swap config (&lt;a href="https://github.com/mostlygeek/llama-swap/wiki/gemma3-27b-100k-context"&gt;source wiki page&lt;/a&gt;): &lt;/p&gt; &lt;p&gt;```yaml macros: &amp;quot;server-latest&amp;quot;: /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap&lt;/p&gt; &lt;p&gt;# quantize KV cache to Q8, increases context but # has a small effect on perplexity # &lt;a href="https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt; &amp;quot;q8-kv&amp;quot;: &amp;quot;--cache-type-k q8_0 --cache-type-v q8_0&amp;quot;&lt;/p&gt; &lt;p&gt;models: # fits on a single 24GB GPU w/ 100K context # requires Q8 KV quantization &amp;quot;gemma&amp;quot;: env: # 3090 - 35 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 11.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1&amp;quot; cmd: | ${server-latest} ${q8-kv} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# Requires 30GB VRAM # - Dual 3090s, 38.6 tok/sec # - Dual P40s, 15.8 tok/sec &amp;quot;gemma-full&amp;quot;: env: # 3090s - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40s # - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-ea4&amp;quot; cmd: | ${server-latest} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 # uncomment if using P40s # -sm row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kze1r6</id>
    <title>Ollama run bob</title>
    <updated>2025-05-30T20:06:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt; &lt;img alt="Ollama run bob" src="https://preview.redd.it/v4krpd9g7z3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2201e590a1b08cca19a9ca4d26c56ddf0e869e85" title="Ollama run bob" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4krpd9g7z3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:06:52+00:00</published>
  </entry>
</feed>
