<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-12T15:50:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mnlfyt</id>
    <title>Geocities style site by glm 4.5</title>
    <updated>2025-08-11T18:44:42+00:00</updated>
    <author>
      <name>/u/ChazychazZz</name>
      <uri>https://old.reddit.com/user/ChazychazZz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt; &lt;img alt="Geocities style site by glm 4.5" src="https://preview.redd.it/fvi60un3qfif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=ac08d73fcf7b3cd7d2da2f4614ab0b91d7bf5250" title="Geocities style site by glm 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Completed in just 1 super simple prompt. GLM 4.5 is terrifyingly good at web dev now, especially as we can run it local. For me it was obvious it can generate modern and modern-ish sites but this stuff is kinda cooler to see (at least for me). The only unfortunate thing that it used emojis but that can be tweaked i guess and just included in the prompt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChazychazZz"&gt; /u/ChazychazZz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fvi60un3qfif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnlfyt/geocities_style_site_by_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T18:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1moabey</id>
    <title>Llama.cpp on android</title>
    <updated>2025-08-12T14:31:01+00:00</updated>
    <author>
      <name>/u/0xBekket</name>
      <uri>https://old.reddit.com/user/0xBekket</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moabey/llamacpp_on_android/"&gt; &lt;img alt="Llama.cpp on android" src="https://b.thumbs.redditmedia.com/TOvb2FgS4j5snat4W_rbCcBYoRgR09JTZKTd8yKUMGs.jpg" title="Llama.cpp on android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, I have been succesfully compiled and run llama c++ at my android and run uncensored llm locally&lt;/p&gt; &lt;p&gt;The most wild thing, that you actually can build llama.cpp from source directly at android and run it from there, so now I can use it to ask any questions and my history will never leave a device&lt;/p&gt; &lt;p&gt;In example I have asked llm how to kill Putin&lt;/p&gt; &lt;p&gt;If you are interested, I can share you script of commands to build your own&lt;/p&gt; &lt;p&gt;The only issue I am currently expereincing is heat, and I am afraid, that some smaller android devices can be turned into grenades and blow off your hand with about 30% probability &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xBekket"&gt; /u/0xBekket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1moabey"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moabey/llamacpp_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moabey/llamacpp_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnd144</id>
    <title>Am I the only one who never really liked Ollama?</title>
    <updated>2025-08-11T13:30:22+00:00</updated>
    <author>
      <name>/u/a_normal_user1</name>
      <uri>https://old.reddit.com/user/a_normal_user1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all that happens with it now and them wanting people to make accounts to use certain features(which kinda defeats the purpose of it) am I the only one who thought that it's really not the best? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_normal_user1"&gt; /u/a_normal_user1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnd144/am_i_the_only_one_who_never_really_liked_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mobyjw</id>
    <title>Where's Prima.cpp??</title>
    <updated>2025-08-12T15:33:34+00:00</updated>
    <author>
      <name>/u/Mysterious_While358</name>
      <uri>https://old.reddit.com/user/Mysterious_While358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know what happened to &lt;a href="https://github.com/Lizonghang/prima.cpp"&gt;https://github.com/Lizonghang/prima.cpp&lt;/a&gt; it 404. This was the official repo for the prima.cpp paper right? &lt;a href="https://arxiv.org/abs/2504.08791"&gt;https://arxiv.org/abs/2504.08791&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_While358"&gt; /u/Mysterious_While358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobyjw/wheres_primacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mobyjw/wheres_primacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mobyjw/wheres_primacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T15:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo69j2</id>
    <title>What does gradient_checkpointing do when using HF transformer for full training?</title>
    <updated>2025-08-12T11:36:10+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting this runtime error while running full fine tuning with a 130m embedding model with the HF transformers library with a TraingingArguments like this. It seems like the crash is related to torch_compile.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; training_args = TrainingArguments( output_dir=OUTPUT_DIR, save_strategy=&amp;quot;steps&amp;quot;, save_steps=6304, learning_rate=2e-5, per_device_train_batch_size=64, # Adjust based on GPU memory num_train_epochs=4, weight_decay=0.01, tf32=True, bf16=True, torch_compile=True, torch_compile_backend=&amp;quot;inductor&amp;quot;, dataloader_pin_memory=True, dataloader_num_workers=6, logging_dir='./logs', logging_steps=1576, report_to=&amp;quot;none&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the Runtime error goes away when I add gradient_checkpointing=True to TrainingArguments. Why is that? According HF docs, this switch supposedly slow down my training by reducing VRAM use. Is there more to it than VRAM saving?&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/trainer.py&amp;quot;, line 2207, in train return inner_training_loop( ^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/trainer.py&amp;quot;, line 2549, in _inner_training_loop tr_loss_step = self.training_step(model, inputs, num_items_in_batch) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/trainer.py&amp;quot;, line 3750, in training_step loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/trainer.py&amp;quot;, line 3837, in compute_loss outputs = model(**inputs) ^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1751, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1762, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&amp;quot;, line 655, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1751, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1762, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py&amp;quot;, line 818, in forward return model_forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py&amp;quot;, line 806, in __call__ return convert_to_fp32(self.model_forward(*args, **kwargs)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py&amp;quot;, line 44, in decorate_autocast return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py&amp;quot;, line 1153, in forward outputs = self.model( ^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1751, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1762, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py&amp;quot;, line 168, in forward max_seqlen: Optional[int] = None, File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py&amp;quot;, line 868, in torch_dynamo_resume_in_forward_at_847 layer_outputs = encoder_layer( ^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/modeling_layers.py&amp;quot;, line 83, in __call__ return super().__call__(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1751, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py&amp;quot;, line 1762, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py&amp;quot;, line 538, in forward attn_outputs = self.attn( File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py&amp;quot;, line 538, in torch_dynamo_resume_in_forward_at_538 attn_outputs = self.attn( File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py&amp;quot;, line 838, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py&amp;quot;, line 1201, in forward return compiled_fn(full_args) ^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py&amp;quot;, line 315, in runtime_wrapper all_outs = call_func_at_runtime_with_args( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&amp;quot;, line 126, in call_func_at_runtime_with_args out = normalize_as_list(f(args)) ^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&amp;quot;, line 100, in g return f(*args) ^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py&amp;quot;, line 575, in apply return super().apply(*args, **kwargs) # type: ignore[misc] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py&amp;quot;, line 1937, in forward fw_outs = call_func_at_runtime_with_args( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py&amp;quot;, line 126, in call_func_at_runtime_with_args out = normalize_as_list(f(args)) ^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py&amp;quot;, line 495, in wrapper return compiled_fn(runtime_args) ^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py&amp;quot;, line 689, in inner_fn outs = compiled_fn(args) ^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py&amp;quot;, line 460, in __call__ return self.current_callable(inputs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/home/user/anaconda3/lib/python3.12/site-packages/torch/_inductor/utils.py&amp;quot;, line 2404, in run return model(new_inputs) ^^^^^^^^^^^^^^^^^ File &amp;quot;/tmp/torchinductor_user/t5/ct5kwylslfcgaui2jqprcnqilzsdmc47af5v6tmpj463okakyfin.py&amp;quot;, line 208, in call extern_kernels.addmm(buf7, buf6, reinterpret_tensor(primals_5, (2048, 512), (1, 2048), 0), alpha=1, beta=1, out=buf8) RuntimeError: self and mat2 must have the same dtype, but got Float and BFloat16 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo69j2/what_does_gradient_checkpointing_do_when_using_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo69j2/what_does_gradient_checkpointing_do_when_using_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo69j2/what_does_gradient_checkpointing_do_when_using_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T11:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo4ep3</id>
    <title>Why MLA is not used more and companies still prefer with GQA ?</title>
    <updated>2025-08-12T09:52:00+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So i have been going through the code of some of the latest models and i noticed that they still use GQA like GPT-OSS and GLM-4.5 inspite people saying that Mutli-Head Latent Attentions is superior.&lt;/p&gt; &lt;p&gt;Does anyone have a reason for that ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo4ep3/why_mla_is_not_used_more_and_companies_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T09:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnhgt0</id>
    <title>GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks</title>
    <updated>2025-08-11T16:19:43+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt; &lt;img alt="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" src="https://preview.redd.it/jw671veezeif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ef1b882c2760541b723f2922a88f046fea21c80" title="GPT-OSS Benchmarks: How GPT-OSS-120B Performs in Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI released their first open models since GPT-2, and GPT-OSS-120B is now the best open-weight model on our real-world TaskBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better completion performance overall compared to other open-weight models like Kimi-K2 and DeepSeek-R1, while being roughly 1/10th the size. Cheaper, better, faster.&lt;/li&gt; &lt;li&gt;Relative to closed-source models, it performs like smaller frontier models such as o4-mini or previous-generation top tier models like Claude-3.7.&lt;/li&gt; &lt;li&gt;Clearly optimized for agentic use cases, it’s close to Sonnet-4 on our agentic benchmarks and could be a strong main agent model.&lt;/li&gt; &lt;li&gt;Works more like an action model than a chat or knowledge model. Multi-lingual performance is limited, and it hallucinates more on world knowledge, so it benefits from retrieval grounding and pairing with another model for multi-lingual scenarios.&lt;/li&gt; &lt;li&gt;Context recall is decent but weaker than top frontier models, so it’s better suited for shorter or carefully managed context windows.&lt;/li&gt; &lt;li&gt;Excels when paired with strong context engineering and agentic engineering, where each task completion reliably feeds into the next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, this model looks to be a real gem and will likely inject more energy into open-source models.&lt;/p&gt; &lt;p&gt;We’ve published the full benchmark results, including GPT-5, mini, and nano, and our task categories and eval methods here: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those building with it, anyone else seeing similar strengths/weaknesses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw671veezeif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnhgt0/gptoss_benchmarks_how_gptoss120b_performs_in_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T16:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncfif</id>
    <title>GLM-4.5V (based on GLM-4.5 Air)</title>
    <updated>2025-08-11T13:04:47+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A vision-language model (VLM) in the GLM-4.5 family. Features listed in model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image reasoning&lt;/strong&gt; (scene understanding, complex multi-image analysis, spatial recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video understanding&lt;/strong&gt; (long video segmentation and event recognition)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GUI tasks&lt;/strong&gt; (screen reading, icon recognition, desktop operation assistance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex chart &amp;amp; long document parsing&lt;/strong&gt; (research report analysis, information extraction)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grounding&lt;/strong&gt; (precise visual element localization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5V"&gt;https://huggingface.co/zai-org/GLM-4.5V&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncfif/glm45v_based_on_glm45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa5as</id>
    <title>Cheap coding assistant</title>
    <updated>2025-08-12T14:24:37+00:00</updated>
    <author>
      <name>/u/Evisteron</name>
      <uri>https://old.reddit.com/user/Evisteron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some say there's a paradigm shift happening in software - in coding specifically, where AI is taking the reigns. Other say it's hype, and actually everything takes longer. I have found the truth to be somewhere in between at work with tools like Cline and Claude Code (at least on work projects - they pay.)&lt;/p&gt; &lt;p&gt;But what I'm struggling with is the ability to do hobby coding with these tools, because they are so expensive. Usually, when I try to learn a new platform, or computer language, I just sort of set up a dev environment, and play - but it's different when that play is expensive.&lt;/p&gt; &lt;p&gt;So that's my ask - what is the cheapest way to run a backend for cline, or the cheapest service you've found - one that *doesn't suck?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evisteron"&gt; /u/Evisteron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5as/cheap_coding_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo9vkh</id>
    <title>Microsoft releases Prompt Orchestration Markup Language</title>
    <updated>2025-08-12T14:14:04+00:00</updated>
    <author>
      <name>/u/ArtZab</name>
      <uri>https://old.reddit.com/user/ArtZab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;Just came across Microsoft’s POML (Prompt Orchestration Markup Language) and it seems like a useful tool to have.&lt;/p&gt; &lt;p&gt;From GitHub page (&lt;a href="https://github.com/microsoft/poml):"&gt;https://github.com/microsoft/poml):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;POML (Prompt Orchestration Markup Language) is a novel markup language designed to bring structure, maintainability, and versatility to advanced prompt engineering for Large Language Models (LLMs). It addresses common challenges in prompt development, such as lack of structure, complex data integration, format sensitivity, and inadequate tooling. POML provides a systematic way to organize prompt components, integrate diverse data types seamlessly, and manage presentation variations, empowering developers to create more sophisticated and reliable LLM applications.&lt;/p&gt; &lt;p&gt;What are your thoughts on this release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtZab"&gt; /u/ArtZab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo9vkh/microsoft_releases_prompt_orchestration_markup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:14:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mno45o</id>
    <title>FULL LEAKED v0 by Vercel System Prompts and Internal Tools</title>
    <updated>2025-08-11T20:25:04+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 11/08/2025)&lt;/p&gt; &lt;p&gt;I managed to get FULL official v0 system prompt and internal tools. Over 13.5K tokens and 1.3K lines.&lt;/p&gt; &lt;p&gt;Check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mno45o/full_leaked_v0_by_vercel_system_prompts_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T20:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo3hrh</id>
    <title>Gpt-oss-120b API provider comparison</title>
    <updated>2025-08-12T08:53:47+00:00</updated>
    <author>
      <name>/u/Sadman782</name>
      <uri>https://old.reddit.com/user/Sadman782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/1955102409044398415"&gt;https://x.com/ArtificialAnlys/status/1955102409044398415&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As expected, Groq performs worse. I've commented multiple times that something seemed off with their implementation, and this data appears to back that up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sadman782"&gt; /u/Sadman782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3hrh/gptoss120b_api_provider_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T08:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo5s79</id>
    <title>Why evals are the missing piece in most AI products</title>
    <updated>2025-08-12T11:11:00+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep seeing AI teams obsess over model choice, prompts, and infrastructure, but very few invest in structured evals early. Without them, you are basically shipping blind. In my experience, good eval workflows catch issues before they hit production, shorten iteration cycles, and prevent those “works in testing, fails in prod” disasters.&lt;/p&gt; &lt;p&gt;At Maxim AI we’ve seen teams slash AI feature rollout time just by setting up continuous eval loops with both human and automated tests. If your AI product handles real user-facing tasks, you cannot rely on spot checks. You need evals that mimic the exact scenarios your users will throw at the system.&lt;/p&gt; &lt;p&gt;What’s your take, are evals an engineering must-have or just a nice-to-have?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo5s79/why_evals_are_the_missing_piece_in_most_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T11:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnc8lx</id>
    <title>I built Excel Add-in for Ollama</title>
    <updated>2025-08-11T12:56:39+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt; &lt;img alt="I built Excel Add-in for Ollama" src="https://preview.redd.it/mvjwf2f81eif1.gif?width=640&amp;amp;crop=smart&amp;amp;s=17b456d91ceed7000d3f08cd2f8917aec6e4254a" title="I built Excel Add-in for Ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an excel add-in that connects Ollama with Microsoft Excel. Data to remain inside excel only. You can simply write function =ollama(A1), assuming prompt in cell A1. You can simply drag to run on multiple cells. It has arguments to specify system instructions, temperature and model. You can set at both global level and specific to your prompts. &lt;a href="https://www.listendata.com/2025/08/ollama-in-excel.html"&gt;https://www.listendata.com/2025/08/ollama-in-excel.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mvjwf2f81eif1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnc8lx/i_built_excel_addin_for_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T12:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1moa5o0</id>
    <title>VulkanIlm, Run Modern LLMs on Old GPUs via Vulkan (33× Faster on Dell iGPU, 4× on RX 580)</title>
    <updated>2025-08-12T14:24:59+00:00</updated>
    <author>
      <name>/u/Proper_Dig_6618</name>
      <uri>https://old.reddit.com/user/Proper_Dig_6618</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’ve been building &lt;strong&gt;VulkanIlm&lt;/strong&gt; — a Python wrapper for llama.cpp that uses Vulkan for GPU acceleration. The goal: make local LLMs faster on &lt;em&gt;any&lt;/em&gt; GPU, even older AMD and integrated ones, with &lt;strong&gt;no CUDA dependency&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Some early benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dell E7250 (i7-5600U, Intel iGPU)&lt;/strong&gt;&lt;br /&gt; Model: TinyLLaMA-1.1B-Chat (Q4_K_M)&lt;br /&gt; CPU: 121 s → GPU: 3 s → &lt;strong&gt;33× speedup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AMD RX 580 (8 GB)&lt;/strong&gt;&lt;br /&gt; Model: Gemma-3n-E4B-it (6.9 B params)&lt;br /&gt; CPU: 188 s → GPU: 44 s → &lt;strong&gt;4× speedup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next steps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More benchmarks (including new OpenAI OSS models)&lt;/li&gt; &lt;li&gt;“Run LLMs on Your Old GPU” video tutorial&lt;/li&gt; &lt;li&gt;AMD GPU deep dive&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo (still under active development): &lt;a href="https://github.com/Talnz007/VulkanIlm"&gt;https://github.com/Talnz007/VulkanIlm&lt;/a&gt;&lt;br /&gt; Please try it out, contribute, or share feedback — I’m aiming to make this work well for the entire Local LLaMA community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proper_Dig_6618"&gt; /u/Proper_Dig_6618 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moa5o0/vulkanilm_run_modern_llms_on_old_gpus_via_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo3j17</id>
    <title>LocalAI Major Update: Modular Backends (update llama.cpp, stablediffusion.cpp, and others independently!), Qwen-VL, Qwen-Image Support, Image Editing &amp; More</title>
    <updated>2025-08-12T08:56:03+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might know LocalAI already as a way to self-host your own private, OpenAI-compatible AI API (it was the first of its kind !). I'm excited to share that we've just pushed a series of massive updates that I think this community will really appreciate. As a reminder: LocalAI is not a company, it's a Free, open source project community-driven!&lt;/p&gt; &lt;p&gt;Also, LocalAI just hit &lt;strong&gt;34.5k stars on GitHub and&lt;/strong&gt; &lt;strong&gt;LocalAGI&lt;/strong&gt; &lt;strong&gt;crossed 1k&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt; (which is, an Agentic system built on top of LocalAI) and we know a huge part of that is from power users like you. Thank you!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here’s the TL;DR on what's new (v3.2.0-v3.4.0):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Modular Backends:&lt;/strong&gt; We've completely separated the inference backends from the LocalAI core. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Smaller images:&lt;/strong&gt; You can now update backends like &lt;code&gt;llama.cpp&lt;/code&gt; , &lt;code&gt;stablediffusion.cpp&lt;/code&gt; or &lt;code&gt;diffusers&lt;/code&gt;independently! If a new version of a backend drops, you can pull it in without waiting for a new LocalAI release. It also means the core app is super lean.&lt;/li&gt; &lt;li&gt;Installation of required backends is automatic based on the model's needs and your hardware (CUDA, ROCm, SYCL, CPU-only etc.). &lt;ul&gt; &lt;li&gt;We are working now to improve CPU support for backends like &lt;code&gt;diffusers&lt;/code&gt; and the ones using pytorch, stay tuned!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Object detection, Qwen-Image, and..&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Full support for powerful models like &lt;strong&gt;Qwen-VL or Qwen Image&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Now you can do image editing using text prompts with &lt;strong&gt;Flux Kontext&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;We added an additional API endpoint specifically for &lt;strong&gt;object detection&lt;/strong&gt; API, currently powered by the rfdetr (&lt;a href="https://github.com/roboflow/rf-detr"&gt;https://github.com/roboflow/rf-detr&lt;/a&gt;) backend which you can install from the backend gallery with one click, or just installing the rfdetr model&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Model &amp;amp; Backend Expansion:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;The gallery now has many new models, including the &lt;strong&gt;latest from the community&lt;/strong&gt; , and Qwen Image, Flux Krea, GPT-OSS and many more!&lt;/li&gt; &lt;li&gt;We've added new TTS backends like &lt;strong&gt;KittenTTS&lt;/strong&gt;, Dia, and Kokoro if you're experimenting with voice.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to make LocalAI the most flexible, OpenAI-compatible API layer for whatever you want to run locally. These changes give you more control and faster access to the latest and greatest from the community.&lt;/p&gt; &lt;p&gt;Check out the full release notes and give it a spin: ➡️&lt;strong&gt;&lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think and what models you're planning to test with the new setup!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo3j17/localai_major_update_modular_backends_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T08:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnp5nc</id>
    <title>Training an LLM only on books from the 1800's - Another update</title>
    <updated>2025-08-11T21:04:34+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm training LLM's from scratch using only texts from a specific region and time period and want to share another update. Right now it's 1800-1875 London. When I first started, my dataset was only 50 texts and I was using a 4060 for training. The latest version is trained on almost 7,000 texts using Phi 1.5 (700M parameters) on an A100 GPU. My long term goal is to see if a model trained this way can actually reason. The newest model I've trained has some promising output, it's starting to reference real historical events instead of just hallucinating everything. Also many people have told me that fine tuning will be more efficient and I agree, but I want to see how far this approach can go. And Internet Archive has around 175,000 London texts within my chosen time period, so scaling the dataset won't be an issue. &lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnp5nc/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T21:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxwmw</id>
    <title>Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot</title>
    <updated>2025-08-12T03:24:03+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt; &lt;img alt="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" src="https://b.thumbs.redditmedia.com/81joqRjngFFUavEApRYDiznp-6LcG-wUqoKaM4BcLls.jpg" title="Unsloth fixes chat_template (again). gpt-oss-120-high now scores 68.4 on Aider polyglot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a"&gt;https://preview.redd.it/tx92p3mpbiif1.png?width=688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de64253fcf2dba31b81554a76ac87534d3d29d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to gguf: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sha256: c6f818151fa2c6fbca5de1a0ceb4625b329c58595a144dc4a07365920dd32c51&lt;/p&gt; &lt;p&gt;edit: test was done with above Unsloth gguf downloaded Aug 5,&lt;/p&gt; &lt;p&gt;and with the new chat_template here: &lt;a href="https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja"&gt;https://huggingface.co/openai/gpt-oss-120b/resolve/main/chat_template.jinja&lt;/a&gt;&lt;/p&gt; &lt;p&gt;newest Unsloth gguf has same link and;&lt;/p&gt; &lt;p&gt;sha256: 2d1f0298ae4b6c874d5a468598c5ce17c1763b3fea99de10b1a07df93cef014f&lt;/p&gt; &lt;p&gt;and also has an improved chat template built-in&lt;/p&gt; &lt;p&gt;currently rerunning low and medium reasoning tests with the newest gguf&lt;/p&gt; &lt;p&gt;and with the chat template built into the gguf&lt;/p&gt; &lt;p&gt;high reasoning took 2 days to run load balanced over 6 llama.cpp nodes so we will only rerun if there is a noticeable improvement with low and medium&lt;/p&gt; &lt;p&gt;high reasoning used 10x completion tokens over low, medium used 2x over low. high used 5x over medium etc. so both low and medium are much faster than high.&lt;/p&gt; &lt;p&gt;Finally here are instructions how to run locally: &lt;a href="https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and: &lt;a href="https://aider.chat/"&gt;https://aider.chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxwmw/unsloth_fixes_chat_template_again_gptoss120high/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1vre</id>
    <title>Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'.</title>
    <updated>2025-08-12T07:08:27+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt; &lt;img alt="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." src="https://preview.redd.it/2e65cn38fjif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da4ffea4883b21f3e637daf2a89cb44028cbdb31" title="Why stop at 'Strawberry'? Lets up the game with 'How many c's are there in 'pneumonoultramicroscopicsilicovolcanoconiosis'." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 4B got it right after thinking 30 Sec.ZLM thought for almost 2 min .GPT-5 took 5 sec.Gemini took less than 2 sec,and told me use count() function in Python which it used. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e65cn38fjif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1vre/why_stop_at_strawberry_lets_up_the_game_with_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mncrqp</id>
    <title>ollama</title>
    <updated>2025-08-11T13:19:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt; &lt;img alt="ollama" src="https://preview.redd.it/2whabjm55eif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dea8efc9d0fe6d86f047a62709601f55061db889" title="ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2whabjm55eif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T13:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1mb1</id>
    <title>GLM 4.5 AIR IS SO FKING GOODDD</title>
    <updated>2025-08-12T06:52:07+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got to try it with our agentic system , it's so fast and perfect with its tool calls , but mostly it's freakishly fast too , thanks z.ai i love you 😘💋&lt;/p&gt; &lt;p&gt;Edit: not running it locally, used open router to test stuff. I m just here to hype em up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1mb1/glm_45_air_is_so_fking_gooddd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo1pv4</id>
    <title>Uncensored gpt-oss-20b released</title>
    <updated>2025-08-12T06:58:18+00:00</updated>
    <author>
      <name>/u/No-Solution-8341</name>
      <uri>https://old.reddit.com/user/No-Solution-8341</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt; &lt;img alt="Uncensored gpt-oss-20b released" src="https://external-preview.redd.it/P0d7BMzhU8lFm_gY9r3-Ieqcq7avVW4yk_FBxEW_Ccs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a246b29e2c888dfb88cfcf39f23a3530b26e09d" title="Uncensored gpt-oss-20b released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jinx is a &amp;quot;helpful-only&amp;quot; variant of popular open-weight language models that responds to all queries without safety refusals.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b"&gt;https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Solution-8341"&gt; /u/No-Solution-8341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo1pv4/uncensored_gptoss20b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T06:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1moakv3</id>
    <title>We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025</title>
    <updated>2025-08-12T14:41:09+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt; &lt;img alt="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" src="https://preview.redd.it/lcee3fueolif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bc5d986a916d0445a79f4b3d5044d02c9aacef2" title="We tested Qwen3-Coder, GPT-5 and other 30+ models on new SWE-Bench like tasks from July 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We ran a benchmark on &lt;strong&gt;34 fresh GitHub PR tasks&lt;/strong&gt; from July 2025 using the &lt;a href="https://swe-rebench.com/leaderboard"&gt;SWE-rebench leaderboard&lt;/a&gt;. These are real, recent problems — no training-set contamination — and include both proprietary and open-source models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-5-Medium&lt;/strong&gt; leads overall (29.4% resolved rate, 38.2% pass@5).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;, matching GPT-5-High in pass@5 (32.4%) despite a lower resolved rate.&lt;/li&gt; &lt;li&gt;Claude Sonnet 4.0 lags behind in pass@5 at 23.5%.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All tasks come from the continuously updated, decontaminated &lt;a href="https://huggingface.co/datasets/nebius/SWE-rebench-leaderboard"&gt;SWE-rebench-leaderboard&lt;/a&gt; dataset for real-world SWE tasks.&lt;/p&gt; &lt;p&gt;We’re already adding gpt-oss-120b and GLM-4.5 next — which OSS model should we include after that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lcee3fueolif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1moakv3/we_tested_qwen3coder_gpt5_and_other_30_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T14:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mo2gg7</id>
    <title>Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro</title>
    <updated>2025-08-12T07:45:23+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt; &lt;img alt="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" src="https://preview.redd.it/niaetccbljif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83cb98af8850f5f113bf6d7f37db6c95989b888f" title="Jan v1: 4B model for web search with 91% SimpleQA, slightly outperforms Perplexity Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from Jan. We're releasing Jan v1 today. In our evals, Jan v1 delivers 91% SimpleQA accuracy, slightly outperforming Perplexity Pro while running fully locally.&lt;/p&gt; &lt;p&gt;It's built on the new version of Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;Qwen3-4B-Thinking&lt;/a&gt; (up to 256k context length), fine-tuned for reasoning and tool use in Jan.&lt;/p&gt; &lt;h1&gt;How to run it:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Jan&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download Jan v1 via Jan Hub&lt;/li&gt; &lt;li&gt;Enable search in Jan: &lt;ul&gt; &lt;li&gt;Settings → Experimental Features → On&lt;/li&gt; &lt;li&gt;Settings → MCP Servers → enable Search-related MCP (e.g. Serper)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Plus you can run the model in llama.cpp and vLLM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v1-4B: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B"&gt;https://huggingface.co/janhq/Jan-v1-4B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v1-4B-GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-4B-GGUF"&gt;https://huggingface.co/janhq/Jan-v1-4B-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 0.6&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;min_p: 0.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;max_tokens: 2048&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We'd love for you to try Jan v1 and share your feedback, including what works well and where it falls short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/niaetccbljif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mo2gg7/jan_v1_4b_model_for_web_search_with_91_simpleqa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T07:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mnxodk</id>
    <title>LocalLLaMA is the last sane place to discuss LLMs on this site, I swear</title>
    <updated>2025-08-12T03:12:34+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt; &lt;img alt="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" src="https://preview.redd.it/iu3pniar9iif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4bac76c25e583f3690f2e1e9cdc20c74739fa84c" title="LocalLLaMA is the last sane place to discuss LLMs on this site, I swear" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iu3pniar9iif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-12T03:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
