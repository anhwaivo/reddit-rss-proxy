<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-11T17:22:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kk4j1u</id>
    <title>Faster and most accurate speech to text models (opensource/local)?</title>
    <updated>2025-05-11T16:08:11+00:00</updated>
    <author>
      <name>/u/TheMarketBuilder</name>
      <uri>https://old.reddit.com/user/TheMarketBuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I am trying to dev an app for real time audio transcription. I need a local model for speech to text transcription (multilingual en, fr) that is fast so I can have live transcription. &lt;/p&gt; &lt;p&gt;Can you orientate me to the best existing models? I tried faster whisper 6 month ago, but I am not sure what are the new ones out their !&lt;/p&gt; &lt;p&gt;Thanks !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheMarketBuilder"&gt; /u/TheMarketBuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4j1u/faster_and_most_accurate_speech_to_text_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4j1u/faster_and_most_accurate_speech_to_text_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4j1u/faster_and_most_accurate_speech_to_text_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T16:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk59cy</id>
    <title>Anyone aware of local AI-assisted tools for reverse engineering legacy .NET or VB6 binaries?</title>
    <updated>2025-05-11T16:40:42+00:00</updated>
    <author>
      <name>/u/Hinged31</name>
      <uri>https://old.reddit.com/user/Hinged31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might be a bit of a long shot, but I figured I’d ask here: is anyone aware of any AI-assisted tools (LLM-integrated or otherwise) that help with reverse engineering old abandoned binaries—specifically legacy VB6 or .NET executables (think PE32 GUIs from the early 2000s, calling into MSVBVM60.DLL, possibly compiled as p-code or using COM controls like VSDraw)?&lt;/p&gt; &lt;p&gt;I’ve tried using Ghidra, but don’t really know what I’m doing, and I’m wondering if there’s anything smarter—something that can recognize VB runtime patterns, trace through p-code or thunked imports, and help reconstruct the app’s logic (especially GUI drawing code). Ideally something that can at least annotate or pseudocode the runtime-heavy stuff for reimplementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hinged31"&gt; /u/Hinged31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk59cy/anyone_aware_of_local_aiassisted_tools_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk59cy/anyone_aware_of_local_aiassisted_tools_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk59cy/anyone_aware_of_local_aiassisted_tools_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T16:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj7l8p</id>
    <title>AMD eGPU over USB3 for Apple Silicon by Tiny Corp</title>
    <updated>2025-05-10T10:58:23+00:00</updated>
    <author>
      <name>/u/zdy132</name>
      <uri>https://old.reddit.com/user/zdy132</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt; &lt;img alt="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" src="https://external-preview.redd.it/2BON-N6TCd_ctm0tqr4moZr228fviTa5r-AUavBUN3Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de7825d238122dad4a6420788d2290a151b8da31" title="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zdy132"&gt; /u/zdy132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/__tinygrad__/status/1920960070055080107"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjd8tg</id>
    <title>Absolute_Zero_Reasoner-Coder-14b / 7b / 3b</title>
    <updated>2025-05-10T15:44:01+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"&gt; &lt;img alt="Absolute_Zero_Reasoner-Coder-14b / 7b / 3b" src="https://external-preview.redd.it/c2vPUFXKhvD_gZRfZocGG6ne7L_maCxsQIvkq5lx_Ec.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6db7591f54489669f2ba77fd228c4034fbc9225" title="Absolute_Zero_Reasoner-Coder-14b / 7b / 3b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/andrewzh/absolute-zero-reasoner-68139b2bca82afb00bc69e5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjd8tg/absolute_zero_reasonercoder14b_7b_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T15:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk66rj</id>
    <title>Jamba mini 1.6 actually outperformed GPT-40 for our RAG support bot</title>
    <updated>2025-05-11T17:21:08+00:00</updated>
    <author>
      <name>/u/NullPointerJack</name>
      <uri>https://old.reddit.com/user/NullPointerJack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These results surprised me. We were testing a few models for a support use case (chat summarization + QA over internal docs) and figured GPT-4o would easily win, but Jamba mini 1.6 (open weights) actually gave us more accurate grounded answers and ran much faster.&lt;/p&gt; &lt;p&gt;Some of the main takeaways -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It beat Jamba 1.5 by a decent margin. About 21% more of our QA outputs were grounded correctly and it was basically tied with GPT-4o in how well it grounded information from our RAG setup&lt;/li&gt; &lt;li&gt;Much faster latency. We're running it quantized with vLLM in our own VPC and it was like 2x faster than GPT-4o for token generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We havent tested math/coding or multilingual yet, just text-heavy internal documents and customer chat logs.&lt;/p&gt; &lt;p&gt;GPT-4o is definitely better for ambiguous questions and slightly more natural in how it phrases answers. But for our exact use case, Jamba Mini handled it better and cheaper.&lt;/p&gt; &lt;p&gt;Is anyone else here running Jamba locally or on-premises?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullPointerJack"&gt; /u/NullPointerJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T17:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjilvd</id>
    <title>AMD's "Strix Halo" APUs Are Being Apparently Sold Separately In China; Starting From $550</title>
    <updated>2025-05-10T19:45:57+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"&gt; &lt;img alt="AMD's &amp;quot;Strix Halo&amp;quot; APUs Are Being Apparently Sold Separately In China; Starting From $550" src="https://external-preview.redd.it/H2PilUsCPrB61jYHu-ehMJ7ez-2xqBlQGK2jAXQtDYs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17801eef3d0bc54803f8d5da3d4b1af4af3e68ce" title="AMD's &amp;quot;Strix Halo&amp;quot; APUs Are Being Apparently Sold Separately In China; Starting From $550" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-apus-are-being-sold-separately-in-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjilvd/amds_strix_halo_apus_are_being_apparently_sold/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T19:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjoc3n</id>
    <title>Is there a specific reason thinking models don't seem to exist in the (or near) 70b parameter range?</title>
    <updated>2025-05-11T00:21:28+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems 30b or less or 200b+. Am I missing something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoc3n/is_there_a_specific_reason_thinking_models_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T00:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjshnd</id>
    <title>Is it possible to generate my own dynamic quant?</title>
    <updated>2025-05-11T04:17:56+00:00</updated>
    <author>
      <name>/u/Lissanro</name>
      <uri>https://old.reddit.com/user/Lissanro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dynamic quants by unsloth are quite good, but they are not available for every model. For example, DeepSeek R1T Chimera has only one Q4_K_M quant (by bullerwins on huggingface) but it fails many tests like solving mazes or have lesser success rate than my own Q6_K quant that I generated locally, which can consistently solve the maze. So I know it is quant issue and not a model issue. Usually failure to solve the maze indicates too much quantization or that it wasn't done perfectly. Unsloth's old R1 quant at Q4_K_M level did not have such issue, and dynamic quants are supposed to be even better. This is why I am interested in learning from their experience creating quants.&lt;/p&gt; &lt;p&gt;I am currently trying to figure out the best way to generate similar high quality Q4 for the Chimera model, so I would like to ask was creation of Dynamic Quants documented somewhere?&lt;/p&gt; &lt;p&gt;I tried searching but I did not find an answer, hence I would like to ask here in the hope someone knows. If it wasn't documented yet, I probably will try experimenting myself with existing Q4 and IQ4 quantization methods and see what gives me the best result.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lissanro"&gt; /u/Lissanro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjshnd/is_it_possible_to_generate_my_own_dynamic_quant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T04:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjvo1t</id>
    <title>Local LLM Build with CPU and DDR5: Thoughts on how to build a Cost Effective Server</title>
    <updated>2025-05-11T07:48:57+00:00</updated>
    <author>
      <name>/u/Xelendor1989</name>
      <uri>https://old.reddit.com/user/Xelendor1989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Local LLM Build with CPU and DDR5: Thoughts on how to build a Cost Effective Server&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The more cost effect fixes/lessons learned I have put below. The build I made here isn't the most &amp;quot;cost effective&amp;quot; build. However it was built as a hybrid serve, in which I was able to think about a better approach to building the CPU/DDR5 based LLM server. I renamed this post so it wouldn't mislead people and think i was proposing my current build as the most &amp;quot;cost effective&amp;quot; approach. It is mostly lessons I learned and thought other people would find useful.&lt;/p&gt; &lt;p&gt;I recently completed what I believe is one of the more efficient local Large Language Model (LLM) builds, particularly if you prioritize these metrics:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low monthly power consumption costs&lt;/li&gt; &lt;li&gt;Scalability for larger, smarter local LLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This setup is also versatile enough to support other use cases on the same server. For instance, I’m using Proxmox to host my gaming desktop, cybersecurity lab, TrueNAS (for storing YouTube content), Plex, and Kubernetes, all running smoothly alongside this build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware Specifications:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DDR5 RAM:&lt;/strong&gt; 576GB (4800MHz, 6 lanes) - Total Cost: $3,500(230.4 gb of bandwidth)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Epyc 8534p (64-core) - Cost: $2,000 USD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; I opted for a high-end motherboard to support this build:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ASUS S14NA-U12&lt;/strong&gt; (imported from Germany) Features include 2x 25GB NICs for future-proof networking.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPU Setup:&lt;/strong&gt;&lt;br /&gt; The GPU is currently passthrough to my gaming PC VM, which houses an RTX 4070 Super. While this configuration doesn’t directly benefit the LLM in this setup, it’s useful for other workloads.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use Cases:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;TrueNAS with OpenWebUI:&lt;/strong&gt; I primarily use this LLM with OpenWebUI to organize my thoughts, brainstorm ideas, and format content into markdown.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Obsidian Copilot Integration:&lt;/strong&gt; The LLM is also utilized to summarize YouTube videos, conduct research, and perform various other tasks through Obsidian Copilot. It’s an incredibly powerful tool for productivity.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This setup balances performance, cost-efficiency, and versatility, making it a solid choice for those looking to run demanding workloads locally.&lt;/p&gt; &lt;h1&gt;Current stats for LLMS:&lt;/h1&gt; &lt;p&gt;prompt:** what is the fastest way to get to china? &lt;strong&gt;system:&lt;/strong&gt; 64core 8534p epyc 6 channel DDR5 4800hz ecc (576gb)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes on LLM performance:&lt;/strong&gt; &lt;strong&gt;qwen3:32b-fp16&lt;/strong&gt; total duration: 20m45.027432852s load duration: 17.510769ms prompt eval count: 17 token(s) prompt eval duration: 636.892108ms prompt eval rate: 26.69 tokens/s eval count: 1424 token(s) eval duration: 20m44.372337587s eval rate: 1.14 tokens/s&lt;/p&gt; &lt;p&gt;Notes: so far fp16 seems to be a very bad performer, speed is super slow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen3:235b-a22b-q8_0&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;total duration: 9m4.279665312s load duration: 18.578117ms prompt eval count: 18 token(s) prompt eval duration: 341.825732ms prompt eval rate: 52.66 tokens/s eval count: 1467 token(s) eval duration: 9m3.918470289s eval rate: 2.70 tokens/s&lt;/p&gt; &lt;p&gt;Note, will compare later, but seemed similar to qwen3:235b in speed&lt;/p&gt; &lt;p&gt;&lt;strong&gt;deepseek-r1:671b&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Note: I ran this with 1.58bit quant version before since I didn't have enough ram, curious to see how it fairs against that version now that I got the faulty ram stick replaced&lt;/p&gt; &lt;p&gt;total duration: 9m0.065311955s load duration: 17.147124ms prompt eval count: 13 token(s) prompt eval duration: 1.664708517s prompt eval rate: 7.81 tokens/s eval count: 1265 token(s) eval duration: 8m58.382699408s eval rate: 2.35 tokens/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SIGJNF/deepseek-r1-671b-1.58bit:latest&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;total duration: 4m15.88028086s load duration: 16.422788ms prompt eval count: 13 token(s) prompt eval duration: 1.190251949s prompt eval rate: 10.92 tokens/s eval count: 829 token(s) eval duration: 4m14.672781876s eval rate: 3.26 tokens/s&lt;/p&gt; &lt;p&gt;Note: 1.58 bit is almost twice as fast for me.&lt;/p&gt; &lt;h1&gt;Lessons Learned for LLM Local CPU and DDR5 Build&lt;/h1&gt; &lt;h1&gt;Key Recommendations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;CPU Selection&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;8xx Gen EPYC CPUs&lt;/strong&gt;: Chosen for low TDP (thermal design power), resulting in minimal monthly electricity costs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;9xx Gen EPYC CPUs (Preferred Option)&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Supports 12 PCIe lanes per CPU and up to 6000 MHz DDR5 memory.&lt;/li&gt; &lt;li&gt;Significantly improves memory bandwidth, critical for LLM performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recommended Model&lt;/strong&gt;: Dual AMD EPYC 9355P 32C (high-performance but ~3x cost of older models).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Budget-Friendly Alternative&lt;/strong&gt;: Dual EPYC 9124 (12 PCIe lanes, ~$1200 total on eBay).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Configuration&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Use &lt;strong&gt;32GB or 64GB DDR5 modules&lt;/strong&gt; (4800 MHz base speed).&lt;/li&gt; &lt;li&gt;Higher DDR5 speeds (up to 6000 MHz) with 9xx series CPUs can alleviate memory bandwidth bottlenecks.&lt;/li&gt; &lt;li&gt;With the higher memory speed(6000MHz) and bandwidth(1000gb/s+), you could achieve the speed of a 3090 with much more loading capacity and less power consumption(if you were to load up 4x 3090's the power draw would be insane).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost vs. Performance Trade-Offs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Older EPYC models (e.g., 9124) offer a balance between PCIe lane support and affordability.&lt;/li&gt; &lt;li&gt;Newer CPUs (e.g., 9355P) prioritize performance but at a steep price premium.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Thermal Management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DDR5 Cooling&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Experimenting with &lt;strong&gt;air cooling&lt;/strong&gt; for DDR5 modules due to high thermal output (&amp;quot;ridiculously hot&amp;quot;).&lt;/li&gt; &lt;li&gt;Plan to install &lt;strong&gt;heat sinks and dedicated fans&lt;/strong&gt; for memory slots adjacent to CPUs.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal Throttling Mitigation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Observed LLM response slowdowns after 5 seconds of sustained workload.&lt;/li&gt; &lt;li&gt;Suspected cause: DDR5/VRAM overheating.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Adding DDR5-specific cooling solutions to maintain sustained performance.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Bandwidth Bottleneck&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Even with newer CPUs, DDR5 bandwidth limitations remain a critical constraint for LLM workloads.&lt;/li&gt; &lt;li&gt;Upgrading to 6000 MHz DDR5 (with compatible 9xx EPYC CPUs) may reduce this bottleneck.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU Generation Impact&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;9xx series CPUs offer marginal performance gains over 8xx series, but benefits depend on DDR5 speed and cooling efficiency.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Prioritize DDR5 speed and cooling for LLM builds.&lt;/li&gt; &lt;li&gt;Balance budget and performance by selecting CPUs with adequate PCIe lanes (12+ per CPU).&lt;/li&gt; &lt;li&gt;Monitor thermal metrics during sustained workloads to prevent throttling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xelendor1989"&gt; /u/Xelendor1989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvo1t/local_llm_build_with_cpu_and_ddr5_thoughts_on_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvo1t/local_llm_build_with_cpu_and_ddr5_thoughts_on_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvo1t/local_llm_build_with_cpu_and_ddr5_thoughts_on_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T07:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjoyrc</id>
    <title>How about this Ollama Chat portal?</title>
    <updated>2025-05-11T00:56:14+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"&gt; &lt;img alt="How about this Ollama Chat portal?" src="https://preview.redd.it/0iyghlhuw10f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=595fe97e58b00087e5706293c48dd73242bd16f9" title="How about this Ollama Chat portal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Greetings everyone, I'm sharing a modern web chat interface for local LLMs, inspired by the visual style and user experience of Claude from Anthropic. It is super easy to use. Supports *.txt file upload, conversation history and Systemas Prompts. &lt;/p&gt; &lt;p&gt;You can play all you want with this 😅&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Oft3r/Ollama-Chat"&gt;https://github.com/Oft3r/Ollama-Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0iyghlhuw10f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjoyrc/how_about_this_ollama_chat_portal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T00:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk4y4c</id>
    <title>Why do runtimes keep the CoT trace in context?</title>
    <updated>2025-05-11T16:26:58+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The CoT traces are the majority of tokens used by any CoT model and all runtimes keep them in context *after* the final answer is produced. Even if the bias to use CoT is not baked deep enough into the model to keep using it after multiple answers without it, you can begin the assistant turn with &amp;lt;think&amp;gt; or whatever CoT special token the model uses. &lt;/p&gt; &lt;p&gt;Is there a specific reason the chain is not dropped after the answer is ready? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4y4c/why_do_runtimes_keep_the_cot_trace_in_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4y4c/why_do_runtimes_keep_the_cot_trace_in_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk4y4c/why_do_runtimes_keep_the_cot_trace_in_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T16:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjigz3</id>
    <title>What happened to Black Forest Labs?</title>
    <updated>2025-05-10T19:39:31+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;theyve been totally silent since november of last year with the release of flux tools and remember when flux 1 first came out they teased that a video generation model was coming soon? what happened with that? Same with stability AI, do they do anything anymore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjigz3/what_happened_to_black_forest_labs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T19:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjlq7g</id>
    <title>I am GPU poor.</title>
    <updated>2025-05-10T22:09:28+00:00</updated>
    <author>
      <name>/u/Khipu28</name>
      <uri>https://old.reddit.com/user/Khipu28</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"&gt; &lt;img alt="I am GPU poor." src="https://preview.redd.it/o61lr9f3310f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71a4af796f1b72d4787c4fcfbebb5815223c9d7a" title="I am GPU poor." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently, I am very GPU poor. How many GPUs of what type can I fit into this available space of the Jonsbo N5 case? All the slots are 5.0x16 the leftmost two slots have re-timers on board. I can provide 1000W for the cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Khipu28"&gt; /u/Khipu28 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o61lr9f3310f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjlq7g/i_am_gpu_poor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T22:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk1dkh</id>
    <title>Time to First Token and Tokens/second</title>
    <updated>2025-05-11T13:45:48+00:00</updated>
    <author>
      <name>/u/TheTideRider</name>
      <uri>https://old.reddit.com/user/TheTideRider</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been seeing lots of benchmarking lately. I just want to make sure that my understandings are correct. TTFT measures the latency of prefilling and t/s measures the average speed of token generation after prefilling. Both of them depend on the context size. Let’s assume there is kv-cache. Prefilling walks through a prompt and its runtime latency is O(n&lt;sup&gt;2)&lt;/sup&gt; where n is the number of input tokens. T/s depends on the context size. It’s O(n) where n is the current context size. As the context gets longer, it gets slower.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTideRider"&gt; /u/TheTideRider &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1dkh/time_to_first_token_and_tokenssecond/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1dkh/time_to_first_token_and_tokenssecond/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1dkh/time_to_first_token_and_tokenssecond/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T13:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjzq9s</id>
    <title>Free Real time AI speech-to-text better than WisperFlow?</title>
    <updated>2025-05-11T12:19:36+00:00</updated>
    <author>
      <name>/u/milkygirl21</name>
      <uri>https://old.reddit.com/user/milkygirl21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Whisper Tiny / V3 Turbo via Buzz and it takes maybe 3-5s to translate my text, and the text gets dropped in Buzz instead of whichever AI app I'm using, say AI Studio. Which other app has a better UI and faster AI transcribing capabilities? Purpose is to have voice chat, but via AI Studio. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/milkygirl21"&gt; /u/milkygirl21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjzq9s/free_real_time_ai_speechtotext_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjzq9s/free_real_time_ai_speechtotext_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjzq9s/free_real_time_ai_speechtotext_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T12:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjts8s</id>
    <title>Why is decoder architecture used for text generation according to a prompt rather than encoder-decoder architecture?</title>
    <updated>2025-05-11T05:40:51+00:00</updated>
    <author>
      <name>/u/darkGrayAdventurer</name>
      <uri>https://old.reddit.com/user/darkGrayAdventurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Learning about LLMs for the first time, and this question is bothering me, I haven't been able to find an answer that intuitively makes sense. &lt;/p&gt; &lt;p&gt;To my understanding, encoder-decoder architectures are good for understanding the text that has been provided in a thorough manner (encoder architecture) as well as for building off of given text (decoder architecture). Using decoder-only will detract from the model's ability to gain a thorough understanding of what is being asked of it -- something that is achieved when using an encoder. &lt;/p&gt; &lt;p&gt;So, why aren't encoder-decoder architectures popular for LLMs when they are used for other common tasks, such as translation and summarization of input texts?&lt;/p&gt; &lt;p&gt;Thank you!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkGrayAdventurer"&gt; /u/darkGrayAdventurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjts8s/why_is_decoder_architecture_used_for_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk1fzx</id>
    <title>Own a RTX3080 10GB, is it good if I sidegrade it to RTX 5060Ti 16GB?</title>
    <updated>2025-05-11T13:49:09+00:00</updated>
    <author>
      <name>/u/akachan1228</name>
      <uri>https://old.reddit.com/user/akachan1228</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Owning an RTX 3080 10GB means sacrificing on VRAM. Very slow output if model exceeded the VRAM limit and start to offset layer to CPU.&lt;/p&gt; &lt;p&gt;Not planning to get the RTX3090 as still very expensive even surveying used market.&lt;/p&gt; &lt;p&gt;Question is, how worthy is the RTX 5060 16gb compared to the RTX 3080 10GB ? I can sale the RTX3080 on the 2nd hand market and get a new RTX 5060 16GB for a slightly similar price.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akachan1228"&gt; /u/akachan1228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1fzx/own_a_rtx3080_10gb_is_it_good_if_i_sidegrade_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1fzx/own_a_rtx3080_10gb_is_it_good_if_i_sidegrade_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk1fzx/own_a_rtx3080_10gb_is_it_good_if_i_sidegrade_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T13:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjn2wv</id>
    <title>Cheap 48GB official Blackwell yay!</title>
    <updated>2025-05-10T23:16:22+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"&gt; &lt;img alt="Cheap 48GB official Blackwell yay!" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="Cheap 48GB official Blackwell yay!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjn2wv/cheap_48gb_official_blackwell_yay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T23:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjy99w</id>
    <title>Tinygrad eGPU for Apple Silicon - Also huge for AMD Ai Max 395?</title>
    <updated>2025-05-11T10:50:03+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a reddit user reported earlier today, George Hotz dropped a very powerful update to the tinygrad master repo, that allows the connection of an AMD eGPU to Apple Silicon Macs.&lt;/p&gt; &lt;p&gt;Since it is using libusb under the hood, this should also work on Windows and Linux. This could be particularly interesting to add GPU capabilities to Ai Mini PCs like the ones from Framework, Asus and other manufacturers, running the AMD Ai Max 395 with up to 128GB of unified Memory.&lt;/p&gt; &lt;p&gt;What's your take? How would you put this to good use?&lt;/p&gt; &lt;p&gt;Reddit Post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/lVfr7TcGph"&gt;https://www.reddit.com/r/LocalLLaMA/s/lVfr7TcGph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/tinygrad/tinygrad"&gt;https://github.com/tinygrad/tinygrad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;X: &lt;a href="https://x.com/tinygrad/status/1920960070055080107"&gt;https://x.com/tinygrad/status/1920960070055080107&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjy99w/tinygrad_egpu_for_apple_silicon_also_huge_for_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjy99w/tinygrad_egpu_for_apple_silicon_also_huge_for_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjy99w/tinygrad_egpu_for_apple_silicon_also_huge_for_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T10:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjwi3w</id>
    <title>How I Run Gemma 3 27B on an RX 7800 XT 16GB Locally!</title>
    <updated>2025-05-11T08:48:00+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I've been successfully running the &lt;strong&gt;Gemma 3 27B&lt;/strong&gt; model locally on my &lt;strong&gt;RX 7800 XT 16GB&lt;/strong&gt; and wanted to share my setup and performance results. It's amazing to be able to run such a powerful model entirely on the GPU!&lt;/p&gt; &lt;p&gt;I opted for the &lt;strong&gt;&lt;code&gt;gemma-3-27B-it-qat-GGUF&lt;/code&gt;&lt;/strong&gt; version provided by the &lt;a href="https://huggingface.co/lmstudio-community/gemma-3-27B-it-qat-GGUF"&gt;lmstudio-community&lt;/a&gt; on HuggingFace. The size of this GGUF model is perfect for my card, allowing it to fit entirely in VRAM.&lt;/p&gt; &lt;h3&gt;My Workflow:&lt;/h3&gt; &lt;p&gt;I mostly use LM Studio for day-to-day interaction (super easy!), but I've been experimenting with running it directly via &lt;code&gt;llama.cpp&lt;/code&gt; server for a bit more control and benchmarking. &lt;/p&gt; &lt;p&gt;Here's a breakdown of my rig:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Case:&lt;/strong&gt; Lian Li A4-H2O&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; MSI H510I&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core i5-11400&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; Netac 32GB DDR4 3200MHz&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Sapphire RX 7800 XT Pulse 16GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cooler:&lt;/strong&gt; ID-Cooling Dashflow 240 Basic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Cooler Master V750 SFX Gold&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Running Gemma with Llama.cpp&lt;/h3&gt; &lt;p&gt;I’m using parameters &lt;a href="https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune"&gt;recommended by the Unsloth team&lt;/a&gt; for inference and aiming for a 16K context size. This is a Windows setup.&lt;/p&gt; &lt;p&gt;Here’s the command I'm using to launch the server:&lt;/p&gt; &lt;p&gt;&lt;code&gt;cmd ~\.llama.cpp\llama-cpp-bin-win-hip-x64\llama-server ^ --host 0.0.0.0 ^ --port 1234 ^ --log-file llama-server.log ^ --alias &amp;quot;gemma-3-27b-it-qat&amp;quot; ^ --model C:\HuggingFace\lmstudio-community\gemma-3-27B-it-qat-GGUF\gemma-3-27B-it-QAT-Q4_0.gguf ^ --threads 5 ^ --ctx-size 16384 ^ --n-gpu-layers 63 ^ --repeat-penalty 1.0 ^ --temp 1.0 ^ --min-p 0.01 ^ --top-k 64 ^ --top-p 0.95 ^ --ubatch-size 512 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important Notes on Parameters:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--host 0.0.0.0&lt;/code&gt;&lt;/strong&gt;: Allows access from other devices on the network.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--port 1234&lt;/code&gt;&lt;/strong&gt;: The port the server will run on.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--log-file llama-server.log&lt;/code&gt;&lt;/strong&gt;: Saves server logs for debugging.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--alias &amp;quot;gemma-3-27b-it-qat&amp;quot;&lt;/code&gt;&lt;/strong&gt;: A friendly name for the model.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt;: Path to the GGUF model file. &lt;em&gt;Make sure to adjust this to your specific directory.&lt;/em&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--threads 5&lt;/code&gt;&lt;/strong&gt;: Number of CPU threads to use, based on your CPU thread count - 1.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--ctx-size 16384&lt;/code&gt;&lt;/strong&gt;: Sets the context length to 16K. Experiment with this based on your RAM! Higher context = more VRAM usage.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--n-gpu-layers 63&lt;/code&gt;&lt;/strong&gt;: This offloads all layers to the GPU. With 16GB of VRAM on the 7800 XT, I'm able to push this to the maximum. Lower this value if you run into OOM errors (Out of Memory).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--repeat-penalty 1.0&lt;/code&gt;&lt;/strong&gt;: Avoids repetitive output.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--temp 1.0&lt;/code&gt;&lt;/strong&gt;: Sampling temperature.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--min-p 0.01&lt;/code&gt;&lt;/strong&gt;: Minimum probability.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--top-k 64&lt;/code&gt;&lt;/strong&gt;: Top-k sampling.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--top-p 0.95&lt;/code&gt;&lt;/strong&gt;: Top-p sampling.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;&lt;code&gt;--ubatch-size 512&lt;/code&gt;&lt;/strong&gt;: Increases batch size for faster inference.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;KV Cache:&lt;/strong&gt; I tested both F16 and Q8_0 KV Cache for performance comparison.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I used these parameters based on the recommendations provided by the Unsloth team for Gemma 3 inference:&lt;/strong&gt; &lt;a href="https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Benchmark Results (Prompt: &amp;quot;What is the reason of life?&amp;quot;)&lt;/h3&gt; &lt;p&gt;I ran a simple benchmark to get a sense of the performance. Here's what I'm seeing:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Runtime&lt;/th&gt; &lt;th&gt;KV Cache&lt;/th&gt; &lt;th&gt;Tokens/Second (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td&gt;F16&lt;/td&gt; &lt;td&gt;17.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ROCm&lt;/td&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;20.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td&gt;F16&lt;/td&gt; &lt;td&gt;14.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;9.9&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;ROCm outperforms Vulkan in my setup.&lt;/strong&gt; I'm not sure why, but it's consistent across multiple runs.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Q8_0 quantization provides a speed boost compared to F16&lt;/strong&gt;, though with a potential (small) tradeoff in quality.&lt;/li&gt; &lt;li&gt; The 7800XT can really push the 27B model, and the results are impressive.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Things to Note:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt; Your mileage may vary depending on your system configuration and specific model quantization.&lt;/li&gt; &lt;li&gt; Ensure you have the latest AMD drivers installed.&lt;/li&gt; &lt;li&gt; Experiment with the parameters to find the optimal balance of speed and quality for your needs.&lt;/li&gt; &lt;li&gt; ROCm support can be tricky to set up on Windows. Make sure you have it configured correctly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm still exploring optimizations and fine-tuning, but I wanted to share these results in case it helps anyone else thinking about running Gemma 3 27B on similar hardware with 16GB GPU. Let me know if you have any questions or suggestions in the comments. Happy inferencing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjwi3w/how_i_run_gemma_3_27b_on_an_rx_7800_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjwi3w/how_i_run_gemma_3_27b_on_an_rx_7800_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjwi3w/how_i_run_gemma_3_27b_on_an_rx_7800_xt_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T08:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk43eo</id>
    <title>Hardware specs comparison to host Mistral small 24B</title>
    <updated>2025-05-11T15:49:07+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt; &lt;img alt="Hardware specs comparison to host Mistral small 24B" src="https://external-preview.redd.it/T4sMn15QFsNG_0KdS2BwGdWLAF8Ie_ulw9XpnsXfqsE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37e0992d289e953d6af5186034ff41b5b4d77814" title="Hardware specs comparison to host Mistral small 24B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am comparing hardware specifications for a customer who wants to host Mistral small 24B locally for inference. He would like to know if it's worth buying a GPU server instead of consuming the MistralAI API, and if so, when the breakeven point occurs. Here are my assumptions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Model weights are FP16 and the 128k context window is fully utilized.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The formula to compute the required VRAM is the product of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context length&lt;/li&gt; &lt;li&gt;Number of layers&lt;/li&gt; &lt;li&gt;Number of key-value heads&lt;/li&gt; &lt;li&gt;Head dimension - 2 (2-bytes per float16) - 2 (one for keys, one for values)&lt;/li&gt; &lt;li&gt;Number of users&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;To calculate the upper bound, the number of users is the maximum number of concurrent users the hardware can handle with the full 128k token context window.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The use of an AI agent consumes approximately 25 times the number of tokens compared to a normal chat (Source: &lt;a href="https://www.businessinsider.com/ai-super-agents-enough-computing-power-openai-deepseek-2025-3"&gt;https://www.businessinsider.com/ai-super-agents-enough-computing-power-openai-deepseek-2025-3&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My comparison resulted in this table. The price of electricity for professionals here is about 0.20€/kWh all taxes included. Because of this, the breakeven point is at least 8.3 years for the Nvidia DGX A100. The Apple Mac Studio M3 Ultra reaches breakeven after 6 months, but it is significantly slower than the Nvidia and AMD products.&lt;/p&gt; &lt;p&gt;Given these data I think this is not worth investing in a GPU server, unless the customer absolutely requires privacy.&lt;/p&gt; &lt;p&gt;Do you think the numbers I found are reasonable? Were my assumptions too far off? I hope this helps the community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0140tgw960f1.png?width=2427&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fdf2d5f2b15d88ef4621a830436459baebbaf3e"&gt;https://preview.redd.it/c0140tgw960f1.png?width=2427&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fdf2d5f2b15d88ef4621a830436459baebbaf3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Below some graphs :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ghlcd725b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=804fe43c28dab4a4cde53a1df5d1ca6b67df3a67"&gt;https://preview.redd.it/ghlcd725b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=804fe43c28dab4a4cde53a1df5d1ca6b67df3a67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3f5x0dk5b60f1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c799d2e711a84b1355cd3b4515560a4450a3e0e"&gt;https://preview.redd.it/3f5x0dk5b60f1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c799d2e711a84b1355cd3b4515560a4450a3e0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7emca9v5b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7295ff311460e0d45dfa3ddd671e188840394c6"&gt;https://preview.redd.it/7emca9v5b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7295ff311460e0d45dfa3ddd671e188840394c6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8bl4pcb6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ed692b1afc9caa440470f8779b44d46130de02f"&gt;https://preview.redd.it/8bl4pcb6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ed692b1afc9caa440470f8779b44d46130de02f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/94h5rso6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc7f3f07abc2f5c9f236e30ff20f300446f3f0c"&gt;https://preview.redd.it/94h5rso6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc7f3f07abc2f5c9f236e30ff20f300446f3f0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wm0y3j37b60f1.png?width=1185&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7af8a86a7fbee60b5028349525fe2430ce2313d4"&gt;https://preview.redd.it/wm0y3j37b60f1.png?width=1185&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7af8a86a7fbee60b5028349525fe2430ce2313d4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T15:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk0ghi</id>
    <title>Speed Comparison with Qwen3-32B-q8_0, Ollama, Llama.cpp, 2x3090, M3Max</title>
    <updated>2025-05-11T12:59:11+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Requested by &lt;a href="/u/MLDataScientist"&gt;/u/MLDataScientist&lt;/a&gt;, here is a comparison test between Ollama and Llama.cpp on 2 x RTX-3090 and M3-Max with 64GB using Qwen3-32B-q8_0.&lt;/p&gt; &lt;p&gt;Just note, if you are interested in a comparison with most optimized setup, it would be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ke26sl/another_attempt_to_measure_speed_for_qwen3_moe_on/"&gt;SGLang/VLLM for 4090 and MLX for M3Max.&lt;/a&gt; This was primarily to compare Ollama and Llama.cpp under the same condition with Qwen3-32b model based on dense architecture. If interested, I also ran another &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;benchmark using Qwen MoE architecture.&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;To ensure consistency, I used a custom Python script that sends requests to the server via the OpenAI-compatible API. Metrics were calculated as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time to First Token (TTFT): Measured from the start of the streaming request to the first streaming event received.&lt;/li&gt; &lt;li&gt;Prompt Processing Speed (PP): Number of prompt tokens divided by TTFT.&lt;/li&gt; &lt;li&gt;Token Generation Speed (TG): Number of generated tokens divided by (total duration - TTFT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The displayed results were truncated to two decimal places, but the calculations used full precision. I made the script to prepend new material in the beginning of next longer prompt to avoid caching effect.&lt;/p&gt; &lt;p&gt;Here's my script for anyone interest. &lt;a href="https://github.com/chigkim/prompt-test"&gt;https://github.com/chigkim/prompt-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses OpenAI API, so it should work in variety setup. Also, this tests one request at a time, so multiple parallel requests could result in higher throughput in different tests.&lt;/p&gt; &lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;Both use the same q8_0 model from Ollama library with flash attention. I'm sure you can further optimize Llama.cpp, but I copied the flags from Ollama log in order to keep it consistent, so both use the exactly same flags when loading the model.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ~/.ollama/models/blobs/sha256... --ctx-size 22000 --batch-size 512 --n-gpu-layers 65 --threads 32 --flash-attn --parallel 1 --tensor-split 33,32 --port 11434&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama.cpp: 5339 (3b24d26c)&lt;/li&gt; &lt;li&gt;Ollama: 0.6.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each row in the results represents a test (a specific combination of machine, engine, and prompt length). There are 4 tests per prompt length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup 1: 2xRTX3090, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 2: 2xRTX3090, Ollama&lt;/li&gt; &lt;li&gt;Setup 3: M3Max, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 4: M3Max, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Result&lt;/h3&gt; &lt;p&gt;Please zoom in to see the graph better.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img 26e05b1zd50f1...&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;1033.18&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;968&lt;/td&gt; &lt;td&gt;21.71&lt;/td&gt; &lt;td&gt;44.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;853.87&lt;/td&gt; &lt;td&gt;0.31&lt;/td&gt; &lt;td&gt;1041&lt;/td&gt; &lt;td&gt;21.44&lt;/td&gt; &lt;td&gt;48.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;153.63&lt;/td&gt; &lt;td&gt;1.72&lt;/td&gt; &lt;td&gt;739&lt;/td&gt; &lt;td&gt;10.41&lt;/td&gt; &lt;td&gt;72.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;152.12&lt;/td&gt; &lt;td&gt;1.74&lt;/td&gt; &lt;td&gt;885&lt;/td&gt; &lt;td&gt;10.35&lt;/td&gt; &lt;td&gt;87.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;1184.75&lt;/td&gt; &lt;td&gt;0.38&lt;/td&gt; &lt;td&gt;1154&lt;/td&gt; &lt;td&gt;21.66&lt;/td&gt; &lt;td&gt;53.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;1013.60&lt;/td&gt; &lt;td&gt;0.44&lt;/td&gt; &lt;td&gt;1177&lt;/td&gt; &lt;td&gt;21.38&lt;/td&gt; &lt;td&gt;55.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;171.37&lt;/td&gt; &lt;td&gt;2.63&lt;/td&gt; &lt;td&gt;1273&lt;/td&gt; &lt;td&gt;10.28&lt;/td&gt; &lt;td&gt;126.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;169.53&lt;/td&gt; &lt;td&gt;2.65&lt;/td&gt; &lt;td&gt;1275&lt;/td&gt; &lt;td&gt;10.33&lt;/td&gt; &lt;td&gt;126.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;1405.67&lt;/td&gt; &lt;td&gt;0.51&lt;/td&gt; &lt;td&gt;1288&lt;/td&gt; &lt;td&gt;21.63&lt;/td&gt; &lt;td&gt;60.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;1292.38&lt;/td&gt; &lt;td&gt;0.56&lt;/td&gt; &lt;td&gt;1343&lt;/td&gt; &lt;td&gt;21.31&lt;/td&gt; &lt;td&gt;63.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;164.83&lt;/td&gt; &lt;td&gt;4.39&lt;/td&gt; &lt;td&gt;1274&lt;/td&gt; &lt;td&gt;10.29&lt;/td&gt; &lt;td&gt;128.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;163.79&lt;/td&gt; &lt;td&gt;4.41&lt;/td&gt; &lt;td&gt;1204&lt;/td&gt; &lt;td&gt;10.27&lt;/td&gt; &lt;td&gt;121.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;1602.61&lt;/td&gt; &lt;td&gt;0.76&lt;/td&gt; &lt;td&gt;1815&lt;/td&gt; &lt;td&gt;21.44&lt;/td&gt; &lt;td&gt;85.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;1498.43&lt;/td&gt; &lt;td&gt;0.81&lt;/td&gt; &lt;td&gt;1445&lt;/td&gt; &lt;td&gt;21.35&lt;/td&gt; &lt;td&gt;68.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;169.15&lt;/td&gt; &lt;td&gt;7.21&lt;/td&gt; &lt;td&gt;1302&lt;/td&gt; &lt;td&gt;10.19&lt;/td&gt; &lt;td&gt;134.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;168.32&lt;/td&gt; &lt;td&gt;7.24&lt;/td&gt; &lt;td&gt;1686&lt;/td&gt; &lt;td&gt;10.11&lt;/td&gt; &lt;td&gt;173.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;1734.46&lt;/td&gt; &lt;td&gt;1.07&lt;/td&gt; &lt;td&gt;1375&lt;/td&gt; &lt;td&gt;21.37&lt;/td&gt; &lt;td&gt;65.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;1635.95&lt;/td&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;1293&lt;/td&gt; &lt;td&gt;21.13&lt;/td&gt; &lt;td&gt;62.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;166.81&lt;/td&gt; &lt;td&gt;11.14&lt;/td&gt; &lt;td&gt;1411&lt;/td&gt; &lt;td&gt;10.09&lt;/td&gt; &lt;td&gt;151.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;166.96&lt;/td&gt; &lt;td&gt;11.13&lt;/td&gt; &lt;td&gt;1450&lt;/td&gt; &lt;td&gt;10.10&lt;/td&gt; &lt;td&gt;154.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;1789.89&lt;/td&gt; &lt;td&gt;1.66&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;21.09&lt;/td&gt; &lt;td&gt;96.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;1735.97&lt;/td&gt; &lt;td&gt;1.72&lt;/td&gt; &lt;td&gt;1628&lt;/td&gt; &lt;td&gt;20.83&lt;/td&gt; &lt;td&gt;79.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;162.22&lt;/td&gt; &lt;td&gt;18.36&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;9.89&lt;/td&gt; &lt;td&gt;220.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;161.46&lt;/td&gt; &lt;td&gt;18.45&lt;/td&gt; &lt;td&gt;1643&lt;/td&gt; &lt;td&gt;9.88&lt;/td&gt; &lt;td&gt;184.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;1791.05&lt;/td&gt; &lt;td&gt;2.61&lt;/td&gt; &lt;td&gt;1326&lt;/td&gt; &lt;td&gt;20.77&lt;/td&gt; &lt;td&gt;66.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;1746.71&lt;/td&gt; &lt;td&gt;2.67&lt;/td&gt; &lt;td&gt;1592&lt;/td&gt; &lt;td&gt;20.47&lt;/td&gt; &lt;td&gt;80.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;154.16&lt;/td&gt; &lt;td&gt;30.29&lt;/td&gt; &lt;td&gt;1593&lt;/td&gt; &lt;td&gt;9.67&lt;/td&gt; &lt;td&gt;194.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;153.03&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;1450&lt;/td&gt; &lt;td&gt;9.66&lt;/td&gt; &lt;td&gt;180.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;1756.76&lt;/td&gt; &lt;td&gt;4.52&lt;/td&gt; &lt;td&gt;1255&lt;/td&gt; &lt;td&gt;20.29&lt;/td&gt; &lt;td&gt;66.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;1706.41&lt;/td&gt; &lt;td&gt;4.66&lt;/td&gt; &lt;td&gt;1404&lt;/td&gt; &lt;td&gt;20.10&lt;/td&gt; &lt;td&gt;74.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;140.11&lt;/td&gt; &lt;td&gt;56.73&lt;/td&gt; &lt;td&gt;1748&lt;/td&gt; &lt;td&gt;9.20&lt;/td&gt; &lt;td&gt;246.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;138.99&lt;/td&gt; &lt;td&gt;57.18&lt;/td&gt; &lt;td&gt;1650&lt;/td&gt; &lt;td&gt;9.18&lt;/td&gt; &lt;td&gt;236.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;1648.97&lt;/td&gt; &lt;td&gt;7.53&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;19.59&lt;/td&gt; &lt;td&gt;109.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;1616.69&lt;/td&gt; &lt;td&gt;7.68&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;19.30&lt;/td&gt; &lt;td&gt;111.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;127.96&lt;/td&gt; &lt;td&gt;97.03&lt;/td&gt; &lt;td&gt;1395&lt;/td&gt; &lt;td&gt;8.60&lt;/td&gt; &lt;td&gt;259.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;127.08&lt;/td&gt; &lt;td&gt;97.70&lt;/td&gt; &lt;td&gt;1778&lt;/td&gt; &lt;td&gt;8.57&lt;/td&gt; &lt;td&gt;305.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;1481.92&lt;/td&gt; &lt;td&gt;13.61&lt;/td&gt; &lt;td&gt;598&lt;/td&gt; &lt;td&gt;18.72&lt;/td&gt; &lt;td&gt;45.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;1458.86&lt;/td&gt; &lt;td&gt;13.83&lt;/td&gt; &lt;td&gt;1627&lt;/td&gt; &lt;td&gt;18.30&lt;/td&gt; &lt;td&gt;102.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;111.18&lt;/td&gt; &lt;td&gt;181.44&lt;/td&gt; &lt;td&gt;1771&lt;/td&gt; &lt;td&gt;7.58&lt;/td&gt; &lt;td&gt;415.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;111.80&lt;/td&gt; &lt;td&gt;180.43&lt;/td&gt; &lt;td&gt;1372&lt;/td&gt; &lt;td&gt;7.53&lt;/td&gt; &lt;td&gt;362.54&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T12:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju0ty</id>
    <title>Why new models feel dumber?</title>
    <updated>2025-05-11T05:57:09+00:00</updated>
    <author>
      <name>/u/SrData</name>
      <uri>https://old.reddit.com/user/SrData</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just me, or do the new models feel… dumber?&lt;/p&gt; &lt;p&gt;I’ve been testing Qwen 3 across different sizes, expecting a leap forward. Instead, I keep circling back to Qwen 2.5. It just feels sharper, more coherent, less… bloated. Same story with Llama. I’ve had long, surprisingly good conversations with 3.1. But 3.3? Or Llama 4? It’s like the lights are on but no one’s home.&lt;/p&gt; &lt;p&gt;Some flaws I have found: They lose thread persistence. They forget earlier parts of the convo. They repeat themselves more. Worse, they feel like they’re trying to sound smarter instead of being coherent.&lt;/p&gt; &lt;p&gt;So I’m curious: Are you seeing this too? Which models are you sticking with, despite the version bump? Any new ones that have genuinely impressed you, especially in longer sessions?&lt;/p&gt; &lt;p&gt;Because right now, it feels like we’re in this strange loop of releasing “smarter” models that somehow forget how to talk. And I’d love to know I’m not the only one noticing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrData"&gt; /u/SrData &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju1y1</id>
    <title>Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset</title>
    <updated>2025-05-11T05:59:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt; &lt;img alt="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" src="https://external-preview.redd.it/8ePyWxYJavtNkgThp-DI68bW9d5fj-oFIybzu4pnoUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b2715032a28656454c9bee39e79aafee721d37" title="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We've uploaded them all now&lt;/p&gt; &lt;p&gt;Also with a new improved calibration dataset :)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0"&gt;https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They updated All Qwen3 ggufs&lt;/p&gt; &lt;p&gt;Plus more gguf variants for Qwen3-30B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476"&gt;https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjvb8i</id>
    <title>I Built a Tool That Tells Me If a Side Project Will Ruin My Weekend</title>
    <updated>2025-05-11T07:24:15+00:00</updated>
    <author>
      <name>/u/IntelligentHope9866</name>
      <uri>https://old.reddit.com/user/IntelligentHope9866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to lie to myself every weekend:&lt;br /&gt; “I’ll build this in an hour.”&lt;/p&gt; &lt;p&gt;Spoiler: I never did.&lt;/p&gt; &lt;p&gt;So I built a tool that tracks how long my features actually take — and uses a local LLM to estimate future ones.&lt;/p&gt; &lt;p&gt;It logs my coding sessions, summarizes them, and tells me:&lt;br /&gt; &amp;quot;Yeah, this’ll eat your whole weekend. Don’t even start.&amp;quot;&lt;/p&gt; &lt;p&gt;It lives in my terminal and keeps me honest.&lt;/p&gt; &lt;p&gt;Full writeup + code: &lt;a href="https://www.rafaelviana.io/posts/code-chrono"&gt;https://www.rafaelviana.io/posts/code-chrono&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentHope9866"&gt; /u/IntelligentHope9866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T07:24:15+00:00</published>
  </entry>
</feed>
