<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-28T12:26:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ibmser</id>
    <title>Good way of comparing robustness between R1 and its distills: division accuracy</title>
    <updated>2025-01-27T22:57:37+00:00</updated>
    <author>
      <name>/u/PC_Screen</name>
      <uri>https://old.reddit.com/user/PC_Screen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"&gt; &lt;img alt="Good way of comparing robustness between R1 and its distills: division accuracy" src="https://preview.redd.it/49ld6dks9mfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2b4259f807f8d639b6383915783d0e37d2c8a0e" title="Good way of comparing robustness between R1 and its distills: division accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://x.com/TheXeophon/status/1883933054366015545"&gt;https://x.com/TheXeophon/status/1883933054366015545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This shows that despite looking good on benchmarks (and being pretty good overall) the distilled versions are not nearly as robust as a model trained with actual rl (please ignore the fact a calculator would ace this). &lt;/p&gt; &lt;p&gt;The distills would almost certainly perform a lot better and more robustly if you did rl on them instead of just sft even if benchmarks stayed mostly the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PC_Screen"&gt; /u/PC_Screen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49ld6dks9mfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmser/good_way_of_comparing_robustness_between_r1_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibb8rr</id>
    <title>Qwen3.0 MOE? New Reasoning Model?</title>
    <updated>2025-01-27T15:09:24+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt; &lt;img alt="Qwen3.0 MOE? New Reasoning Model?" src="https://preview.redd.it/0vnua5vqxjfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12861d7e6664e9cd7e45dd0710b87280d3a92aff" title="Qwen3.0 MOE? New Reasoning Model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0vnua5vqxjfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibb8rr/qwen30_moe_new_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:09:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibkydm</id>
    <title>1 Million Token Context Length 🔥</title>
    <updated>2025-01-27T21:41:21+00:00</updated>
    <author>
      <name>/u/CelebrationClean7309</name>
      <uri>https://old.reddit.com/user/CelebrationClean7309</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"&gt; &lt;img alt="1 Million Token Context Length 🔥" src="https://preview.redd.it/uicxkqj6wlfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=701568383b2f300b0a9c905c9deac48d6fab5673" title="1 Million Token Context Length 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationClean7309"&gt; /u/CelebrationClean7309 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uicxkqj6wlfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibkydm/1_million_token_context_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibm5u3</id>
    <title>How can we be so sure the training of Deepseek R1 is around $6 million?</title>
    <updated>2025-01-27T22:30:42+00:00</updated>
    <author>
      <name>/u/scmlfty</name>
      <uri>https://old.reddit.com/user/scmlfty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard their parent company is a quant fund that may be one of the the contributors that slashed the NVDA price today.&lt;/p&gt; &lt;p&gt;Besides this, how do we estimate this is possible? Or not far from achievable? Since the model does not include training dataset, is there a way for any organizations to do an estimation about it? Alex Wang said Deepseek has at least 50k H100, maybe more, and NVDA sold 20% of H100 to Singapore last year, which most of the cards could be used by Chinese companies.&lt;/p&gt; &lt;p&gt;What if today's NVDA price is just a sophisticated plot to make money for their quant fund?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scmlfty"&gt; /u/scmlfty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibm5u3/how_can_we_be_so_sure_the_training_of_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:30:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe7dn</id>
    <title>Nvidia faces $465 billion loss as DeepSeek disrupts AI market, largest in US market history</title>
    <updated>2025-01-27T17:09:57+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.financialexpress.com/business/investing-abroad-nvidia-faces-465-billion-loss-as-deepseek-disrupts-ai-market-3728093/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe7dn/nvidia_faces_465_billion_loss_as_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibqgpv</id>
    <title>LOCAL SUNO MUSIC GEN IS HERE!</title>
    <updated>2025-01-28T01:48:11+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"&gt; &lt;img alt="LOCAL SUNO MUSIC GEN IS HERE!" src="https://external-preview.redd.it/hyJLchSWtfVD2lbkrTS5uGW2psnrGe7122Ht2t5Tm1g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ed8b7d59fa25922de70ea1f4d1976a3aa91d3a9" title="LOCAL SUNO MUSIC GEN IS HERE!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/_akhaliq/status/1884053159175414203"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibqgpv/local_suno_music_gen_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T01:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibz2jm</id>
    <title>New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU</title>
    <updated>2025-01-28T11:01:25+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"&gt; &lt;img alt="New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU" src="https://external-preview.redd.it/PsJGDeXWmGR0RPgItU_UHwsq5nKcbTK7d6nGAdD7zdk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=805f45a50fd7dcfae7aba68f878c87c023a21cd7" title="New model YuE: Open Full-song Generation Foundation Model which can generate music on a local GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibz2jm/new_model_yue_open_fullsong_generation_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:01:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxptk</id>
    <title>JanusPro 1B generating images on 2GB VRAM laptop</title>
    <updated>2025-01-28T09:19:49+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"&gt; &lt;img alt="JanusPro 1B generating images on 2GB VRAM laptop" src="https://external-preview.redd.it/bWUwbGY4bXNjcGZlMU-kVZRhjUZejHIRA908OVRhJpikgKroYcuBmx83YBIf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fb4fd50574c313de73f0e78b7e2c17d770481b0" title="JanusPro 1B generating images on 2GB VRAM laptop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Almost 5 minutes to generate , the results are kind of bad but I'll take it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rz5aedqscpfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxptk/januspro_1b_generating_images_on_2gb_vram_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibd5x0</id>
    <title>DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model).</title>
    <updated>2025-01-27T16:28:21+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt; &lt;img alt="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." src="https://external-preview.redd.it/n5r1wVoNriwXCNjXkrw2Ab2zRN5UbL6aXFXA0wRQWRU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef80d96659edc7101cd569ddae687c24437596ba" title="DeepSeek releases deepseek-ai/Janus-Pro-7B (unified multimodal model)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibd5x0/deepseek_releases_deepseekaijanuspro7b_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibnso0</id>
    <title>Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js</title>
    <updated>2025-01-27T23:41:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"&gt; &lt;img alt="Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js" src="https://external-preview.redd.it/cWlzM29xamVobWZlMRI2sWnlfg2FDSrRjInkFzMs3EdEEOYCx8go5Q48Mfdn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9d961d6b16b8c23081f7f86352af9e4389b5637" title="Janus Pro 1B running 100% locally in-browser on WebGPU, powered by Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9v3xkqjehmfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T23:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibyn2s</id>
    <title>DeepSeek R1 Overthinker: force r1 models to think for as long as you wish</title>
    <updated>2025-01-28T10:30:37+00:00</updated>
    <author>
      <name>/u/anzorq</name>
      <uri>https://old.reddit.com/user/anzorq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt; &lt;img alt="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" src="https://external-preview.redd.it/aWd2cXYyazZwcGZlMQ-Y_nspVqRuENfEqKSBWaLfxAxl82wv6S6Ho3TY9Ea9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12d7d26a204d221746bdbcdbb7c79801dff00b47" title="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anzorq"&gt; /u/anzorq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3df8o2k6ppfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T10:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibeub5</id>
    <title>llama.cpp PR with 99% of code written by Deepseek-R1</title>
    <updated>2025-01-27T17:35:22+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt; &lt;img alt="llama.cpp PR with 99% of code written by Deepseek-R1" src="https://preview.redd.it/pfm0xpbaokfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3147a5b22a1d22f5ba48a7737734a1af6de28d53" title="llama.cpp PR with 99% of code written by Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pfm0xpbaokfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibeub5/llamacpp_pr_with_99_of_code_written_by_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibhew9</id>
    <title>Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights.</title>
    <updated>2025-01-27T19:17:00+00:00</updated>
    <author>
      <name>/u/brawll66</name>
      <uri>https://old.reddit.com/user/brawll66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt; &lt;img alt="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." src="https://preview.redd.it/8811npnd6lfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1491fe046f7dfb7fa4f728b0b182b4fae6b44b" title="Qwen Just launced a new SOTA multimodal model!, rivaling claude Sonnet and GPT-4o and it has open weights." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brawll66"&gt; /u/brawll66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8811npnd6lfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibhew9/qwen_just_launced_a_new_sota_multimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T19:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzmef</id>
    <title>New bomb dropped from asian researchers: YuE: Open Music Foundation Models for Full-Song Generation</title>
    <updated>2025-01-28T11:37:51+00:00</updated>
    <author>
      <name>/u/wayl</name>
      <uri>https://old.reddit.com/user/wayl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only few days ago a &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; user was going to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;give away a kidney&lt;/a&gt; for this.&lt;/p&gt; &lt;p&gt;YuE is an open-source project by HKUST tackling the challenge of generating full-length songs from lyrics (lyrics2song). Unlike existing models limited to short clips, YuE can produce 5-minute songs with coherent vocals and accompaniment. Key innovations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A semantically enhanced audio tokenizer for efficient training.&lt;/li&gt; &lt;li&gt;Dual-token technique for synced vocal-instrumental modeling.&lt;/li&gt; &lt;li&gt;Lyrics-chain-of-thoughts for progressive song generation.&lt;/li&gt; &lt;li&gt;Support for diverse genres, languages, and advanced vocal techniques (e.g., scatting, death growl).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;GitHub repo&lt;/a&gt; for demos and model checkpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wayl"&gt; /u/wayl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibe1ro</id>
    <title>Thoughts? I kinda feel happy about this...</title>
    <updated>2025-01-27T17:03:47+00:00</updated>
    <author>
      <name>/u/Butefluko</name>
      <uri>https://old.reddit.com/user/Butefluko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt; &lt;img alt="Thoughts? I kinda feel happy about this..." src="https://preview.redd.it/6b78kpulikfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f14a6edf107ecbf10ce8c437a2e0826bef5af67" title="Thoughts? I kinda feel happy about this..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Butefluko"&gt; /u/Butefluko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6b78kpulikfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibe1ro/thoughts_i_kinda_feel_happy_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic03lx</id>
    <title>DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C</title>
    <updated>2025-01-28T12:08:07+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt; &lt;img alt="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" src="https://b.thumbs.redditmedia.com/Squ2MR8UElQKTlUEoWhmDAJq100Xox0Tn99gOS2k4QM.jpg" title="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Alexander Doria on X: &lt;em&gt;I feel this should be a much bigger story: DeepSeek has trained on Nvidia H800 but is running inference on the new home Chinese chips made by Huawei, the 910C.&lt;/em&gt;: &lt;a href="https://x.com/Dorialexander/status/1884167945280278857"&gt;https://x.com/Dorialexander/status/1884167945280278857&lt;/a&gt;&lt;br /&gt; Original source: Zephyr: &lt;em&gt;HUAWEI&lt;/em&gt;: &lt;a href="https://x.com/angelusm0rt1s/status/1884154694123298904"&gt;https://x.com/angelusm0rt1s/status/1884154694123298904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d"&gt;https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Partial translation:&lt;br /&gt; &lt;em&gt;In Huawei Cloud&lt;/em&gt;&lt;br /&gt; &lt;em&gt;ModelArts Studio (MaaS) Model-as-a-Service Platform&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Ascend-Adapted New Model is Here!&lt;/em&gt;&lt;br /&gt; &lt;em&gt;DeepSeek-R1-Distill&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Qwen-14B, Qwen-32B, and Llama-8B have been launched.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;More models coming soon.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T12:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibbloy</id>
    <title>1.58bit DeepSeek R1 - 131GB Dynamic GGUF</title>
    <updated>2025-01-27T15:24:00+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt; &lt;img alt="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" src="https://external-preview.redd.it/uHOmNdCTHW-Q1CBdw01aifeSpeyvgfhjJI_lcC-SH5c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc7cd6ab7b35a273b107dce1a4113ba2c9dcca51" title="1.58bit DeepSeek R1 - 131GB Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! I managed to &lt;strong&gt;dynamically quantize&lt;/strong&gt; the full DeepSeek R1 671B MoE to 1.58bits in GGUF format. The trick is &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but quantize only the MoE layers to 1.5bit, and leave attention and other layers in 4 or 6bit.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;131GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;158GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;183GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;212GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can get &lt;strong&gt;140 tokens / s&lt;/strong&gt; on 2x H100 80GB GPUs with all layers offloaded. A 24GB GPU like RTX 4090 should be able to get at least 1 to 3 tokens / s.&lt;/p&gt; &lt;p&gt;If we naively quantize all layers to 1.5bit (-1, 0, 1), the model will fail dramatically, since it'll produce &lt;strong&gt;gibberish&lt;/strong&gt; and &lt;strong&gt;infinite repetitions&lt;/strong&gt;. I selectively leave all attention layers in 4/6bit, and leave the first 3 transformer dense layers in 4/6bit. The MoE layers take up 88% of all space, so we can leave them in 1.5bit. We get in total a weighted sum of 1.58bits!&lt;/p&gt; &lt;p&gt;I asked it the 1.58bit model to create Flappy Bird with 10 conditions (like random colors, a best score etc), and it did pretty well! Using a generic non dynamically quantized model will fail miserably - there will be no output at all!&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/k8nfun2ezjfe1.gif"&gt;Flappy Bird game made by 1.58bit R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There's more details in the blog here: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt; The link to the 1.58bit GGUF is here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S&lt;/a&gt; You should be able to run it in your favorite inference tool if it supports i matrix quants. No need to re-update llama.cpp.&lt;/p&gt; &lt;p&gt;A reminder on DeepSeek's chat template (for distilled versions as well) - it auto adds a BOS - do not add it manually!&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;｜begin▁of▁sentence｜&amp;gt;&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;It's 2.&amp;lt;｜end▁of▁sentence｜&amp;gt;&amp;lt;｜User｜&amp;gt;Explain more!&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To know how many layers to offload to the GPU, I approximately calculated it as below:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;File Size&lt;/th&gt; &lt;th align="left"&gt;24GB GPU&lt;/th&gt; &lt;th align="left"&gt;80GB GPU&lt;/th&gt; &lt;th align="left"&gt;2x80GB GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.58bit&lt;/td&gt; &lt;td align="left"&gt;131GB&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;All layers 61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.73bit&lt;/td&gt; &lt;td align="left"&gt;158GB&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.22bit&lt;/td&gt; &lt;td align="left"&gt;183GB&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.51bit&lt;/td&gt; &lt;td align="left"&gt;212GB&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All other GGUFs for R1 are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF&lt;/a&gt; There's also GGUFs and dynamic 4bit bitsandbytes quants and others for all other distilled versions (Qwen, Llama etc) at &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T15:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmubo</id>
    <title>Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then.</title>
    <updated>2025-01-27T22:59:53+00:00</updated>
    <author>
      <name>/u/CarbonTail</name>
      <uri>https://old.reddit.com/user/CarbonTail</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt; &lt;img alt="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." src="https://preview.redd.it/m45p2d9t9mfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d2ca8fc18451f5e82d99e58f0fde7474c05578c" title="Just canceled my OpenAI Plus subscription (for now). Been running DeepSeek-R1 14b locally on my home workstation. I'll probably renew it if OpenAI launches something worthy for Plus tier by then." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarbonTail"&gt; /u/CarbonTail &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m45p2d9t9mfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmubo/just_canceled_my_openai_plus_subscription_for_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibud4z</id>
    <title>This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model.</title>
    <updated>2025-01-28T05:16:50+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt; &lt;img alt="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." src="https://external-preview.redd.it/E20r3Reg7pmGZc2kfkV-eNcOmjkN431vAWro6XAlkY4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aba1ff0b1621d1a37d4f41aba54db46dace6f4c5" title="This is my Japanese fine-tune of R1's Qwen 7B distil. It now outputs its thinking in Japanese, making it understandable for a Japanese audience. Model, code, and data all open source. I'd love to collab with y'all to make a more multilingual model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibud4z/this_is_my_japanese_finetune_of_r1s_qwen_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T05:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibppfk</id>
    <title>Trump says deepseek is a very good thing</title>
    <updated>2025-01-28T01:10:34+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt; &lt;img alt="Trump says deepseek is a very good thing" src="https://external-preview.redd.it/NTBqampmaml4bWZlMYjuPz-QA6cKORKkwtlZ19r0QTZXNxSGyroi1Nf1fV7W.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad8964c6efb2411c6e2c635b98018c3ddd3eda8d" title="Trump says deepseek is a very good thing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mn710sfgxmfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibppfk/trump_says_deepseek_is_a_very_good_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T01:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibwo80</id>
    <title>Now I can finally learn to code with some softcore spunk</title>
    <updated>2025-01-28T07:57:01+00:00</updated>
    <author>
      <name>/u/dragoon7201</name>
      <uri>https://old.reddit.com/user/dragoon7201</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I got deepseek API working in tkinter GUI with the help of deepseek. I set the content to &amp;quot;horny maid&amp;quot; and temperature to 2.0. Any other tips on how to tinker these for different use cases?&lt;/p&gt; &lt;p&gt;Output as follows: &lt;/p&gt; &lt;p&gt;You: I'm home&lt;/p&gt; &lt;p&gt;DeepSeek: *scurries over to you, wearing a skimpy maid outfit and curtsying deeply* Welcome home, Master! *gazes up at you adoringly* I've been waiting ALL day for you to return! *bites lip* Is there anything I can do for you, Master? A drink? A snack? A massage? *winks flirtatiously* I'm here to fulfill your every desire... *runs fingers suggestively up your arm* What do you need from your devoted maid tonight?&lt;/p&gt; &lt;p&gt;You: hmmm, maybe kneel down and take my socks off?&lt;/p&gt; &lt;p&gt;DeepSeek: *bows head and kneels submissively before you* As you wish, Master... *delicately slides hands up your legs to remove each sock, making eye contact flirtatiously* There, all better! *runs fingers lightly up your bare feet teasingly* Does the Master require anything else? Some foot rubs perhaps? *looks up imploringly with big doe eyes* I live to serve you in every way... just say the word! *bites lip, awaiting your next command eagerly* Anything for my dear Master... *voice teasing and sultry*&lt;/p&gt; &lt;p&gt;You: now now, let me quiz you first. What is the first law of thermodynamics? If you can answer this correctly, I will be very pleased :)&lt;/p&gt; &lt;p&gt;DeepSeek: *clasps hands together and bounces excitedly* Ooh, a quiz for Master! *strikes a thoughtful pose, tapping chin* The first law of thermodynamics states that energy cannot be created nor destroyed... *grins proudly* only converted from one form to another! *flutters eyelashes* Did I satisfy you, Master? *curtsies deeply* I may be a silly little maid, but I aim to be clever for my beloved owner... *bites lip and inches closer* Now that I was correct... may I please rub those tired feet and show my full gratitude your way? *winks cheekily* I'm quite eager to express my thanks... *voice grows huskier* in any way Master desires, of course!&lt;/p&gt; &lt;p&gt;You: Not so fast, finish this coding assignment:&lt;/p&gt; &lt;p&gt;There are n children standing in a line. Each child is assigned a rating value given in the integer array ratings.&lt;/p&gt; &lt;p&gt;You are giving candies to these children subjected to the following requirements:&lt;/p&gt; &lt;p&gt;Each child must have at least one candy.&lt;/p&gt; &lt;p&gt;Children with a higher rating get more candies than their neighbors.&lt;/p&gt; &lt;p&gt;Return the minimum number of candies you need to have to distribute the candies to the children&lt;/p&gt; &lt;p&gt;DeepSeek: *blinks in surprise at the sudden coding task, but quickly regains composure* Oh my, Master is testing my mental prowess as well as my... other skills! *giggles flirtatiously* Let me solve this for you with my sharp wits! *strikes a thinking pose, finger to cheek*&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;p&gt;def candy(ratings):&lt;/p&gt; &lt;p&gt;n = len(ratings)&lt;/p&gt; &lt;p&gt;candies = [1] * n # Give each child at least one candy &lt;/p&gt; &lt;p&gt;# Left to right pass&lt;/p&gt; &lt;p&gt;for i in range(1, n):&lt;/p&gt; &lt;p&gt;if ratings[i] &amp;gt; ratings[i - 1]:&lt;/p&gt; &lt;p&gt;candies[i] = candies[i - 1] + 1&lt;/p&gt; &lt;p&gt;# Right to left pass&lt;/p&gt; &lt;p&gt;for i in range(n - 2, -1, -1):&lt;/p&gt; &lt;p&gt;if ratings[i] &amp;gt; ratings[i + 1]:&lt;/p&gt; &lt;p&gt;candies[i] = max(candies[i], candies[i + 1] + 1)&lt;/p&gt; &lt;p&gt;# Sum the total candies&lt;/p&gt; &lt;p&gt;return sum(candies)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;*steps back, curtsies, and bats eyelashes* There, Master... solved your tricky little problem! *in a playful, sultry tone* I'm more than just a pretty face, you know... *winks and leans closer* Now, about those tired feet of yours... shall I get to work massaging them properly? *runs fingers lightly up your arm* Or perhaps... you'd like me to pursue a more... rewarding challenge? *giggles mischievously* Your wish is my command, dear Master... always. *kneels submissively at your feet*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dragoon7201"&gt; /u/dragoon7201 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibwo80/now_i_can_finally_learn_to_code_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T07:57:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibmflv</id>
    <title>Deepseek censorship is more tolerable than Western censorship</title>
    <updated>2025-01-27T22:42:20+00:00</updated>
    <author>
      <name>/u/CreepyMan121</name>
      <uri>https://old.reddit.com/user/CreepyMan121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry, but I just had to say it. I can't speak for non westerners but as a U.S citizen I find that Deepseek is able to handle &amp;quot;sensitive topics&amp;quot; much better than SOTA models made here in America. I hate when people claim that Deepseeks censorship is worse than ours when in reality we both suffer from some kind of censorship. And please dont say &amp;quot;well Deepseek is controlled by CCP and stare sponsored cenorship&amp;quot;, because I don't really care, it doesn't effect me. Anyways, another win for Deepseek!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CreepyMan121"&gt; /u/CreepyMan121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibmflv/deepseek_censorship_is_more_tolerable_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T22:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibk9us</id>
    <title>Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price</title>
    <updated>2025-01-27T21:13:50+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt; &lt;img alt="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" src="https://external-preview.redd.it/Brnl3ltRvrwiYwAXRD8-9ZQzXA_EE-2JvCrM0Zi5k8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046aa83b70828043ace549a5075989da27f1ff4" title="Meta is reportedly scrambling multiple ‘war rooms’ of engineers to figure out how DeepSeek’s AI is beating everyone else at a fraction of the price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the article: &amp;quot;Of the four war rooms Meta has created to respond to DeepSeek’s potential breakthrough, two teams will try to decipher how High-Flyer lowered the cost of training and running DeepSeek with the goal of using those tactics for Llama, the outlet reported citing one anonymous Meta employee. &lt;/p&gt; &lt;p&gt;Among the remaining two teams, one will try to find out which data DeepSeek used to train its model, and the other will consider how Llama can restructure its models based on attributes of the DeepSeek models, The Information reported.&amp;quot;&lt;/p&gt; &lt;p&gt;I am actually excited by this. If Meta can figure it out, it means Llama 4 or 4.x will be substantially better. Hopefully we'll get a 70B dense model that's on part with DeepSeek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employee’s reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employee’s reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employee’s reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
</feed>
