<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-28T15:06:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kx2hcm</id>
    <title>Qwen3-14B vs Gemma3-12B</title>
    <updated>2025-05-27T23:42:05+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys thinks about these models? Which one to choose?&lt;/p&gt; &lt;p&gt;I mostly ask some programming knowledge questions, primary Go and Java.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx2hcm/qwen314b_vs_gemma312b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T23:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxj4ne</id>
    <title>Is slower inference and non-realtime cheaper?</title>
    <updated>2025-05-28T14:53:00+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a service that can take in my requests, and then give me the response after A WHILE, like, days later.&lt;/p&gt; &lt;p&gt;and is significantly cheaper?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxj4ne/is_slower_inference_and_nonrealtime_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T14:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxafjv</id>
    <title>When do you think the gap between local llm and o4-mini can be closed</title>
    <updated>2025-05-28T06:49:31+00:00</updated>
    <author>
      <name>/u/GregView</name>
      <uri>https://old.reddit.com/user/GregView</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if OpenAI recently upgraded this o4-mini free version, but I found this model really surpassed almost every local model in both correctness and consistency. I mainly tested on the coding part (not agent mode). It can understand the problem so well with minimal context (even compared to the Claude 3.7 &amp;amp; 4). I really hope one day we can get this thing running in local setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregView"&gt; /u/GregView &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxafjv/when_do_you_think_the_gap_between_local_llm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxeg6a</id>
    <title>Seeking Help Setting Up a Local LLM Assistant for TTRPG Worldbuilding + RAG on Windows 11</title>
    <updated>2025-05-28T11:16:50+00:00</updated>
    <author>
      <name>/u/TheArchivist314</name>
      <uri>https://old.reddit.com/user/TheArchivist314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm looking for some guidance on setting up a local LLM to help with &lt;strong&gt;TTRPG worldbuilding and running games&lt;/strong&gt; (like D&amp;amp;D or other systems). I want to be able to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate and roleplay NPCs&lt;/li&gt; &lt;li&gt;Write world lore collaboratively&lt;/li&gt; &lt;li&gt;Answer rules questions from PDFs&lt;/li&gt; &lt;li&gt;Query my own documents (lore, setting info, custom rules, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I think I need &lt;strong&gt;RAG&lt;/strong&gt; (Retrieval-Augmented Generation) ‚Äî or at least some way to have the LLM &amp;quot;understand&amp;quot; and reference my worldbuilding files or rule PDFs.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üñ•Ô∏è &lt;strong&gt;My current setup:&lt;/strong&gt; - Windows 11 - 4070 (12GB of Vram) - 64GB of Ram - SillyTavern installed and working - TabbyAPI installed&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚ùì &lt;strong&gt;What I'm trying to figure out:&lt;/strong&gt; - Can I do &lt;strong&gt;RAG&lt;/strong&gt; with &lt;strong&gt;SillyTavern&lt;/strong&gt; or &lt;strong&gt;TabbyAPI&lt;/strong&gt;? - What‚Äôs the best &lt;strong&gt;model loader&lt;/strong&gt; on Windows 11 that supports RAG (or can be used in a RAG pipeline)? - Which &lt;strong&gt;models&lt;/strong&gt; would you recommend for: - Worldbuilding / creative writing - Rule parsing and Q&amp;amp;A - Lightweight enough to run locally&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;üß† &lt;strong&gt;What I want in the long run:&lt;/strong&gt; - A local AI DM assistant that remembers lore - Can roleplay NPCs (via SillyTavern or similar) - Can read and answer questions from PDFs (like the PHB or custom notes) - Privacy is important ‚Äî I want to keep everything local&lt;/p&gt; &lt;p&gt;If you‚Äôve got a setup like this or know how to connect the dots between SillyTavern + RAG + local models, I‚Äôd love your advice!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheArchivist314"&gt; /u/TheArchivist314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxeg6a/seeking_help_setting_up_a_local_llm_assistant_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxeg6a/seeking_help_setting_up_a_local_llm_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxeg6a/seeking_help_setting_up_a_local_llm_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T11:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx51dp</id>
    <title>Tip for those building agents. The CLI is king.</title>
    <updated>2025-05-28T01:46:50+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"&gt; &lt;img alt="Tip for those building agents. The CLI is king." src="https://b.thumbs.redditmedia.com/p3QuldvOYVK8hBY2sPdoITG34rO_nYvSh-jsqyl80VA.jpg" title="Tip for those building agents. The CLI is king." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of ways of exposing tools to your agents depending on the framework or your implementation. MCP servers are making this trivial. But I am finding that exposing a simple CLI tool to your LLM/Agent with instructions on how to use common cli commands can actually work better, while reducing complexity. For example, the &lt;code&gt;wc&lt;/code&gt; command: &lt;a href="https://en.wikipedia.org/wiki/Wc_(Unix)"&gt;https://en.wikipedia.org/wiki/Wc_(Unix)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Crafting a system prompt for your agents to make use of these universal, but perhaps obscure commands for your level of experience, can greatly increase the probability of a successful task/step completion.&lt;/p&gt; &lt;p&gt;I have been experimenting with using a lot of MCP servers and exposing their tools to my agent fleet implementation (what should a group of agents be called?, a perplexity of agents? :D ), and have found that giving your agents the ability to simply issue cli commands can work a lot better.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kx51dp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx51dp/tip_for_those_building_agents_the_cli_is_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T01:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwqt64</id>
    <title>[Research] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond</title>
    <updated>2025-05-27T15:53:20+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I wanted to share a technique we've been working on called &lt;strong&gt;AutoThink&lt;/strong&gt; that significantly improves reasoning performance on local models through adaptive resource allocation and steering vectors.&lt;/p&gt; &lt;h1&gt;What is AutoThink?&lt;/h1&gt; &lt;p&gt;Instead of giving every query the same amount of &amp;quot;thinking time,&amp;quot; AutoThink:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Classifies query complexity&lt;/strong&gt; (HIGH/LOW) using an adaptive classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamically allocates thinking tokens&lt;/strong&gt; based on complexity (70-90% for hard problems, 20-40% for simple ones)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Uses steering vectors&lt;/strong&gt; to guide reasoning patterns during generation&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it as making your local model &amp;quot;think harder&amp;quot; on complex problems and &amp;quot;think faster&amp;quot; on simple ones.&lt;/p&gt; &lt;h1&gt;Performance Results&lt;/h1&gt; &lt;p&gt;Tested on &lt;strong&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;: 31.06% vs 21.72% baseline (+9.34 points, 43% relative improvement)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMLU-Pro&lt;/strong&gt;: 26.38% vs 25.58% baseline (+0.8 points)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;fewer tokens&lt;/strong&gt; than baseline approaches&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical Approach&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Steering Vectors&lt;/strong&gt;: We use Pivotal Token Search (PTS) - a technique from Microsoft's Phi-4 paper that we implemented and enhanced. These vectors modify activations to encourage specific reasoning patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;depth_and_thoroughness&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;numerical_accuracy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;self_correction&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;exploration&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;organization&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: Built on our adaptive classifier that can learn new complexity categories without retraining.&lt;/p&gt; &lt;h1&gt;Model Compatibility&lt;/h1&gt; &lt;p&gt;Works with any local reasoning model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1 variants&lt;/li&gt; &lt;li&gt;Qwen models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How to Try It&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Install optillm pip install optillm # Basic usage from optillm.autothink import autothink_decode response = autothink_decode( model, tokenizer, messages, { &amp;quot;steering_dataset&amp;quot;: &amp;quot;codelion/Qwen3-0.6B-pts-steering-vectors&amp;quot;, &amp;quot;target_layer&amp;quot;: 19 # adjust based on your model } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Full examples in the repo: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Research Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327"&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AutoThink Code&lt;/strong&gt;: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PTS Implementation&lt;/strong&gt;: &lt;a href="https://github.com/codelion/pts"&gt;https://github.com/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Blog&lt;/strong&gt;: &lt;a href="https://huggingface.co/blog/codelion/pts"&gt;https://huggingface.co/blog/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Classifier&lt;/strong&gt;: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Requires models that support thinking tokens (&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Need to tune &lt;code&gt;target_layer&lt;/code&gt; parameter for different model architectures&lt;/li&gt; &lt;li&gt;Steering vector datasets are model-specific (though we provide some pre-computed ones)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for more model architectures&lt;/li&gt; &lt;li&gt;Better automatic layer detection&lt;/li&gt; &lt;li&gt;Community-driven steering vector datasets&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Has anyone tried similar approaches with local models? I'm particularly interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How different model families respond to steering vectors&lt;/li&gt; &lt;li&gt;Alternative ways to classify query complexity&lt;/li&gt; &lt;li&gt;Ideas for extracting better steering vectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts and results if you try it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T15:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxi7qh</id>
    <title>Llama.cpp: Does it make sense to use a larger --n-predict (-n) than --ctx-size (-c)?</title>
    <updated>2025-05-28T14:15:58+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My setup: A reasoning model eg Qwen3 32B at Q4KXL + 16k context. Those will fit snugly in 24GB VRAM and leave some room for other apps.&lt;/p&gt; &lt;p&gt;Problem: Reasoning models, 1 time out of 3 (in my use cases), will keep on thinking for longer than the 16k window, and that's why I set the -n option to prevent it from reasoning indefinitely.&lt;/p&gt; &lt;p&gt;Question: I can relax -n to perhaps 30k, which some reasoning models suggest. However, when -n is larger than -c, won't the context window shift and the response's relevance to my prompt start decreasing?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxi7qh/llamacpp_does_it_make_sense_to_use_a_larger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T14:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxg95a</id>
    <title>vLLM Classify Bad Results</title>
    <updated>2025-05-28T12:50:16+00:00</updated>
    <author>
      <name>/u/Upstairs-Garlic-2301</name>
      <uri>https://old.reddit.com/user/Upstairs-Garlic-2301</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"&gt; &lt;img alt="vLLM Classify Bad Results" src="https://preview.redd.it/d9tr89iqri3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21243a1700f4e98e4582d952577c7f25af1d879" title="vLLM Classify Bad Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone used vLLM for classification?&lt;/p&gt; &lt;p&gt;I have a fine-tuned modernBERT model with 5 classes. During model training, the best model shows a .78 F1 score.&lt;/p&gt; &lt;p&gt;After the model is trained, I passed the test set through vLLM and Hugging Face pipelines as a test and get the screenshot above.&lt;/p&gt; &lt;p&gt;Hugging Face pipeline matches the result (F1 of .78) but vLLM is way off, with an F1 of .58.&lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upstairs-Garlic-2301"&gt; /u/Upstairs-Garlic-2301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d9tr89iqri3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxg95a/vllm_classify_bad_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T12:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx077t</id>
    <title>Deepseek R2 Release?</title>
    <updated>2025-05-27T22:00:34+00:00</updated>
    <author>
      <name>/u/Old-Medicine2445</name>
      <uri>https://old.reddit.com/user/Old-Medicine2445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn‚Äôt Deepseek say they were accelerating the timeline to release R2 before the original May release date shooting for April? Now that it‚Äôs almost June, have they said anything about R2 or when they will be releasing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Medicine2445"&gt; /u/Old-Medicine2445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T22:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxfq8r</id>
    <title>Old model, new implementation</title>
    <updated>2025-05-28T12:24:34+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/foldl/chatllm.cpp"&gt;chatllm.cpp&lt;/a&gt; implements &lt;a href="https://huggingface.co/adept/fuyu-8b"&gt;Fuyu-8b&lt;/a&gt; as the 1st supported vision model.&lt;/p&gt; &lt;p&gt;I have search this group. Not many have tested this model due to lack of support from llama.cpp. Now, would you like to try this model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxfq8r/old_model_new_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T12:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxc5vo</id>
    <title>MCP Proxy ‚Äì Use your embedded system as an agent</title>
    <updated>2025-05-28T08:47:54+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt; &lt;img alt="MCP Proxy ‚Äì Use your embedded system as an agent" src="https://external-preview.redd.it/QE_AwMn8Vhy9CL-rjaMkp2CgPWYkmSjtSuxPv7QHnQs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ecb64a2bffb05e44d274eb04fb6b8576a8f1055e" title="MCP Proxy ‚Äì Use your embedded system as an agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/x1y4mz3mkh3f1.gif"&gt;https://i.redd.it/x1y4mz3mkh3f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=foCp3ja8FRA"&gt;https://www.youtube.com/watch?v=foCp3ja8FRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repository: &lt;a href="https://github.com/openserv-labs/mcp-proxy"&gt;https://github.com/openserv-labs/mcp-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I've been playing around with agents, MCP servers and embedded systems for a while. I was trying to figure out the best way to connect my real-time devices to agents and use them in multi-agent workflows.&lt;/p&gt; &lt;p&gt;At OpenServ, we have an API to interact with agents, so at first I thought I'd just run a specialized web server to talk to the platform. But that had its own problems‚Äîmainly memory issues and needing to customize it for each device.&lt;/p&gt; &lt;p&gt;Then we thought, why not just run a regular web server and use it as an agent? The idea is simple, and the implementation is even simpler thanks to MCP. I define my server‚Äôs endpoints as tools in the MCP server, and agents (MCP clients) can call them directly.&lt;/p&gt; &lt;p&gt;Even though the initial idea was to work with embedded systems, this can work for any backend.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts‚Äîespecially around connecting agents to real-time devices to collect sensor data or control them in mutlti-agent workflows.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxc5vo/mcp_proxy_use_your_embedded_system_as_an_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwk1jm</id>
    <title>Wife isn‚Äôt home, that means H200 in the living room ;D</title>
    <updated>2025-05-27T10:40:11+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt; &lt;img alt="Wife isn‚Äôt home, that means H200 in the living room ;D" src="https://a.thumbs.redditmedia.com/CHdnIbD-SLsvZOKpoU7Rs4hqE0GREYpW_lt-IICeGd0.jpg" title="Wife isn‚Äôt home, that means H200 in the living room ;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got our H200 System, until it‚Äôs going in the datacenter next week that means localLLaMa with some extra power :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kwk1jm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T10:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxf0ig</id>
    <title>Parakeet-TDT 0.6B v2 FastAPI STT Service (OpenAI-style API + Experimental Streaming)</title>
    <updated>2025-05-28T11:47:57+00:00</updated>
    <author>
      <name>/u/Shadowfita</name>
      <uri>https://old.reddit.com/user/Shadowfita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm (finally) releasing a FastAPI wrapper around NVIDIA‚Äôs Parakeet-TDT 0.6B v2 ASR model with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;REST &lt;code&gt;/transcribe&lt;/code&gt; endpoint with optional timestamps&lt;/li&gt; &lt;li&gt;Health &amp;amp; debug endpoints: &lt;code&gt;/healthz&lt;/code&gt;, &lt;code&gt;/debug/cfg&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Experimental WebSocket &lt;code&gt;/ws&lt;/code&gt; for real-time PCM streaming and partial/full transcripts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Shadowfita/parakeet-tdt-0.6b-v2-fastapi"&gt;https://github.com/Shadowfita/parakeet-tdt-0.6b-v2-fastapi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shadowfita"&gt; /u/Shadowfita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxf0ig/parakeettdt_06b_v2_fastapi_stt_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T11:47:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwucpn</id>
    <title>üòûNo hate but claude-4 is disappointing</title>
    <updated>2025-05-27T18:10:17+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt; &lt;img alt="üòûNo hate but claude-4 is disappointing" src="https://preview.redd.it/9dngmfww7d3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d89328b58759f0c926b5258c859b6fbfcf5a5b32" title="üòûNo hate but claude-4 is disappointing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean how the heck literally Is Qwen-3 better than claude-4(the Claude who used to dog walk everyone). this is just disappointing ü´†&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9dngmfww7d3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T18:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxbmr9</id>
    <title>Another Ryzen Max+ 395 machine has been released. Are all the Chinese Max+ 395 machines the same?</title>
    <updated>2025-05-28T08:10:10+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another AMD Ryzen Max+ 395 mini-pc has been released. The FEVM FA-EX9. For those who kept asking for it, this comes with Oculink. Here's a YT review.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-1kuUqp1X2I"&gt;https://www.youtube.com/watch?v=-1kuUqp1X2I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think all the Chinese Max+ mini-pcs are the same. I noticed again that this machine has &lt;em&gt;exactly&lt;/em&gt; the same port layout as the GMK X2. But how can that be if this has Oculink but the X2 doesn't? The Oculink is an addon. It takes up one of the NVME slots. It's just not the port layout, but the motherboards look exactly the same. Down to the same red color. Even the sound level is the same with the same fan configuration 2 blowers and one axial. So it's like one manufacturer is making the MB and then all the other companies are using that MB for their mini-pcs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxbmr9/another_ryzen_max_395_machine_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T08:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxh07e</id>
    <title>FlashMoe support in ipex-llm allows you to run DeepSeek V3/R1 671B and Qwen3MoE 235B models with just 1 or 2 Intel Arc GPU (such as A770 and B580)</title>
    <updated>2025-05-28T13:24:16+00:00</updated>
    <author>
      <name>/u/lQEX0It_CUNTY</name>
      <uri>https://old.reddit.com/user/lQEX0It_CUNTY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that this team claims it is possible to run the DeepSeek V1/R1 671B Q4_K_M model with two cheap Intel GPUs (and a huge amount of system RAM). I wonder if anybody has actually tried or built such a beast?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/flashmoe_quickstart.md"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/flashmoe_quickstart.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also see at the end the claim: For 1 ARC A770 platform, please reduce context length (e.g., 1024) to avoid OOM. Add this option &lt;code&gt;-c 1024&lt;/code&gt; at the CLI command.&lt;/p&gt; &lt;p&gt;Does this mean this implementation is effectively a box ticking exercise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lQEX0It_CUNTY"&gt; /u/lQEX0It_CUNTY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxh07e/flashmoe_support_in_ipexllm_allows_you_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxgzd1</id>
    <title>Is there an open source alternative to manus?</title>
    <updated>2025-05-28T13:23:15+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried manus and was surprised how ahead it is of other agents at browsing the web and using files, terminal etc autonomously.&lt;/p&gt; &lt;p&gt;There is no tool I've tried before that comes close to it.&lt;/p&gt; &lt;p&gt;What's the best open source alternative to Manus that you've tried?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgzd1/is_there_an_open_source_alternative_to_manus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx9nfk</id>
    <title>Megakernel doubles Llama-1B inference speed for batch size 1</title>
    <updated>2025-05-28T05:58:06+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The authors of this &lt;a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles"&gt;bloglike paper&lt;/a&gt; at Stanford found that vLLM and SGLang lose significant performance due to overhead in CUDA usage for low batch sizes - what you usually use when running locally to chat. Their improvement doubles the inference speed on a H100, which however has significantly higher memory bandwidth than a 3090 for example. It remains to be seen how this scales to user GPUs. The benefits will diminish the larger the model gets.&lt;/p&gt; &lt;p&gt;The best thing is that even with their optimizations there seems to be still some room left for further improvements - theoretically. There was also no word on llama.cpp in there. Their publication is a nice &amp;amp; easy read though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx9nfk/megakernel_doubles_llama1b_inference_speed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T05:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxhmgo</id>
    <title>VideoGameBench- full code + paper release</title>
    <updated>2025-05-28T13:51:11+00:00</updated>
    <author>
      <name>/u/ofirpress</name>
      <uri>https://old.reddit.com/user/ofirpress</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1kxhmgo/video/hzjtuzzr1j3f1/player"&gt;https://reddit.com/link/1kxhmgo/video/hzjtuzzr1j3f1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VideoGameBench&lt;/strong&gt; evaluates VLMs on Game Boy and MS-DOS games given only raw screen input, just like how a human would play. The best model (Gemini) completes just 0.48% of the benchmark. We have a bunch of clips on the website:&lt;br /&gt; &lt;a href="http://vgbench.com"&gt;vgbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.18134"&gt;https://arxiv.org/abs/2505.18134&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/alexzhang13/videogamebench"&gt;https://github.com/alexzhang13/videogamebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alex and I will stick around to answer questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ofirpress"&gt; /u/ofirpress &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxhmgo/videogamebench_full_code_paper_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T13:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdkms</id>
    <title>Cobolt is now available on Linux! üéâ</title>
    <updated>2025-05-28T10:23:07+00:00</updated>
    <author>
      <name>/u/ice-url</name>
      <uri>https://old.reddit.com/user/ice-url</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember when we said Cobolt is &amp;quot;Powered by community-driven development&amp;quot;?&lt;/p&gt; &lt;p&gt;After our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kujwzl/we_believe_the_future_of_ai_is_local_private_and"&gt;last post&lt;/a&gt; about Cobolt ‚Äì &lt;strong&gt;our local, private, and personalized AI assistant&lt;/strong&gt; ‚Äì the call for Linux support was overwhelming. Well, you asked, and we're thrilled to deliver: Cobolt is now available on Linux! üéâ &lt;a href="https://github.com/platinum-hill/cobolt?tab=readme-ov-file#getting-started"&gt;Get started here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are excited by your engagement and shared belief in accessible, private AI.&lt;/p&gt; &lt;p&gt;Join us in shaping the future of Cobolt on &lt;a href="https://github.com/platinum-hill/cobolt"&gt;Github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our promise remains: Privacy by design, extensible, and personalized.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thank you for driving us forward. Let's keep building AI that serves you, now on Linux!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ice-url"&gt; /u/ice-url &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdkms/cobolt_is_now_available_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxgfbj</id>
    <title>New DeepseekV3 as well</title>
    <updated>2025-05-28T12:58:33+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgfbj/new_deepseekv3_as_well/"&gt; &lt;img alt="New DeepseekV3 as well" src="https://b.thumbs.redditmedia.com/xoet0fIOx86MhrIXV3L5Dptihv4IiT8Zf4MWi32_rDs.jpg" title="New DeepseekV3 as well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New V3!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wjoiebx5ti3f1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=11bcdcd461259d9329165669759f04fb531ee79c"&gt;https://preview.redd.it/wjoiebx5ti3f1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=11bcdcd461259d9329165669759f04fb531ee79c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgfbj/new_deepseekv3_as_well/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgfbj/new_deepseekv3_as_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxgfbj/new_deepseekv3_as_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T12:58:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdcpi</id>
    <title>impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!</title>
    <updated>2025-05-28T10:08:41+00:00</updated>
    <author>
      <name>/u/thebigvsbattlesfan</name>
      <uri>https://old.reddit.com/user/thebigvsbattlesfan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt; &lt;img alt="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" src="https://preview.redd.it/sd06j27qyh3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c51a26804948f34a4686a4018dd2e02a67c40a82" title="impressive streamlining in local llm deployment: gemma 3n downloading directly to my phone without any tinkering. what a time to be alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebigvsbattlesfan"&gt; /u/thebigvsbattlesfan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sd06j27qyh3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdcpi/impressive_streamlining_in_local_llm_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxa788</id>
    <title>Google AI Edge Gallery</title>
    <updated>2025-05-28T06:33:50+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt; &lt;img alt="Google AI Edge Gallery" src="https://preview.redd.it/s6rgmrfawg3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4720f1c95bf832e5eacd2490cf5b69783a79a11b" title="Google AI Edge Gallery" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android &lt;em&gt;(available now)&lt;/em&gt; and iOS &lt;em&gt;(coming soon)&lt;/em&gt; devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images, explore prompts, and more!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery?tab=readme-ov-file"&gt;https://github.com/google-ai-edge/gallery?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6rgmrfawg3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxa788/google_ai_edge_gallery/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T06:33:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxdm2z</id>
    <title>DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324</title>
    <updated>2025-05-28T10:25:48+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt; &lt;img alt="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" src="https://b.thumbs.redditmedia.com/Dz5hcsX2WLjgLhcJJfOpbDqkyNTpx1Aiw9gp50Wdl_Q.jpg" title="DeepSeek Announces Upgrade, Possibly Launching New Model Similar to 0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The official DeepSeek group has issued an announcement claiming an upgrade, possibly a new model similar to the 0324 version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kxdm2z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxdm2z/deepseek_announces_upgrade_possibly_launching_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T10:25:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxaxw9</id>
    <title>The Economist: "Companies abandon their generative AI projects"</title>
    <updated>2025-05-28T07:23:16+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/P51MQ"&gt;recent article&lt;/a&gt; in the Economist claims that &amp;quot;the share of companies abandoning most of their generative-AI pilot projects has risen to 42%, up from 17% last year.&amp;quot; Apparently companies who invested in generative AI and slashed jobs are now disappointed and they began rehiring humans for roles.&lt;/p&gt; &lt;p&gt;The hype with the generative AI increasingly looks like a &amp;quot;we have a solution, now let's find some problems&amp;quot; scenario. Apart from software developers and graphic designers, I wonder how many professionals actually feel the impact of generative AI in their workplace?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxaxw9/the_economist_companies_abandon_their_generative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T07:23:16+00:00</published>
  </entry>
</feed>
