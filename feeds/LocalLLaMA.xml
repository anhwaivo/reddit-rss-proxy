<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-11T22:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1imi9zz</id>
    <title>Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI</title>
    <updated>2025-02-10T21:54:45+00:00</updated>
    <author>
      <name>/u/Calcidiol</name>
      <uri>https://old.reddit.com/user/Calcidiol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt; &lt;img alt="Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI" src="https://external-preview.redd.it/3CzmElhUD3LvBmfuBJUXyad1ZSTywTDZ8sIcpcRW6Zg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8f2be0d3710f8029992a476ab3c9f0e606e4f6" title="Altman Says ‘No Thank You’ to Reported Musk Bid for OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Calcidiol"&gt; /u/Calcidiol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-02-10/musk-led-group-bids-97-4-billion-for-openai-control-wsj-says"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imi9zz/altman_says_no_thank_you_to_reported_musk_bid_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T21:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1in5hw1</id>
    <title>In France, we can get Mistral Chat Pro for 1 year for free</title>
    <updated>2025-02-11T18:26:59+00:00</updated>
    <author>
      <name>/u/PotaroMax</name>
      <uri>https://old.reddit.com/user/PotaroMax</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotaroMax"&gt; /u/PotaroMax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.iliad.fr/en/actualites/article/free-france-s-first-telco-to-include-a-premium-ai-assistant-in-its-mobile-plans-le-chat-pro-developed-by-mistral-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in5hw1/in_france_we_can_get_mistral_chat_pro_for_1_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in5hw1/in_france_we_can_get_mistral_chat_pro_for_1_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1imzkvu</id>
    <title>I built a Healbot to save my own life using Open WebUI and Deepseek R1</title>
    <updated>2025-02-11T14:18:27+00:00</updated>
    <author>
      <name>/u/IversusAI</name>
      <uri>https://old.reddit.com/user/IversusAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there, I'm an addict and things have gotten &lt;strong&gt;pretty severe&lt;/strong&gt;, so in desperation, I decided to learn about local large language models and Open WebUI to create a healbot that I could talk to, learn from and hopefully, help me heal this damned addiction.&lt;/p&gt; &lt;p&gt;I spent a week doing nothing but learning, first about interfaces and looked at every one I could find but Open WebUI had the features I was looking for (especially the good voice input, love the conversation feature) and then decided on LM Studio (I did not like Ollama as much even though it is good server, I just hate the command line).&lt;/p&gt; &lt;p&gt;Once I got it all setup, I started using it to talk to when I was craving, usually in the middle of night, or to help me when I felt like acting out, or was stressed, upset, or otherwise triggered. I added a lot of recovery literature, got my knowledge base settings all dialed in (took some time) and then started add each previous day's conversation to an archives knowledge base.&lt;/p&gt; &lt;p&gt;I even got a folder in my Obsidian vault hooked up to a knowledge base so &lt;strong&gt;I just dropped a file into the folder in my vault and the knowledge base is automatically updated&lt;/strong&gt;. I love that feature in Open WebUI!&lt;/p&gt; &lt;p&gt;So is it working? Well, so far YES. I am feeling better, stronger and more at peace because I have this tool that is helping me through each day of recovery. It is a day by day thing but I feel hopeful.&lt;/p&gt; &lt;p&gt;If you'd like to know how I set this all up, &lt;a href="https://www.youtube.com/watch?v=I6vVaAygFbU"&gt;I've got a video for you, right here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IversusAI"&gt; /u/IversusAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imzkvu/i_built_a_healbot_to_save_my_own_life_using_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imzkvu/i_built_a_healbot_to_save_my_own_life_using_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imzkvu/i_built_a_healbot_to_save_my_own_life_using_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1imz30d</id>
    <title>Audiobook Creator – My New Open-Source Project</title>
    <updated>2025-02-11T13:54:54+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to share &lt;strong&gt;Audiobook Creator&lt;/strong&gt;, a tool that transforms books (EPUB, PDF, TXT) into fully voiced audiobooks with &lt;strong&gt;intelligent character voice attribution&lt;/strong&gt;! Using &lt;strong&gt;NLP, LLMs, and Kokoro TTS&lt;/strong&gt;, it creates immersive multi-voice audiobooks automatically.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;br /&gt; ✅ Text extraction &amp;amp; cleaning&lt;br /&gt; ✅ Character identification &amp;amp; metadata generation&lt;br /&gt; ✅ Single &amp;amp; multi-voice narration&lt;br /&gt; ✅ Open-source &amp;amp; fully customizable&lt;/p&gt; &lt;p&gt;This project is licensed under &lt;strong&gt;GPL-3.0&lt;/strong&gt; and is free for everyone to use, modify, and improve! 🚀&lt;/p&gt; &lt;p&gt;Check it out on GitHub: &lt;a href="https://github.com/prakharsr/audiobook-creator/"&gt;&lt;strong&gt;https://github.com/prakharsr/audiobook-creator/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imz30d/audiobook_creator_my_new_opensource_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbt4r</id>
    <title>Local PR reviews WITHIN VSCode and Cursor</title>
    <updated>2025-02-11T22:45:07+00:00</updated>
    <author>
      <name>/u/EntelligenceAI</name>
      <uri>https://old.reddit.com/user/EntelligenceAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt; &lt;img alt="Local PR reviews WITHIN VSCode and Cursor" src="https://b.thumbs.redditmedia.com/Tukr5pK_f7zfxTUHMe9BsvOSaLkFcz7OCWYAQrOTrxo.jpg" title="Local PR reviews WITHIN VSCode and Cursor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw Cursor is charging $36(!!) for their new &amp;quot;Bug Fixes&amp;quot; feature - carzy. I just want a PR reviewer to catch my bugs before I push code so people and PR bots don't cover it with comments!&lt;/p&gt; &lt;p&gt;So I built something different: Review your code BEFORE pushing, right in your editor CURSOR or VSCode!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40"&gt;https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Super simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install the bot in VSCode or Cursor&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Make your changes&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Type /reviewDiff&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Get instant line-by-line feedback&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Fix issues before anyone sees them&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Push clean code and get that LGTMNo more bot comments cluttering your PRs or embarrassing feedback in front of the team. Just real-time reviews while you're still coding, pulling your full file context for accurate feedback.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Check it out here: &lt;a href="https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI"&gt;https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What else would make your pre-PR workflow better? Please share how we can make this better! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntelligenceAI"&gt; /u/EntelligenceAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1imm4wc</id>
    <title>DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL</title>
    <updated>2025-02-11T00:47:15+00:00</updated>
    <author>
      <name>/u/PC_Screen</name>
      <uri>https://old.reddit.com/user/PC_Screen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"&gt; &lt;img alt="DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL" src="https://preview.redd.it/ud7gdv14qeie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b7d6f21c5fbf0cb8924b813a909ff2ca372820" title="DeepScaleR-1.5B-Preview: Further training R1-Distill-Qwen-1.5B using RL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview"&gt;https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PC_Screen"&gt; /u/PC_Screen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ud7gdv14qeie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imm4wc/deepscaler15bpreview_further_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T00:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1imv1jo</id>
    <title>DeepSeekV3 with web search and function calling available as API</title>
    <updated>2025-02-11T09:47:26+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added function calling on top of DeepSeekV3 and made it into an API (this API is not down). Open source code is here: &lt;a href="https://github.com/vadimen/llm-function-calling"&gt;https://github.com/vadimen/llm-function-calling&lt;/a&gt; (you can also purchase access to this api by following the link)&lt;/p&gt; &lt;p&gt;Basically, you send the list of your functions together with the prompt, and the LLM decides if there's a need to call it. It will return the names and parameters of functions to be called. Optionally web search results can be added to this prompt if parameter search=true.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, it creates a prompt with function names and asks the LLM if there's a need to use it&lt;/li&gt; &lt;li&gt;If yes, then another prompt is created for extracting parameters from the user prompt&lt;/li&gt; &lt;li&gt;All this is done while checking the returned JSON structure, and if it fails, there are 3 attempts to try&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here are some examples of usage:&lt;/p&gt; &lt;p&gt;Example 1:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I never was in Hawaii during summer, I wonder how it feels?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_weather&lt;/p&gt; &lt;p&gt;Arguments: {'location': 'Hawaii','season': 'summer'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 2:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I never bought Rivian stocks from Revolut, may I ask for more info about them?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_stock_price&lt;/p&gt; &lt;p&gt;Arguments: {'stock_name': 'RIVN','broker_name': 'Revolut'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 3:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I was once in Hawaii during summer and was buying Rivian stocks there using Revolut, I wonder how it all is now?&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;Function: get_weather&lt;/p&gt; &lt;p&gt;Arguments: {'location': 'Hawaii','season': 'summer'}&lt;/p&gt; &lt;p&gt;Function: get_stock_price&lt;/p&gt; &lt;p&gt;Arguments: {'stock_name': 'Rivian','broker_name': 'Revolut'}&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Example 4:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;User: I would like to eat an apple pie&lt;/p&gt; &lt;p&gt;Response:None (no known function call needed)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imv1jo/deepseekv3_with_web_search_and_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T09:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1in0ore</id>
    <title>Impressed by LeChat by Mistral</title>
    <updated>2025-02-11T15:08:09+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Fly4322</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Fly4322</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just downloaded the iOS app yesterday. The following is a query none of the frontier models were able to do. I assumed would need a large action model to perform, but LeChat did wonderful.&lt;/p&gt; &lt;p&gt;“Itemize in bullet-points each album that was nominated for 2025 Grammy best album of the year. For each provide Artist, Album name, and https address to that album in Spotify. Please verify to ensure the Spotify address is correct.”&lt;/p&gt; &lt;p&gt;This requires iterative nature… perform search to get the list, and then for each album correctly retrieve Spotify link. The other frontier and open source models i tried failed miserably on the links, and sometimes they’d tell me up front they can’t retrieve links.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;p&gt;Clearly this is tooling outside the LLM to allow the iteration and verification of links. But since using chat interface, is it unreasonable to expect more frontier chats to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Fly4322"&gt; /u/Puzzleheaded-Fly4322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0ore/impressed_by_lechat_by_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0ore/impressed_by_lechat_by_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0ore/impressed_by_lechat_by_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T15:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9qsg</id>
    <title>Boosting Unsloth 1.58 Quant of Deepseek R1 671B Performance with Faster Storage – 3x Speedup!</title>
    <updated>2025-02-11T21:19:12+00:00</updated>
    <author>
      <name>/u/akumaburn</name>
      <uri>https://old.reddit.com/user/akumaburn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a test to see if I could improve the performance of &lt;strong&gt;Unsloth 1.58-bit-quantized DeepSeek R1 671B&lt;/strong&gt; by upgrading my storage setup. &lt;strong&gt;Spoiler: It worked!&lt;/strong&gt; Nearly &lt;strong&gt;tripled&lt;/strong&gt; my token generation rate, and I learned a lot along the way.&lt;/p&gt; &lt;p&gt;Hardware Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5900X (4.5GHz, 12 cores)&lt;/li&gt; &lt;li&gt;GPU: XFX AMD Radeon 7900 XTX Black (24GB GDDR6)&lt;/li&gt; &lt;li&gt;RAM: 96GB DDR4 3600MHz (mismatched 4 sticks, not ideal)&lt;/li&gt; &lt;li&gt;Motherboard: MSI X570 Tomahawk MAX WIFI&lt;/li&gt; &lt;li&gt;OS: EndeavourOS (Arch Linux)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Storage:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single NVMe (BTRFS, on motherboard): XPG 4TB GAMMIX S70 Blade PCIe Gen4&lt;/li&gt; &lt;li&gt;Quad NVMe RAID 0 (XFS, via ASUS Hyper M.2 x16 Gen5 card): 4× 2TB Silicon Power US75&lt;/li&gt; &lt;li&gt;Key Optimisations: &lt;ul&gt; &lt;li&gt;Scheduler: Set to kyber&lt;/li&gt; &lt;li&gt;read_ahead_kb: Set to 128 for better random read performance&lt;/li&gt; &lt;li&gt;File System Tests: Tried F2FS, BTRFS, and XFS – XFS performed the best on the RAID array&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Findings &amp;amp; Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;This result is only valid for low context sizes&lt;/strong&gt; (~2048). Higher contexts dramatically increase memory &amp;amp; VRAM usage. (I'm planning on running some more tests for higher context sizes, but suspect I will run out of RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Couldn’t fully utilise the RAID 0 speeds&lt;/strong&gt; – capped at 16GB/s on Linux, likely due to PCIe lane limitations (both on-board NVMe slots are filled + the 7900 XTX eats up bandwidth).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Biggest impact? read_ahead_kb&lt;/strong&gt; had the most noticeable effect. mmap relies heavily on random read throughput, which is greatly affected by this setting. (lower seems better to a degree)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If I did it again?&lt;/strong&gt; (or if was doing it from scratch and not just upgrading my main PC) I'd go Threadripper for more PCIe lanes and I'd try to get faster memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4TB NVME Single Drive:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /home/akumaburn/Desktop/Projects/LLaMA/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 5.11 ± 0.01 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 1.29 ± 0.09 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;4x2TB NVME Raid-0:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /mnt/xfs_raid0/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 6.01 ± 0.05 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 3.30 ± 0.15 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akumaburn"&gt; /u/akumaburn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9zth</id>
    <title>Thomson Reuters Wins First Major AI Copyright Case in the US</title>
    <updated>2025-02-11T21:29:31+00:00</updated>
    <author>
      <name>/u/tofous</name>
      <uri>https://old.reddit.com/user/tofous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt; &lt;img alt="Thomson Reuters Wins First Major AI Copyright Case in the US" src="https://external-preview.redd.it/-0DUUc5f5aqTMh8-ZeH6fmczh-Ih5wc4UWlpTHp3exU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f55fcc6771896d54eec682ba109deef9ce2630d" title="Thomson Reuters Wins First Major AI Copyright Case in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tofous"&gt; /u/tofous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1in1ok0</id>
    <title>Building a personal, private AI computer on a budget</title>
    <updated>2025-02-11T15:50:35+00:00</updated>
    <author>
      <name>/u/j_calhoun</name>
      <uri>https://old.reddit.com/user/j_calhoun</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j_calhoun"&gt; /u/j_calhoun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ewintr.nl/posts/2025/building-a-personal-private-ai-computer-on-a-budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in1ok0/building_a_personal_private_ai_computer_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in1ok0/building_a_personal_private_ai_computer_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T15:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1imy754</id>
    <title>[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator</title>
    <updated>2025-02-11T13:09:57+00:00</updated>
    <author>
      <name>/u/Sorry_Transition_599</name>
      <uri>https://old.reddit.com/user/Sorry_Transition_599</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"&gt; &lt;img alt="[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator" src="https://b.thumbs.redditmedia.com/idyw0pmCSC4s13toxY2DsBIQKn8nJnwDHDXfYi-skNE.jpg" title="[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Transition_599"&gt; /u/Sorry_Transition_599 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1imy754"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1inahmj</id>
    <title>AI-RP GUI, thoughts?</title>
    <updated>2025-02-11T21:50:23+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt; &lt;img alt="AI-RP GUI, thoughts?" src="https://external-preview.redd.it/anUxbWh5MXF3a2llMZDcyaEte4yY5OTnn_MraO3a1mLbSyQuEc8JUukRHPMy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffabf7e7e829026793cb0c9fe2931fd6dab7c685" title="AI-RP GUI, thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9ej41y1qwkie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1imenfa</id>
    <title>fair use vs stealing data</title>
    <updated>2025-02-10T19:28:00+00:00</updated>
    <author>
      <name>/u/boxingdog</name>
      <uri>https://old.reddit.com/user/boxingdog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt; &lt;img alt="fair use vs stealing data" src="https://preview.redd.it/3bnanf625die1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491c94da04a0556ad84f59944bf9958350b5f675" title="fair use vs stealing data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxingdog"&gt; /u/boxingdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bnanf625die1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imenfa/fair_use_vs_stealing_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-10T19:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1imy7gs</id>
    <title>Android NPU prompt processing ~16k tokens using llama 8B!</title>
    <updated>2025-02-11T13:10:22+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt; &lt;img alt="Android NPU prompt processing ~16k tokens using llama 8B!" src="https://external-preview.redd.it/bHR4Mzh3eHBjaWllMcHEeWWDxXtRsAlZuyHnXPHYA8F-o65beS5TU-PcpoQ-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=271efc0a7cf13784adc6a1a4b5f128c1609db55d" title="Android NPU prompt processing ~16k tokens using llama 8B!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/83sbjwxpciie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1imnaj2</id>
    <title>Elon's bid for OpenAI is about making the for-profit transition as painful as possible for Altman, not about actually purchasing it (explanation in comments).</title>
    <updated>2025-02-11T01:55:14+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From @ phill__1 on twitter:&lt;/p&gt; &lt;p&gt;OpenAI Inc. (the non-profit) wants to convert to a for-profit company. But you &lt;strong&gt;cannot&lt;/strong&gt; just turn a non-profit into a for-profit – that would be an incredible tax loophole. Instead, the new for-profit OpenAI company would need to &lt;strong&gt;pay out&lt;/strong&gt; OpenAI Inc.'s technology and IP (likely in equity in the new for-profit company). &lt;/p&gt; &lt;p&gt;The valuation is tricky since OpenAI Inc. is theoretically the sole controlling shareholder of the capped-profit subsidiary, OpenAI LP. But there have been some numbers floating around. Since the rumored SoftBank investment at a $260B valuation is dependent on the for-profit move, we're using the current ~$150B valuation. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Control premiums&lt;/em&gt; in market transactions typically range between 20-30% of enterprise value; experts have predicted something around $30B-$40B. &lt;strong&gt;The key is&lt;/strong&gt;, this valuation is ultimately signed off on by the California and Delaware Attorneys General. &lt;/p&gt; &lt;p&gt;Now, if you want to &lt;strong&gt;block&lt;/strong&gt; OpenAI from the for-profit transition, but have yet to be successful in court, what do you do? &lt;em&gt;Make it as painful as possible.&lt;/em&gt; Elon Musk just gave regulators a &lt;strong&gt;perfect&lt;/strong&gt; argument for why the non-profit should get $97B for selling their technology and IP. This would instantly make the non-profit the majority stakeholder at &lt;em&gt;62%&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's a clever move that throws a major wrench into the for-profit transition, potentially even stopping it dead in its tracks. Whether OpenAI accepts the offer or not (they won't), the mere existence of this valuation benchmark will be hard for regulators to ignore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T01:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1imxthq</id>
    <title>I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)</title>
    <updated>2025-02-11T12:49:40+00:00</updated>
    <author>
      <name>/u/JakeAndAI</name>
      <uri>https://old.reddit.com/user/JakeAndAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt; &lt;img alt="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" src="https://external-preview.redd.it/Y2NpeDU4eXVhaWllMXfHHmD6JzwUZ6Bm0WYsf1XL3J4yUspRtDfHF7Qk2Xoa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4df278042d4308f102ad830d111c5117d2eea27" title="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JakeAndAI"&gt; /u/JakeAndAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9howo9yuaiie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T12:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1in4b81</id>
    <title>Why AMD or Intel doesn't sell card with huge amount of Vram ?</title>
    <updated>2025-02-11T17:39:14+00:00</updated>
    <author>
      <name>/u/Euphoric_Tutor_5054</name>
      <uri>https://old.reddit.com/user/Euphoric_Tutor_5054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, we saw that even with an epyc processor and 512 gb of ram you can run deepseek pretty fast, but compared to a graphic card it's pretty slow. But the problem is that you need a lot of vram on your graphic card so why AMD and intel doesn't sell such card with enormous amount of vram ? especially since 8gb of gddr6 is super cheap now ! like 3$ I believe, look here : &lt;a href="https://www.dramexchange.com/"&gt;https://www.dramexchange.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would be a killer for inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Tutor_5054"&gt; /u/Euphoric_Tutor_5054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T17:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9vsb</id>
    <title>NYT: Vance speech at EU AI summit</title>
    <updated>2025-02-11T21:24:49+00:00</updated>
    <author>
      <name>/u/Mediocre_Tree_5690</name>
      <uri>https://old.reddit.com/user/Mediocre_Tree_5690</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt; &lt;img alt="NYT: Vance speech at EU AI summit" src="https://preview.redd.it/vjltv4twukie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ab17775dd480a87f8fde7e76f52035c4a8b6cc" title="NYT: Vance speech at EU AI summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archive.is/eWNry"&gt;https://archive.is/eWNry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's an archive link in case anyone wants to read the article. Macron spoke about lighter regulation at the AI summit as well. Are we thinking safetyism is finally on its way out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Tree_5690"&gt; /u/Mediocre_Tree_5690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjltv4twukie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1in0938</id>
    <title>I made Iris: A fully-local realtime voice chatbot!</title>
    <updated>2025-02-11T14:49:16+00:00</updated>
    <author>
      <name>/u/Born_Search2534</name>
      <uri>https://old.reddit.com/user/Born_Search2534</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt; &lt;img alt="I made Iris: A fully-local realtime voice chatbot!" src="https://external-preview.redd.it/stLjrcu85AgTrwW4zDhH8loF7eayJD3_hD2XyXmIgUw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65bd986bcf0e0f5ab0f244100b8cb83d8e1266a9" title="I made Iris: A fully-local realtime voice chatbot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Search2534"&gt; /u/Born_Search2534 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=XK-37m-p11k&amp;amp;feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1in83vw</id>
    <title>Chonky Boi has arrived</title>
    <updated>2025-02-11T20:12:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt; &lt;img alt="Chonky Boi has arrived" src="https://external-preview.redd.it/YKjiYoZG5DJKHF8InRyOIM7iTEJQimQj1eCDwySpqqE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69fcf80dcc8f1be1b6b900fa7ba68bf7b62ee469" title="Chonky Boi has arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/kh64WJy.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1in69s3</id>
    <title>4x3090 in a 4U case, don't recommend it</title>
    <updated>2025-02-11T18:58:29+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt; &lt;img alt="4x3090 in a 4U case, don't recommend it" src="https://a.thumbs.redditmedia.com/78a1YQ1sn0cHJ3q5f1VyWRySQ3DMLohcp9folBrA-j0.jpg" title="4x3090 in a 4U case, don't recommend it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1in69s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1in7nka</id>
    <title>ChatGPT 4o feels straight up stupid after using o1 and DeepSeek for awhile</title>
    <updated>2025-02-11T19:54:09+00:00</updated>
    <author>
      <name>/u/Getabock_</name>
      <uri>https://old.reddit.com/user/Getabock_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And to think I used to be really impressed with 4o. Crazy. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Getabock_"&gt; /u/Getabock_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1in8vya</id>
    <title>EU mobilizes $200 billion in AI race against US and China</title>
    <updated>2025-02-11T20:44:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt; &lt;img alt="EU mobilizes $200 billion in AI race against US and China" src="https://external-preview.redd.it/X_db72AfOkvUPacuwPMCLwkrfkqSycSFXdfcaogRMnw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6dff9e983d7903f62f20823b94ae4fd34bac0f8" title="EU mobilizes $200 billion in AI race against US and China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1imyp19</id>
    <title>If you want my IT department to block HF, just say so.</title>
    <updated>2025-02-11T13:35:20+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt; &lt;img alt="If you want my IT department to block HF, just say so." src="https://preview.redd.it/h1dbbwhxiiie1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1bafacbc3514f10c5b939ade16c607722c1d9b0" title="If you want my IT department to block HF, just say so." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1dbbwhxiiie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:35:20+00:00</published>
  </entry>
</feed>
