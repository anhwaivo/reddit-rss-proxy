<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-24T15:50:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m7dtpm</id>
    <title>Local llm build, 144gb vram monster</title>
    <updated>2025-07-23T16:25:16+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"&gt; &lt;img alt="Local llm build, 144gb vram monster" src="https://b.thumbs.redditmedia.com/VGM2yiS76HMEN0da0De5H87rkjtR_9prbewrkSRRamQ.jpg" title="Local llm build, 144gb vram monster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still taking a few cables out doing management but just built this beast! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7dtpm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ypyb</id>
    <title>Why is B200 performing similarly to H200? (ArtificialAnalysis)</title>
    <updated>2025-07-24T08:19:04+00:00</updated>
    <author>
      <name>/u/Cyp9715</name>
      <uri>https://old.reddit.com/user/Cyp9715</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;According to ArtificialAnalysis data (from their hardware benchmarks, like at &lt;a href="https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1"&gt;https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1&lt;/a&gt;), the performance difference between NVIDIA's 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don't show a huge gap despite B200's superior specs on paper.&lt;/p&gt; &lt;p&gt;Is this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I'd love to hear your thoughts or any insights from real-world usage!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyp9715"&gt; /u/Cyp9715 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T08:19:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7pqln</id>
    <title>Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s</title>
    <updated>2025-07-24T00:14:43+00:00</updated>
    <author>
      <name>/u/FalseMap1582</name>
      <uri>https://old.reddit.com/user/FalseMap1582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"&gt; &lt;img alt="Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s" src="https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361" title="Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt; &lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt; &lt;p&gt;Here is the full execution command I used:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ./llama-server \ --model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \ --port 11433 \ --host &amp;quot;0.0.0.0&amp;quot; \ --verbose \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --n-gpu-layers 999 \ -ot &amp;quot;blk\.(?:[1-8]?[1379])\.ffn_.*_exps\.weight=CPU&amp;quot; \ --prio 3 \ --threads 32 \ --ctx-size 32768 \ --temp 0.6 \ --min-p 0.0 \ --top-p 0.95 \ --top-k 20 \ --repeat-penalty 1 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FalseMap1582"&gt; /u/FalseMap1582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7HXCQ-4F_oQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T00:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7yswh</id>
    <title>Vibe Coding Anonymous - Satirical take on Vibe Coding</title>
    <updated>2025-07-24T08:24:31+00:00</updated>
    <author>
      <name>/u/Sad_Bandicoot_6925</name>
      <uri>https://old.reddit.com/user/Sad_Bandicoot_6925</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7yswh/vibe_coding_anonymous_satirical_take_on_vibe/"&gt; &lt;img alt="Vibe Coding Anonymous - Satirical take on Vibe Coding" src="https://external-preview.redd.it/NnN1bGpqdTk4c2VmMUXUTLumsNNF9cjJi_w3n1JWDKrihqTu6hcB78F4gCsV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78fd23ef4098cf00609f599db3a32872c097a5cc" title="Vibe Coding Anonymous - Satirical take on Vibe Coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Bandicoot_6925"&gt; /u/Sad_Bandicoot_6925 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vui02yr68sef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7yswh/vibe_coding_anonymous_satirical_take_on_vibe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7yswh/vibe_coding_anonymous_satirical_take_on_vibe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T08:24:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7o3u8</id>
    <title>Is there a future for local models?</title>
    <updated>2025-07-23T23:01:46+00:00</updated>
    <author>
      <name>/u/ASTRdeca</name>
      <uri>https://old.reddit.com/user/ASTRdeca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASTRdeca"&gt; /u/ASTRdeca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T23:01:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7fwhl</id>
    <title>Google DeepMind release Mixture-of-Recursions</title>
    <updated>2025-07-23T17:43:58+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : &lt;a href="https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR"&gt;https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T17:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m87q21</id>
    <title>Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!</title>
    <updated>2025-07-24T15:38:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"&gt; &lt;img alt="Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!" src="https://external-preview.redd.it/NzhobXVycW5idWVmMYlhzbFataawvWX66V9K_jpKKHiNyf2rK1xSvPZF5vG3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76f3889ea17a471e18de4adc6fcaeee9fddc9d20" title="Voxtral WebGPU: State-of-the-art audio transcription directly in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This demo runs Voxtral-Mini-3B, a new audio language model from Mistral, enabling state-of-the-art audio transcription directly in your browser! Everything runs locally, meaning none of your data is sent to a server (and your transcripts are stored on-device).&lt;/p&gt; &lt;p&gt;Important links: - Model: &lt;a href="https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX"&gt;https://huggingface.co/onnx-community/Voxtral-Mini-3B-2507-ONNX&lt;/a&gt; - Demo: &lt;a href="https://huggingface.co/spaces/webml-community/Voxtral-WebGPU"&gt;https://huggingface.co/spaces/webml-community/Voxtral-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9p0p7mqnbuef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m87q21/voxtral_webgpu_stateoftheart_audio_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T15:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7wqi3</id>
    <title>Tool Use Reasoning Dataset Release on Huggingface</title>
    <updated>2025-07-24T06:15:23+00:00</updated>
    <author>
      <name>/u/interstellar-ninja</name>
      <uri>https://old.reddit.com/user/interstellar-ninja</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7wqi3/tool_use_reasoning_dataset_release_on_huggingface/"&gt; &lt;img alt="Tool Use Reasoning Dataset Release on Huggingface" src="https://preview.redd.it/w54k1k58lref1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14ab53a6d727323250320d7b6f742e07264054cb" title="Tool Use Reasoning Dataset Release on Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;🚀 Released: 50k Rows of Tool-Use Reasoning Dataset on Huggingface!&lt;/h2&gt; &lt;p&gt;I've just published a &lt;strong&gt;50,000-row dataset compilation&lt;/strong&gt; focused on &lt;strong&gt;tool-use reasoning&lt;/strong&gt;, now live on Huggingface!&lt;/p&gt; &lt;h3&gt;🧠 What’s Inside?&lt;/h3&gt; &lt;p&gt;This dataset covers key &lt;strong&gt;BFCL scenarios&lt;/strong&gt; for tool-use reasoning: - 🔧 &lt;strong&gt;Single-turn tool-use&lt;/strong&gt; - 🔁 &lt;strong&gt;Multi-turn tool-use&lt;/strong&gt; - 🧩 &lt;strong&gt;Multi-step tool-use&lt;/strong&gt; - 🎯 &lt;strong&gt;Relevance reasoning&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;We've enhanced previous &lt;strong&gt;Hermes function calling datasets&lt;/strong&gt; and other &lt;strong&gt;open-source tool-use datasets&lt;/strong&gt;, enriching them with &lt;strong&gt;reasoning traces&lt;/strong&gt; for deeper learning.&lt;/h2&gt; &lt;h3&gt;📂 Dataset:&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Hermes Tool Use Reasoning Dataset&lt;/strong&gt;&lt;br /&gt; 🔗 &lt;a href="https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use"&gt;https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;🛠️ How It Was Built:&lt;/h3&gt; &lt;p&gt;We used &lt;a href="https://github.com/NousResearch/atropos/pull/160"&gt;&lt;strong&gt;Nous Research's Atropos&lt;/strong&gt;&lt;/a&gt; to create a &lt;strong&gt;multi-turn tool-use RL environment&lt;/strong&gt; with: - ✅ &lt;strong&gt;Turn-based &amp;amp; trajectory-based rewards&lt;/strong&gt; - 🔄 &lt;strong&gt;Rejection sampling-based SFT dataset generation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This supports better generalization for models needing structured multi-turn reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/interstellar-ninja"&gt; /u/interstellar-ninja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w54k1k58lref1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7wqi3/tool_use_reasoning_dataset_release_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7wqi3/tool_use_reasoning_dataset_release_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T06:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m86wxa</id>
    <title>had to fine-tune qwen since llama sucks at summarizing</title>
    <updated>2025-07-24T15:07:53+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m86wxa/had_to_finetune_qwen_since_llama_sucks_at/"&gt; &lt;img alt="had to fine-tune qwen since llama sucks at summarizing" src="https://external-preview.redd.it/MndnZnFrMjNkc2VmMZ2wNVo-OE2bciGH4sJ2rG79auy1VwP-dEBcS0EBYyDD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ead9548cdb025060e86d842022befdbc957edbe1" title="had to fine-tune qwen since llama sucks at summarizing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; - Fine-tuned Qwen3 1.7B - called HyprLLM - which outperforms llama 3.2 3B in summarization for user experience because &amp;quot;vanilla&amp;quot; models suck at summarization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt; - I am building an &lt;a href="https://github.com/fastrepl/hyprnote"&gt;open-source&lt;/a&gt; privacy-first AI notetaker for people in compliance-sensitive environments. It uses on-device AI models to process everything locally. Used to use llama 3.2 3B q8 which sucks at summarizing so had to post-train a new model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Selection&lt;/strong&gt; - Juggled between Gemma and Qwen. But found Qwen to show more promising results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Preparing&lt;/strong&gt; - Since I can't get user data, I had to create a pipeline for synthetic data generation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt; - Just boring stuff. Used Modal.&lt;/p&gt; &lt;p&gt;Planning to fine-tune whisper as well. Also trying to create next version for HyprLLM for multi-lingual support; our user base is global.&lt;/p&gt; &lt;p&gt;Would love to get any tips on synthetic dataset generation or suggestions on models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/37dhjk23dsef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m86wxa/had_to_finetune_qwen_since_llama_sucks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m86wxa/had_to_finetune_qwen_since_llama_sucks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T15:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7u02i</id>
    <title>Vibe Coded with Qwen 3 Coder in &lt;1 hour</title>
    <updated>2025-07-24T03:42:26+00:00</updated>
    <author>
      <name>/u/ryanwang4thepeople</name>
      <uri>https://old.reddit.com/user/ryanwang4thepeople</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"&gt; &lt;img alt="Vibe Coded with Qwen 3 Coder in &amp;lt;1 hour" src="https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0673937350b3a69d4a36c45bd18cf02f98922d3b" title="Vibe Coded with Qwen 3 Coder in &amp;lt;1 hour" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It's not perfect, but it doesn't have to be for my use case. &lt;/p&gt; &lt;p&gt;I tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You're losing if you're not using AI for at least prototyping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryanwang4thepeople"&gt; /u/ryanwang4thepeople &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vr5d47x6tqef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T03:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7kkyn</id>
    <title>Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.</title>
    <updated>2025-07-23T20:40:28+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"&gt; &lt;img alt="Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity." src="https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7" title="Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/krjfba3oqoef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T20:40:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m820ry</id>
    <title>I used a local LLM and http proxy to create a "Digital Twin" from my web browsing for my AI agents</title>
    <updated>2025-07-24T11:37:24+00:00</updated>
    <author>
      <name>/u/kuaythrone</name>
      <uri>https://old.reddit.com/user/kuaythrone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m820ry/i_used_a_local_llm_and_http_proxy_to_create_a/"&gt; &lt;img alt="I used a local LLM and http proxy to create a &amp;quot;Digital Twin&amp;quot; from my web browsing for my AI agents" src="https://external-preview.redd.it/hcMsPXl9sueJ7whcsnuoPIiGx_1AAO5_J25JC13RT88.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c930ae840780c055e12d46587ee78dbe04d08779" title="I used a local LLM and http proxy to create a &amp;quot;Digital Twin&amp;quot; from my web browsing for my AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open-source tool called &lt;strong&gt;Digital Twin Proxy&lt;/strong&gt; that uses a local LLM (via Ollama) to analyze my browsing history and create a personal &amp;quot;digital twin.&amp;quot; This gives my other AI agents real-time context about what I'm working on.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/kstonekuan/digital-twin-proxy"&gt;https://github.com/kstonekuan/digital-twin-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It works by routing traffic through a Squid proxy, and then a Rust app sends the logs to a local model (I'm using Llama 3) for analysis. This way, I can create a more personalized AI experience without my data ever leaving my machine.&lt;/p&gt; &lt;p&gt;The goal is to enable &amp;quot;context engineering,&amp;quot; where agents can anticipate needs or tailor responses based on my current web activity.&lt;/p&gt; &lt;p&gt;I'd love to get feedback, let me know what you think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuaythrone"&gt; /u/kuaythrone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kstonekuan/digital-twin-proxy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m820ry/i_used_a_local_llm_and_http_proxy_to_create_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m820ry/i_used_a_local_llm_and_http_proxy_to_create_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T11:37:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ufyb</id>
    <title>KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly.</title>
    <updated>2025-07-24T04:05:19+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"&gt; &lt;img alt="KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly." src="https://preview.redd.it/nylqnllzxqef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10b88450320c1a803baf4cb0625160a4299439c8" title="KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am not affiliated with the model creators&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nylqnllzxqef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T04:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m84s47</id>
    <title>How do you keep AI outputs from sounding AI?</title>
    <updated>2025-07-24T13:43:00+00:00</updated>
    <author>
      <name>/u/resiros</name>
      <uri>https://old.reddit.com/user/resiros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI-generated content is easy to spot these days: &lt;/p&gt; &lt;p&gt;– The em dashes&lt;br /&gt; – The “It’s not X, but Y”&lt;br /&gt; – Snappy one-line sentences&lt;br /&gt; – Lots of emojis&lt;br /&gt; ...&lt;/p&gt; &lt;p&gt;Many of us use AI to edit text, build chatbots, write reports...&lt;br /&gt; What technique do you use to make sure the output isn't generic AI slop?&lt;/p&gt; &lt;p&gt;Do you use specific prompts? Few-shot examples? Guardrails? Certain models? Fine-tuning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/resiros"&gt; /u/resiros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m84s47/how_do_you_keep_ai_outputs_from_sounding_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m84s47/how_do_you_keep_ai_outputs_from_sounding_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m84s47/how_do_you_keep_ai_outputs_from_sounding_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T13:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7dmy2</id>
    <title>Encouragement of "Open-Source and Open-Weight AI" is now the official policy of the U.S. government.</title>
    <updated>2025-07-23T16:18:12+00:00</updated>
    <author>
      <name>/u/GlowiesEatShitAndDie</name>
      <uri>https://old.reddit.com/user/GlowiesEatShitAndDie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"&gt; &lt;img alt="Encouragement of &amp;quot;Open-Source and Open-Weight AI&amp;quot; is now the official policy of the U.S. government." src="https://preview.redd.it/736cx17efnef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b6dc537086eca79402f273c84f9cfeda0bb9e59" title="Encouragement of &amp;quot;Open-Source and Open-Weight AI&amp;quot; is now the official policy of the U.S. government." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full text: &lt;a href="https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf"&gt;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlowiesEatShitAndDie"&gt; /u/GlowiesEatShitAndDie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/736cx17efnef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:18:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7k4ix</id>
    <title>Google has shared the system prompt that got Gemini 2.5 Pro IMO 2025 Gold Medal 🏅</title>
    <updated>2025-07-23T20:23:02+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alphaxiv.org/abs/2507.15855"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T20:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m82lwo</id>
    <title>Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs</title>
    <updated>2025-07-24T12:07:03+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"&gt; &lt;img alt="Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs" src="https://external-preview.redd.it/4EDmTvD3Q75TbsFu3bs2bpESx7dmxOeoPUkJraEGC8I.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42380fe0539f546fdb60963fca95595cf9e80e4c" title="Leaked List Shows Which Websites Contractors Can Use to Train Anthropic's LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BI obtained an internal list of websites that could and couldn't be used for training Anthropic's latest AI models. &lt;/p&gt; &lt;p&gt;Anthropic's contractor Surge AI left the list fully public on Google Docs. &lt;/p&gt; &lt;p&gt;'Sites you can use' include Bloomberg, Harvard, &amp;amp; the Mayo Clinic.&lt;/p&gt; &lt;p&gt;Many of the whitelisted sources copyright or otherwise restrict their content. &lt;/p&gt; &lt;p&gt;At least 3 - the Mayo Clinic, Cornell University, &amp;amp; Morningstar - told BI they didn't have any AI training agreements with Anthropic.&lt;/p&gt; &lt;p&gt;The spreadsheet also includes a blacklist of websites that Surge AI's gig workers were &amp;quot;now disallowed&amp;quot; from using. &lt;/p&gt; &lt;p&gt;The blacklist includes companies like the NYT &amp;amp; Reddit which have sued AI startups for scraping without permission.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.businessinsider.com/anthropic-surge-ai-leaked-list-sites-2025-7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m82lwo/leaked_list_shows_which_websites_contractors_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:07:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m85v3a</id>
    <title>Running an LLM on the Wii</title>
    <updated>2025-07-24T14:27:00+00:00</updated>
    <author>
      <name>/u/leavesandautumn222</name>
      <uri>https://old.reddit.com/user/leavesandautumn222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"&gt; &lt;img alt="Running an LLM on the Wii" src="https://external-preview.redd.it/ejZqYmpubncwdWVmMUYlGZuGY306YnN9DXXslwFYWelDZ0l3sy5Wrjfi7S4v.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe0ab84525e2cc55b31752cd441c3263e798f588" title="Running an LLM on the Wii" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leavesandautumn222"&gt; /u/leavesandautumn222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8hvd0nnw0uef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m85v3a/running_an_llm_on_the_wii/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m868na</id>
    <title>The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license.</title>
    <updated>2025-07-24T14:42:00+00:00</updated>
    <author>
      <name>/u/ru_cyber</name>
      <uri>https://old.reddit.com/user/ru_cyber</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt; &lt;img alt="The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license." src="https://external-preview.redd.it/vq0t4Qoop35yGxU9mHt1fkSFdKkcFltnPl3gzZiILug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7af7edb7da1fe811056d8c05b8f8d8acd8fdb89" title="The agent-based RP UI 'Astrisk' is now fully open-source under a GPL license." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Just wanted to share some exciting news for anyone here who's into deep, long-form roleplaying. The team behind &lt;a href="https://astrsk.ai"&gt;Astrsk&lt;/a&gt;, a desktop app for RP that's been in development for about six months, has just announced they are going &lt;strong&gt;fully open source&lt;/strong&gt; under the GPL license!&lt;/p&gt; &lt;p&gt;As a fan of the project, I think this is a huge deal for the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The most important link first:&lt;/strong&gt; &lt;a href="https://github.com/astrskai/astrsk"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1m868na/video/zk1ui4ctytef1/player"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So, what is Astrsk and why is it interesting?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;At its core, Astrsk is a UI for RP, but its main differentiator is the &lt;strong&gt;agentic workflow&lt;/strong&gt;. I've been following it, and the concept is very cool because it moves beyond a simple prompt-response loop.&lt;/p&gt; &lt;p&gt;To make this concrete, let's look at the default workflow it comes with, called &lt;strong&gt;SAGA&lt;/strong&gt;. It's a four-step pipeline that mimics how a human Game Master thinks, breaking down the task of generating a response into logical steps.&lt;/p&gt; &lt;p&gt;Here's how it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Step 1: The Analyzer Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the GM's logical brain. It looks at what your character just did and analyzes it against the current game state.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It answers the questions: &amp;quot;Is the player's action possible? What are the immediate consequences based on game rules or a dice roll?&amp;quot; It validates the action and determines the outcome.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 2: The Planner Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the creative storyteller. It takes the Analyzer's output and designs the narrative response.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It decides how NPCs will react to the player's action (e.g., with anger, surprise, or a counter-move). It plans the scene, sets the emotional tone, and prepares the key information for the next agent.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 3: The Actor Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the performer. It takes the Planner's script and turns it into the actual text you read.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It writes the scene narration and performs the detailed dialogue for one main NPC, giving them a distinct voice and personality. Other NPCs are handled through the narration, keeping the focus clear.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step 4: The Formatter Agent&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Job:&lt;/strong&gt; This is the final editor. It’s a non-AI, rule-based agent that ensures readability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In Practice:&lt;/strong&gt; It takes the text from the Actor and cleans it up with simple markdown. It automatically wraps actions in italics, dialogue in &amp;quot;quotes&amp;quot;, and adds &lt;strong&gt;bold&lt;/strong&gt; for emphasis, making the final output clean and easy to read without changing the content.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This pipeline approach allows for incredible consistency and detail. And since you can assign different models to different agents (a key feature!), you could use a large, powerful model for the creative Planner and a faster, smaller model for the structured Analyzer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it compare to the greats like SillyTavern / Agnaistic?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;From what I've seen, while projects like ST/Agnaistic are amazing for chat-based RP, Astrsk seems to aim for a different goal. It feels less like a chat interface and more like a tool for collaborative storytelling, almost like having an AI Dungeon Master powered by a framework of agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent-based generation:&lt;/strong&gt; The core of Astrsk, designed for more coherent and long-term storytelling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sleek, Customizable UI:&lt;/strong&gt; A really polished interface where you can tweak settings directly in the app. No more digging through config files to change things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-Agent Model Assignment:&lt;/strong&gt; This is a killer feature. You can assign a different LLM endpoint to each agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;True Cross-Platform Support:&lt;/strong&gt; The team provides native builds for Windows, macOS, and Linux. This means you can just download and run it — no need to be an engineer or fight with dependencies to get started.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend Agnostic:&lt;/strong&gt; Connects to any OpenAI-compatible API, so it works with your existing setup (Oobabooga, KoboldCPP, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Open Source Move&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;According to their announcement, the team wants to build the project out in the open, getting feedback and contributions from the community, which is fantastic news for all of us. The project is still young, but the foundation is solid.&lt;/p&gt; &lt;p&gt;I'm not affiliated with the developers, just a user who is really excited about the project's potential and wanted to share it with a community that might appreciate the tech.&lt;/p&gt; &lt;p&gt;Definitely worth checking out the &lt;a href="https://github.com/astrskai/astrsk"&gt;https://github.com/astrskai/astrsk&lt;/a&gt;, especially if the idea of an agentic approach to RP sounds interesting to you. The team is looking for feedback, bug reports, and contributors.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ru_cyber"&gt; /u/ru_cyber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m868na/the_agentbased_rp_ui_astrisk_is_now_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7p7ek</id>
    <title>I optimized a Flappy Bird diffusion world model to run locally on my phone</title>
    <updated>2025-07-23T23:50:32+00:00</updated>
    <author>
      <name>/u/fendiwap1234</name>
      <uri>https://old.reddit.com/user/fendiwap1234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"&gt; &lt;img alt="I optimized a Flappy Bird diffusion world model to run locally on my phone" src="https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3065cd09eac2517feb68e9966241504d4fbb9eb4" title="I optimized a Flappy Bird diffusion world model to run locally on my phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;demo: &lt;a href="https://flappybird.njkumar.com/"&gt;https://flappybird.njkumar.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blogpost: &lt;a href="https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/"&gt;https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. &lt;/p&gt; &lt;p&gt;World models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.&lt;/p&gt; &lt;p&gt;Let me know what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fendiwap1234"&gt; /u/fendiwap1234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71l2pz57opef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T23:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ts5g</id>
    <title>Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found</title>
    <updated>2025-07-24T03:30:49+00:00</updated>
    <author>
      <name>/u/West-Chocolate2977</name>
      <uri>https://old.reddit.com/user/West-Chocolate2977</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt; &lt;img alt="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" src="https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688" title="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt; &lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt; &lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt; &lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt; &lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt; &lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Chocolate2977"&gt; /u/West-Chocolate2977 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T03:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7vlpn</id>
    <title>Anthropic’s New Research: Giving AI More "Thinking Time" Can Actually Make It Worse</title>
    <updated>2025-07-24T05:09:23+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt; &lt;img alt="Anthropic’s New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" src="https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69b7dca05f4a287acca18082926d12008127ef3d" title="Anthropic’s New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read a fascinating—and honestly, a bit unsettling—research paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt; &lt;p&gt;Turns out, that’s not always true.&lt;/p&gt; &lt;p&gt;Their paper, “Inverse Scaling in Test-Time Compute,” reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt; &lt;p&gt;So what’s going wrong?&lt;/p&gt; &lt;p&gt;The paper breaks it down across several models and tasks. Here's what they found:&lt;/p&gt; &lt;p&gt;🧠 More Thinking, More Problems&lt;/p&gt; &lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy—especially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt; &lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt; &lt;p&gt;Overfit: OpenAI’s o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt; &lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt; &lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt; &lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors—like self-preservation in Claude Sonnet 4.&lt;/p&gt; &lt;p&gt;Tasks Where This Shows Up&lt;/p&gt; &lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt; &lt;p&gt;Simple counting with distractors&lt;/p&gt; &lt;p&gt;Regression with spurious features&lt;/p&gt; &lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt; &lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt; &lt;p&gt;🧩 Why This Matters&lt;/p&gt; &lt;p&gt;This isn’t just a weird performance quirk—it has deep implications for AI safety, reliability, and interpretability. The paper also points out “Chain-of-Thought Faithfulness” issues: the reasoning steps models output often don’t reflect what’s actually driving their answer.&lt;/p&gt; &lt;p&gt;That’s a huge deal for alignment and safety. If we can’t trust the model’s step-by-step logic, then we can’t audit or guide their reasoning—even if it looks rational on the surface.&lt;/p&gt; &lt;p&gt;⚠️ Bottom Line&lt;/p&gt; &lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI’s reasoning tokens and Anthropic’s extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn’t always better—and can sometimes make things worse&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.14417"&gt;Research Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/srk1p5og9ref1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T05:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m85vhw</id>
    <title>new mistralai/Magistral-Small-2507 !?</title>
    <updated>2025-07-24T14:27:29+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt; &lt;img alt="new mistralai/Magistral-Small-2507 !?" src="https://external-preview.redd.it/tgkQSXgEmVg0U0WBS2WE-yi3ZEgfIauWskF7DtJUClg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aa4619d7f8ff888c9274c7c014531dcd45ff12e" title="new mistralai/Magistral-Small-2507 !?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m85vhw/new_mistralaimagistralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T14:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m80gsn</id>
    <title>GLM-4.5 Is About to Be Released</title>
    <updated>2025-07-24T10:10:17+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt; &lt;img alt="GLM-4.5 Is About to Be Released" src="https://external-preview.redd.it/y8jq7uBPn8clvEuxpE2Zw8a3WYZcpZ0Z-EaGdldn7RM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9908a35901687f5249e56f8b7bb3e593bf9a82e" title="GLM-4.5 Is About to Be Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vLLM commit: &lt;a href="https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29"&gt;https://github.com/vllm-project/vllm/commit/85bda9e7d05371af6bb9d0052b1eb2f85d3cde29&lt;/a&gt;&lt;/p&gt; &lt;p&gt;modelscope/ms-swift commit: &lt;a href="https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7"&gt;https://github.com/modelscope/ms-swift/commit/a26c6a1369f42cfbd1affa6f92af2514ce1a29e7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e"&gt;https://preview.redd.it/hda2uymxqsef1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f058ef9a9e5e86553ef702ab8914c00fdb0763e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're going to get a 106B-A12B (Air) model and a 355B-A32B model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m80gsn/glm45_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T10:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m83644</id>
    <title>China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks</title>
    <updated>2025-07-24T12:33:36+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt; &lt;img alt="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" src="https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa69848c81b950052de8eb2024c390e13024272" title="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:33:36+00:00</published>
  </entry>
</feed>
