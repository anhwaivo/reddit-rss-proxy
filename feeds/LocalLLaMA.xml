<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-06T23:48:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ijet6p</id>
    <title>Any interest in a poker engine?</title>
    <updated>2025-02-06T22:02:03+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I was playing around a bit using rust and I was thinking like, there are already models that are better than most players, so creating a model for Texas Holdem is definitely something that should/would be feasible. &lt;/p&gt; &lt;p&gt;First thing, no, I don't have a model (yet) I could share. But I thought, maybe others are also interested in the environment without having to program a whole environment?&lt;/p&gt; &lt;p&gt;The engine itself is able to play ~180k hands per second on my server with an AMD 8700GE. Of course, it's optimized for multiprocessing, and I tried to keep the heap usage as low as possible. The performance goes down ~40-50%, when cloning the state for further usage with the model, so 90-100k hands per second are still possible in a full simulation on my server. &lt;/p&gt; &lt;p&gt;The project is divided into multiple crates for the core, engine, cli, simulation, and agents. All with comprehensive unit tests and benchmarks for Criterion/Flamegraph, traits to keep things generic, and so on. The whole project is laid out for reinforcement learning, so the traits I have match those things you'll need for that.&lt;/p&gt; &lt;p&gt;If people are interested in it, I'll clean up the code a bit and probably release it this weekend. If nobody is interested, the code will stay dirty on my machine.&lt;/p&gt; &lt;p&gt;So let me know if you're interested in it (or not)!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijet6p/any_interest_in_a_poker_engine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijet6p/any_interest_in_a_poker_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijet6p/any_interest_in_a_poker_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T22:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirej3</id>
    <title>The New Gemini Pro 2.0 Experimental sucks Donkey Balls.</title>
    <updated>2025-02-06T01:58:43+00:00</updated>
    <author>
      <name>/u/Odd-Environment-7193</name>
      <uri>https://old.reddit.com/user/Odd-Environment-7193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wow. Last night, after a long coding bender I heard the great news that Gemini were releasing some new models. I woke up this morning super excited to try them.&lt;/p&gt; &lt;p&gt;My first attempt was a quick OCR with Flesh light 2.0 and I was super impressed with the Speed. This thing is going to make complex OCR an absolute breeze. I cannot wait to incorporate this into my apps. I reckon it's going to cut the processing times in half. (Christmas came early)&lt;/p&gt; &lt;p&gt;Then I moved onto testing the Gemini 2.0 Pro Experimental.&lt;/p&gt; &lt;p&gt;How disappointing... This is such a regression from 1206. I could immediately see the drop in the quality of the tasks I've been working on daily like coding.&lt;/p&gt; &lt;p&gt;It makes shit tons of mistakes. The code that comes out doesn't have valid HTML (Super basic task) and it seems to want to interject and refactor code all the time without permission.&lt;/p&gt; &lt;p&gt;I don't know what the fuck these people are doing. Every single release it's like this. They just can't seem to get it right. 1206 has been a great model, and I've been using it as my daily driver for quite some time. I was actually very impressed with it and had they just released 1206 as Gemini 2.0 pro EXP I would have been stoked. This is an absolute regression.&lt;/p&gt; &lt;p&gt;I have seen this multiple times now with Google products. The previous time the same thing happened with 0827 and then Gemini 002.&lt;/p&gt; &lt;p&gt;For some reason at that time, they chose to force concise answers into everything, basically making it impossible to get full lengthy responses. Even with system prompts, it would just keep shortening code, adding comments into everything and basically forcing this dogshit concise mode behavior into everything.&lt;/p&gt; &lt;p&gt;Now they've managed to do it again. This model is NOT better than 1206. The benchmarks or whatever these people are aiming to beat are just an illusion. If your model cannot do simple tasks like outputting valid code without trying to force refactoring it is just a hot mess.&lt;/p&gt; &lt;p&gt;Why can't they get this right? They seem to regress a lot on updates. I've had discussions with people in the know, and apparently it's difficult to juggle the various needs of all the different types of people. Where some might like lengthy thorough answers for example, others might find that annoying and &amp;quot;too verbose&amp;quot;. So basically we get stuck with these half arsed models that don't seem to excel in anything in particular.&lt;/p&gt; &lt;p&gt;I use these models for coding and for writing, which has always been the case. I might be in the minority of users and just be too entitled about this. But jesus, what a disappointment.&lt;/p&gt; &lt;p&gt;I am not shitting you, when I say I would rather use deepseek than whatever this is. It's ability to give long thorough answers, without changing parts of code unintentionally is extremely valuable to my use cases.&lt;/p&gt; &lt;p&gt;Google is the biggest and most reliable when it comes to serving their models though, and I absolutely love the flash models for building apps. So you could say I am a major lover and hater of them. It's always felt this way. A genuine love-hate relationship. I am secretly rooting for their success but I absolutely loathe some of the things they do and am really surprised they haven't surpassed chatgpt/claude yet.. Like how the fuck?&lt;/p&gt; &lt;p&gt;Maybe it's time to outsource their LLM production to CHHHIIIIINNAAAA. Just like everything else. Hahahaa&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Environment-7193"&gt; /u/Odd-Environment-7193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T01:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij4c7h</id>
    <title>Unpopular opinion. The chatbot arena benchmark is not useless, rather it is misunderstood. It is not necessarily an hard benchmark, rather it is a benchmark of "what if the LLM would answer common queries for search engines"?</title>
    <updated>2025-02-06T14:54:12+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From another thread:&lt;/p&gt; &lt;p&gt;The gemini flash thinking is great on chatbot arena. But why this? Before one jumps on the bandwagon &amp;quot;chatbot arena sucks&amp;quot; one has to understand what is tested there. Many say &amp;quot;human preferences&amp;quot; but I think it is a bit different.&lt;/p&gt; &lt;p&gt;Most likely on chatbot arena people test the LLMs with relatively simple questions. Akin to &amp;quot;tell me how to write a function in X&amp;quot; rather than &amp;quot;this function doesn't work, fix it&amp;quot;.&lt;/p&gt; &lt;p&gt;Chatbot arena (at least for the category overall) is great to say &amp;quot;which model would be great for everyday use instead of searching the web&amp;quot;.&lt;/p&gt; &lt;p&gt;And I think that some companies, like google, are optimizing exactly for that. Hence Chatbot arena is relevant for them. They want to have models that can substitute or complement their search engine.&lt;/p&gt; &lt;p&gt;More often than not on reddit people complain that Claude or other models do not excel in chatbot arena (again, the overall category), and thus the benchmark sucks. But that is because those people use the LLMs differently from the voters in chatbot arena.&lt;/p&gt; &lt;p&gt;Asking an LLM to help on a niche (read: not that common in internet) coding or debugging problem is harder than a &amp;quot;I use the LLM rather than the search&amp;quot; request. Hence some models are good in hard benchmarks but less good in a benchmark that at the end measures the &amp;quot;substitute a search engine for common questions&amp;quot; metric.&lt;/p&gt; &lt;p&gt;Therefore the point &amp;quot;I have a feeling all the current evals those model releases are using are just too far away from real work/life scenarios.&amp;quot; is somewhat correct. If a model optimizes for Chatbot arena / search engine usage, then of course it is unlikely to be trained to solve consistently niche problems.&lt;/p&gt; &lt;p&gt;And even if one has a benchmark that is more relevant to the use case (say: aider, livebench and what not). If one has a LLM that is right 60% of the time, there is still a lot of work to do for the person to fill the gaps.&lt;/p&gt; &lt;p&gt;Then it also depends on the prompts - I found articles in the past where prompts where compared and some could really extract from from an LLM. Those prompts are standardized and optimized in &amp;quot;ad hoc&amp;quot; benchmarks. In Chatbot arena the prompts could be terrible, hence once again what is tested is &amp;quot;what people would type in a LLM based search engine&amp;quot;.&lt;/p&gt; &lt;p&gt;IMO what the people from LMSYS offer as hard human based benchmarking offers are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the category hard prompts for general cases&lt;/li&gt; &lt;li&gt;the category longer query for general cases (most of the bullshit prompts IMO are short)&lt;/li&gt; &lt;li&gt;(a bit unsure here) the category multi turn. In a 1:1 usage, we ask many questions in the same conversation with a model. On chatbot arena people vote mostly on one shot questions, end of it. That is also a huge difference from personal LLM use.&lt;/li&gt; &lt;li&gt;for coding, the WebDev Arena Leaderboard - there Claude is #1 by a mile (so far) . Claude 3.5 (from October 24) has 1250 Elo points, Deepseek R1 1210, o3 mini-high 1161, the next non-thinking model, Gemini exp 1206 has 1025. The distance Claude 3.5 vs Gemini exp is over 200 points, is massive and thus I think that actually Claude &amp;quot;thinks&amp;quot;, at least in some domains. It cannot be that is so strong without thinking.&lt;/li&gt; &lt;li&gt;It would be cool if Chatbot Arena would add &amp;quot;hard prompts&amp;quot; for each specific subcategory. For example &amp;quot;math hard prompts&amp;quot;, &amp;quot;coding hard prompts&amp;quot; and so on. But I guess that would dilute the votes too much and would require too much classification every week.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This to say, I think chatbot arena is very useful IF seen in the proper context, that is mostly &amp;quot;search engine / stack overflow replacement&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij4c7h/unpopular_opinion_the_chatbot_arena_benchmark_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T14:54:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iisj7j</id>
    <title>Open WebUI drops 3 new releases today. Code Interpreter, Native Tool Calling, Exa Search added</title>
    <updated>2025-02-06T02:55:42+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;0.5.8 had a slew of new adds. 0.5.9 and 0.5.10 seemed to be minor bug fixes for the most part. From their release page:&lt;/p&gt; &lt;p&gt;🖥️ Code Interpreter: Models can now execute code in real time to refine their answers dynamically, running securely within a sandboxed browser environment using Pyodide. Perfect for calculations, data analysis, and AI-assisted coding tasks!&lt;/p&gt; &lt;p&gt;💬 Redesigned Chat Input UI: Enjoy a sleeker and more intuitive message input with improved feature selection, making it easier than ever to toggle tools, enable search, and interact with AI seamlessly.&lt;/p&gt; &lt;p&gt;🛠️ Native Tool Calling Support (Experimental): Supported models can now call tools natively, reducing query latency and improving contextual responses. More enhancements coming soon!&lt;/p&gt; &lt;p&gt;🔗 Exa Search Engine Integration: A new search provider has been added, allowing users to retrieve up-to-date and relevant information without leaving the chat interface.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T02:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbnit</id>
    <title>Tiny Data, Strong Reasoning if you have $50</title>
    <updated>2025-02-06T19:54:23+00:00</updated>
    <author>
      <name>/u/Xiwei</name>
      <uri>https://old.reddit.com/user/Xiwei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;s1K&lt;/h1&gt; &lt;p&gt;Uses a small, curated dataset (1,000 samples) and &amp;quot;budget forcing&amp;quot; to achieve competitive AI reasoning, rivalling larger models like OpenAI's o1.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sample Efficiency: Shows that quality &amp;gt; quantity in data. Training the s1-32B model on the s1K dataset only took &lt;strong&gt;26 minutes on 16 NVIDIA H100 GPUs&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Test-Time Scaling: Inspired by o1, increasing compute at inference boosts performance.&lt;/li&gt; &lt;li&gt;Open Source: Promotes transparency and research.&lt;/li&gt; &lt;li&gt;Distillation: s1K leverages a distillation procedure from Gemini 2.0. The s1-32B model, fine-tuned on s1K, nearly matches Gemini 2.0 Thinking on AIME24.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It suggests that AI systems can be more efficient, transparent and controllable.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;p&gt;#AI #MachineLearning #Reasoning #OpenSource #s1K&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2501.19393"&gt;https://arxiv.org/pdf/2501.19393&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiwei"&gt; /u/Xiwei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbnit/tiny_data_strong_reasoning_if_you_have_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbnit/tiny_data_strong_reasoning_if_you_have_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbnit/tiny_data_strong_reasoning_if_you_have_50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij205h</id>
    <title>Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!</title>
    <updated>2025-02-06T13:02:20+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt; &lt;img alt="Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!" src="https://external-preview.redd.it/MI3THquRgDiXtkGwCynZSuvm0Cdq3csr60FcHLC_Xk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ae6c4d987ccc5b9eff4fb95ca0547ed5c8a98a2" title="Experience DeepSeek-R1-Distill-Llama-8B on Your Smartphone with PowerServe and Qualcomm NPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/powerserve-project/PowerServe"&gt;PowerServe&lt;/a&gt; is a &lt;strong&gt;high-speed&lt;/strong&gt; and &lt;strong&gt;easy-to-use&lt;/strong&gt; LLM serving framework for local deployment. You can deploy popular LLMs with our &lt;a href="https://github.com/powerserve-project/PowerServe/blob/main/docs/end_to_end.md"&gt;one-click compilation and deployment&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;PowerServe offers the following advantages:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Lightning-Fast Prefill and Decode&lt;/strong&gt;: Optimized for NPU, achieving over 10x faster prefill speeds compared to llama.cpp, significantly accelerating model warm-up.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Efficient NPU Speculative Inference&lt;/strong&gt;: Supports speculative inference, delivering 2x faster inference speeds compared to traditional autoregressive decoding.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Seamless OpenAI API Compatibility&lt;/strong&gt;: Fully compatible with OpenAI API, enabling effortless migration of existing applications to the PowerServe platform.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Model Support&lt;/strong&gt;: Compatible with mainstream large language models such as &lt;strong&gt;Llama3&lt;/strong&gt;, &lt;strong&gt;Qwen2.5&lt;/strong&gt;, and &lt;strong&gt;InternLM3&lt;/strong&gt;, catering to diverse application needs.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Ease of Use&lt;/strong&gt;: Features one-click deployment for quick setup, making it accessible to everyone.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ij205h/video/dsu4qf4doihe1/player"&gt;Running DeepSeek-R1-Distill-Llama-8B with NPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij205h/experience_deepseekr1distillllama8b_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T13:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ija355</id>
    <title>A Gentle Intro to Running a Local LLM (For Complete Beginners)</title>
    <updated>2025-02-06T18:50:42+00:00</updated>
    <author>
      <name>/u/contextbot</name>
      <uri>https://old.reddit.com/user/contextbot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"&gt; &lt;img alt="A Gentle Intro to Running a Local LLM (For Complete Beginners)" src="https://external-preview.redd.it/htZiSEyUB16U9gyyKeUrjXb4JczPOO0TCspO30BQGiQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8beee75085a759caf2ded5e73f1204392da5d02e" title="A Gentle Intro to Running a Local LLM (For Complete Beginners)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contextbot"&gt; /u/contextbot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.dbreunig.com/2025/02/04/a-gentle-intro-to-running-a-local-llm.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiveqd</id>
    <title>So, Google has no state-of-the-art frontier model now?</title>
    <updated>2025-02-06T05:34:09+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt; &lt;img alt="So, Google has no state-of-the-art frontier model now?" src="https://preview.redd.it/64r0glzkgghe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4b6ad82ec54e92060d2226f5e8ec28c2f2eaf9b" title="So, Google has no state-of-the-art frontier model now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64r0glzkgghe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T05:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiio9u</id>
    <title>Anthropic: ‘Please don’t use AI’</title>
    <updated>2025-02-05T19:36:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt; &lt;img alt="Anthropic: ‘Please don’t use AI’" src="https://external-preview.redd.it/XdLbwNiaDfP6hGsSmn44MWaR_4YQK7L36Ar5RuZkt4s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f5b6dec6d423a65124ea27edb0de0e52f12e6ef" title="Anthropic: ‘Please don’t use AI’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process. We want to understand your personal interest in Anthropic without mediation through an AI system, and we also want to evaluate your non-AI-assisted communication skills. Please indicate ‘Yes’ if you have read and agree.&amp;quot;&lt;/p&gt; &lt;p&gt;There's a certain irony in having one of the biggest AI labs coming against AI applications and acknowledging the enshittification of the whole job application process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/9b1e6af4-94f2-41c6-bb91-96a74b9b2da1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijaxeo</id>
    <title>GitHub Copilot: The agent awakens</title>
    <updated>2025-02-06T19:24:03+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"&gt; &lt;img alt="GitHub Copilot: The agent awakens" src="https://external-preview.redd.it/FNfBYXEZ6fJCMtp9hdpnkrJv53x_UkhrdS6GkDW1sh8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=000f375a894580879f1aa2a41815b8914c25cff3" title="GitHub Copilot: The agent awakens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Today, we are upgrading GitHub Copilot with the force of even more agentic AI – introducing agent mode and announcing the General Availability of Copilot Edits, both in VS Code. We are adding Gemini 2.0 Flash to the model picker for all Copilot users. And we unveil a first look at Copilot’s new autonomous agent, codenamed Project Padawan. From code completions, chat, and multi-file edits to workspace and agents, Copilot puts the human at the center of the creative work that is software development. AI helps with the things you don’t want to do, so you have more time for the things you do.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilrym</id>
    <title>Gemma 3 on the way!</title>
    <updated>2025-02-05T21:43:33+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt; &lt;img alt="Gemma 3 on the way!" src="https://preview.redd.it/q2q4555s4ehe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be6c986a18108bcff251eb781a9cd1a0f4bcbd3" title="Gemma 3 on the way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19"&gt;https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2q4555s4ehe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T21:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1ew9</id>
    <title>lineage-bench benchmark results updated with recently released models</title>
    <updated>2025-02-06T12:29:58+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt; &lt;img alt="lineage-bench benchmark results updated with recently released models" src="https://preview.redd.it/pgf54p7ddihe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02d2e9ae3c8b4d6c2160f1ebcb0f9b3a96964947" title="lineage-bench benchmark results updated with recently released models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pgf54p7ddihe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1ew9/lineagebench_benchmark_results_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwgou</id>
    <title>For coders! free&amp;open DeepSeek R1 &gt; $20 o3-mini with rate-limit!</title>
    <updated>2025-02-06T06:43:10+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt; &lt;img alt="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" src="https://preview.redd.it/n9sntvkvsghe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a41575abce1f1f8a8cb02b9965f65a613e1a0174" title="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9sntvkvsghe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijfskv</id>
    <title>Mistral AI CEO Interview</title>
    <updated>2025-02-06T22:43:59+00:00</updated>
    <author>
      <name>/u/SignalCompetitive582</name>
      <uri>https://old.reddit.com/user/SignalCompetitive582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"&gt; &lt;img alt="Mistral AI CEO Interview" src="https://external-preview.redd.it/19CD9Zbziz4wjyY-KLNZm0d_AXIRPDRzjWqBsfq2Fg8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3051c11f352f91dde0f5af22f09fb4d29e44376e" title="Mistral AI CEO Interview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This interview with Arthur Mensch, CEO of Mistral AI, is incredibly comprehensive and detailed. I highly recommend watching it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalCompetitive582"&gt; /u/SignalCompetitive582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/bzs0wFP_6ck"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T22:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwmsq</id>
    <title>Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost</title>
    <updated>2025-02-06T06:55:03+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt; &lt;img alt="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" src="https://b.thumbs.redditmedia.com/U7IbKXWllKMESakzdcsFfg82O-BgJ0wgsGCf2i_dXrc.jpg" title="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iiwmsq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ija3v4</id>
    <title>DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!</title>
    <updated>2025-02-06T18:51:33+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt; &lt;img alt="DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!" src="https://external-preview.redd.it/GEk7Ll7QhkvaFAkkOayBbV1OyQKQVaWruZ9jP8F9VEw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=342c762f73e43798fe1835ee49e5c48ce5e3e306" title="DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1iqbdg4y3khe1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7695c218715a211f47f5bc37aa8309fc6bb8cc62"&gt;React Renderer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vb2iknfy3khe1.png?width=1370&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a2226fcf5d30f59a1d3453c374e48c16fd82156"&gt;Full tailwind support w/ preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/an4w4onrekhe1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc67cfdb95a8c1990a3f7aaf821bf4297d962977"&gt;Difference viewer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone! I have been getting a lot of real world use this week now with the open-webui-artifacts-overhaul version of open-webui. It has been AMAZING at work and it completely replaced my need for Claude or OpenAI's artifacts. Of course, full disclaimer: I am the creator of this fork -- but all the features requested were from YOU, the community. I didn't realize how much I needed these features in my life, it really brings Open-WebUI up to par with the UI's used provided by SOTA models. &lt;/p&gt; &lt;p&gt;Feel free to try it out yourself! &lt;a href="https://www.github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;https://www.github.com/nick-tonjum/open-webui-artifacts-overhaul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I believe this will be another couple of weeks of real world testing to iron out bugs and implement more features requested by the community. Please feel free to help out and submit Issues and Feature requests.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1xge</id>
    <title>Autiobooks: Automatically convert epubs to audiobooks (kokoro)</title>
    <updated>2025-02-06T12:58:49+00:00</updated>
    <author>
      <name>/u/vosFan</name>
      <uri>https://old.reddit.com/user/vosFan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt; &lt;img alt="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" src="https://external-preview.redd.it/ZWtwaHU5YzBvaWhlMV54VUX8u6k6pXdX7L9L_cCrxwAtDjHSCnwLZyQNJRce.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7c76ff926c140c89393b924cb7e4fd0e235e742" title="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;https://github.com/plusuncold/autiobooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a GUI frontend for Kokoro for generating audiobooks from epubs. The results are pretty good!&lt;/p&gt; &lt;p&gt;PRs are very welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vosFan"&gt; /u/vosFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w21l2ag0oihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:58:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbqky</id>
    <title>Mistral’s new “Flash Answers”</title>
    <updated>2025-02-06T19:57:49+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"&gt; &lt;img alt="Mistral’s new “Flash Answers”" src="https://external-preview.redd.it/Oqw5kk3lifQ1HwlLen4W6BZPZtqltu9AUU6wWFbOBbg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=687ea1cc1b90ede67d331463612f5148431106fd" title="Mistral’s new “Flash Answers”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/onetwoval/status/1887547069956845634?s=46&amp;amp;t=4i240TMN9BFmGRKFS4WP1A"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijauz4</id>
    <title>Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)</title>
    <updated>2025-02-06T19:21:19+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"&gt; &lt;img alt="Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)" src="https://b.thumbs.redditmedia.com/6OGmjsnhti76DWcguefqugQlxvuIkCmz7tZF5JbY5Uo.jpg" title="Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ijauz4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij96e5</id>
    <title>deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++</title>
    <updated>2025-02-06T18:13:29+00:00</updated>
    <author>
      <name>/u/reasonableklout</name>
      <uri>https://old.reddit.com/user/reasonableklout</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"&gt; &lt;img alt="deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++" src="https://external-preview.redd.it/xxUqvQ7bjDufrBkxeXC-RZ_b54GSDiLzEjIobqu9d1M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99ba4ad51d925840f322676fb0bc2f13784c90d1" title="deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reasonableklout"&gt; /u/reasonableklout &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/andrewkchan/deepseek.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iizbxs</id>
    <title>Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way.</title>
    <updated>2025-02-06T10:14:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt; &lt;img alt="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." src="https://external-preview.redd.it/bDJtMXNycmt1aGhlMQxr13kQ4l494R_6FN5L7tr44dIiu9kzOIdUQI5GS5Z5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5b432ad2dfeb081934154500e3fccbe230c81d" title="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/50vlqmrkuhhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T10:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5sma</id>
    <title>Mistral AI just released a mobile app</title>
    <updated>2025-02-06T15:56:41+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"&gt; &lt;img alt="Mistral AI just released a mobile app" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral AI just released a mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/en/news/all-new-le-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T15:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij35u7</id>
    <title>Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN</title>
    <updated>2025-02-06T13:59:31+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt; &lt;img alt="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" src="https://external-preview.redd.it/Z3lrdWp0dmx5aWhlMaQ4EUN4_AgLY98885pUW0pYP7vfo05dn6YTgI9m58bO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=379782dc21a6714fa7105716d9fd647dc31f82ba" title="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gpawbnvlyihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T13:59:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijab77</id>
    <title>Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)</title>
    <updated>2025-02-06T18:59:49+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt; &lt;img alt="Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)" src="https://external-preview.redd.it/to7Gx1lMl0voSDkT7id5Fh2N7SEb6nUJ2HQzl2en4NU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=598db52b5cff8719b6abbc6affa07d300858717d" title="Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;]()! We're excited to introduce reasoning in &lt;a href="https://github.com/unslothai/unsloth/releases/tag/2025-02"&gt;Unsloth&lt;/a&gt; so you can now reproduce R1's &amp;quot;aha&amp;quot; moment locally. You'll only need &lt;strong&gt;7GB of VRAM&lt;/strong&gt; to do it with Qwen2.5 (1.5B).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is done through &lt;strong&gt;GRPO&lt;/strong&gt;, and we've enhanced the entire process to make it use &lt;strong&gt;80% less VRAM&lt;/strong&gt;. Try it in the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;Colab notebook&lt;/a&gt;-GRPO.ipynb) for Llama 3.1 8B!&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Jiayi-Pan/TinyZero"&gt;Tiny-Zero&lt;/a&gt; demonstrated that you could achieve your own &amp;quot;aha&amp;quot; moment with Qwen2.5 (1.5B) - but it required a minimum 4xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same &amp;quot;aha&amp;quot; moment using just a single 7GB VRAM GPU&lt;/li&gt; &lt;li&gt;Previously GRPO only worked with FFT, but we made it work with QLoRA and LoRA.&lt;/li&gt; &lt;li&gt;With 15GB VRAM, you can transform Phi-4 (14B), Llama 3.1 (8B), Mistral (12B), or any model up to 15B parameters into a reasoning model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Blog for more details: &lt;a href="https://unsloth.ai/blog/r1-reasoning"&gt;https://unsloth.ai/blog/r1-reasoning&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;Llama 3.1 8B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B"&gt;Phi-4 14B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B"&gt;Qwen 2.5 3B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B needs ~ 13GB&lt;/td&gt; &lt;td align="left"&gt;Phi-4 14B needs ~ 15GB&lt;/td&gt; &lt;td align="left"&gt;Qwen 3B needs ~7GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I plotted the rewards curve for a specific run:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xj5rtk69fkhe1.png?width=2057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a25a3a96393be54bc9687258df49329a56d530d7"&gt;https://preview.redd.it/xj5rtk69fkhe1.png?width=2057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a25a3a96393be54bc9687258df49329a56d530d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth also now has 20x faster inference via vLLM! Please update Unsloth and vLLM via:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install --upgrade --no-cache-dir --force-reinstall unsloth_zoo unsloth vllm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;P.S. thanks for all your overwhelming love and support for our R1 Dynamic 1.58-bit GGUF last week! Things like this really keep us going so thank you again.&lt;/p&gt; &lt;p&gt;Happy reasoning!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5yf2</id>
    <title>How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use</title>
    <updated>2025-02-06T16:03:05+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt; &lt;img alt="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" src="https://a.thumbs.redditmedia.com/5GvbBLMtQKog3tISnQ2IpuYVRJEYPT5-0ptjxlTHnR4.jpg" title="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to share something I built after my long health journey. For 5 years, I struggled with mysterious symptoms - getting injured easily during workouts, slow recovery, random fatigue, joint pain. I spent over $100k visiting more than 30 hospitals and specialists, trying everything from standard treatments to experimental protocols at longevity clinics. Changed diets, exercise routines, sleep schedules - nothing seemed to help.&lt;/p&gt; &lt;p&gt;The most frustrating part wasn't just the lack of answers - it was how fragmented everything was. Each doctor only saw their piece of the puzzle: the orthopedist looked at joint pain, the endocrinologist checked hormones, the rheumatologist ran their own tests. No one was looking at the whole picture. It wasn't until I visited a rheumatologist who looked at the combination of my symptoms and genetic test results that I learned I likely had an autoimmune condition.&lt;/p&gt; &lt;p&gt;Interestingly, when I fed all my symptoms and medical data from before the rheumatologist visit into GPT, it suggested the same diagnosis I eventually received. After sharing this experience, I discovered many others facing similar struggles with fragmented medical histories and unclear diagnoses. That's what motivated me to turn this into an open source tool for anyone to use. While it's still in early stages, it's functional and might help others in similar situations.&lt;/p&gt; &lt;p&gt;Here's what it looks like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/v6j508rxkjhe1.gif"&gt;https://i.redd.it/v6j508rxkjhe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenHealthForAll/open-health"&gt;https://github.com/OpenHealthForAll/open-health&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**What it can do:**&lt;/p&gt; &lt;p&gt;* Upload medical records (PDFs, lab results, doctor notes)&lt;/p&gt; &lt;p&gt;* Automatically parses and standardizes lab results:&lt;/p&gt; &lt;p&gt;- Converts different lab formats to a common structure&lt;/p&gt; &lt;p&gt;- Normalizes units (mg/dL to mmol/L etc.)&lt;/p&gt; &lt;p&gt;- Extracts key markers like CRP, ESR, CBC, vitamins&lt;/p&gt; &lt;p&gt;- Organizes results chronologically&lt;/p&gt; &lt;p&gt;* Chat to analyze everything together:&lt;/p&gt; &lt;p&gt;- Track changes in lab values over time&lt;/p&gt; &lt;p&gt;- Compare results across different hospitals&lt;/p&gt; &lt;p&gt;- Identify patterns across multiple tests&lt;/p&gt; &lt;p&gt;* Works with different AI models:&lt;/p&gt; &lt;p&gt;- Local models like Deepseek (runs on your computer)&lt;/p&gt; &lt;p&gt;- Or commercial ones like GPT4/Claude if you have API keys&lt;/p&gt; &lt;p&gt;**Getting Your Medical Records:**&lt;/p&gt; &lt;p&gt;If you don't have your records as files:&lt;/p&gt; &lt;p&gt;- Check out [Fasten Health](&lt;a href="https://github.com/fastenhealth/fasten-onprem"&gt;https://github.com/fastenhealth/fasten-onprem&lt;/a&gt;) - it can help you fetch records from hospitals you've visited&lt;/p&gt; &lt;p&gt;- Makes it easier to get all your history in one place&lt;/p&gt; &lt;p&gt;- Works with most US healthcare providers&lt;/p&gt; &lt;p&gt;**Current Status:**&lt;/p&gt; &lt;p&gt;- Frontend is ready and open source&lt;/p&gt; &lt;p&gt;- Document parsing is currently on a separate Python server&lt;/p&gt; &lt;p&gt;- Planning to migrate this to run completely locally&lt;/p&gt; &lt;p&gt;- Will add to the repo once migration is done&lt;/p&gt; &lt;p&gt;Let me know if you have any questions about setting it up or using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T16:03:05+00:00</published>
  </entry>
</feed>
