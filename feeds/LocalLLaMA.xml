<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-16T16:55:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m0rk8t</id>
    <title>Notes on Kimi K2: A Deepseek derivative but the true Sonnet 3.6 Succesor</title>
    <updated>2025-07-15T19:40:06+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just like that, out of nowhere, we have an open-source Claude 4 Sonnet, or better yet, and this is no joke. I have been using the Kimi model for some time, and it truly feels the rightful successor to Claude 3.6 Sonnet. What Deepseek is to OpenAI, Kimi is to Anthropic.&lt;/p&gt; &lt;p&gt;K2 isn't truly a different model; it uses Deepseek v3 architecture. You can find that in the model config, but there are some subtle yet key improvements that resulted in such drastic improvements.&lt;/p&gt; &lt;h1&gt;Kimi K2 vs. DsV3 architecture&lt;/h1&gt; &lt;p&gt;This is from Liu Shaowei's Zhihu post.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Number of experts = 384 vs. 256&lt;/strong&gt;: 1.5x more experts for improving overall model ability, and helps lower the train/val loss, yielding better quality at the same &lt;em&gt;activated-parameter&lt;/em&gt; cost and inference FLOPs. But also a 50% spike in memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Number of attention heads = 64 vs 128&lt;/strong&gt;: They halve the attention-head count, shrinking the QKV projection weights from 10 GB to 5 GB per EP rank, which more than offsets the 50 % memory spike by yielding a net 2.5 GB saving while simultaneously halving pre-fill latency and leaving the KV-cache size unchanged.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;first_k_dense = 1 vs 3:&lt;/strong&gt; Kimi replaced the first layer with a dense layer after observing that the router in layer-1 consistently produced severe load imbalance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n_group = 1 vs. 8&lt;/strong&gt;: Dropping expert grouping frees every GPU to route to any of the 384 experts, letting EPLB handle load balancing while shrinking memory and widening the model’s effective capacity.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;MuonCLIP&lt;/h1&gt; &lt;p&gt;One of the key contributor of Kimi's success. Kimi went with Muon, more token efficient than AdamW. But it wasn't before tested for such a large model. To overcome they added a drop-in extension qk-clip. This helped to transplant Muon’s 2× token-efficiency into a 1-trillion-parameter regime without its historical Achilles’ heel: qk-clip rescales the query and key projections after every Muon update.&lt;/p&gt; &lt;h1&gt;How good in comparison to Claude 4 Sonnet?&lt;/h1&gt; &lt;p&gt;Kimi k2's positioning directly challenged Claude 4 Sonnet, the current SOTA agentic model. The k2 was specifically RL'd for extensive tool-use scenarios. However, it's not just good at tool use, it is surprisingly creative at writing and coding.&lt;/p&gt; &lt;p&gt;Some observations&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The K2 feels most natural to talk to than any available models. Zero sycophancy, no assumption, it just sticks to the point. Though I still find Sonnet 4 to be more attentive to instructions.&lt;/li&gt; &lt;li&gt;It has the simillar vibes of Claude 3.6 Sonnet, understands user intention better and more grounded response.&lt;/li&gt; &lt;li&gt;K2 has a better taste.&lt;/li&gt; &lt;li&gt;The coding is surprisingly good, though Sonnet will still be better at raw coding as for some task I found myself going back to it.&lt;/li&gt; &lt;li&gt;The best part it is roughly 1/12th of Sonnet's cost. Crazy times indeed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find the complete note here: &lt;a href="https://composio.dev/blog/notes-on-kimi-k2"&gt;Notes on Kimi K2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know your experience with the new Kimi K2 and how do you think it compares to Claude for agentic coding and other agentic tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0rk8t/notes_on_kimi_k2_a_deepseek_derivative_but_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T19:40:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1dzqj</id>
    <title>IMO 2025 LLM Mathematical Reasoning Evaluation</title>
    <updated>2025-07-16T14:26:34+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the conclusion of IMO 2025 in Australia today, I tested the performance of three frontier models: Anthropic Sonnet 4 (with thinking), ByteDance Seed 1.6 (with thinking), and Gemini 2.5 Pro. The results weren't as impressive as expected - &lt;strong&gt;only two models&lt;/strong&gt; correctly solved &lt;strong&gt;Problem 5&lt;/strong&gt; with proper reasoning processes. While some models got correct answers for other problems, their reasoning processes still had flaws. This demonstrates that these probability-based text generation reasoning models still have significant room for improvement in rigorous mathematical problem-solving and proof construction.&lt;/p&gt; &lt;h1&gt;Repository&lt;/h1&gt; &lt;p&gt;The complete evaluation is available at: &lt;a href="https://github.com/PaperPlaneDeemo/IMO2025-LLM"&gt;https://github.com/PaperPlaneDeemo/IMO2025-LLM&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Problem classification&lt;/h1&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6t1881404f6h3609787_sunny_lines"&gt;Problem 1&lt;/a&gt; – Combinatorial Geometry&lt;/p&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6t1881404f6h3609790_imo_2025_p2"&gt;Problem 2 &lt;/a&gt;– Geometry&lt;/p&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6t1881404f6h3609789_bonza_functions"&gt;Problem 3 &lt;/a&gt;– Algebra&lt;/p&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6h3610484_2025_imo_p4"&gt;Problem 4&lt;/a&gt; – Number Theory&lt;/p&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6h3610486_the_inekoalaty_game"&gt;Problem 5&lt;/a&gt; – Game Theory&lt;/p&gt; &lt;p&gt;&lt;a href="https://artofproblemsolving.com/community/c6t1881404f6h3610487_i_miss_turbo"&gt;Problem 6&lt;/a&gt; – Combinatorics&lt;/p&gt; &lt;h1&gt;Correct Solutions:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4: 2/6 problems (Problems 1, 3)&lt;/li&gt; &lt;li&gt;Gemini 2.5 Pro: 2/6 problems (Problems 1, 5)&lt;/li&gt; &lt;li&gt;Seed 1.6: 2/6 problems (Problems 3, 5)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Complete Solutions:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Only Seed 1.6 and Gemini 2.5 Pro provided complete solutions for Problem 5&lt;/li&gt; &lt;li&gt;Most solutions were partial, showing reasoning attempts but lacking full rigor&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Token Usage &amp;amp; Cost:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4: ~235K tokens, $3.50 total&lt;/li&gt; &lt;li&gt;Gemini 2.5 Pro: ~184K tokens, $1.84 total&lt;/li&gt; &lt;li&gt;Seed 1.6: ~104K tokens, $0.21 total&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seed 1.6 was remarkably efficient, achieving comparable performance at ~17% of Claude's cost.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;While LLMs have made impressive progress in mathematical reasoning, IMO problems remain a significant challenge.&lt;/p&gt; &lt;p&gt;This reminds me of a paper that Ilya once participated in: &lt;a href="https://arxiv.org/abs/2305.20050"&gt;Let's Verify Step by Step&lt;/a&gt;. Although DeepSeek R1's paper indicates they considered Process Reward Models as &amp;quot;Unsuccessful Attempts&amp;quot; during R1's development (paper at &lt;a href="https://arxiv.org/abs/2501.12948"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;), I believe that in complex reasoning processes, we still need to gradually supervise the model's reasoning steps. Today, OpenAI's official Twitter also shared a similar viewpoint: &amp;quot;Chain of Thought (CoT) monitoring could be a powerful tool for overseeing future AI systems—especially as they become more agentic. That's why we're backing a new research paper from a cross-institutional team of researchers pushing this work forward.&amp;quot; Link: &lt;a href="https://x.com/OpenAI/status/1945156362859589955"&gt;https://x.com/OpenAI/status/1945156362859589955&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1dzqj/imo_2025_llm_mathematical_reasoning_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1dzqj/imo_2025_llm_mathematical_reasoning_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1dzqj/imo_2025_llm_mathematical_reasoning_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T14:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0onbu</id>
    <title>Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding — and it costs less</title>
    <updated>2025-07-15T17:51:19+00:00</updated>
    <author>
      <name>/u/Aralknight</name>
      <uri>https://old.reddit.com/user/Aralknight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"&gt; &lt;img alt="Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding — and it costs less" src="https://external-preview.redd.it/0_FBAWufqqOTn0mcZPRsZJsFSHO9y993GsJn8w3P0XM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf6c0f2e02a4bd86aa09677603889292d8fa2d0e" title="Alibaba-backed Moonshot releases new Kimi AI model that beats ChatGPT, Claude in coding — and it costs less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aralknight"&gt; /u/Aralknight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/07/14/alibaba-backed-moonshot-releases-kimi-k2-ai-rivaling-chatgpt-claude.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0onbu/alibababacked_moonshot_releases_new_kimi_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1efdo</id>
    <title>Built an Agent That Replaced My Financial Advisor and Now My Realtor Too</title>
    <updated>2025-07-16T14:43:45+00:00</updated>
    <author>
      <name>/u/InitialChard8359</name>
      <uri>https://old.reddit.com/user/InitialChard8359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back, I built a small app to track stocks. It pulled market data and gave me daily reports on what to buy or sell based on my risk tolerance. It worked so well that I kept iterating it for bigger decisions. Now I’m using it to figure out my next house purchase, stuff like which neighborhoods are hot, new vs. old homes, flood risks, weather, school ratings… you get the idea. Tons of variables, but exactly the kind of puzzle these agents crush!&lt;/p&gt; &lt;p&gt;Why not just use Grok 4 or ChatGPT? My app remembers my preferences, learns from my choices, and pulls real-time data to give answers that actually fit me. It’s like a personal advisor that never forgets. I’m building it with the mcp-agent framework, which makes it super easy:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Orchestrato&lt;/strong&gt;r: Manages agents and picks the right tools for the job.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;EvaluatorOptimizer&lt;/strong&gt;: Quality-checks the research to keep it sharp.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Elicitation&lt;/strong&gt;: Adds a human-in-the-loop to make sure the research stays on track.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;mcp-agent as a server&lt;/strong&gt;: I can turn it into an mcp-server and run it from any client. I’ve got a Streamlit dashboard, but I also love using it on my cloud desktop too.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Memory&lt;/strong&gt;: Stores my preferences for smarter results over time.&lt;/p&gt; &lt;p&gt;The code’s built on the same logic as my financial analyzer but leveled up with an API and human-in-the-loop features. With mcp-agent, you can create an expert for any domain and share it as an mcp-server.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lastmile-ai/mcp-agent/tree/main/examples/usecases/mcp_realtor_agent"&gt;Code for realtor App&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lastmile-ai/mcp-agent/tree/main/examples/usecases/mcp_financial_analyzer"&gt;Code for financial analyzer App&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InitialChard8359"&gt; /u/InitialChard8359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1efdo/built_an_agent_that_replaced_my_financial_advisor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1efdo/built_an_agent_that_replaced_my_financial_advisor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1efdo/built_an_agent_that_replaced_my_financial_advisor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T14:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0zy1a</id>
    <title>New documentation / explainer for GGUF quantization</title>
    <updated>2025-07-16T01:35:32+00:00</updated>
    <author>
      <name>/u/mojojojo_24</name>
      <uri>https://old.reddit.com/user/mojojojo_24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's surprisingly little documentation on how GGUF quantization works, including legacy / I-quants / K-quants and the importance matrix.&lt;/p&gt; &lt;p&gt;The maintainers made it &lt;a href="https://github.com/ggml-org/llama.cpp/pull/1684#issuecomment-2474462323"&gt;pretty clear&lt;/a&gt; it's not their priority to write a paper either. Currently, people are just piecing information together from Reddit threads and Medium articles (which are often wrong). So I spent some time combing through the llama.cpp quantization code and put together a public GitHub repo that hopefully brings some clarity and can function as an unofficial explainer / documentation.&lt;/p&gt; &lt;p&gt;Contributions are welcome, as long as they are backed by reliable sources! &lt;a href="https://github.com/iuliaturc/gguf-docs"&gt;https://github.com/iuliaturc/gguf-docs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mojojojo_24"&gt; /u/mojojojo_24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0zy1a/new_documentation_explainer_for_gguf_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T01:35:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0mg5b</id>
    <title>Least sycophantic AI yet? Kimi K2</title>
    <updated>2025-07-15T16:30:01+00:00</updated>
    <author>
      <name>/u/PrimaryBalance315</name>
      <uri>https://old.reddit.com/user/PrimaryBalance315</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy crap this thing has sass. First time I've ever engaged with an AI that replied &amp;quot;No.&amp;quot;&lt;br /&gt; That's it. It was fantastic.&lt;/p&gt; &lt;p&gt;Actually let me grab some lines from the conversation -&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Thermodynamics kills the romance&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Everything else is commentary&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;If your 'faith' can be destroyed by a single fMRI paper or a bad meditation session, it's not faith, it's a hypothesis&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Bridges that don't creak aren't being walked on&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And my favorite zinger - &lt;strong&gt;&amp;quot;Beautiful scaffolding with no cargo yet&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Fucking Killing it Moonshot. Like this thing never once said &amp;quot;that's interesting&amp;quot; or &amp;quot;great question&amp;quot; - it just went straight for the my intelligence every single time. It's like talking to someone that genuinely doesn't give a shit if you can handle the truth or not. Just pure &amp;quot;Show me or shut up&amp;quot;. It makes me think instead of feeling good about thinking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrimaryBalance315"&gt; /u/PrimaryBalance315 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0mg5b/least_sycophantic_ai_yet_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T16:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m19igi</id>
    <title>getting acceleration on Intel integrated GPU/NPU</title>
    <updated>2025-07-16T10:56:14+00:00</updated>
    <author>
      <name>/u/a_postgres_situation</name>
      <uri>https://old.reddit.com/user/a_postgres_situation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp on CPU is easy.&lt;/p&gt; &lt;p&gt;AMD and integrated graphics is also easy, run via Vulkan (not ROCm) and receive noteable speedup. :-)&lt;/p&gt; &lt;p&gt;Intel integrated graphics via Vulkan is actually slower than CPU! :-(&lt;/p&gt; &lt;p&gt;For Intel there is Ipex-LLM (&lt;a href="https://github.com/intel/ipex-llm"&gt;https://github.com/intel/ipex-llm&lt;/a&gt;), but I just can't figure out how to get all these dependencies properly installed - intel-graphics-runtime, intel-compute-runtime, oneAPI, ... this is complicated.&lt;/p&gt; &lt;p&gt;TL;DR; platform Linux, Intel Arrowlake CPU with integrated graphics (Xe/Arc 140T) and NPU ([drm] Firmware: intel/vpu/vpu_37xx_v1.bin, version: 20250415).&lt;/p&gt; &lt;p&gt;How to get a speedup over CPU-only for llama.cpp?&lt;/p&gt; &lt;p&gt;If anyone got this running, how much speedup one can expect on Intel? Are there some memory mapping kernel options GPU-CPU like with AMD?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_postgres_situation"&gt; /u/a_postgres_situation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19igi/getting_acceleration_on_intel_integrated_gpunpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19igi/getting_acceleration_on_intel_integrated_gpunpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m19igi/getting_acceleration_on_intel_integrated_gpunpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T10:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0y3a6</id>
    <title>Fine-tuning Leaderboard!</title>
    <updated>2025-07-16T00:07:13+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"&gt; &lt;img alt="Fine-tuning Leaderboard!" src="https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477a0620594fc1ec25f6cb09693ff29925108ee4" title="Fine-tuning Leaderboard!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does &lt;em&gt;not&lt;/em&gt; correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There's something to leaving the model less-trained like a blank canvas.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://predibase.com/fine-tuning-index"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0slrh</id>
    <title>support for Kimi-K2 has been merged into llama.cpp</title>
    <updated>2025-07-15T20:19:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support for Kimi-K2 has been merged into llama.cpp" src="https://external-preview.redd.it/AvRP7gL4hJa_Rr6omxHq_5JILyovt6Ibkrc-oDo7cE0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1f61a192157fcd7aff2e2fa3bf1c60d78f2fa97" title="support for Kimi-K2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14654"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0slrh/support_for_kimik2_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T20:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0twqa</id>
    <title>Alternative to llama.cpp for Apple Silicon</title>
    <updated>2025-07-15T21:09:43+00:00</updated>
    <author>
      <name>/u/darkolorin</name>
      <uri>https://old.reddit.com/user/darkolorin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"&gt; &lt;img alt="Alternative to llama.cpp for Apple Silicon" src="https://external-preview.redd.it/hgPKgy_3Vizk0MqLpC77xRGYB-9mpWo_rx5vN9M9zVU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3332c993dd26519a4ef1b63d265d7a6c44d33516" title="Alternative to llama.cpp for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community,&lt;/p&gt; &lt;p&gt;We wrote our own inference engine based on Rust for Apple Silicon. It's open sourced under MIT license.&lt;/p&gt; &lt;p&gt;Why we do this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;should be easy to integrate&lt;/li&gt; &lt;li&gt;believe that app UX will completely change in a recent years&lt;/li&gt; &lt;li&gt;it faster than llama.cpp in most of the cases&lt;/li&gt; &lt;li&gt;sometimes it is even faster than MLX from Apple&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speculative decoding right now tightened with platform (trymirai). Feel free to try it out.&lt;/p&gt; &lt;p&gt;Would really appreciate your feedback. Some benchmarks are in readme of the repo. More and more things we will publish later (more benchmarks, support of VLM &amp;amp; TTS/STT is coming soon).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkolorin"&gt; /u/darkolorin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/trymirai/uzu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0twqa/alternative_to_llamacpp_for_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T21:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0nutb</id>
    <title>Totally lightweight local inference...</title>
    <updated>2025-07-15T17:22:09+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt; &lt;img alt="Totally lightweight local inference..." src="https://preview.redd.it/r05r0wfvn2df1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c622b493252fd700bcdd538ffef56559bdbbcd5" title="Totally lightweight local inference..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r05r0wfvn2df1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0nutb/totally_lightweight_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T17:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1hixa</id>
    <title>Playing around with the design of my pet project - does this look decent or nah?</title>
    <updated>2025-07-16T16:39:57+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt; &lt;img alt="Playing around with the design of my pet project - does this look decent or nah?" src="https://a.thumbs.redditmedia.com/oijm0DyxoxFre_g3otVxcpGYCKM5xFhT79TCi5btsZ8.jpg" title="Playing around with the design of my pet project - does this look decent or nah?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a showcase of my project recently, would be glad to hear opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m1hixa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1hixa/playing_around_with_the_design_of_my_pet_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m18nke</id>
    <title>GitHub - boneylizard/Eloquent: A local front-end for open-weight LLMs with memory, RAG, TTS/STT, Elo ratings, and dynamic research tools. Built with React and FastAPI.</title>
    <updated>2025-07-16T10:04:05+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;🚀 Just Dropped: Eloquent – A Local LLM Powerhouse&lt;/h1&gt; &lt;p&gt;Hey LocalLLaMA! Just dropped &lt;strong&gt;Eloquent&lt;/strong&gt; after 4 months of &amp;quot;just one more feature&amp;quot; syndrome.&lt;/p&gt; &lt;p&gt;Started as a basic chat interface... ended up as a full-stack, dual-GPU, memory-retaining AI companion.&lt;br /&gt; Built entirely for local model users — by someone who actually uses local models.&lt;/p&gt; &lt;h1&gt;🧠 Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual-GPU architecture with memory offloading&lt;/li&gt; &lt;li&gt;Persistent memory system that learns who you are over time&lt;/li&gt; &lt;li&gt;Model ELO testing (head-to-head tournaments + scoring)&lt;/li&gt; &lt;li&gt;Auto-character creator (talk to an AI → get a JSON persona)&lt;/li&gt; &lt;li&gt;Built-in SD support (EloDiffusion + ADetailer)&lt;/li&gt; &lt;li&gt;60+ TTS voices, fast voice-to-text&lt;/li&gt; &lt;li&gt;RAG support for PDFs, DOCX, and more&lt;/li&gt; &lt;li&gt;Focus &amp;amp; Call modes (clean UI &amp;amp; voice-only UX)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;…and probably a dozen other things I forgot I built.&lt;/p&gt; &lt;h1&gt;🛠️ Install &amp;amp; Run&lt;/h1&gt; &lt;p&gt;Quick setup (Windows):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/boneylizard/Eloquent.git cd Eloquent install.bat run.bat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Works with any GGUF model. Supports single GPU, but flies with two.&lt;/p&gt; &lt;h1&gt;🧬 Why?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I wanted real memory, so it remembers your background, style, vibe.&lt;/li&gt; &lt;li&gt;I wanted model comparisons that aren’t just vibes-based.&lt;/li&gt; &lt;li&gt;I wanted persona creation without filling out forms.&lt;/li&gt; &lt;li&gt;I wanted it modular, so anyone can build on top of it.&lt;/li&gt; &lt;li&gt;I wanted it local, private, and fast.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔓 Open Source &amp;amp; Yours to Break&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;100% local — nothing phones home&lt;/li&gt; &lt;li&gt;AGPL-3.0 licensed&lt;/li&gt; &lt;li&gt;Everything's in backend/app or frontend/src&lt;/li&gt; &lt;li&gt;The rest is just dependencies — over 300 of them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please, try it out. Break it. Fork it. Adapt it.&lt;br /&gt; I genuinely think people will build cool stuff on top of this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/boneylizard/Eloquent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m18nke/github_boneylizardeloquent_a_local_frontend_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m18nke/github_boneylizardeloquent_a_local_frontend_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T10:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m19upn</id>
    <title>So how do I fine-time a local model?</title>
    <updated>2025-07-16T11:14:57+00:00</updated>
    <author>
      <name>/u/ImYoric</name>
      <uri>https://old.reddit.com/user/ImYoric</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm a newb, please forgive me if I'm missing some obvious documentation.&lt;/p&gt; &lt;p&gt;For the sake of fun and learning, I'd like to fine-tune a local model (haven't decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we're not there yet.&lt;/p&gt; &lt;p&gt;Right now, I'd like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it's not clear to me where to start. I'm not afraid of Python or PyTorch (or Rust, or C++), but I'm entirely lost on the process.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Any suggestion for a model to use as base? I'd like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don't need something particularly fancy.&lt;/li&gt; &lt;li&gt;How large a corpus do I need to get started?&lt;/li&gt; &lt;li&gt;Let's assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?&lt;/li&gt; &lt;li&gt;How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?&lt;/li&gt; &lt;li&gt;Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?&lt;/li&gt; &lt;li&gt;Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?&lt;/li&gt; &lt;li&gt;What else am I missing?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImYoric"&gt; /u/ImYoric &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T11:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1au28</id>
    <title>Vllm vs. llama.cpp</title>
    <updated>2025-07-16T12:06:46+00:00</updated>
    <author>
      <name>/u/Agreeable-Prompt-666</name>
      <uri>https://old.reddit.com/user/Agreeable-Prompt-666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Prompt-666"&gt; /u/Agreeable-Prompt-666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T12:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m118is</id>
    <title>Use claudecode with local models</title>
    <updated>2025-07-16T02:38:02+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month. So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code. Well, many folks are already doing that with local. So if you don't know, now you know. This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.&lt;/p&gt; &lt;p&gt;Start your local LLM API &lt;/p&gt; &lt;p&gt;Install claude code&lt;/p&gt; &lt;p&gt;install a proxy - &lt;a href="https://github.com/1rgs/claude-code-proxy"&gt;https://github.com/1rgs/claude-code-proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit the &lt;a href="http://server.py"&gt;server.py&lt;/a&gt; proxy and point it to your OpenAI endpoint, could be llama.cpp, ollama, vllm, whatever you are running. &lt;/p&gt; &lt;p&gt;Add the line above load_dotenv&lt;br /&gt; +litellm.api_base = &amp;quot;http://yokujin:8083/v1&amp;quot; # use your localhost name/IP/ports&lt;/p&gt; &lt;p&gt;Start the proxy according to the docs which will run it in localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_BASE_URL=http://localhost:8082&lt;/p&gt; &lt;p&gt;export ANTHROPIC_AUTH_TOKEN=&amp;quot;sk-localkey&amp;quot;&lt;/p&gt; &lt;p&gt;run claude code&lt;/p&gt; &lt;p&gt;I just created my first code then decided to post this. I'm running the latest mistral-small-24b on that host. I'm going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T02:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m16o6r</id>
    <title>Official Local LLM support by AMD released. Lemonade</title>
    <updated>2025-07-16T07:53:22+00:00</updated>
    <author>
      <name>/u/grigio</name>
      <uri>https://old.reddit.com/user/grigio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can somebody test the performance of Gemma3 12B / 27B q4 on different modes ONNX, llamacpp, GPU, CPU, NPU ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=mcf7dDybUco"&gt;https://www.youtube.com/watch?v=mcf7dDybUco&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grigio"&gt; /u/grigio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m16o6r/official_local_llm_support_by_amd_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T07:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1foz1</id>
    <title>CUDA is coming to MLX</title>
    <updated>2025-07-16T15:31:43+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt; &lt;img alt="CUDA is coming to MLX" src="https://external-preview.redd.it/w8edStcv8JcRcgUOJ4-eZrp8x-ns7z_4bZz-mt8i8eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9049533811e0bc40173ac835b90eb4f9943876f0" title="CUDA is coming to MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will soon get CUDA support in MLX - this means that we’ll be able to run MLX programs on both Apple Silicon and CUDA GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ml-explore/mlx/pull/1983"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1foz1/cuda_is_coming_to_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T15:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1h0fy</id>
    <title>Support for diffusion models (Dream 7B) has been merged into llama.cpp</title>
    <updated>2025-07-16T16:20:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt; &lt;img alt="Support for diffusion models (Dream 7B) has been merged into llama.cpp" src="https://external-preview.redd.it/OqAAbOs6fFLPZaNF0M6vIqHJqNLZwtArB7hBcX1IZ7M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8971ce6047ae48ffddcf53ec22de6523ddaa226e" title="Support for diffusion models (Dream 7B) has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion models are a new kind of language model that generate text by denoising random noise step-by-step, instead of predicting tokens left to right like traditional LLMs.&lt;/p&gt; &lt;p&gt;This PR adds basic support for diffusion models, using Dream 7B instruct as base. DiffuCoder-7B is built on the same arch so it should be trivial to add after this.&lt;br /&gt; [...]&lt;br /&gt; &lt;strong&gt;Another cool/gimmicky thing is you can see the diffusion unfold&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In a joint effort with Huawei Noah’s Ark Lab, we release &lt;strong&gt;Dream 7B&lt;/strong&gt; (Diffusion reasoning model), the most powerful open diffusion large language model to date.&lt;/p&gt; &lt;p&gt;In short, Dream 7B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;consistently outperforms existing diffusion language models by a large margin;&lt;/li&gt; &lt;li&gt;matches or exceeds top-tier Autoregressive (AR) language models of similar size on the general, math, and coding abilities;&lt;/li&gt; &lt;li&gt;demonstrates strong planning ability and inference flexibility that naturally benefits from the diffusion modeling.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1h0fy/support_for_diffusion_models_dream_7b_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T16:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1aj8n</id>
    <title>📢 [RELEASE] LoFT CLI: Fine-tune &amp; Deploy LLMs on CPU (8GB RAM, No GPU, No Cloud)</title>
    <updated>2025-07-16T11:51:46+00:00</updated>
    <author>
      <name>/u/diptanshu1991</name>
      <uri>https://old.reddit.com/user/diptanshu1991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/"&gt;previous post&lt;/a&gt; — the repo is &lt;strong&gt;finally public&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;🔥 TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/diptanshu1991/LoFT"&gt;diptanshu1991/LoFT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What you get&lt;/strong&gt;: 5 CLI commands: &lt;code&gt;loft finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;export&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: Tested on 8GB MacBook Air — peak RAM &lt;strong&gt;330MB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 300 Dolly samples, 2 epochs → 1.5 hrs total wall-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference speed&lt;/strong&gt;: 6.9 tok/sec (Q4_0) on CPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT – 100% open-source&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🧠 What is LoFT?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;LoFT CLI&lt;/strong&gt; is a lightweight, CPU-friendly toolkit that lets you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ Finetune 1–3B LLMs like TinyLlama using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;🔄 Merge and export models to &lt;strong&gt;GGUF&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;🧱 Quantize models (Q4_0, Q5_1, etc.)&lt;/li&gt; &lt;li&gt;💬 Run &lt;strong&gt;offline inference&lt;/strong&gt; using &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All from a &lt;strong&gt;command-line interface&lt;/strong&gt; on your &lt;strong&gt;local laptop&lt;/strong&gt;. No Colab. No GPUs. No cloud.&lt;/p&gt; &lt;h1&gt;📊 Benchmarks (8GB MacBook Air)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Step&lt;/th&gt; &lt;th align="left"&gt;Output&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Peak RAM&lt;/th&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Finetune&lt;/td&gt; &lt;td align="left"&gt;LoRA Adapter&lt;/td&gt; &lt;td align="left"&gt;4.3 MB&lt;/td&gt; &lt;td align="left"&gt;308 MB&lt;/td&gt; &lt;td align="left"&gt;23 min&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Merge&lt;/td&gt; &lt;td align="left"&gt;HF Model&lt;/td&gt; &lt;td align="left"&gt;4.2 GB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;4.7 min&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Export&lt;/td&gt; &lt;td align="left"&gt;GGUF (FP16)&lt;/td&gt; &lt;td align="left"&gt;2.1 GB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;83 sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Quantize&lt;/td&gt; &lt;td align="left"&gt;GGUF (Q4_0)&lt;/td&gt; &lt;td align="left"&gt;607 MB&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;21 sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Chat&lt;/td&gt; &lt;td align="left"&gt;6.9 tok/sec&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;td align="left"&gt;322 MB&lt;/td&gt; &lt;td align="left"&gt;79 sec&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;🧪 Trained on: 300 Dolly samples, 2 epochs → loss &amp;lt; 1.0&lt;/p&gt; &lt;h1&gt;🧪 5-Command Lifecycle&lt;/h1&gt; &lt;p&gt;LoFT runs the complete LLM workflow — from training to chat — in just 5 commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;loft finetune loft merge loft export loft quantize loft chat &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;🧪 Coming Soon in LoFT&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;📦 Plug-and-Play Recipes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Legal Q&amp;amp;A bots (air-gapped, offline)&lt;/li&gt; &lt;li&gt;Customer support assistants&lt;/li&gt; &lt;li&gt;Contract summarizers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;🌱 Early Experiments&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-turn finetuning&lt;/li&gt; &lt;li&gt;Adapter-sharing for niche domains&lt;/li&gt; &lt;li&gt;Dataset templating tools&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;LoFT is built for indie builders, researchers, and OSS devs who want &lt;strong&gt;local GenAI without GPU constraints&lt;/strong&gt;. Would love your feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What models/datasets you would like to see supported next&lt;/li&gt; &lt;li&gt;Edge cases or bugs during install/training&lt;/li&gt; &lt;li&gt;Use cases where this unlocks new workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔗 GitHub: &lt;a href="https://github.com/diptanshu1991/LoFT"&gt;https://github.com/diptanshu1991/LoFT&lt;/a&gt;&lt;br /&gt; 🪪 MIT licensed — feel free to fork, contribute, and ship your own CLI tools on top&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diptanshu1991"&gt; /u/diptanshu1991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T11:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0v9m1</id>
    <title>Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!</title>
    <updated>2025-07-15T22:04:18+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt; &lt;img alt="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="Incoming late summer: 8B and 70B models trained on 15T tokens, fluent in 1000+ languages, open weights and code, Apache 2.0. Thanks Switzerland!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ETH Zurich &amp;amp; EPFL Public LLM – Technical Specs • Release: Late summer 2025 • Developers: EPFL, ETH Zurich, Swiss National Supercomputing Centre (CSCS), Swiss universities • Model sizes: 8B and 70B parameters (fully open weights and code, Apache 2.0 license) • Multilinguality: Fluency in 1,000+ languages (trained on &amp;gt;1,500 languages; ~60% English, ~40% non-English; code and math included) • Training data: &amp;gt;15 trillion tokens, high-quality, transparent, reproducible, with web-crawling opt-outs respected • Training hardware: Alps supercomputer (CSCS, Lugano), &amp;gt;10,000 NVIDIA Grace Hopper Superchips, 100% carbon-neutral electricity • Compliance: Swiss data protection and copyright laws, EU AI Act transparency • Intended use: Science, society, industry; fully public download, detailed documentation on model architecture and training • Initiative: Swiss AI Initiative, 800+ researchers, 20M+ GPU hours/year, funded by ETH Board (2025–2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0v9m1/incoming_late_summer_8b_and_70b_models_trained_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-15T22:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m16kdm</id>
    <title>T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog</title>
    <updated>2025-07-16T07:46:29+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"&gt; &lt;img alt="T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog" src="https://external-preview.redd.it/Ua_6ve9KH9QIRkyagh-mJMhThsokU2TkQCZbS7Cxv8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbbccb5f228138b777b853e12fc432178fff5f50" title="T5Gemma: A new collection of encoder-decoder Gemma models- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma released a new encoder-decoder model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/t5gemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m16kdm/t5gemma_a_new_collection_of_encoderdecoder_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T07:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m13eb2</id>
    <title>AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO "Blackwell" With 24 GB VRAM</title>
    <updated>2025-07-16T04:28:39+00:00</updated>
    <author>
      <name>/u/Rich_Repeat_22</name>
      <uri>https://old.reddit.com/user/Rich_Repeat_22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" src="https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82787d21e7b0821fdce5a034706e0598040c7cc4" title="AMD Radeon AI PRO R9700 32 GB GPU Listed Online, Pricing Expected Around $1250, Half The Price of NVIDIA's RTX PRO &amp;quot;Blackwell&amp;quot; With 24 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Said it when this was presented that will have MSRP around RTX5080 since AMD decided to bench it against that card and not some workstation grade RTX.... 🥳&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Repeat_22"&gt; /u/Rich_Repeat_22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-radeon-ai-pro-r9700-32-gb-gpu-listed-pricing-around-1250-half-price-nvidia-rtx-pro-blackwell-24-gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m13eb2/amd_radeon_ai_pro_r9700_32_gb_gpu_listed_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T04:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m14a9j</id>
    <title>Meta's new ASI team discussed about abandoning Meta's powerful Open-source and focus on developing close</title>
    <updated>2025-07-16T05:23:16+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html"&gt;https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m14a9j/metas_new_asi_team_discussed_about_abandoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T05:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m0z1zx</id>
    <title>Your unpopular takes on LLMs</title>
    <updated>2025-07-16T00:52:41+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mine are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;All the popular public benchmarks are nearly worthless when it comes to a model's general ability. Literaly the only good thing we get out of them is a rating for &amp;quot;can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up&amp;quot;, which does have some value. I think the people who maintain the benchmarks know this too, but we're all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. That's because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don't even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON'T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T00:52:41+00:00</published>
  </entry>
</feed>
