<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-08T15:25:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mky4jd</id>
    <title>Why Open Source is Needed</title>
    <updated>2025-08-08T15:22:58+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"&gt; &lt;img alt="Why Open Source is Needed" src="https://preview.redd.it/k8n9e70mcthf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=756f312a40164e6bd66cc44530c38aea4caaa12b" title="Why Open Source is Needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With this new launch, OpenAI cut total weekly reasoning model requests from 2900 to 200(!!!!) also with a huge reduction in context window length. &lt;/p&gt; &lt;p&gt;Yes, $200 a month for a measly 128k context window length. Just goes to show why open source models and more companies being able to host these models protects the consumer from this disingenuous behavior.&lt;/p&gt; &lt;p&gt;(Also, GPT5 isn‚Äôt even that impressive‚Ä¶)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k8n9e70mcthf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk5n89</id>
    <title>HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide</title>
    <updated>2025-08-07T17:02:37+00:00</updated>
    <author>
      <name>/u/Tango-Down766</name>
      <uri>https://old.reddit.com/user/Tango-Down766</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt; &lt;img alt="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" src="https://b.thumbs.redditmedia.com/-NXaX5EHmxxb7GBcWRIp5vrkgUBJaiSRrpm9inzqlEM.jpg" title="HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://civitaiarchive.com/"&gt;https://civitaiarchive.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tango-Down766"&gt; /u/Tango-Down766 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk5n89"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkvaon</id>
    <title>Transformer Lab now supports training OpenAI‚Äôs open models (gpt-oss)</title>
    <updated>2025-08-08T13:31:27+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Transformer Lab is an open source toolkit to train, tune and chat with UI for common tasks. We just shipped gpt-oss support to Transformer Lab.&lt;/p&gt; &lt;p&gt;We currently support the original gpt-oss models and the gpt-oss GGUFs (from Ollama) across NVIDIA, AMD and Apple silicon as long as you have adequate hardware. We even got it to run on a T4.&lt;/p&gt; &lt;p&gt;Check it out and let us know how it could be more useful to you. &lt;/p&gt; &lt;p&gt;üîó Try it here ‚Üí&lt;a href="https://transformerlab.ai/docs/intro"&gt; &lt;/a&gt;&lt;a href="https://transformerlab.ai/"&gt;https://transformerlab.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó Useful? Give us a star on GitHub ‚Üí &lt;a href="https://github.com/transformerlab"&gt;https://github.com/transformerlab/transformerlab-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó Ask for help on our Discord Community ‚Üí &lt;a href="https://discord.gg/transformerlab"&gt;https://discord.gg/transformerlab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvaon/transformer_lab_now_supports_training_openais/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvaon/transformer_lab_now_supports_training_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvaon/transformer_lab_now_supports_training_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T13:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk3rj1</id>
    <title>Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop</title>
    <updated>2025-08-07T15:52:01+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt; &lt;img alt="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" src="https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f53b202537c26df370b20f3e2c66f92c5b25828" title="Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While the setup looks √ºber cool, the software is still not ready to make good use of the hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N5xhOqlvRh4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T15:52:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mknjzx</id>
    <title>[Showoff] I made an AI that understands where things are, not just what they are ‚Äì live demo on Hugging Face üöÄ</title>
    <updated>2025-08-08T06:11:55+00:00</updated>
    <author>
      <name>/u/scheitelpunk1337</name>
      <uri>https://old.reddit.com/user/scheitelpunk1337</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know how most LLMs can tell you what a &amp;quot;keyboard&amp;quot; is, but if you ask &lt;em&gt;&amp;quot;where‚Äôs the keyboard relative to the monitor?&amp;quot;&lt;/em&gt; you get‚Ä¶ ü§∑?&lt;br /&gt; That‚Äôs the &lt;strong&gt;Spatial Intelligence Gap&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I‚Äôve been working for months on &lt;strong&gt;GASM&lt;/strong&gt; (Geometric Attention for Spatial &amp;amp; Mathematical Understanding) ‚Äî and yesterday I finally ran the example that‚Äôs been stuck in my head:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Raw output:&lt;/strong&gt;&lt;br /&gt; üìç Sensor: &lt;code&gt;(-1.25, -0.68, -1.27)&lt;/code&gt; m&lt;br /&gt; üìç Conveyor: &lt;code&gt;(-0.76, -1.17, -0.78)&lt;/code&gt; m&lt;br /&gt; üìê 45¬∞ angle: Extracted &amp;amp; encoded ‚úì&lt;br /&gt; üîó Spatial relationships: 84.7% confidence ‚úì&lt;/p&gt; &lt;p&gt;No simulation. No smoke. Just &lt;strong&gt;plain English ‚Üí 3D coordinates&lt;/strong&gt;, all CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs cool:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First &lt;em&gt;public&lt;/em&gt; SE(3)-invariant AI for natural language ‚Üí geometry&lt;/li&gt; &lt;li&gt;Works for robotics, AR/VR, engineering, scientific modeling&lt;/li&gt; &lt;li&gt;Optimized for curvature calculations so it runs on CPU (because I like the planet)&lt;/li&gt; &lt;li&gt;Mathematically correct spatial relationships under rotations/translations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Live demo here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/scheitelpunk/GASM"&gt;huggingface.co/spaces/scheitelpunk/GASM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Drop &lt;em&gt;any&lt;/em&gt; spatial description in the comments (&amp;quot;put the box between the two red chairs next to the window&amp;quot;) ‚Äî I‚Äôll run it and post the raw coordinates + visualization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scheitelpunk1337"&gt; /u/scheitelpunk1337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mknjzx/showoff_i_made_an_ai_that_understands_where/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T06:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk26rk</id>
    <title>Llama.cpp now supports GLM 4.5 Air</title>
    <updated>2025-08-07T14:52:12+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt; &lt;img alt="Llama.cpp now supports GLM 4.5 Air" src="https://b.thumbs.redditmedia.com/jawkehNzIT0a-enbiD4fQc_KPJ-dSoMI8t5allPhBfU.jpg" title="Llama.cpp now supports GLM 4.5 Air" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from our hero sammcj&lt;/p&gt; &lt;p&gt;Pictured, Cuda v1.45 engine in LM Studio. (the cuda 12 1.44 runtime still not working--the GLM 4.5 PR was merged in the past 8 hours or so).&lt;/p&gt; &lt;p&gt;As an aside, my initial vibe is it is far too wordy and overthinks, though, and gpt oss 120b is better and also faster in pure t/s but that's very much early vibe so take with a heavy dose of salt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mk26rk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T14:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mk74wq</id>
    <title>Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models</title>
    <updated>2025-08-07T17:58:02+00:00</updated>
    <author>
      <name>/u/agentcubed</name>
      <uri>https://old.reddit.com/user/agentcubed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt; &lt;img alt="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" src="https://b.thumbs.redditmedia.com/TJcHxiajHwTxrtNoLptpQfkKcoLceehEHpbxyUdzAnI.jpg" title="Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People need to stop expecting a 5b model to outperform 30b models. Like do they think OpenAI is god?&lt;/p&gt; &lt;p&gt;R1 670b, &lt;strong&gt;37b activ&lt;/strong&gt;e&lt;br /&gt; Kimi K2 1t, &lt;strong&gt;32b active&lt;/strong&gt;&lt;br /&gt; Qwen3 235b, &lt;strong&gt;22b active&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;-- limit of a single average gpu --&lt;/em&gt;&lt;br /&gt; GLM 4.5 Air 106b, &lt;strong&gt;12b active&lt;/strong&gt; (very pushing it but fine)&lt;br /&gt; Qwen3 14b&lt;br /&gt; oss 120b, &lt;strong&gt;5b active&lt;/strong&gt;&lt;br /&gt; Qwen3 30b, &lt;strong&gt;3b active&lt;/strong&gt;&lt;br /&gt; oss 20b, &lt;strong&gt;3b active&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I would rather have a model that I can actually run locally than a good model that needs providers. To be clear, I hate and won't use gpt-oss, but because it's censored and not because models many times larger are better.&lt;/p&gt; &lt;p&gt;I LOVED Qwen3 30b/3b was local-friendly and fast and nobody compared it to bigger models, but when OpenAI releases a local model and suddenly everyone is comparing it to non-local models.&lt;/p&gt; &lt;p&gt;It's an expected model for it's size. It's not beating models 4x larger, but it's not garbage compared to similar sizes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47a39c27091e2beb9233c783989cf7305269027e"&gt;Graph of all local-friendly models (GLM Air would be tough)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agentcubed"&gt; /u/agentcubed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T17:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkokj2</id>
    <title>GMK X2(AMD Max+ 395 w/128GB) third impressions, RPC and Image/Video gen.</title>
    <updated>2025-08-08T07:13:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is pretty much a catchall post for things people asked about in my first two posts about the Max+ 395. That being how/if it works for distributed LLM inference and image/video gen. It works for both those things.&lt;/p&gt; &lt;p&gt;Let's start with distributed LLM inference. TBH, I'm pretty surprise the numbers hold up as well as they do. Since IME there's a pretty significant performance penalty for going multi-gpu. I ballpark it to be about 50%. In this case, though, it's better than that. That is probably because I'm using a dynamic quant of a MOE. Where the heavy lifting is done by the X2 and the leftovers are on the Mac. Anyways here are the numbers first for the X2 alone and then working with a M1 Max.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Max+ ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 | 112.27 ¬± 0.38 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 | 20.29 ¬± 0.02 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 @ d10000 | 60.61 ¬± 0.34 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 @ d10000 | 15.36 ¬± 0.03 | Max+ with M1 Max ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 | 101.53 ¬± 2.69 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 | 13.90 ¬± 4.29 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 @ d10000 | 56.71 ¬± 0.33 | | glm4moe 106B.A12B Q5_K - Medium | 77.75 GiB | 110.47 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 @ d10000 | 9.56 ¬± 0.12 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are the numbers for doing SD 1.5 image gens. Both at 512x512 and 1024x1024.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SD 1.5 512x512&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Max+ 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01&amp;lt;00:00, 11.58it/s] Prompt executed in 2.21 seconds 7900xtx 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01&amp;lt;00:00, 18.54it/s] Prompt executed in 1.24 seconds 3060 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02&amp;lt;00:00, 8.86it/s] Prompt executed in 2.60 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SD 1.5 1024x1024&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Max+ 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11&amp;lt;00:00, 1.69it/s] Prompt executed in 13.70 seconds 7900xtx 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07&amp;lt;00:00, 2.58it/s] Prompt executed in 8.69 seconds 3060 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10&amp;lt;00:00, 1.84it/s] Prompt executed in 12.12 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lastly, here are some video gen numbers. This is for Wan 2.2. It's at 480x320 resolution since ROCm support for the Max+ 395 is still a work in progress. Under Windows it's fast but only works with about 32GB of RAM max before things go bad. Under Linux it doesn't seem to have that RAM limit but it's really really slow. Like 200 secs/iteration slow. Yes, I verified that it is using the GPU and not the CPU. So these results are from Windows. But because of the memory limit, I had to crank down the resolution. I'm using the Phr00t Wan 2.2 14B AIO.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Wan 2.2 480x320x41&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Max+ 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:42&amp;lt;00:00, 25.69s/it] Prompt executed in 194.01 seconds 7900xtx 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:19&amp;lt;00:00, 4.77s/it] Prompt executed in 140.08 seconds 3060 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:01&amp;lt;00:00, 15.44s/it] Prompt executed in 133.89 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So just like with the other two posts in this series, the Max+ 395 is basically a 128GB 3060.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mky54y</id>
    <title>Qwen Code Now Offering 2000 free Qwen Code runs daily</title>
    <updated>2025-08-08T15:23:35+00:00</updated>
    <author>
      <name>/u/z1xto</name>
      <uri>https://old.reddit.com/user/z1xto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"&gt; &lt;img alt="Qwen Code Now Offering 2000 free Qwen Code runs daily" src="https://preview.redd.it/0qdg1xmncthf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9082f0ec66dc87ddcf7e2f275893ea58d944c27" title="Qwen Code Now Offering 2000 free Qwen Code runs daily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tweet link: &lt;a href="https://x.com/Alibaba_Qwen/status/1953835877555151134"&gt;https://x.com/Alibaba_Qwen/status/1953835877555151134&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z1xto"&gt; /u/z1xto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0qdg1xmncthf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjxx6j</id>
    <title>GPT-OSS is Another Example Why Companies Must Build a Strong Brand Name</title>
    <updated>2025-08-07T11:49:08+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please, for the love of God, convince me that GPT-OSS is the best open-source model that exists today. I dare you to convince me. There's no way the GPT-OSS 120B is better than Qwen-235B-A22B-2507, let alone DeepSeek R1. So why do 90% of YouTubers, and even Two Minute Papers (a guy I respect), praise GPT-OSS as the most beautiful gift to humanity any company ever gave? &lt;/p&gt; &lt;p&gt;It's not even multimodal, and they're calling it a gift? WTF for? Isn't that the same coriticim when Deepseek-R1 was released, that it was text-based only? In about 2 weeks, Alibaba released a video model (Wan2.2) , an image model (Qwen-Image) that are the best open-source models in their categories, two amazing 30B models that are super fast and punch above their weight, and two incredible 4B models ‚Äì yet barely any YouTubers covered them. Meanwhile, OpenAI launches a rather OK model and hell broke loose everywhere. How do you explain this? I can't find any rational explanation except OpenAI built a powerful brand name.&lt;/p&gt; &lt;p&gt;When DeepSeek-R1 was released, real innovation became public ‚Äì innovation GPT-OSS clearly built upon. How can a model have 120 Experts all stable without DeepSeek's paper? And to make matters worse, OpenAI dared to show their 20B model trained for under $500K! As if that's an achievement when DeepSeek R1 cost just $5.58 million ‚Äì 89x cheaper than OpenAI's rumored budgets. &lt;/p&gt; &lt;p&gt;Remember when every outlet (especially American ones) criticized DeepSeek: 'Look, the model is censored by the Communist Party. Do you want to live in a world of censorship?' Well, ask GPT-OSS about the Ukraine war and see if it answers you. The hypocrisy is rich. User &lt;a href="/u/Final_Wheel_7486"&gt;u/Final_Wheel_7486&lt;/a&gt; posted about this.&lt;/p&gt; &lt;p&gt;I'm not a coder or mathematician, and even if I were, these models wouldn't help much ‚Äì they're too limited. So I DON'T CARE ABOUT CODING SCORES ON BENCHMARKS. Don't tell me 'these models are very good at coding' as if a 20B model can actually code. Coders are a niche group. We need models that help average people.&lt;/p&gt; &lt;p&gt;This whole situation reminds me of that greedy guy who rarely gives to charity, then gets praised for doing the bare minimum when he finally does.&lt;/p&gt; &lt;p&gt;I am notsaying the models OpenAI released are bad, they simply aren't. But, what I am saying is that the hype is through the roof for an OK product. I want to hear your thoughts. &lt;/p&gt; &lt;p&gt;P.S. OpenAI fanboys, please keep it objective and civil!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjxx6j/gptoss_is_another_example_why_companies_must/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T11:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkhbs9</id>
    <title>OpenAI new open-source model is basically Phi-5</title>
    <updated>2025-08-08T00:50:55+00:00</updated>
    <author>
      <name>/u/ik-when-that-hotline</name>
      <uri>https://old.reddit.com/user/ik-when-that-hotline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ik-when-that-hotline"&gt; /u/ik-when-that-hotline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44828884"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T00:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkcwiv</id>
    <title>OpenAI open washing</title>
    <updated>2025-08-07T21:38:35+00:00</updated>
    <author>
      <name>/u/gwyngwynsituation</name>
      <uri>https://old.reddit.com/user/gwyngwynsituation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they‚Äôre not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior. And that concludes my rant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gwyngwynsituation"&gt; /u/gwyngwynsituation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T21:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkvks4</id>
    <title>How Attention Sinks Keep Language Models Stable</title>
    <updated>2025-08-08T13:43:01+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hanlab.mit.edu/blog/streamingllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T13:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke7ef</id>
    <title>120B runs awesome on just 8GB VRAM!</title>
    <updated>2025-08-07T22:32:04+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU (&lt;del&gt;~17T/s&lt;/del&gt; 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt; &lt;p&gt;You can offload just the attention layers to GPU (requiring about 5 to 8GB of VRAM) for fast prefill.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache for the sequence&lt;/li&gt; &lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt; &lt;li&gt;Routing tables&lt;/li&gt; &lt;li&gt;LayerNorms and other ‚Äúnon-expert‚Äù parameters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt; &lt;p&gt;This yields an amazing snappy system for a 120B model! Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt; &lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;with 5GB of vram usage!&lt;/p&gt; &lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt; &lt;p&gt;edit: with this latest PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 36 \ #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster. --n-gpu-layers 999 \ #everything else on the GPU, about 8GB -c 0 -fa \ #max context (128k), flash attention --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 94593.62 ms / 12717 tokens ( 7.44 ms per token, 134.44 tokens per second) eval time = 76741.17 ms / 1966 tokens ( 39.03 ms per token, 25.62 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hitting above 25T/s with only 8GB VRAM use!&lt;/p&gt; &lt;p&gt;Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 28 \ --n-gpu-layers 999 \ -c 0 -fa \ --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 78003.66 ms / 12715 tokens ( 6.13 ms per token, 163.01 tokens per second) eval time = 70376.61 ms / 2169 tokens ( 32.45 ms per token, 30.82 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkavhy</id>
    <title>random bar chart made by Qwen3-235B-A22B-2507</title>
    <updated>2025-08-07T20:19:55+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt; &lt;img alt="random bar chart made by Qwen3-235B-A22B-2507" src="https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd853635222d78299767b459957da8a9ae9f30b5" title="random bar chart made by Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rka3lhpnonhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkp0am</id>
    <title>Granite 3 8B is seriously underrated - still outperforming newer models</title>
    <updated>2025-08-08T07:42:17+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building AI pipelines using the 12 factor agent approach (shoutout to Dex - check out his YouTube talk and GitHub), and I have to say IBM's Granite 3 8B continues to impress me nearly a year after release.This model consistently outperforms newer closed-source options (yes, I talked about GPT-5 mini/nano) on specific tasks. Where it really shines:&lt;/p&gt; &lt;p&gt;Task classification with structured outputs - It's incredibly reliable at categorizing user requests into the right buckets&lt;/p&gt; &lt;p&gt;Keyword generation for search/RAG - Produces solid results for information retrieval&lt;/p&gt; &lt;p&gt;If you haven't tried Granite 3 8B yet, it's worth adding to your toolkit. I still use larger models for the final aggregation and presentation layer, but for these specialized tasks, Granite punches well above its weight class.&lt;/p&gt; &lt;p&gt;Anyone else having similar experiences with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkxmoa</id>
    <title>GLM-4.5 series new models will be open source soon</title>
    <updated>2025-08-08T15:03:54+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt; &lt;img alt="GLM-4.5 series new models will be open source soon" src="https://preview.redd.it/mmvy25c79thf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff0519caa8ad5551e7e553775eb89d38a4aeb2c0" title="GLM-4.5 series new models will be open source soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mmvy25c79thf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkon92</id>
    <title>Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China</title>
    <updated>2025-08-08T07:18:22+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt; &lt;img alt="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" src="https://preview.redd.it/u7fdqw6zwqhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3114187ca53c4b97153190460034166fc59eaccc" title="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I started &lt;a href="https://www.designarena.ai/"&gt;my benchmark&lt;/a&gt; just about a month and a half ago, it has been interesting to see just how well the open weight / open source models are competing with their proprietary counterparts when evaluated on how user comparisons of different generations from each model. &lt;/p&gt; &lt;p&gt;Based on the benchmark, Qwen3 Coder, DeepSeek R1-0528, DeepSeek V3-2024, Qwen3 Instruct 2507, and GLM 4.5 could all be considered to be SOTA. I do think this ranking will change slightly though with one of the OS models being pushed out for GPT-5 (which was recently added, so sample size is too small). &lt;/p&gt; &lt;p&gt;That said, it really feels like we're in a golden age of open source models right now. We're also see a good amount of stagnation right now in the improvements being made by the proprietary models. Do you think OS will continue to keep pace? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7fdqw6zwqhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkw4ug</id>
    <title>GLM45 vs GPT-5, Claude Sonnet 4, Gemini 2.5 Pro ‚Äî live coding test, same prompt</title>
    <updated>2025-08-08T14:05:13+00:00</updated>
    <author>
      <name>/u/darkageofme</name>
      <uri>https://old.reddit.com/user/darkageofme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre running a live benchmark today with &lt;strong&gt;GLM45&lt;/strong&gt; in the mix against three major proprietary LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every model gets the same prompt for each task&lt;/li&gt; &lt;li&gt;Multiple attempts: simple builds, bug fixes, complex projects, and possibly planning tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll record:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How GLM45 performs on speed and accuracy&lt;/li&gt; &lt;li&gt;Where it matches or beats closed models&lt;/li&gt; &lt;li&gt;Debug handling in a live environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;16:00 UTC / 19:00 EEST&lt;/p&gt; &lt;p&gt;You'll find us here: &lt;a href="https://live.biela.dev"&gt;https://live.biela.dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkageofme"&gt; /u/darkageofme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkwkcd</id>
    <title>What do you think it will be?</title>
    <updated>2025-08-08T14:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt; &lt;img alt="What do you think it will be?" src="https://preview.redd.it/it28f5ns1thf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e5ae37fbf6696b49a3a1409b98db6c6507247a" title="What do you think it will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/it28f5ns1thf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkq4i4</id>
    <title>Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507</title>
    <updated>2025-08-08T08:55:44+00:00</updated>
    <author>
      <name>/u/acec</name>
      <uri>https://old.reddit.com/user/acec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt; &lt;img alt="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They claim that &amp;quot;On sequences approaching 1M tokens, the system achieves up to a &lt;strong&gt;3√ó speedup&lt;/strong&gt; compared to standard attention implementations.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acec"&gt; /u/acec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507/commit/3ffd1f50b179e643d839c86df9ffbbefcb0d5018"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T08:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkowrw</id>
    <title>Llama.cpp just added a major 3x performance boost.</title>
    <updated>2025-08-08T07:35:50+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama cpp just merged the final piece to fully support attention sinks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My prompt processing speed went from 300 to 1300 with a 3090 for the new oss model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkf543</id>
    <title>To all GPT-5 posts</title>
    <updated>2025-08-07T23:11:59+00:00</updated>
    <author>
      <name>/u/Danny_Davitoe</name>
      <uri>https://old.reddit.com/user/Danny_Davitoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt; &lt;img alt="To all GPT-5 posts" src="https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1" title="To all GPT-5 posts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please. I don‚Äôt care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny_Davitoe"&gt; /u/Danny_Davitoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v08gwidjohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkngs6</id>
    <title>I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results.</title>
    <updated>2025-08-08T06:06:41+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt; &lt;img alt="I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results." src="https://preview.redd.it/n3tapryqkqhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef40b2d7394bb222878d6008796d540bdf41673e" title="I had to try the ‚Äúblueberry‚Äù thing myself with GPT5. I merely report the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT5 keep saying it is the real deal lol. Is working but still far from the real deal in my opinion. &lt;/p&gt; &lt;p&gt;Credit: Kieran Healy‚Ä™@kjhealy.co‚Ä¨&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3tapryqkqhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T06:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkrb18</id>
    <title>üöÄ Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context‚Äîup to 1 million tokens!</title>
    <updated>2025-08-08T10:11:45+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt; &lt;img alt="üöÄ Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context‚Äîup to 1 million tokens!" src="https://preview.redd.it/ud233u23trhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9666b41b192bef19c1d95e2dc31745f398def8d7" title="üöÄ Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context‚Äîup to 1 million tokens!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context‚Äîup to 1 million tokens!&lt;/p&gt; &lt;p&gt;üîß Powered by:&lt;/p&gt; &lt;p&gt;‚Ä¢ Dual Chunk Attention (DCA) ‚Äì A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence. &lt;/p&gt; &lt;p&gt;‚Ä¢ MInference ‚Äì Sparse attention that cuts overhead by focusing on key token interactions&lt;/p&gt; &lt;p&gt;üí° These innovations boost both generation quality and inference speed, delivering up to 3√ó faster performance on near-1M token sequences.&lt;/p&gt; &lt;p&gt;‚úÖ Fully compatible with vLLM and SGLang for efficient deployment.&lt;/p&gt; &lt;p&gt;üìÑ See the update model cards for how to enable this feature.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ud233u23trhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
