<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-26T14:38:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jk64d7</id>
    <title>Installation commands for whisper.cpp's talk-llama on Android's termux</title>
    <updated>2025-03-26T07:18:50+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt; &lt;img alt="Installation commands for whisper.cpp's talk-llama on Android's termux" src="https://external-preview.redd.it/MnlnNO1j_5bv5TAC7lTjiCkmmkBFYOpH0cfI5V_na2M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae5685e95d73e7f40e3ed12ad1d509c1c9bf2ff1" title="Installation commands for whisper.cpp's talk-llama on Android's termux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whisper.cpp is a project to run openai's speech-to-text models. It uses the same machine learning library as llama.cpp: &lt;strong&gt;ggml&lt;/strong&gt; - maintained by ggerganov and contributors.&lt;/p&gt; &lt;p&gt;In this project exists a simple executable: which you can create and run on any device. This post provides further details for creating and running the executable on Android phones. Here is the example provided in whisper.cpp:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama"&gt;https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk-llama&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Pre-requisites:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Download f-droid from here: &lt;a href="https://f-droid.org"&gt;https://f-droid.org&lt;/a&gt; refresh to update the app list to newest.&lt;/li&gt; &lt;li&gt;Download &amp;quot;Termux&amp;quot; and &amp;quot;termux-api&amp;quot; apps using f-droid. &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;1. Install Dependencies:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pkg update # (hit return on all) pkg install termux-api wget git cmake clang x11-repo -y pkg install sdl2 pulseaudio espeak -y # enable Microphone permissions termux-microphone-record -d -f /tmp/audio_recording.wav # records with microphone for 10 seconds &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;2. Build it:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggerganov/whisper.cpp cd whisper.cpp cmake -B build -S . -DWHISPER_SDL2=ON cmake --build build --config Release cp build/bin/whisper-talk-llama . cp examples/talk-llama/speak . chmod +x speak touch speak_file wget -c https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.en.bin wget -c https://huggingface.co/mradermacher/SmolLM-135M-GGUF/resolve/main/SmolLM-135M.Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;3. Run with this command:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pulseaudio --start &amp;amp;&amp;amp; pactl load-module module-sles-source &amp;amp;&amp;amp; ./whisper-talk-llama -c 0 -mw ggml-tiny.en.bin -ml SmolLM-135M.Q4_K_M.gguf -s speak -sf speak_file &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Next steps:&lt;/h1&gt; &lt;p&gt;Try larger models until response time becomes too slow: &lt;code&gt;wget -c&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf"&gt;&lt;code&gt;https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_0.gguf&lt;/code&gt;&lt;/a&gt; Replace your -ml flag with your model.&lt;/p&gt; &lt;p&gt;You can get the realtime interruption and sentence-wise tts operation by running the glados project in a more proper debian linux environment within termux. There is currently a bug where the models don't download consistently.&lt;/p&gt; &lt;p&gt;Both talk-llama and glados can be run properly while under load. Here's an example where I chat with gemma 1B and play a demanding 3D game.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jk64d7/video/df8l0ncmgzqe1/player"&gt;https://reddit.com/link/1jk64d7/video/df8l0ncmgzqe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hope you benefit from this tutorial. Cancel the process with Ctrl+C, or the phone will keep models in RAM, which uses battery while sleeping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T07:18:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkbmj5</id>
    <title>X99 huananzhi f8d PLUS PCIE spacing vs 3090 Blower cards</title>
    <updated>2025-03-26T13:21:07+00:00</updated>
    <author>
      <name>/u/Judtoff</name>
      <uri>https://old.reddit.com/user/Judtoff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, the huananzhi f8d PLUS has 6 PCIE slots, but the documentation isn't great. I can't tell what the spacing is. I was debating on using this motherboard to run 6x 3090 with blowers (they look like they're just 2 slots wide, not 3). It looks like it'll fit, but I'm trying to dig up some documentation to confirm.&lt;/p&gt; &lt;p&gt;Although there might be a minimum clearance on the blower style 3090s, so even it it fits it might not be great thermally. &lt;/p&gt; &lt;p&gt;Currently I'm using 4x P40, which don't have their own fans, so I'm used to being able to densely pack the GPUs with no clearance.&lt;/p&gt; &lt;p&gt;The goal is to fit this in an existing eatx case, I want to avoid risers etc.&lt;/p&gt; &lt;p&gt;If it isn't feasible that's fine. I just thought I'd check here since I couldn't find anything successfully on the internet. &lt;/p&gt; &lt;p&gt;Thanks guys.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Judtoff"&gt; /u/Judtoff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbmj5/x99_huananzhi_f8d_plus_pcie_spacing_vs_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbmj5/x99_huananzhi_f8d_plus_pcie_spacing_vs_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbmj5/x99_huananzhi_f8d_plus_pcie_spacing_vs_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk69ns</id>
    <title>ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning</title>
    <updated>2025-03-26T07:29:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Agent-RL/ReSearch"&gt;https://github.com/Agent-RL/ReSearch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.19470"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk69ns/research_learning_to_reason_with_search_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk69ns/research_learning_to_reason_with_search_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T07:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjjv8k</id>
    <title>DeepSeek official communication on X: DeepSeek-V3-0324 is out now!</title>
    <updated>2025-03-25T13:53:11+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt; &lt;img alt="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" src="https://b.thumbs.redditmedia.com/__aOAn3RMb1pB4WQ7nZaRtP8KJ2vjbYZROoq35jWyoc.jpg" title="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjjv8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjqa9a</id>
    <title>AMD Is Reportedly Bringing Strix Halo To Desktop; CEO Lisa Su Confirms In An Interview.</title>
    <updated>2025-03-25T18:21:29+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/"&gt;https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is so awesome. You will be able to have up to 96Gb dedicated to Vram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkahkc</id>
    <title>Guide to work with 5080/90 Nvidia cards For Local Setup (linux/windows), For lucky/desperate ones to find one.</title>
    <updated>2025-03-26T12:24:04+00:00</updated>
    <author>
      <name>/u/ditpoo94</name>
      <uri>https://old.reddit.com/user/ditpoo94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing details for working with 50xx nvidia cards for Ai (Deep learning) etc.&lt;/p&gt; &lt;p&gt;I checked and no one has shared details for this, took some time for, sharing for other looking for same.&lt;/p&gt; &lt;p&gt;Sharing my findings from building and running a multi gpu 5080/90 Linux (debian/ubuntu) Ai rig (As of March'25) for the lucky one to get a hold of them.&lt;/p&gt; &lt;p&gt;(This is work related so couldn't get older cards and had to buy them at premium, sadly had no other option)&lt;/p&gt; &lt;p&gt;- Install latest drivers and cuda stuff from nvidia &lt;/p&gt; &lt;p&gt;- Works and tested with Ubuntu 24 lts, kernel v 6.13.6, gcc-14&lt;/p&gt; &lt;p&gt;- Multi gpu setup also works and tested with a combination of 40xx series and 50xx series Nvidia card&lt;/p&gt; &lt;p&gt;- For pytorch current version don't work fully, use the nightyly version for now, Will be stable in few weeks/month&lt;/p&gt; &lt;p&gt;pip install --pre torch torchvision torchaudio --index-url &lt;a href="https://download.pytorch.org/whl/nightly/cu128"&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- For local serving and use with llama.cpp/ollama and vllm you have to build them locally for now, support will be available in few weeks/month&lt;/p&gt; &lt;p&gt;Build llama.cpp locally &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md"&gt;https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Build vllm locally / guide for 5000 series card&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/issues/14452"&gt;https://github.com/vllm-project/vllm/issues/14452&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- For local runing of image/diffusion based model and ui with AUTOMATIC1111 &amp;amp; ComfyUI, following are for windows but if you get pytorch working on linux then it works on them as well with latest drivers and cuda&lt;/p&gt; &lt;p&gt;AUTOMATIC1111 guide for 5000 series card on windows&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/16824"&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/16824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ComfyUI guide for 5000 series card on windows&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/discussions/6643"&gt;https://github.com/comfyanonymous/ComfyUI/discussions/6643&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ditpoo94"&gt; /u/ditpoo94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkahkc/guide_to_work_with_508090_nvidia_cards_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkahkc/guide_to_work_with_508090_nvidia_cards_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkahkc/guide_to_work_with_508090_nvidia_cards_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T12:24:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjusya</id>
    <title>Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands</title>
    <updated>2025-03-25T21:24:35+00:00</updated>
    <author>
      <name>/u/AmbitiousSeaweed101</name>
      <uri>https://old.reddit.com/user/AmbitiousSeaweed101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt; &lt;img alt="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" src="https://preview.redd.it/laea7v40lwqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258ada96674ff89666373dd77c431418f63438cc" title="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmbitiousSeaweed101"&gt; /u/AmbitiousSeaweed101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/laea7v40lwqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjpsfp</id>
    <title>Google's new Gemini 2.5 beats all other thinking model as per their claims in their article . What are your views on this?</title>
    <updated>2025-03-25T18:01:43+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking"&gt;https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgje5</id>
    <title>We got competition</title>
    <updated>2025-03-25T10:50:03+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt; &lt;img alt="We got competition" src="https://preview.redd.it/bamkfj1yftqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c43810bb7e5d8ea7891aeebc79e47a801d562c8" title="We got competition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bamkfj1yftqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgi8y</id>
    <title>Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys.</title>
    <updated>2025-03-25T10:47:48+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt; &lt;img alt="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." src="https://preview.redd.it/4hh6ys9gftqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01ab49fd276d31a93696fb2a9a9f51d5ad35c7" title="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hh6ys9gftqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjuu78</id>
    <title>New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context</title>
    <updated>2025-03-25T21:25:54+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt; &lt;img alt="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" src="https://preview.redd.it/ks0djm85lwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c523a2ac3957a50567391d0e0d6a09816702e7" title="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ks0djm85lwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk9bn7</id>
    <title>So i just received my new Rig</title>
    <updated>2025-03-26T11:15:59+00:00</updated>
    <author>
      <name>/u/getmevodka</name>
      <uri>https://old.reddit.com/user/getmevodka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk9bn7/so_i_just_received_my_new_rig/"&gt; &lt;img alt="So i just received my new Rig" src="https://preview.redd.it/ao248wdhp0re1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fea7a17b64042d53e0184c9139870d33626c8721" title="So i just received my new Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;currently its updating but i will be able to test plenty after that i guess. &lt;/p&gt; &lt;p&gt;its a 28/60 256 2tb model. &lt;/p&gt; &lt;p&gt;what would you like to see me test if any ? &lt;/p&gt; &lt;p&gt;i know many people still holding off between the 256 and 512 model regarding inference because they think 256 may be not enough. &lt;/p&gt; &lt;p&gt;shoot at me ;) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getmevodka"&gt; /u/getmevodka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ao248wdhp0re1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk9bn7/so_i_just_received_my_new_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk9bn7/so_i_just_received_my_new_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjsiiw</id>
    <title>Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!</title>
    <updated>2025-03-25T19:51:51+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt; &lt;img alt="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" src="https://external-preview.redd.it/N283c2VudGQ0d3FlMV2EuLTbyq8GSEyVM5EFne5QcU-eiwqTnkibTrWsMAGW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=749ce5ef623cae53e2677afd031ff951b8a489ce" title="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/955pvmtd4wqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T19:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk857w</id>
    <title>Chonkie, the "no-nonsense RAG chunking library" just vanished from GitHub</title>
    <updated>2025-03-26T09:56:52+00:00</updated>
    <author>
      <name>/u/SK33LA</name>
      <uri>https://old.reddit.com/user/SK33LA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using chonkie at work, and today we were looking for its docs. Then we realized that the GitHub repository was either deleted or marked as private, their website is down, and I couldn't find any mention of this on reddit or linkedin. Was I really the only one using it? I don't think so.&lt;/p&gt; &lt;p&gt;I still found the library on pypi, &lt;a href="https://github.com/GPTim/chonkie"&gt;here&lt;/a&gt; a GH repository with the latest pushed version 0.5.1&lt;/p&gt; &lt;p&gt;Does anyone have any news about what happened?&lt;/p&gt; &lt;p&gt;Original GH repository: &lt;a href="https://github.com/chonkie-ai/chonkie"&gt;Page not found · GitHub&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SK33LA"&gt; /u/SK33LA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk857w/chonkie_the_nononsense_rag_chunking_library_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk5udi</id>
    <title>Jensen Huang on GPUs - Computerphile</title>
    <updated>2025-03-26T06:58:20+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"&gt; &lt;img alt="Jensen Huang on GPUs - Computerphile" src="https://external-preview.redd.it/phUXsHRgF1E2nsygi2QqQyxnIknoI3fD9D7Q_h5m5rs.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879ede20cdcf5fe1c140d267b9146bf0d41f481a" title="Jensen Huang on GPUs - Computerphile" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=G6R7UOFx1bw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk5udi/jensen_huang_on_gpus_computerphile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T06:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk57au</id>
    <title>How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way.</title>
    <updated>2025-03-26T06:10:37+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt; &lt;img alt="How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way." src="https://preview.redd.it/f5ex9ltz6zqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d8be8b19f9bc0bb0f4e9c01fe98d94cdbb16ebf" title="How I adapted a 1B function calling LLM for fast routing and agent hand -off scenarios in a framework agnostic way." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might have heard a thing or two about agents. Things that have high level goals and usually run in a loop to complete a said task - the trade off being latency for some powerful automation work&lt;/p&gt; &lt;p&gt;Well if you have been building with agents then you know that users can switch between them.Mid context and expect you to get the routing and agent hand off scenarios right. So now you are focused on not only working on the goals of your agent you are also working on thus pesky work on fast, contextual routing and hand off &lt;/p&gt; &lt;p&gt;Well I just adapted Arch-Function a SOTA function calling LLM that can make precise tools calls for common agentic scenarios to support routing to more coarse-grained or high-level agent definitions&lt;/p&gt; &lt;p&gt;The project can be found here: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt; and the models are listed in the README. &lt;/p&gt; &lt;p&gt;Happy bulking 🛠️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5ex9ltz6zqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk57au/how_i_adapted_a_1b_function_calling_llm_for_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T06:10:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjv68r</id>
    <title>Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)</title>
    <updated>2025-03-25T21:39:40+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt; &lt;img alt="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" src="https://preview.redd.it/vnkynyqrnwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b966146c75053316ab0b7e51083981dd658f31bd" title="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnkynyqrnwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkcd5l</id>
    <title>😲 DeepSeek-V3-4bit &gt;20tk/s, &lt;200w on M3 Ultra 512GB, MLX</title>
    <updated>2025-03-26T13:56:26+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might be the best and most user-friendly way to run DeepSeek-V3 on consumer hardware, possibly the most affordable too.&lt;/p&gt; &lt;p&gt;It sounds like you can finally run a GPT-4o level model locally at home, possibly with even better quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/"&gt;https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvo4e</id>
    <title>we are just 3 months into 2025</title>
    <updated>2025-03-25T22:00:40+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt; &lt;img alt="we are just 3 months into 2025" src="https://b.thumbs.redditmedia.com/UkF78GvO1l_Iu4ZikUoUTSJvE6F25Fvn1d1yTgP75FU.jpg" title="we are just 3 months into 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81"&gt;https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk97sp</id>
    <title>Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making</title>
    <updated>2025-03-26T11:08:59+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt; &lt;img alt="Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making" src="https://external-preview.redd.it/KYF-69e6kbGPdV1zCvq_5UYTDAc8xutzlnroB8C0qjQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c787cc38ce9acd3a889f5eb94343d6bba3f4c51b" title="Fin-R1:A Specialized Large Language Model for Financial Reasoning and Decision-Making" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fin-R1 is a large financial reasoning language model designed to tackle key challenges in financial AI, including fragmented data, inconsistent reasoning logic, and limited business generalization. It delivers state-of-the-art performance by utilizing a two-stage training process—SFT and RL—on the high-quality Fin-R1-Data dataset. With a compact 7B parameter scale, it achieves scores of 85.0 in ConvFinQA and 76.0 in FinQA, outperforming larger models. Future work aims to enhance financial multimodal capabilities, strengthen regulatory compliance, and expand real-world applications, driving innovation in fintech while ensuring efficient and intelligent financial decision-making.&lt;/p&gt; &lt;p&gt;The reasoning abilities of Fin-R1 in financial scenarios were evaluated through a comparative analysis against several state-of-the-art models, including DeepSeek-R1, Fin-R1-SFT, and various Qwen and Llama-based architectures. Despite its compact 7B parameter size, Fin-R1 achieved a notable average score of 75.2, ranking second overall. It outperformed all models of similar scale and exceeded DeepSeek-R1-Distill-Llama-70B by 8.7 points. Fin-R1 ranked highest in FinQA and ConvFinQA with scores of 76.0 and 85.0, respectively, demonstrating strong financial reasoning and cross-task generalization, particularly in benchmarks like Ant_Finance, TFNS, and Finance-Instruct-500K.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h3ykrngjn0re1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bb2dd12be4e245ce360cbb2d4aa48265958f9dd"&gt;https://preview.redd.it/h3ykrngjn0re1.png?width=617&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bb2dd12be4e245ce360cbb2d4aa48265958f9dd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lbr6y8kun0re1.gif"&gt;https://i.redd.it/lbr6y8kun0re1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p1hgmlwwn0re1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=579c66b858a8b13260e56cdcf3d181fb6d3a6e91"&gt;https://preview.redd.it/p1hgmlwwn0re1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=579c66b858a8b13260e56cdcf3d181fb6d3a6e91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1"&gt;HuggingFace (only Chinese)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.16252"&gt;Paper &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1/blob/main/README_en.md"&gt;HuggingFace (eng)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk97sp/finr1a_specialized_large_language_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T11:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjorwd</id>
    <title>I think we’re going to need a bigger bank account.</title>
    <updated>2025-03-25T17:20:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt; &lt;img alt="I think we’re going to need a bigger bank account." src="https://preview.redd.it/zr3syf8mdvqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27187b0a5f34d831c3e26fc2978cc6ab6324cf35" title="I think we’re going to need a bigger bank account." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zr3syf8mdvqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T17:20:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk0qjs</id>
    <title>1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF</title>
    <updated>2025-03-26T01:51:16+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt; &lt;img alt="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" src="https://b.thumbs.redditmedia.com/VQUhjwmdzkKwJU-pPDHGMZROeUu65PNj2UGT0ZKAjUg.jpg" title="1.78bit DeepSeek-V3-0324 - 230GB Unsloth Dynamic GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA! We're back again to release DeepSeek-V3-0324 (671B) dynamic quants in &lt;strong&gt;1.78-bit and more GGUF formats&lt;/strong&gt; so you can run them locally. All GGUFs are at &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/67rxi0wo3yqe1.gif"&gt;https://i.redd.it/67rxi0wo3yqe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We initially provided the &lt;strong&gt;1.58-bit version&lt;/strong&gt;, which you can still use but its outputs weren't the best. So, we found it necessary to upcast to 1.78-bit by increasing the down proj size to achieve much better performance.&lt;/p&gt; &lt;p&gt;To ensure the best tradeoff between accuracy and size, we do &lt;strong&gt;not to quantize all layers&lt;/strong&gt;, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit. This time we also added 3.5 + 4.5-bit dynamic quants.&lt;/p&gt; &lt;p&gt;Read our Guide on How To Run the GGUFs on llama.cpp: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also found that if you use convert all layers to 2-bit (standard 2-bit GGUF), the model is still very bad, producing endless loops, gibberish and very poor code. Our Dynamic 2.51-bit quant largely solves this issue. The same applies for 1.78-bit however is it recommended to use our 2.51 version for best results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model uploads:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.78bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;151GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_S"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.93bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;178GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.42-bit (prelim)&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;203GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ2_XXS"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2.71-bit (best)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;231GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;321GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;406GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;For recommended settings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Temperature of 0.3&lt;/strong&gt; (Maybe 0.0 for coding as &lt;a href="https://api-docs.deepseek.com/quick_start/parameter_settings"&gt;seen here&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)&lt;/li&gt; &lt;li&gt;Chat template: &lt;code&gt;&amp;lt;｜User｜&amp;gt;Create a simple playable Flappy Bird Game in Python. Place the final game inside of a markdown section.&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;A BOS token of &lt;code&gt;&amp;lt;｜begin▁of▁sentence｜&amp;gt;&lt;/code&gt; is auto added during tokenization (do NOT add it manually!)&lt;/li&gt; &lt;li&gt;DeepSeek mentioned using a &lt;strong&gt;system prompt&lt;/strong&gt; as well (optional) - it's in Chinese: &lt;code&gt;该助手为DeepSeek Chat，由深度求索公司创造。\n今天是3月24日，星期一。&lt;/code&gt; which translates to: &lt;code&gt;The assistant is DeepSeek Chat, created by DeepSeek.\nToday is Monday, March 24th.&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;For KV cache quantization, use 8bit, NOT 4bit - we found it to do noticeably worse.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I suggest people to run the 2.71bit for now - the other other bit quants (listed as prelim) are still processing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# !pip install huggingface_hub hf_transfer import os os.environ[&amp;quot;HF_HUB_ENABLE_HF_TRANSFER&amp;quot;] = &amp;quot;1&amp;quot; from huggingface_hub import snapshot_download snapshot_download( repo_id = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, local_dir = &amp;quot;unsloth/DeepSeek-V3-0324-GGUF&amp;quot;, allow_patterns = [&amp;quot;*UD-Q2_K_XL*&amp;quot;], # Dynamic 2.7bit (230GB) ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I did both the Flappy Bird and Heptagon test (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i%5C_just%5C_made%5C_an%5C_animation%5C_of%5C_a%5C_ball%5C_bouncing/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i\_just\_made\_an\_animation\_of\_a\_ball\_bouncing/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk0qjs/178bit_deepseekv30324_230gb_unsloth_dynamic_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T01:51:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk96ei</id>
    <title>Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License.</title>
    <updated>2025-03-26T11:06:27+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt; &lt;img alt="Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License." src="https://external-preview.redd.it/to4DeErJUd6nuRuuLqu9lyq844P83bUoZfIxhOq-ba0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89c5f9dbe9205ff855bef1629d1286873f325f34" title="Ling: A new MoE model series - including Ling-lite, Ling-plus and Ling-Coder-lite. Instruct + Base models available. MIT License." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Ling Lite and Ling Plus:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-Lite and Ling-Plus. Ling-Lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling Coder Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ling-Coder-Lite is a MoE LLM provided and open-sourced by InclusionAI, which has 16.8 billion parameters with 2.75 billion activated parameters. Ling-Coder-Lite performs impressively on coding tasks compared to existing models in the industry. Specifically, Ling-Coder-Lite further pre-training from an intermediate checkpoint of Ling-Lite, incorporating an additional 3 trillion tokens. This extended pre-training significantly boosts the coding abilities of Ling-Lite, while preserving its strong performance in general language tasks. More details are described in the technique report &lt;a href="https://huggingface.co/papers/2503.17793"&gt;Ling-Coder-TR&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32"&gt;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.05139"&gt;https://arxiv.org/abs/2503.05139&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/inclusionAI/Ling"&gt;https://github.com/inclusionAI/Ling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I would really recommend reading the paper, there's a section called &amp;quot;Bitter Lessons&amp;quot; which covers some of the problems someone might run into making models from scratch. It was insightful to read.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am not affiliated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some benchmarks (more in the paper):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dbqo9n1in0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0bb829ad58a67e12305675b519dfd6cca8354d6"&gt;https://preview.redd.it/dbqo9n1in0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0bb829ad58a67e12305675b519dfd6cca8354d6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Plus:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rexwjtuin0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01cc607e77117c025817add398ddc5329337275c"&gt;https://preview.redd.it/rexwjtuin0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01cc607e77117c025817add398ddc5329337275c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-Lite:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eka1kg0fp0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d069b0069e96400737104c065c3851628543f8b4"&gt;https://preview.redd.it/eka1kg0fp0re1.png?width=1661&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d069b0069e96400737104c065c3851628543f8b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk96ei/ling_a_new_moe_model_series_including_linglite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T11:06:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkbh4f</id>
    <title>Google releases TxGemma, open models for therapeutic applications</title>
    <updated>2025-03-26T13:13:38+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt; &lt;img alt="Google releases TxGemma, open models for therapeutic applications" src="https://external-preview.redd.it/w4gdQx4Lq4f5EYpFmMZH_AWSFA12WrreFd38e_AppFM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4444c30b578119b10e027ae74a8e53550f6800b" title="Google releases TxGemma, open models for therapeutic applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We're excited to share TxGemma! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemma 2-based model for multiple therapeutic tasks &lt;ul&gt; &lt;li&gt;Classification (will molecule cross blood-brain barrier)&lt;/li&gt; &lt;li&gt;Regression (drug's binding affinity)&lt;/li&gt; &lt;li&gt;Generation (given product of some reaction, generate reactant set)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2B, 9B, and 27B, with 27B being SOTA for many tasks, including versus single-task models&lt;/li&gt; &lt;li&gt;Chat version for general reasoning, to answer questions and engage in discussions&lt;/li&gt; &lt;li&gt;Fine-tunable with transformers, with an example notebook&lt;/li&gt; &lt;li&gt;Agentic-Tx for agentic systems, powered with Gemini, and using TxGemma as a tool&lt;/li&gt; &lt;li&gt;Models on HF: &lt;a href="https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"&gt;https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/?linkId=13647386"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7cka</id>
    <title>Plenty 3090 FE's for sale in the Netherlands</title>
    <updated>2025-03-26T08:56:31+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt; &lt;img alt="Plenty 3090 FE's for sale in the Netherlands" src="https://preview.redd.it/3bxpnick00re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7295d7a24ea4033f0e4d96d4cdbf0b662770a23a" title="Plenty 3090 FE's for sale in the Netherlands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bxpnick00re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T08:56:31+00:00</published>
  </entry>
</feed>
