<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-17T06:07:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i2ww9q</id>
    <title>benchmarks and real world comparisons QwQ 72B vs. DeepSeek V3 vs. Claude 3.5 Sonnet vs. Llama405B</title>
    <updated>2025-01-16T19:20:51+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking specifically at these models and want to understand how they compare in real world situations. Hoping someone has a good table and details on what model did best for a particular task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ww9q/benchmarks_and_real_world_comparisons_qwq_72b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T19:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i30bjx</id>
    <title>What LLMs Do You Recommend For a RTX 2060 (6 GB) For Roleplay?</title>
    <updated>2025-01-16T21:48:24+00:00</updated>
    <author>
      <name>/u/AdvertisingOk6742</name>
      <uri>https://old.reddit.com/user/AdvertisingOk6742</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i’m interested in both sfw, nsfw and even nsfl roleplay&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdvertisingOk6742"&gt; /u/AdvertisingOk6742 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i30bjx/what_llms_do_you_recommend_for_a_rtx_2060_6_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T21:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i37x87</id>
    <title>Whats the current State-of-The-Art for voice cloning?</title>
    <updated>2025-01-17T04:02:10+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;last time i checked which was quite a while voice cloning and like making AI song covers and etc used RVC v2 but im sure a LOT has changed since then Ive heard a lot of stuff about tts models like the new 82M model but i dont think ive heard specifically about voice cloning and cover tools&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i37x87/whats_the_current_stateoftheart_for_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T04:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i340rd</id>
    <title>GPU Enclosure Experiences?</title>
    <updated>2025-01-17T00:39:15+00:00</updated>
    <author>
      <name>/u/ilovepolthavemybabie</name>
      <uri>https://old.reddit.com/user/ilovepolthavemybabie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for the noob question, but will an eGPU enclosure work as well for LLM loading as it would for gaming?&lt;/p&gt; &lt;p&gt;I have a 4070ti incompatible with my PC (OEM XPS PSU can’t handle it). The card I have now is a 3060Ti. I got the 4070 so cheap that even with an enclosure it’d be less than avg used price. &lt;/p&gt; &lt;p&gt;If anyone has good/bad eGPU experience, that might sway me on keeping vs selling. It’s just been sitting in the box for awhile. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilovepolthavemybabie"&gt; /u/ilovepolthavemybabie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i340rd/gpu_enclosure_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T00:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2pvpc</id>
    <title>Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!</title>
    <updated>2025-01-16T14:17:23+00:00</updated>
    <author>
      <name>/u/Zealousideal_Bad_52</name>
      <uri>https://old.reddit.com/user/Zealousideal_Bad_52</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt; &lt;img alt="Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!" src="https://external-preview.redd.it/vqjX5SwLNlGMS1SSiHAT804_sRvPBHuMxyMU2GJpb1U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fe0aaff34a61b4f8f3fa0a386d0f9d14fa68f78" title="Now you can running InternLM3 8B using Qualcomm NPU with PowerServe!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We introduced &lt;strong&gt;PowerServe&lt;/strong&gt;, a serving framework designed specifically for Qualcomm NPU. Now we have already support &lt;strong&gt;Qwen, Llama&lt;/strong&gt; and &lt;strong&gt;InternLM3 8B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/powerserve-project/PowerServe"&gt;powerserve-project/PowerServe: High-speed and easy-use LLM serving framework for local deployment (github.com)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current open-source serving frameworks perform poorly in prefill speed on mobile devices, mainly due to limited CPU computing power. So we design PowerServe, a serving framework designed specifically for Qualcomm NPU, which achieves a prefill speed of &lt;strong&gt;1000&lt;/strong&gt; tokens/s of tokens per second for &lt;strong&gt;3B&lt;/strong&gt; models. This represents a &lt;strong&gt;100x&lt;/strong&gt; speedup compared to llama.cpp's &lt;strong&gt;15&lt;/strong&gt; tokens per second. For InternLM 8B, you can run it with &lt;strong&gt;250&lt;/strong&gt; tokens/s, significantly accelerating the prefill speed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i2pvpc/video/y93hx9ss6dde1/player"&gt;Running InternLM3 8B with Qualcomm 8Gen3 NPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pzxssjtw6dde1.png?width=2056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2918387653f5940983a6718fd10bf9659d458e52"&gt;Performance comparison between Llama.cpp and PowerServe.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Bad_52"&gt; /u/Zealousideal_Bad_52 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pvpc/now_you_can_running_internlm3_8b_using_qualcomm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T14:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31sxj</id>
    <title>How to use chat templates for multicharacter roleplays?</title>
    <updated>2025-01-16T22:54:17+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have implemented my own roleplay front-end for KoboldCpp. In contrast to SillyTavern and BackyardAI, my approach is not character-centric but rather scenario-centric. Both AI and the user can control multiple characters, and AI makes its own choice of who should speak next.&lt;/p&gt; &lt;p&gt;At first, I did not even bother to figure out how to use chat templates. I just send a simple example dialogue to the LLM together with my scenario:&lt;/p&gt; &lt;p&gt;Bob: Hi!&lt;/p&gt; &lt;p&gt;Anna: Hello!&lt;/p&gt; &lt;p&gt;Then I launch the generation and poll the API to check for the result. I look for a valid `Character Name:` marker in the response and allow only the characters that are setup for AI control. If I receive one more character marker, I stop the generation to avoid the infamous &amp;quot;speaking for others&amp;quot; issue, and clean up the response to remove the unnecessary text.&lt;/p&gt; &lt;p&gt;I'm testing it now and even Llama 3.2 3B seems to work quite OK with this setup.&lt;/p&gt; &lt;p&gt;However, I've heard that some models benefit from system prompts, and, as I understand, to pass the system prompt to the model, I need to use a proper chat template for the specific model. &lt;/p&gt; &lt;p&gt;And now we come to the root of the problem. &lt;strong&gt;Chat templates seem to be centered on the idea of only two parties - the user and the assistant. I have more parties. How would I encode their messages in a chat template?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A naive approach would be to send the system prompt with the proper formatting for the template, and then just dump the entire accumulated context with the scenario, character descriptions and all the chat messages into a single &amp;quot;assistant&amp;quot; message and ignore the user part of the template completely. &lt;/p&gt; &lt;p&gt;But wouldn't this somehow make the model less smart and not obey the scenario as well as it would if I separate the chat messages and create a single assistant (or user) message for every character's reply? &lt;/p&gt; &lt;p&gt;What are the practical effects of the chat template on the inference quality? Is the chat template just a convenient wrapper to properly separate messages in more complex situations or does it actually improve the model's behavior?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31sxj/how_to_use_chat_templates_for_multicharacter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2lh3b</id>
    <title>All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01</title>
    <updated>2025-01-16T09:48:38+00:00</updated>
    <author>
      <name>/u/bidet_enthusiast</name>
      <uri>https://old.reddit.com/user/bidet_enthusiast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"&gt; &lt;img alt="All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01" src="https://external-preview.redd.it/CaDa8UUx90v9PcEOeKGi-HSkE2urc6XyHG74Upv4XCw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=951db4cb8b0737b142d2311a74092a236c8f4e90" title="All new SOTA MOE open source model, up to 4M context. - MiniMax-AI/MiniMax-01" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bidet_enthusiast"&gt; /u/bidet_enthusiast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2lh3b/all_new_sota_moe_open_source_model_up_to_4m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T09:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i27l37</id>
    <title>Deepseek is overthinking</title>
    <updated>2025-01-15T20:57:13+00:00</updated>
    <author>
      <name>/u/Mr_Jericho</name>
      <uri>https://old.reddit.com/user/Mr_Jericho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt; &lt;img alt="Deepseek is overthinking" src="https://preview.redd.it/rz378lgd18de1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deff4f920457d1affd3bc98d78e4fc3601dda4b9" title="Deepseek is overthinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Jericho"&gt; /u/Mr_Jericho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rz378lgd18de1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i27l37/deepseek_is_overthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T20:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2nkui</id>
    <title>Do you think that LLMs can do better natural language translation than services like DeepL, GoogleTranslate, Microsoft Translate etc.?</title>
    <updated>2025-01-16T12:14:11+00:00</updated>
    <author>
      <name>/u/sassyhusky</name>
      <uri>https://old.reddit.com/user/sassyhusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My personal experience (which could be very subjective) with these translators is that even regular old chat bots with not much prompt engineering already produce better results with translations. Is this really just an unpopular opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sassyhusky"&gt; /u/sassyhusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2nkui/do_you_think_that_llms_can_do_better_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T12:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i34642</id>
    <title>Why do tools like Perplexity struggle to calculate accurate stats from different sources when the exact number is not posted online?</title>
    <updated>2025-01-17T00:46:39+00:00</updated>
    <author>
      <name>/u/vamos-viendo</name>
      <uri>https://old.reddit.com/user/vamos-viendo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been wondering why tools like Perplexity seem to fall short on calculating stats that don’t already exist online. Perplexity tries—with its reasoning steps—but the results often fail in accuracy or iterative depth. &lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;“What percentage of countries with universal healthcare also have female leaders?”&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If this functionality exists, I haven’t seen it work well. Curious—what do you think is the blocker here?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is it a complexity or cost issue (the multi-step iterative reasoning)?&lt;/li&gt; &lt;li&gt;Is the demand just not there?&lt;/li&gt; &lt;li&gt;Are these tools just focusing elsewhere?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vamos-viendo"&gt; /u/vamos-viendo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i34642/why_do_tools_like_perplexity_struggle_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T00:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2e23v</id>
    <title>I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!</title>
    <updated>2025-01-16T01:57:31+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt; &lt;img alt="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" src="https://external-preview.redd.it/ajBjajZ2YTFpOWRlMdVERFdEQKrY8cptLv00gyZBVqtju60x3iy8w-FpWSZ2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7aa615b1ccbb81cee65b5735b41605e27fcb9ed" title="I used Kokoro-82M, Llama 3.2, and Whisper Small to build a real-time speech-to-speech chatbot that runs locally on my MacBook!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yw01bva1i9de1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2e23v/i_used_kokoro82m_llama_32_and_whisper_small_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T01:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2ucxu</id>
    <title>I created a vscode extension that does inline edits using deepseek</title>
    <updated>2025-01-16T17:34:09+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt; &lt;img alt="I created a vscode extension that does inline edits using deepseek" src="https://external-preview.redd.it/c2Y1NHdiamk1ZWRlMSXLRLoBTWH7BkELeo8cMATHejXfU-O8HPWWGk2XwKZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=822e126be45f676ab516da632460316140e9e985" title="I created a vscode extension that does inline edits using deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wo2fucji5ede1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2ucxu/i_created_a_vscode_extension_that_does_inline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T17:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i29wz5</id>
    <title>Google just released a new architecture</title>
    <updated>2025-01-15T22:38:26+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like a big deal? &lt;a href="https://x.com/behrouz_ali/status/1878859086227255347"&gt;Thread by lead author&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i29wz5/google_just_released_a_new_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-15T22:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2pbyp</id>
    <title>Seems like used 3090 price is up near $850/$900?</title>
    <updated>2025-01-16T13:50:36+00:00</updated>
    <author>
      <name>/u/Synaps3</name>
      <uri>https://old.reddit.com/user/Synaps3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a bit of a sanity check here; it seems like used 3090's on eBay are up from around $650-$700 two weeks ago to $850-$1000 depending on the model after the disappointing 5090 announcement. Is this still a decent value proposition for an inference box? I'm about to pull the trigger on an H12SSL-i, but am on the fence about whether to wait for a potentially non-existent price drop on 3090 after 5090's are actually available and people try to flip their current cards. Short term goal is 70b Q4 inference server and NVLink for training non-language models. Any thoughts from secondhand GPU purchasing veterans?&lt;/p&gt; &lt;p&gt;Edit: also, does anyone know how long NVIDIA tends to provide driver support for their cards? I read somehow that 3090s inherit A100 driver support but I haven't been able to find any verification of this. It'd be a shame to buy two and have them be end-of-life in a year or two.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synaps3"&gt; /u/Synaps3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2pbyp/seems_like_used_3090_price_is_up_near_850900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i38jih</id>
    <title>Which do you think will be better: Qwen-3 or Llama-4</title>
    <updated>2025-01-17T04:37:46+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and which do you think will come out first? and more importantly will llama-4 actually have a middle ground size between 8 and 70 like ~30 so i can run it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i38jih/which_do_you_think_will_be_better_qwen3_or_llama4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i38jih/which_do_you_think_will_be_better_qwen3_or_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i38jih/which_do_you_think_will_be_better_qwen3_or_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T04:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i357ov</id>
    <title>4x AMD Instinct AI Server + Mistral 7B + vLLM</title>
    <updated>2025-01-17T01:38:35+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"&gt; &lt;img alt="4x AMD Instinct AI Server + Mistral 7B + vLLM" src="https://external-preview.redd.it/OXZzbzY0dmNrZ2RlMYrnczNrVsQkdH3BrjnNDBSvBen7AmAirsnxCxjuWUYQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1928bed39a3edac8783d4face60cb49bffa9a3a0" title="4x AMD Instinct AI Server + Mistral 7B + vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1sni53vckgde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i357ov/4x_amd_instinct_ai_server_mistral_7b_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T01:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i30yy4</id>
    <title>Where do people get news about upcoming LLM releases?</title>
    <updated>2025-01-16T22:17:01+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m curious about how people stay up-to-date on news about upcoming LLM releases, especially ones that haven’t been released yet. Are there specific websites, forums, newsletters, or communities you follow to learn about this kind of stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i30yy4/where_do_people_get_news_about_upcoming_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i35hs3</id>
    <title>My Tesla P40 just caught on fire and exploded… help?</title>
    <updated>2025-01-17T01:52:52+00:00</updated>
    <author>
      <name>/u/Cressio</name>
      <uri>https://old.reddit.com/user/Cressio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://imgur.com/a/1ViaFVL"&gt;https://imgur.com/a/1ViaFVL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Um… so, this GPU has an insanely long lore. To summarize, I ended up trying to sell it, UPS ravaged the box and the buyer claimed the GPU didn’t work anymore (wouldn’t power on), I received it back, tried to power it up, and it immediately caught on fire in catastrophic fashion and shot flames into my motherboard. &lt;/p&gt; &lt;p&gt;I’m powering them with a good quality PCIe to EPS adapter, which I just used again to try and check if it was indeed dead. Well, it sure as hell is now.&lt;/p&gt; &lt;p&gt;Uh, what the hell happened? What is the component that exploded? It looks to be power related and it had a thermal pad on the backplate that is now scorched. &lt;/p&gt; &lt;p&gt;I actually have ANOTHER P40 from this shipment that I’m wanting to test and I’m absolutely mortified to plug it in now. I don’t think I’ll ever trust a PC build again.&lt;/p&gt; &lt;p&gt;Edit: just to super clarify, these P40s worked before with this exact same setup and adapters. It just… happened to explode this time for some reason. System still powers on just fine without the GPU, thank god.&lt;/p&gt; &lt;p&gt;Edit 2: I was right, the other one works and he claimed both didn’t. I bet the one that just exploded worked too. Fuck. My life. The one that’s powered on right now is using the other (I had 2) adapter and a different PCIe cable. I am so absolutely terrified to try the other cables. This is almost a worst outcome than them both being DOA. Now I don’t even know what to do or trust lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cressio"&gt; /u/Cressio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i35hs3/my_tesla_p40_just_caught_on_fire_and_exploded_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-17T01:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2p6n3</id>
    <title>Why can't GPUs have removable memory like PC ram?</title>
    <updated>2025-01-16T13:43:00+00:00</updated>
    <author>
      <name>/u/Delicious-Farmer-234</name>
      <uri>https://old.reddit.com/user/Delicious-Farmer-234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was thinking, why doesn't Intel, Nvidia, or AMD come up with the idea of being able to expand the memory? I get it that DDR6 is pricey but if one of them were to create modules and sell them wouldn't they be able to profit? Image if Intel came out with this first, I bet most of us will max out the vram and the whole community will push away from Nvidia and create better or comparable frameworks other cuda. Thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Farmer-234"&gt; /u/Delicious-Farmer-234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2p6n3/why_cant_gpus_have_removable_memory_like_pc_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T13:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2xk8h</id>
    <title>Context &gt;</title>
    <updated>2025-01-16T19:49:05+00:00</updated>
    <author>
      <name>/u/MrCyclopede</name>
      <uri>https://old.reddit.com/user/MrCyclopede</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"&gt; &lt;img alt="Context &amp;gt;" src="https://preview.redd.it/281mgak4uede1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cb3b1f73fa5516de74f745937554917437aeb73" title="Context &amp;gt;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrCyclopede"&gt; /u/MrCyclopede &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/281mgak4uede1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2xk8h/context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T19:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2y810</id>
    <title>Is DeepSeek V3 overhyped?</title>
    <updated>2025-01-16T20:17:06+00:00</updated>
    <author>
      <name>/u/YourAverageDev0</name>
      <uri>https://old.reddit.com/user/YourAverageDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been using DeepSeek V3 for some time after all the time it came out. Coding wise (I work on web frontend, mostly react/svelte), I do not find it nearly as impressive as 3.5 Sonnet. The benchmarks seems to be matching, but the feel is just different, sometimes DeepSeek does give interesting stuff when asked. For me personally, it feels like a base 405B that has even been further scaled, it has little scars of brutal human RLHF (unlike OAI, LLaMa and etc Models). It just doesn't have that taste of Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;Curious what you guys think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YourAverageDev0"&gt; /u/YourAverageDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2y810/is_deepseek_v3_overhyped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T20:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2qokt</id>
    <title>Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM.</title>
    <updated>2025-01-16T14:55:36+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt; &lt;img alt="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." src="https://external-preview.redd.it/c2Y2dHB4cGdkZGRlMblxftDnj1ubBLQxBS031TPNonm7GOuytqVIBIDUD3XU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b475390d835ebe3032e27539598bb0f968273c4" title="Introducing Kokoro.js: a new JavaScript library for running Kokoro TTS (82M) locally in the browser w/ WASM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uv6trvpgddde1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2qokt/introducing_kokorojs_a_new_javascript_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T14:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2n0il</id>
    <title>How would you build an LLM agent application without using LangChain?</title>
    <updated>2025-01-16T11:37:48+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt; &lt;img alt="How would you build an LLM agent application without using LangChain?" src="https://preview.redd.it/q1d445cdecde1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53957b0b5cd83b245d383aec699f6fb075f1d50" title="How would you build an LLM agent application without using LangChain?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q1d445cdecde1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2n0il/how_would_you_build_an_llm_agent_application/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T11:37:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i31ji5</id>
    <title>What is ElevenLabs doing? How is it so good?</title>
    <updated>2025-01-16T22:42:26+00:00</updated>
    <author>
      <name>/u/Independent_Aside225</name>
      <uri>https://old.reddit.com/user/Independent_Aside225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. What's their trick? On everything but voice, local models are pretty good for what they are, but ElevenLabs just blows everyone out of the water. &lt;/p&gt; &lt;p&gt;Is it full Transformer? Some sort of Diffuser? Do they model the human anatomy to add accuracy to the model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Aside225"&gt; /u/Independent_Aside225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i2t82i</id>
    <title>Introducing Wayfarer: a brutally challenging roleplay model trained to let you fail and die.</title>
    <updated>2025-01-16T16:46:20+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One frustration we’ve heard from many AI Dungeon players is that AI models are too nice, never letting them fail or die. So we decided to fix that. We trained a model we call Wayfarer where adventures are much more challenging with failure and death happening frequently.&lt;/p&gt; &lt;p&gt;We released it on AI Dungeon several weeks ago and players loved it, so we’ve decided to open source the model for anyone to experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-12B"&gt;https://huggingface.co/LatitudeGames/Wayfarer-12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i2t82i/introducing_wayfarer_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-16T16:46:20+00:00</published>
  </entry>
</feed>
