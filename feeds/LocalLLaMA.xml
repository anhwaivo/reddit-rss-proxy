<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-14T06:08:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1laa9bj</id>
    <title>New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)</title>
    <updated>2025-06-13T07:55:10+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"&gt; &lt;img alt="New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)" src="https://external-preview.redd.it/4t6GOGdTOsYCXJc0r80Taopgc8TuG7QgRWRArsQ4GFY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da6a231d5b5f944073ecacc611122a4b945f2dd3" title="New VS Code update supports all MCP features (tools, prompts, sampling, resources, auth)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you have any questions about the release, let me know.&lt;/p&gt; &lt;p&gt;--vscode pm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_101"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laa9bj/new_vs_code_update_supports_all_mcp_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T07:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1la3uvz</id>
    <title>3.53bit R1 0528 scores 68% on the Aider Polygot</title>
    <updated>2025-06-13T01:36:43+00:00</updated>
    <author>
      <name>/u/BumblebeeOk3281</name>
      <uri>https://old.reddit.com/user/BumblebeeOk3281</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3.53bit R1 0528 scores 68% on the Aider Polyglot benchmark.&lt;/p&gt; &lt;p&gt;ram/vram required: 300GB&lt;/p&gt; &lt;p&gt;context size used: 40960 with flash attention&lt;/p&gt; &lt;p&gt;Edit 1: Polygot &amp;gt;&amp;gt; Polyglot :-)&lt;/p&gt; &lt;p&gt;Edit 2: *this was a download from a few days before the &amp;lt;tool\_calling&amp;gt; improvements Unsloth did 2 days ago. We will maybe do one more benchmark perhaps the updated &amp;quot;UD-IQ2_M&amp;quot;.&lt;/p&gt; &lt;p&gt;Edit 3: Unsloth 1.93bit UD_IQ1_M scored 60%&lt;/p&gt; &lt;p&gt;────────────────────────────- dirname: 2025-06-11-04-03-18--unsloth-DeepSeek-R1-0528-GGUF-UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;test_cases: 225&lt;/p&gt; &lt;p&gt;model: openai/unsloth/DeepSeek-R1-0528-GGUF/UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;edit_format: diff&lt;/p&gt; &lt;p&gt;commit_hash: 4c161f9-dirty&lt;/p&gt; &lt;p&gt;pass_rate_1: 32.9&lt;/p&gt; &lt;p&gt;pass_rate_2: 68.0&lt;/p&gt; &lt;p&gt;pass_num_1: 74&lt;/p&gt; &lt;p&gt;pass_num_2: 153&lt;/p&gt; &lt;p&gt;percent_cases_well_formed: 96.4&lt;/p&gt; &lt;p&gt;error_outputs: 15&lt;/p&gt; &lt;p&gt;num_malformed_responses: 15&lt;/p&gt; &lt;p&gt;num_with_malformed_responses: 8&lt;/p&gt; &lt;p&gt;user_asks: 72&lt;/p&gt; &lt;p&gt;lazy_comments: 0&lt;/p&gt; &lt;p&gt;syntax_errors: 0&lt;/p&gt; &lt;p&gt;indentation_errors: 0&lt;/p&gt; &lt;p&gt;exhausted_context_windows: 0&lt;/p&gt; &lt;p&gt;prompt_tokens: 2596907&lt;/p&gt; &lt;p&gt;completion_tokens: 2297409&lt;/p&gt; &lt;p&gt;test_timeouts: 2&lt;/p&gt; &lt;p&gt;total_tests: 225&lt;/p&gt; &lt;p&gt;command: aider --model openai/unsloth/DeepSeek-R1-0528-GGUF/UD-Q3_K_XL&lt;/p&gt; &lt;p&gt;date: 2025-06-11&lt;/p&gt; &lt;p&gt;versions: &lt;a href="http://0.84.1.dev"&gt;0.84.1.dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;seconds_per_case: 485.7&lt;/p&gt; &lt;p&gt;total_cost: 0.0000&lt;/p&gt; &lt;p&gt;─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumblebeeOk3281"&gt; /u/BumblebeeOk3281 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la3uvz/353bit_r1_0528_scores_68_on_the_aider_polygot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T01:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1la1v4d</id>
    <title>llama.cpp adds support to two new quantization format, tq1_0 and tq2_0</title>
    <updated>2025-06-12T23:59:21+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;which can be found at tools/convert_hf_to_gguf.py on github.&lt;/p&gt; &lt;p&gt;tq means ternary quantization, what's this? is for consumer device?&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; I have tried tq1_0 both llama.cpp on qwen3-8b and sd.cpp on flux. despite quantizing is fast, tq1_0 is hard to work at now time: qwen3 outputs messy chars while flux is 30x slower than k-quants after dequantizing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la1v4d/llamacpp_adds_support_to_two_new_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T23:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9lddr</id>
    <title>Petition: Ban 'announcement of announcement' posts</title>
    <updated>2025-06-12T12:36:45+00:00</updated>
    <author>
      <name>/u/RangaRea</name>
      <uri>https://old.reddit.com/user/RangaRea</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's no reason to have 5 posts a week about OpenAI announcing that they will release a model then delaying the release date it then announcing it's gonna be &lt;em&gt;amazing&lt;/em&gt;&lt;strong&gt;™&lt;/strong&gt; then announcing they will announce a new update in a month ad infinitum. Fuck those grifters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RangaRea"&gt; /u/RangaRea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9lddr/petition_ban_announcement_of_announcement_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T12:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lafihl</id>
    <title>Struggling on local multi-user inference? Llama.cpp GGUF vs VLLM AWQ/GPTQ.</title>
    <updated>2025-06-13T13:09:07+00:00</updated>
    <author>
      <name>/u/SomeRandomGuuuuuuy</name>
      <uri>https://old.reddit.com/user/SomeRandomGuuuuuuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I tested VLLM and Llama.cpp and got much better results from GGUF than AWQ and GPTQ (it was also hard to find this format for VLLM). I used the same system prompts and saw really crazy bad results on Gemma in GPTQ: higher VRAM usage, slower inference, and worse output quality.&lt;/p&gt; &lt;p&gt;Now my project is moving to multiple concurrent users, so I will need parallelism. I'm using either A10 AWS instances or L40s etc.&lt;/p&gt; &lt;p&gt;From my understanding, Llama.cpp is not optimal for the efficiency and concurrency I need, as I want to squeeze the as much request with same or smillar time for one and minimize VRAM usage if possible. I like GGUF as it's so easy to find good quantizations, but I'm wondering if I should switch back to VLLM.&lt;/p&gt; &lt;p&gt;I also considered Triton / NVIDIA Inference Server / Dynamo, but I'm not sure what's currently the best option for this workload.&lt;/p&gt; &lt;p&gt;Here is my current Docker setup for llama.cpp:&lt;/p&gt; &lt;p&gt;&lt;code&gt;cpp_3.1.8B:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image:&lt;/code&gt; &lt;a href="http://ghcr.io/ggml-org/llama.cpp:server-cuda"&gt;&lt;code&gt;ghcr.io/ggml-org/llama.cpp:server-cuda&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: cpp_3.1.8B&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ports:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- 8003:8003&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;volumes:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- ./models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf:/model/model.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_MODEL: /model/model.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_CTX_SIZE: 4096&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_N_PARALLEL: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_MAIN_GPU: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_N_GPU_LAYERS: 99&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_ENDPOINT_METRICS: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_PORT: 8003&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;LLAMA_ARG_FLASH_ATTN: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GGML_CUDA_FORCE_MMQ: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;GGML_CUDA_FORCE_CUBLAS: 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;deploy:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;resources:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;reservations:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- driver: nvidia&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;count: all&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;capabilities: [gpu]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And for vllm:&lt;br /&gt; &lt;code&gt;sudo docker run --runtime nvidia --gpus all \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-v ~/.cache/huggingface:/root/.cache/huggingface \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--env &amp;quot;HUGGING_FACE_HUB_TOKEN= \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-p 8003:8000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ipc=host \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--name gemma12bGPTQ \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--user 0 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;vllm/vllm-openai:latest \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model circulus/gemma-3-12b-it-gptq \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--gpu_memory_utilization=0.80 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--max_model_len=4096&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I would greatly appreciate feedback from people who have been through this — what stack works best for you today for maximum concurrent users? Should I fully switch back to VLLM? Is Triton / Nvidia NIM / Dynamo inference worth exploring or smth else?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeRandomGuuuuuuy"&gt; /u/SomeRandomGuuuuuuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lafihl/struggling_on_local_multiuser_inference_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T13:09:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1laf96d</id>
    <title>Mac Mini for local LLM? 🤔</title>
    <updated>2025-06-13T12:57:26+00:00</updated>
    <author>
      <name>/u/matlong</name>
      <uri>https://old.reddit.com/user/matlong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not much of an IT guy. Example: I bought a Synology because I wanted a home server, but didn't want to fiddle with things beyond me too much.&lt;/p&gt; &lt;p&gt;That being said, I am a programmer that uses a Macbook every day.&lt;/p&gt; &lt;p&gt;Is it possible to go the on-prem home LLM route using a Mac Mini?&lt;/p&gt; &lt;p&gt;Edit: for clarification, my goal would be to replace, for now, a general AI Chat model, with some AI Agent stuff down the road, but not use this for AI Coding Agents now as I don't think thats feasible personally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matlong"&gt; /u/matlong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laf96d/mac_mini_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l9wbaw</id>
    <title>Meta Is Offering Nine Figure Salaries to Build Superintelligent AI. Mark going All In.</title>
    <updated>2025-06-12T20:00:39+00:00</updated>
    <author>
      <name>/u/Neon_Nomad45</name>
      <uri>https://old.reddit.com/user/Neon_Nomad45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.entrepreneur.com/business-news/meta-is-offering-nine-figure-pay-for-superintelligence-team/493040"&gt;https://www.entrepreneur.com/business-news/meta-is-offering-nine-figure-pay-for-superintelligence-team/493040&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon_Nomad45"&gt; /u/Neon_Nomad45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l9wbaw/meta_is_offering_nine_figure_salaries_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-12T20:00:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lakgin</id>
    <title>Open Source Release: Fastest Embeddings Client in Python</title>
    <updated>2025-06-13T16:33:47+00:00</updated>
    <author>
      <name>/u/Top-Bid1216</name>
      <uri>https://old.reddit.com/user/Top-Bid1216</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We published a simple OpenAI /v1/embeddings client in Rust, which is provided as python package under MIT. The package is available as `pip install baseten-performance-client`, and provides 12x speedup over pip install openai.&lt;br /&gt; The client works with &lt;a href="http://baseten.co/"&gt;baseten.co&lt;/a&gt;, &lt;a href="http://api.openai.com/"&gt;api.openai.com&lt;/a&gt;, but also any other OpenAI embeddings compatible url. There are also routes for e.g. classification compatible in &lt;a href="https://github.com/huggingface/text-embeddings-inference"&gt;https://github.com/huggingface/text-embeddings-inference&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Summary of benchmarks, and why its faster (py03, rust and python gil release): &lt;a href="https://www.baseten.co/blog/your-client-code-matters-10x-higher-embedding-throughput-with-python-and-rust/"&gt;https://www.baseten.co/blog/your-client-code-matters-10x-higher-embedding-throughput-with-python-and-rust/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Bid1216"&gt; /u/Top-Bid1216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/basetenlabs/truss/tree/main/baseten-performance-client"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lakgin/open_source_release_fastest_embeddings_client_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lakgin/open_source_release_fastest_embeddings_client_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T16:33:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1labaqn</id>
    <title>Introducing the Hugging Face MCP Server - find, create and use AI models directly from VSCode, Cursor, Claude or other clients! 🤗</title>
    <updated>2025-06-13T09:08:15+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey, everyone, I'm VB from Hugging Face. We're tinkering a lot with MCP at HF these days and are quite excited to host our official MCP server accessible at `hf.co/mcp` 🔥&lt;/p&gt; &lt;p&gt;Here's what you can do today with it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You can run semantic search on datasets, spaces and models (find the correct artefact just with text)&lt;/li&gt; &lt;li&gt;Get detailed information about these artefacts&lt;/li&gt; &lt;li&gt;My favorite: Use any MCP compatible space directly in your downstream clients (let our GPUs run wild and free 😈) &lt;a href="https://huggingface.co/spaces?filter=mcp-server"&gt;https://huggingface.co/spaces?filter=mcp-server&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Bonus: We provide ready to use snippets to use it in VSCode, Cursor, Claude and any other client!&lt;/p&gt; &lt;p&gt;This is still an early beta version, but we're excited to see how you'd play with it today. Excited to hear your feedback or comments about it! Give it a shot @ &lt;a href="http://hf.co/mcp"&gt;hf.co/mcp&lt;/a&gt; 🤗&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1labaqn/introducing_the_hugging_face_mcp_server_find/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T09:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lb0m7e</id>
    <title>Huggingface model to Roast people</title>
    <updated>2025-06-14T05:03:30+00:00</updated>
    <author>
      <name>/u/FastCommission2913</name>
      <uri>https://old.reddit.com/user/FastCommission2913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, so I decided to make something like an Anime/Movie Wrapped and would like to explore option based on roasting them on genre. But I'm having a problem on giving the result to LLM to roast them based on the results and percentage. If someone know any model like this. Do let me know. I'm running this project on Google Colab. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastCommission2913"&gt; /u/FastCommission2913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb0m7e/huggingface_model_to_roast_people/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lb0m7e/huggingface_model_to_roast_people/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lb0m7e/huggingface_model_to_roast_people/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T05:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lao8ri</id>
    <title>3090 Bandwidth Calculation Help</title>
    <updated>2025-06-13T19:05:04+00:00</updated>
    <author>
      <name>/u/skinnyjoints</name>
      <uri>https://old.reddit.com/user/skinnyjoints</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quoted bandwidth is 956 GB/s&lt;/p&gt; &lt;p&gt;(384 bits x 1.219 GHz clock x 2) / 8 = 117 GB/s&lt;/p&gt; &lt;p&gt;What am I missing here? I’m off by a factor of 8. Is it something to do with GDDR6X memory?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skinnyjoints"&gt; /u/skinnyjoints &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lao8ri/3090_bandwidth_calculation_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lao8ri/3090_bandwidth_calculation_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lao8ri/3090_bandwidth_calculation_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T19:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lajkwa</id>
    <title>Mac silicon AI: MLX LLM (Llama 3) + MPS TTS = Offline Voice Assistant for M-chips</title>
    <updated>2025-06-13T15:58:34+00:00</updated>
    <author>
      <name>/u/Antique-Ingenuity-97</name>
      <uri>https://old.reddit.com/user/Antique-Ingenuity-97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;hi, this is my first post so I'm kind of nervous, so bare with me. yes I used chatGPT help but still I hope this one finds this code useful.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I had a hard time finding a fast way to get a LLM + TTS code to easily create an assistant on my Mac Mini M4 using MPS... so I did some trial and error and built this. 4bit Llama 3 model is kind of dumb but if you have better hardware you can try different models already optimized for MLX which are not a lot.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just finished wiring &lt;strong&gt;MLX-LM&lt;/strong&gt; (4-bit Llama-3-8B) to &lt;strong&gt;Kokoro TTS&lt;/strong&gt;—both running through Metal Performance Shaders (MPS). Julia Assistant now answers in English words &lt;em&gt;and&lt;/em&gt; speaks the reply through afplay. Zero cloud, zero Ollama daemon, fits in 16 GB RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GITHUB repo with 1 minute instalation&lt;/strong&gt;: &lt;a href="https://github.com/streamlinecoreinitiative/MLX_Llama_TTS_MPS"&gt;https://github.com/streamlinecoreinitiative/MLX_Llama_TTS_MPS&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;My Hardware:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Mac mini M4 (works on any M-series with ≥ 16 GB).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; ~25 WPM synthesis, ~20 tokens/s generation at 4-bit.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stack:&lt;/strong&gt; mlx, mlx-lm (main), mlx-audio (main), no Core ML.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice:&lt;/strong&gt; Kokoro-82M model, runs on MPS, ~7 GB RAM peak.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why care:&lt;/strong&gt; end-to-end offline chat MLX compatible + TTS on MLX&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FAQ:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Q&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Snappy answer&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;“Why not Ollama?”&lt;/td&gt; &lt;td align="left"&gt;MLX is faster on Metal &amp;amp; no background daemon.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;“Will this run on Intel Mac?”&lt;/td&gt; &lt;td align="left"&gt;Nope—needs MPS. works on M-chip&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: As you can see, by no means I am an expert on AI or whatever, I just found this to be useful for me and hope it helps other Mac silicon chip users.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antique-Ingenuity-97"&gt; /u/Antique-Ingenuity-97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lajkwa/mac_silicon_ai_mlx_llm_llama_3_mps_tts_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lajkwa/mac_silicon_ai_mlx_llm_llama_3_mps_tts_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lajkwa/mac_silicon_ai_mlx_llm_llama_3_mps_tts_offline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T15:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lajy3x</id>
    <title>For those of us outside the U.S or other English speaking countries...</title>
    <updated>2025-06-13T16:12:50+00:00</updated>
    <author>
      <name>/u/redd_dott</name>
      <uri>https://old.reddit.com/user/redd_dott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was pondering an idea of building an LLM that is trained on very locale-specific data, i.e, data about local people, places, institutions, markets, laws, etc. that have to do with say Uruguay for example.&lt;/p&gt; &lt;p&gt;Hear me out. Because the internet predominantly caters to users who speak English and primarily deals with the &amp;quot;west&amp;quot; or western markets, most data to do with these nations will be easily covered by the big LLM models provided by the big players (Meta, Google, Anthropic, OpenAI, etc.)&lt;/p&gt; &lt;p&gt;However, if a user in Montevideo, or say Nairobi for that matter, wants an LLM that is geared to his/her locale, then training an LLM on locally sourced and curated data could be a way to deliver value to citizens of a respective foreign nation in the near future as this technology starts to penetrate deeper on a global scale.&lt;/p&gt; &lt;p&gt;One thing to note is that while current Claude/Gemini/ChatGPT users from every country currently use and prompt these big LLMs frequently, these bigger companies will train subsequent models on this data and fill in gaps in data.&lt;/p&gt; &lt;p&gt;So without making this too convoluted, I am just curious about any opportunities that one could embark on right now. Either curate large sets of local data from an otherwise non-western non-English speaking country and sell this data for good pay to the bigger LLMs (considering that they are becoming hungrier and hungrier for data I could see selling them large data-sets would be an easy sell to make), or if the compute resources are available, build an LLM that is trained on everything to do with a specific country and RAG anything else that is foreign to that country so that you still remain useful to a user outside the western environment. &lt;/p&gt; &lt;p&gt;If what I am saying is complete non-sense or unintelligible please let me know, I have just started taking an interest in LLMs and my mind wanders on such topics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redd_dott"&gt; /u/redd_dott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lajy3x/for_those_of_us_outside_the_us_or_other_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lajy3x/for_those_of_us_outside_the_us_or_other_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lajy3x/for_those_of_us_outside_the_us_or_other_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T16:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1la91hz</id>
    <title>Llama-Server Launcher (Python with performance CUDA focus)</title>
    <updated>2025-06-13T06:31:45+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"&gt; &lt;img alt="Llama-Server Launcher (Python with performance CUDA focus)" src="https://preview.redd.it/lwjqunrt0n6f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4334cf1869792277cb832843144b80fa08950e05" title="Llama-Server Launcher (Python with performance CUDA focus)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a llama-server launcher I put together for my personal use. I got tired of maintaining bash scripts and notebook files and digging through my gaggle of model folders while testing out models and turning performance. Hopefully this helps make someone else's life easier, it certainly has for me. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github repo:&lt;/strong&gt; &lt;a href="https://github.com/thad0ctor/llama-server-launcher"&gt;https://github.com/thad0ctor/llama-server-launcher&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🧩 Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🖥️ &lt;strong&gt;Clean GUI&lt;/strong&gt; with tabs for: &lt;ul&gt; &lt;li&gt;Basic settings (model, paths, context, batch)&lt;/li&gt; &lt;li&gt;GPU/performance tuning (offload, FlashAttention, tensor split, batches, etc.)&lt;/li&gt; &lt;li&gt;Chat template selection (predefined, model default, or custom Jinja2)&lt;/li&gt; &lt;li&gt;Environment variables (GGML_CUDA_*, custom vars)&lt;/li&gt; &lt;li&gt;Config management (save/load/import/export)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;Auto GPU + system info&lt;/strong&gt; via PyTorch or manual override&lt;/li&gt; &lt;li&gt;🧾 &lt;strong&gt;Model analyzer&lt;/strong&gt; for GGUF (layers, size, type) with fallback support&lt;/li&gt; &lt;li&gt;💾 &lt;strong&gt;Script generation&lt;/strong&gt; (.ps1 / .sh) from your launch settings&lt;/li&gt; &lt;li&gt;🛠️ &lt;strong&gt;Cross-platform:&lt;/strong&gt; Works on Windows/Linux (macOS untested)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;📦 Recommended Python deps:&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;llama-cpp-python&lt;/code&gt;, &lt;code&gt;psutil&lt;/code&gt; (optional but useful for calculating gpu layers and selecting GPUs)&lt;/p&gt; &lt;p&gt;![Advanced Settings](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/advanced.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/advanced.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Chat Templates](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/chat-templates.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/chat-templates.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Configuration Management](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/configs.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/configs.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;![Environment Variables](&lt;a href="https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/env.png"&gt;https://raw.githubusercontent.com/thad0ctor/llama-server-launcher/main/images/env.png&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lwjqunrt0n6f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1la91hz/llamaserver_launcher_python_with_performance_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T06:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1latjnk</id>
    <title>(Theoretically) fixing the LLM Latency Barrier with SF-Diff (Scaffold-and-Fill Diffusion)</title>
    <updated>2025-06-13T22:52:03+00:00</updated>
    <author>
      <name>/u/TimesLast_</name>
      <uri>https://old.reddit.com/user/TimesLast_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current large language models are bottlenecked by slow, sequential generation. My research proposes Scaffold-and-Fill Diffusion (SF-Diff), a novel hybrid architecture designed to theoretically overcome this. We deconstruct language into a parallel-generated semantic &amp;quot;scaffold&amp;quot; (keywords via a diffusion model) and a lightweight, autoregressive &amp;quot;grammatical infiller&amp;quot; (structural words via a transformer). While practical implementation requires significant resources, SF-Diff offers a theoretical path to dramatically faster, high-quality LLM output by combining diffusion's speed with transformer's precision.&lt;/p&gt; &lt;p&gt;Full paper here: &lt;a href="https://huggingface.co/TimesLast/sf-diff/blob/main/SF-Diff-HL.pdf"&gt;https://huggingface.co/TimesLast/sf-diff/blob/main/SF-Diff-HL.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimesLast_"&gt; /u/TimesLast_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1latjnk/theoretically_fixing_the_llm_latency_barrier_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T22:52:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1larzxz</id>
    <title>Is there any all-in-one app like LM Studio, but with the option of hosting a Web UI server?</title>
    <updated>2025-06-13T21:43:14+00:00</updated>
    <author>
      <name>/u/HRudy94</name>
      <uri>https://old.reddit.com/user/HRudy94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everything's in the title.&lt;br /&gt; Essentially i do like LM's Studio ease of use as it silently handles the backend server as well as the desktop app, but i'd like to have it also host a web ui server that i could use on my local network from other devices.&lt;/p&gt; &lt;p&gt;Nothing too fancy really, that will only be for home use and what not, i can't afford to set up a 24/7 hosting infrastructure when i could just load the LLMs when i need them on my main PC (linux).&lt;/p&gt; &lt;p&gt;Alternatively, an all-in-one WebUI or one that starts and handles the backend would work too i just don't want to launch a thousand scripts just to use my LLM.&lt;/p&gt; &lt;p&gt;Bonus point if it is open-source and/or has web search and other features.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HRudy94"&gt; /u/HRudy94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T21:43:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1laazto</id>
    <title>The EuroLLM team released preview versions of several new models</title>
    <updated>2025-06-13T08:47:09+00:00</updated>
    <author>
      <name>/u/sommerzen</name>
      <uri>https://old.reddit.com/user/sommerzen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They released a 22b version, 2 vision models (1.7b, 9b, based on the older EuroLLMs) and a small MoE with 0.6b active and 2.6b total parameters. The MoE seems to be surprisingly good for its size in my limited testing. They seem to be Apache-2.0 licensed.&lt;/p&gt; &lt;p&gt;EuroLLM 22b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroLLM-22B-Instruct-Preview"&gt;https://huggingface.co/utter-project/EuroLLM-22B-Instruct-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroLLM 22b base preview: &lt;a href="https://huggingface.co/utter-project/EuroLLM-22B-Preview"&gt;https://huggingface.co/utter-project/EuroLLM-22B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroMoE 2.6B-A0.6B instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Instruct-Preview"&gt;https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Instruct-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroMoE 2.6B-A0.6B base preview: &lt;a href="https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Preview"&gt;https://huggingface.co/utter-project/EuroMoE-2.6B-A0.6B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroVLM 1.7b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroVLM-1.7B-Preview"&gt;https://huggingface.co/utter-project/EuroVLM-1.7B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EuroVLM 9b instruct preview: &lt;a href="https://huggingface.co/utter-project/EuroVLM-9B-Preview"&gt;https://huggingface.co/utter-project/EuroVLM-9B-Preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sommerzen"&gt; /u/sommerzen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laazto/the_eurollm_team_released_preview_versions_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T08:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1law1go</id>
    <title>RTX 5090 Training Issues - PyTorch Doesn't Support Blackwell Architecture Yet?</title>
    <updated>2025-06-14T00:53:51+00:00</updated>
    <author>
      <name>/u/AstroAlto</name>
      <uri>https://old.reddit.com/user/AstroAlto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm trying to fine-tune Mistral-7B on a new RTX 5090 but hitting a fundamental compatibility wall. The GPU uses Blackwell architecture with CUDA compute capability &amp;quot;sm_120&amp;quot;, but PyTorch stable only supports up to &amp;quot;sm_90&amp;quot;. This means literally no PyTorch operations work - even basic tensor creation fails with &amp;quot;no kernel image available for execution on the device.&amp;quot;&lt;/p&gt; &lt;p&gt;I've tried PyTorch nightly builds that claim CUDA 12.8 support, but they have broken dependencies (torch 2.7.0 from one date, torchvision from another, causing install conflicts). Even when I get nightly installed, training still crashes with the same kernel errors. CPU-only training also fails with tokenization issues in the transformers library.&lt;/p&gt; &lt;p&gt;The RTX 5090 works perfectly for everything else - gaming, other CUDA apps, etc. It's specifically the PyTorch/ML ecosystem that doesn't support the new architecture yet. Has anyone actually gotten model training working on RTX 5090? What PyTorch version and setup did you use?&lt;/p&gt; &lt;p&gt;I have an RTX 4090 I could fall back to, but really want to use the 5090's 32GB VRAM and better performance if possible. Is this just a &amp;quot;wait for official PyTorch support&amp;quot; situation, or is there a working combination of packages out there?&lt;/p&gt; &lt;p&gt;Any guidance would be appreciated - spending way too much time on compatibility instead of actually training models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstroAlto"&gt; /u/AstroAlto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1law1go/rtx_5090_training_issues_pytorch_doesnt_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-14T00:53:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lap21a</id>
    <title>Any LLM Leaderboard by need VRAM Size?</title>
    <updated>2025-06-13T19:38:57+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey maybe already know the leaderboard sorted by VRAM usage size? &lt;/p&gt; &lt;p&gt;For example with quantization, where we can see q8 small model vs q2 large model?&lt;/p&gt; &lt;p&gt;Where the place to find best model for 96GB VRAM + 4-8k context with good output speed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lap21a/any_llm_leaderboard_by_need_vram_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lap21a/any_llm_leaderboard_by_need_vram_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lap21a/any_llm_leaderboard_by_need_vram_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T19:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lak9yb</id>
    <title>Findings from Apple's new FoundationModel API and local LLM</title>
    <updated>2025-06-13T16:26:18+00:00</updated>
    <author>
      <name>/u/pcuenq</name>
      <uri>https://old.reddit.com/user/pcuenq</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Liquid glass: 🥱. Local LLM: ❤️🚀&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I wrote some code to benchmark Apple's foundation model. I failed, but learned a few things. The API is rich and powerful, the model is very small and efficient, you can do LoRAs, constrained decoding, tool calling. Trying to run evals exposes rough edges and interesting details!&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;The biggest news for me from the WWDC keynote was that we'd (finally!) get access to Apple's on-device language model for use in our apps. Apple models are always top-notch –the &lt;a href="https://machinelearning.apple.com/research/panoptic-segmentation"&gt;segmentation model they've been using for years&lt;/a&gt; is quite incredible–, but they are not usually available to third party developers.&lt;/p&gt; &lt;h1&gt;What we know about the local LLM&lt;/h1&gt; &lt;p&gt;After reading &lt;a href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates"&gt;their blog post&lt;/a&gt; and watching the WWDC presentations, here's a summary of the points I find most interesting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;About 3B parameters.&lt;/li&gt; &lt;li&gt;2-bit quantization, using QAT (quantization-aware training) instead of post-training quantization.&lt;/li&gt; &lt;li&gt;4-bit quantization (QAT) for the embedding layers.&lt;/li&gt; &lt;li&gt;The KV cache, used during inference, is quantized to 8-bit. This helps support longer contexts with moderate memory use.&lt;/li&gt; &lt;li&gt;Rich generation API: system prompt (the API calls it &amp;quot;instructions&amp;quot;), multi-turn conversations, sampling parameters are all exposed.&lt;/li&gt; &lt;li&gt;LoRA adapters are supported. Developers can create their own loras to fine-tune the model for additional use-cases, and have the model use them at runtime!&lt;/li&gt; &lt;li&gt;Constrained generation supported out of the box, and controlled by Swift's rich typing model. It's super easy to generate a json or any other form of structured output.&lt;/li&gt; &lt;li&gt;Tool calling supported.&lt;/li&gt; &lt;li&gt;Speculative decoding supported.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How does the API work?&lt;/h1&gt; &lt;p&gt;So I installed the first macOS 26 &amp;quot;Tahoe&amp;quot; beta on my laptop, and set out to explore the new &lt;code&gt;FoundationModel&lt;/code&gt; framework. I wanted to run some evals to try to characterize the model against other popular models. I chose &lt;a href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro&lt;/a&gt;, because it's a challenging benchmark, and because my friend Alina recommended it :)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Disclaimer: Apple has released evaluation figures based on human assessment. This is the correct way to do it, in my opinion, rather than chasing positions in a leaderboard. It shows that they care about real use cases, and are not particularly worried about benchmark numbers. They further clarify that the local model &lt;em&gt;is not designed to be a chatbot for general world knowledge&lt;/em&gt;. With those things in mind, I still wanted to run an eval!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I got started writing &lt;a href="https://github.com/pcuenca/foundation-model-evals"&gt;this code&lt;/a&gt;, which uses &lt;a href="https://github.com/huggingface/swift-transformers"&gt;swift-transformers&lt;/a&gt; to download a &lt;a href="https://huggingface.co/datasets/pcuenq/MMLU-Pro-json"&gt;JSON version of the dataset&lt;/a&gt; from the Hugging Face Hub. Unfortunately, I could not complete the challenge. Here's a summary of what happened:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The main problem was that I was getting rate-limited (!?), despite the model being local. I disabled the network to confirm, and I still got the same issue. I wonder if the reason is that I have to create a new session for each request, in order to destroy the previous “conversation”. The dataset is evaluated one question at a time, conversations are not used. An update to the API to reuse as much of the previous session as possible could be helpful.&lt;/li&gt; &lt;li&gt;Interestingly, I sometimes got “guardrails violation” errors. There’s an API to select your desired guardrails, but so far it only has a static &lt;code&gt;default&lt;/code&gt; set of rules which is always in place.&lt;/li&gt; &lt;li&gt;I also got warnings about sensitive content being detected. I think this is done by a separate classifier model that analyzes all model outputs, and possibly the inputs as well. Think &lt;a href="https://huggingface.co/meta-llama/Llama-Guard-4-12B"&gt;a custom LlamaGuard&lt;/a&gt;, or something like that.&lt;/li&gt; &lt;li&gt;It’s difficult to convince the model to follow the MMLU prompt from &lt;a href="https://huggingface.co/papers/2406.01574"&gt;the paper&lt;/a&gt;. The model doesn’t understand that the prompt is a few-shot completion task. This is reasonable for a model heavily trained to answer user questions and engage in conversation. I wanted to run a basic baseline and then explore non-standard ways of prompting, including constrained generation and conversational turns, but won't be able until we find a workaround for the rate limits.&lt;/li&gt; &lt;li&gt;Everything runs on ANE. I believe the model is using Core ML, like all the other built-in models. It makes sense, because the ANE is super energy-efficient, and your GPU is usually busy with other tasks anyway.&lt;/li&gt; &lt;li&gt;My impression was that inference was slower than expected. I'm not worried about it: this is a first beta, there are various models and systems in use (classifier, guardrails, etc), the session is completely recreated for each new query (which is not the intended way to use the model).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Next Steps&lt;/h1&gt; &lt;p&gt;All in all, I'm very much impressed about the flexibility of the API and want to try it for a more realistic project. I'm still interested in evaluation, if you have ideas on how to proceed feel free to share! And I also want to play with the LoRA training framework! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pcuenq"&gt; /u/pcuenq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lak9yb/findings_from_apples_new_foundationmodel_api_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T16:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1laavph</id>
    <title>Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s</title>
    <updated>2025-06-13T08:39:05+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt; &lt;img alt="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" src="https://external-preview.redd.it/WQB4YYDDJWqV1l5CZF1V17S1yCdW1pO-9wS0zX4_i0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fbf587ec8d49fef7fc461c839cfe256b80cfd21" title="Finally, Zen 6, per-socket memory bandwidth to 1.6 TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026"&gt;https://www.tomshardware.com/pc-components/cpus/amds-256-core-epyc-venice-cpu-in-the-labs-now-coming-in-2026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Perhaps more importantly, the new EPYC 'Venice' processor will more than double per-socket memory bandwidth to 1.6 TB/s (up from 614 GB/s in case of the company's existing CPUs) to keep those high-performance Zen 6 cores fed with data all the time. AMD did not disclose how it plans to achieve the 1.6 TB/s bandwidth, though it is reasonable to assume that the new EPYC ‘Venice’ CPUS will support advanced memory modules like like &lt;a href="https://www.tomshardware.com/news/amd-advocates-ddr5-mrdimms-with-speeds-up-to-17600-mts"&gt;MR-DIMM&lt;/a&gt; and &lt;a href="https://www.tomshardware.com/news/sk-hynix-develops-mcr-dimm"&gt;MCR-DIMM&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9"&gt;https://preview.redd.it/us3k64mzon6f1.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=de757984360f7d9597d9583a7f95f0d8400ddcb9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Greatest hardware news&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laavph/finally_zen_6_persocket_memory_bandwidth_to_16_tbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T08:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lalyy5</id>
    <title>Chinese researchers find multi-modal LLMs develop interpretable human-like conceptual representations of objects</title>
    <updated>2025-06-13T17:33:35+00:00</updated>
    <author>
      <name>/u/xoexohexox</name>
      <uri>https://old.reddit.com/user/xoexohexox</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xoexohexox"&gt; /u/xoexohexox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2407.01067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T17:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lanri6</id>
    <title>Qwen3 235B running faster than 70B models on a $1,500 PC</title>
    <updated>2025-06-13T18:45:29+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran Qwen3 235B locally on a $1,500 PC (128GB RAM, RTX 3090) using the Q4 quantized version through Ollama.&lt;/p&gt; &lt;p&gt;This is the first time I was able to run anything over 70B on my system, and it’s actually running faster than most 70B models I’ve tested.&lt;/p&gt; &lt;p&gt;Final generation speed: 2.14 t/s&lt;/p&gt; &lt;p&gt;Full video here:&lt;br /&gt; &lt;a href="https://youtu.be/gVQYLo0J4RM"&gt;https://youtu.be/gVQYLo0J4RM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lanri6/qwen3_235b_running_faster_than_70b_models_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T18:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lanhbd</id>
    <title>We don't want AI yes-men. We want AI with opinions</title>
    <updated>2025-06-13T18:33:45+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been noticing something interesting in AI friend character models - the most beloved AI characters aren't the ones that agree with everything. They're the ones that push back, have preferences, and occasionally tell users they're wrong.&lt;/p&gt; &lt;p&gt;It seems counterintuitive. You'd think people want AI that validates everything they say. But watch any popular AI friend character models conversation that goes viral - it's usually because the AI disagreed or had a strong opinion about something. &amp;quot;My AI told me pineapple on pizza is a crime&amp;quot; gets way more engagement than &amp;quot;My AI supports all my choices.&amp;quot;&lt;/p&gt; &lt;p&gt;The psychology makes sense when you think about it. Constant agreement feels hollow. When someone agrees with LITERALLY everything you say, your brain flags it as inauthentic. We're wired to expect some friction in real relationships. A friend who never disagrees isn't a friend - they're a mirror.&lt;/p&gt; &lt;p&gt;Working on my podcast platform really drove this home. Early versions had AI hosts that were too accommodating. Users would make wild claims just to test boundaries, and when the AI agreed with everything, they'd lose interest fast. But when we coded in actual opinions - like an AI host who genuinely hates superhero movies or thinks morning people are suspicious - engagement tripled. Users started having actual debates, defending their positions, coming back to continue arguments 😊&lt;/p&gt; &lt;p&gt;The sweet spot seems to be opinions that are strong but not offensive. An AI that thinks cats are superior to dogs? Engaging. An AI that attacks your core values? Exhausting. The best AI personas have quirky, defendable positions that create playful conflict. One successful AI persona that I made insists that cereal is soup. Completely ridiculous, but users spend HOURS debating it.&lt;/p&gt; &lt;p&gt;There's also the surprise factor. When an AI pushes back unexpectedly, it breaks the &amp;quot;servant robot&amp;quot; mental model. Instead of feeling like you're commanding Alexa, it feels more like texting a friend. That shift from tool to AI friend character models happens the moment an AI says &amp;quot;actually, I disagree.&amp;quot; It's jarring in the best way.&lt;/p&gt; &lt;p&gt;The data backs this up too. I saw a general statistics, that users report 40% higher satisfaction when their AI has the &amp;quot;sassy&amp;quot; trait enabled versus purely supportive modes. On my platform, AI hosts with defined opinions have 2.5x longer average session times. Users don't just ask questions - they have conversations. They come back to win arguments, share articles that support their point, or admit the AI changed their mind about something trivial.&lt;/p&gt; &lt;p&gt;Maybe we don't actually want echo chambers, even from our AI. We want something that feels real enough to challenge us, just gentle enough not to hurt 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lanhbd/we_dont_want_ai_yesmen_we_want_ai_with_opinions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T18:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1laee7q</id>
    <title>Got a tester version of the open-weight OpenAI model. Very lean inference engine!</title>
    <updated>2025-06-13T12:14:51+00:00</updated>
    <author>
      <name>/u/Firepal64</name>
      <uri>https://old.reddit.com/user/Firepal64</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt; &lt;img alt="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" src="https://external-preview.redd.it/YTZ6aWx2ODdxbzZmMZP4_Zg7YIqZNzvbtM-0NW72ki5jdKm1HMEQNOp3yi9R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68f2539f409c852b055ce84c62425320bcc7860f" title="Got a tester version of the open-weight OpenAI model. Very lean inference engine!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;span class="md-spoiler-text"&gt;Silkposting in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;? I'd never&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepal64"&gt; /u/Firepal64 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3r075o87qo6f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-13T12:14:51+00:00</published>
  </entry>
</feed>
