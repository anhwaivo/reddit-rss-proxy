<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-09T22:24:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mm199n</id>
    <title>gpt-o more people will join the locals knowing that they are not the only ones</title>
    <updated>2025-08-09T21:47:54+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm199n/gpto_more_people_will_join_the_locals_knowing/"&gt; &lt;img alt="gpt-o more people will join the locals knowing that they are not the only ones" src="https://b.thumbs.redditmedia.com/c6tVv_8HRofsskTaMN3f-_sM8f5I7L8FzlRXjdzxPXg.jpg" title="gpt-o more people will join the locals knowing that they are not the only ones" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks to the emergence of GPT-OSS, many people have become aware of the existence of a wide range of artificial intelligence models, beyond closed and centralized systems. This project has served as an accessible entry point for new users, accelerating the adoption of solutions based on local inference models.&lt;/p&gt; &lt;p&gt;As a result, recognition is growing about the potential of these models as a viable, and in many cases preferable, alternative to large proprietary models. The combination of low access costs, guaranteed privacy, customization, and optimized performance on local hardware is positioning open models as a key pillar in the next generation of decentralized AI applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b21rrbb4e2if1.png?width=608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=837c0ebe9fff21c8e25310c56bff5a2a1fbf33ce"&gt;https://preview.redd.it/b21rrbb4e2if1.png?width=608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=837c0ebe9fff21c8e25310c56bff5a2a1fbf33ce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm199n/gpto_more_people_will_join_the_locals_knowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm199n/gpto_more_people_will_join_the_locals_knowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm199n/gpto_more_people_will_join_the_locals_knowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:47:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlys5f</id>
    <title>Looking for advice for AI for grandma with dementia</title>
    <updated>2025-08-09T20:02:07+00:00</updated>
    <author>
      <name>/u/SirGalahadOfCamelot</name>
      <uri>https://old.reddit.com/user/SirGalahadOfCamelot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my grandmother has dementia and drives my parents insane asking the same questions 500 times a day. I’d like to build a local LLM she can talk to all she wants and set up an easy way to remotely update a file with context so the AI knows whatever the newest things are that she’s asking about. It would need to be running 24/7. She has a private room that she spends time in when they are away, so any conversation it hears it can safely assume is meant for it. Or we might be able to get her to remember she needs to say its name to talk to it, but I’d worry she’d forget. I’d gone fairly far down the rabbit hole of setting up Home Assistant on Raspberry Pi’s in to use local LLM as a voice assistant. Is that the best strategy or does something better exist for using a local LLM for voice? If you were going to try to build this, how would you do it? What models would you use for the LLM and what’s the best system for TTS at this point? I seem to recall the sesame thing being a flop; has anything like that landed that is solid now? I’m just out of the loop on what’s out there for this kind of task and figure you people probably know the scoop!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirGalahadOfCamelot"&gt; /u/SirGalahadOfCamelot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlys5f/looking_for_advice_for_ai_for_grandma_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlys5f/looking_for_advice_for_ai_for_grandma_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlys5f/looking_for_advice_for_ai_for_grandma_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T20:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1zcg</id>
    <title>i'm making dating simulator game with ai npc using open source llm</title>
    <updated>2025-08-09T22:20:10+00:00</updated>
    <author>
      <name>/u/aziib</name>
      <uri>https://old.reddit.com/user/aziib</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt; &lt;img alt="i'm making dating simulator game with ai npc using open source llm" src="https://external-preview.redd.it/ZjR3N2g2MmhjMmlmMW57LOTD_dYSJqtRCJKUw8LsjdMQCiQ_6aIAc3dh1JZp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18b063cf71d62103bbeb0fff25811ee3533f54d1" title="i'm making dating simulator game with ai npc using open source llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you can play on your browser: &lt;a href="https://romram.itch.io/break-time"&gt;https://romram.itch.io/break-time&lt;/a&gt;&lt;br /&gt; you need LM Studio as a local server: &lt;a href="https://lmstudio.ai/"&gt;https://lmstudio.ai/&lt;/a&gt;&lt;br /&gt; use uncensored llama 8b model or more and 8k context window or more for better experience.&lt;br /&gt; i use blacksheep gguf models:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-RP-8B-i1-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF"&gt;https://huggingface.co/mradermacher/BlackSheep-24B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the game engine is using rpg maker mz with some of my modified custom plugins&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aziib"&gt; /u/aziib &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hchl142hc2if1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1zcg/im_making_dating_simulator_game_with_ai_npc_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlqylw</id>
    <title>AI max+ 395</title>
    <updated>2025-08-09T14:36:43+00:00</updated>
    <author>
      <name>/u/megadonkeyx</name>
      <uri>https://old.reddit.com/user/megadonkeyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone using a 128gb version with a local model as a serious replacement for commercial apis?&lt;/p&gt; &lt;p&gt;If so, what device? What model? What tokens / second and context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/megadonkeyx"&gt; /u/megadonkeyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlqylw/ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlqylw/ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlqylw/ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T14:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mljlmu</id>
    <title>Another uncensored gpt-oss to try</title>
    <updated>2025-08-09T07:33:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/s/BN3K0zDR4B"&gt;https://www.reddit.com/r/LocalLLaMA/s/BN3K0zDR4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huizimao/gpt-oss-20b-uncensored-bf16"&gt;https://huggingface.co/huizimao/gpt-oss-20b-uncensored-bf16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huizimao/gpt-oss-20b-uncensored-mxfp4"&gt;https://huggingface.co/huizimao/gpt-oss-20b-uncensored-mxfp4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs in progress &lt;a href="https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF"&gt;https://huggingface.co/mradermacher/gpt-oss-20b-uncensored-bf16-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mljlmu/another_uncensored_gptoss_to_try/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mljlmu/another_uncensored_gptoss_to_try/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mljlmu/another_uncensored_gptoss_to_try/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T07:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml77rq</id>
    <title>The LLM world is an illusion of progress</title>
    <updated>2025-08-08T21:11:49+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's my previous rant in which I was saying that LLMs were trapped in monolingualism and the assistant paradigm: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[Mini Rant] Are LLMs trapped in English and the assistant paradigms?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To update this: I feel like things evolved toward bilingualism (Chinese and English), while multilingualism is still at the bottom of the benchmarks of popular released LLMs, and generally not in the lesser-known LLMs.&lt;/p&gt; &lt;p&gt;To address what I call the assistant paradigm: it is now more than ever a cluster*ck because everything you'll want to generate a simple chunk of text will try to make tool calls, and to be fair, there is no normalized template used by more than one provider, which complicates things even more. Merging LLMs at this point may be totally magical, hoping that Frankenstein may not come out at the end of the process, lol.&lt;/p&gt; &lt;p&gt;Anyway, here are other points I want to address this time. Working generally in academia has made me pretty critical of these few points, which I think are underrepresented. They may not be the general community view or criteria of choice, but they're mine, and maybe others, so I wanted to share those with you, beloved LocalLlama community. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comparing LLMs is a total illusion at this point&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;As highlighted in a recent paper &amp;quot;&lt;a href="http://arxiv.org/abs/2408.04667"&gt;Non-Determinism of Deterministic LLM Settings&lt;/a&gt;&amp;quot;, LLMs configured to be deterministic can still show significant variations in outputs for the same inputs. This makes comparing LLMs a very tricky task.. if not impossible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks are flawed&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I'm aware of the abundance of benchmarks available, but when I look at the most interesting ones for my use cases, like &lt;a href="https://artificialanalysis.ai/evaluations/gpqa-diamond"&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt; &lt;/a&gt;(which only covers physics, biology, and chemistry) or &lt;a href="https://lastexam.ai/"&gt;Humanity's Last Exam (HLE)&lt;/a&gt;, the issues are glaring&lt;/p&gt; &lt;p&gt;HLE is supposed to be a rigorous benchmark, but it has a major flaw: the answers provided by LLMs are evaluated by... another LLM. This introduces bias and makes the results non-reproducible. How can we trust a benchmark where the judge is as fallible as the models being tested? We now know how LLMs are fallible : Research here showed that using LLMs as judges introduces significant biases and reliability issues. These models tend to favor responses that match their own style or position and struggle with detecting hallucinations without external verification &lt;a href="https://www.semanticscholar.org/paper/Humans-or-LLMs-as-the-Judge-A-Study-on-Judgement-Chen-Chen/a28071c63963cc59ba500cd00c140ac08eb5ccb0"&gt;[1]&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.07791"&gt;[2]&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Moreover, my first point stands as is in English, then, to be crude, its assessment of an LLM's skills is only relevant to about 20% of the world's population. It's a step up in difficulty, but far from a neutral or universally applicable benchmark, which then again marketing and the general peep tend to forget. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The agent era is a clusterf*ck&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The current trend of integrating tool calls into LLM outputs is creating a mess. Calling it simply function calls before agents was better. Then marketing kicked in. Also, there is no standardized template or protocol (MCP? Lol), making it evermore difficult to compare different tool usage by LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Proprietary platforms are the devil&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was a heavy consumer of gemini-2.5-pro 03-26, like.. addicted to it. Then removed in favour of a more code / math oriented model.. which was less better but ok. Then removed in favour of .. etc.&lt;/p&gt; &lt;p&gt;OpenAI just did the same things to consumers worldwide, and they even won't let them chose between models, and the nomenclature is even blurrier than ever .. According to the model sheet, the GPT-5 family consists of six separate models (gpt-5-main, gpt-5-main-mini, gpt-5-thinking, gpt-5-thinking-mini, gpt-5-thinking-nano, gpt-5-thinking-pro). Just.. omg just let your consumers choose. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Internet will implode with slop&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;There's no other considerations here to make other than there is an ever going increase of mess being generated. Dead Internet Theory holds more than ever and the new &lt;a href="https://blog.cloudflare.com/introducing-pay-per-crawl/"&gt;pay-per-crawl from cloudflare &lt;/a&gt;is a new artefact designing how the web space will be consumed. I seriously hope things will get better, but don't know how&lt;/p&gt; &lt;h1&gt;During this journey I've learned to keep it local and build my own benchmarks&lt;/h1&gt; &lt;p&gt;After all these observations, what I've concluded is that the most reliable approach is to keep LLMs local. After having headache on prompting the simplest use case of harmonizing academic texts with the models in the upper leaderboard of LMArena.. I'm finally back to my earlier loves of local LLMs. At least they don't change unexpectedly, and you control their configuration. More importantly, I needed to build my own benchmarks, individually, in which outputs are validated by myself. Public benchmarks have too many limitations and biases. The best approach is to create private, customized benchmarks tailored to our specific use cases. This way, we can ensure our evaluations are relevant, unbiased, and actually meaningful for our work.&lt;/p&gt; &lt;p&gt;This was cowritten with unsloth/Mistral-Small-3.2-24B-Instruct-2506 at Q_8. Thanks for the whole community for driving such a neat technology !&lt;/p&gt; &lt;p&gt;Edit: typos&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T21:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlf2ch</id>
    <title>Miro ODR: Another Deep Research Agent model just went open source</title>
    <updated>2025-08-09T03:11:34+00:00</updated>
    <author>
      <name>/u/MiroMindAI</name>
      <uri>https://old.reddit.com/user/MiroMindAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlf2ch/miro_odr_another_deep_research_agent_model_just/"&gt; &lt;img alt="Miro ODR: Another Deep Research Agent model just went open source" src="https://external-preview.redd.it/Z3llNzlpYXl1d2hmMeEv19zbIXINheGDldYeDNIrs4cqZFopneM09cd1xdoM.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7197bef97c7a36abb0b4038e3b75ec45646ce0c1" title="Miro ODR: Another Deep Research Agent model just went open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! 👋&lt;/p&gt; &lt;p&gt;We just dropped MiroMind Open Deep Research v0.1 - and we mean ACTUALLY open this time&lt;/p&gt; &lt;p&gt;So we've been grinding on this deep research project for months, and we're finally ready to share what we've built. Unlike the usual &amp;quot;open source&lt;em&gt;&amp;quot; (&lt;/em&gt;terms and conditions apply) releases, we're giving you literally everything:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we're releasing:&lt;/strong&gt; MiroFlow: Agent framework that doesn't suck to work with MiroThinker: 8B/14B/32B models that can actually do multi-step research MiroVerse: 147k training samples (not just &amp;quot;we used proprietary data lol&amp;quot;) MiroTrain/MiroRL: Full training pipeline including RL setup&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The numbers that matter:&lt;/strong&gt; MiroFlow scores GAIA validation: 82.4% (current SOTA for reproducible open agent framework) MiroThinker tops GAIA-Text-103: 60.2% (getting close to OpenAI's thing) All runnable on consumer hardware if you're patient enough&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we're doing this:&lt;/strong&gt; Honestly? We're tired of the &amp;quot;trust us bro&amp;quot; approach to AI research. Every time someone drops a paper with incredible results but no way to reproduce it, a local llama dies. We want to build this WITH the community, not just dump models and disappear.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new here:&lt;/strong&gt; End-to-end reproducible deep research (like, actually reproducible) Models that can use tools without losing their minds Training code that won't make you want to throw your GPU out the window We're planning monthly drops with community feedback driving what we build next. Got ideas? Hate something? Found a bug that makes you question our life choices? Hit us up.&lt;/p&gt; &lt;p&gt;🖥️ Agent Demo(TRY IT!): &lt;a href="https://dr.miromind.ai"&gt;MiroThinker Agent Online Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔗 Blog: &lt;a href="https://miromind.ai/blog/miromind-open-deep-research"&gt;MiroMind Open Deep Research&lt;/a&gt; &lt;/p&gt; &lt;p&gt;💻 GitHub: &lt;a href="https://github.com/MiroMindAI"&gt;MiroMind Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 Hugging Face: &lt;a href="https://huggingface.co/miromind-ai"&gt;MiroMind HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiroMindAI"&gt; /u/MiroMindAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iqh4s2cyuwhf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlf2ch/miro_odr_another_deep_research_agent_model_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlf2ch/miro_odr_another_deep_research_agent_model_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T03:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlhryw</id>
    <title>Finally, I Wrote a 600-Page Book About My Mad LLM fine-tuning experiments</title>
    <updated>2025-08-09T05:40:07+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may or may not be aware that I wrote Training Pro and Playground and Virtual Lora and a lot of other insane code that some of you use every day to muck about with LLMs or to idly goof off. And not only that, but I have also created, in my own pathetic home, thousands and thousands of LoRAs and all kinds of strange, mutant models, some of which are actually pretty ok.&lt;/p&gt; &lt;p&gt;I have been wanting to write this for some time, but have been saving it until I had some time on my hands, which is what I am doing right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My last few years of feverish, frustrating, and occasionally glorious LLM experiments have been distilled into a real, live, actual book!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I sort of got carried away, as always, and it would be close to 600 pages if printed in a big format. This is because, you know, once I get started, I cannot be stopped.&lt;/p&gt; &lt;p&gt;It is a gigantic compendium of my own personal notes, ideas, lessons learned and tons of epic failures which I proudly present as shining examples of how not to do things.&lt;/p&gt; &lt;p&gt;And I put in every of my secret tip and trick that I could think of.&lt;/p&gt; &lt;p&gt;I even reproduced some of my old experiments, like Sydney, step by step, or the Plot Bot (even down to code on github to acquire and augment dataset), or the totally insane Style Transfer thing where I cruelly taunt Jane Austen mercilessly. (You can tell by the cowardly qualifier &amp;quot;totally,&amp;quot; that I am still kind of hung up about doing that.)&lt;/p&gt; &lt;p&gt;But everything in there is real, I swear it, and I ran my computer around the clock, 24/7, to make sure that I could reproduce it all not just spew BS.&lt;/p&gt; &lt;p&gt;It starts with a very pre-chewed &amp;quot;bathroom theory&amp;quot; of LLMs for super-newbs, (absolutely no math or highfalutin intellectual mumbo jumbo), and ends with how to gracefully handle all the delightful error messages and segfaults that are an integral part of the LLM fine-tuning experience.&lt;/p&gt; &lt;p&gt;I don't know how it will be received, but this book contains Everything. I. Know.&lt;/p&gt; &lt;p&gt;So I put the damned thing up on Amazon, apple, kobo..., and I don't expect it to make me famous or rich or anything, but if you would just look it up, and maybe even taking a cursory peek at a few pages, I would be, like, soooooo grateful. And while you are at it, you could, you know, buy it, and then write a raving review about how it made you instantly wise and enlightened, and how it opened your mind to the profound beauty and mystery of the universe and everything in it... and stuff.&lt;/p&gt; &lt;p&gt;The book is titled, appropriately:&lt;br /&gt; &lt;a href="https://www.amazon.com/Cranky-Mans-Guide-LoRA-QLoRA-ebook/dp/B0FLBTR2FS/"&gt;The Cranky Man's Guide to LoRA &amp;amp; QLoRA: Personal Lessons from a Thousand LLM Fine-Tuning Fails&lt;/a&gt;&lt;/p&gt; &lt;p&gt;by F.P. Ham&lt;/p&gt; &lt;p&gt;And he has a nice picture of a burning GPU on the cover, which I lovingly toiled over all weekend!&lt;/p&gt; &lt;p&gt;It's also on apple book, B&amp;amp;N and so on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlhryw/finally_i_wrote_a_600page_book_about_my_mad_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlhryw/finally_i_wrote_a_600page_book_about_my_mad_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlhryw/finally_i_wrote_a_600page_book_about_my_mad_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T05:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlftxf</id>
    <title>Gamers Nexus did an investigation into the videocard blackmarket in China.</title>
    <updated>2025-08-09T03:51:32+00:00</updated>
    <author>
      <name>/u/MrWeirdoFace</name>
      <uri>https://old.reddit.com/user/MrWeirdoFace</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlftxf/gamers_nexus_did_an_investigation_into_the/"&gt; &lt;img alt="Gamers Nexus did an investigation into the videocard blackmarket in China." src="https://external-preview.redd.it/kuBTlUBt-VJKMlkA3iq90XNuL9L7HP85dkd9iGjm14Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40b256f1cf755faf1b9118aed1ef47f70416f27e" title="Gamers Nexus did an investigation into the videocard blackmarket in China." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWeirdoFace"&gt; /u/MrWeirdoFace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ltgyS8oJC8g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlftxf/gamers_nexus_did_an_investigation_into_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlftxf/gamers_nexus_did_an_investigation_into_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T03:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1mcj</id>
    <title>A personal take: LLMs are stuck, but local might win?</title>
    <updated>2025-08-09T22:04:08+00:00</updated>
    <author>
      <name>/u/Truncleme</name>
      <uri>https://old.reddit.com/user/Truncleme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some personal rambling thoughts on LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretraining decides what the model knows. By chewing through a massive slice of the internet, it picks up world knowledge, language patterns, common sense, and the core generalization that underwrites its “understanding.” That’s the source—and the cap—of its intelligence.&lt;/li&gt; &lt;li&gt;Post‑training (RLHF/SFT/etc.) teaches the model what to do. With curated examples, it learns to follow instructions, take on styles, and execute tasks. Think of it as behavior shaping and skill training: making the model useful and controllable.&lt;/li&gt; &lt;li&gt;Inference is where it actually does the work, conditioned on your prompt/context. It pulls from pretrained knowledge and post‑trained skills to generate an answer. The context determines what gets activated, but the model isn’t learning new knowledge or skills at this step.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Where things feel stuck: the real limiter is broad generalization/learning—the thing we hand‑wave as “emergence.” So far we mostly “unlocked” it via Scaling Laws (bigger models during pretraining). But we all know the data produced by human is not infinite, and those ultra-large model releases since 2024 haven’t delivered the jump people expected. And because we don’t actually know what causes emergence, we can’t even say for sure it’s just a data ceiling; maybe the next tier of emergence simply doesn’t happen.&lt;/p&gt; &lt;p&gt;So then, every AI compony turn their eyes to the post‑training and inference‑time tricks. Without fresh emergence, though, what we get is “skill generalization,” not “cognitive generalization.” That pushes us toward stronger narrow AI rather than stronger AGI. The most broadly useful “narrow” skill right now is coding. I’m still not overly bullish on this path: it can go far on those well‑specified tasks, but fails on comparing 9.9 and 9.11, which some called jagged intelligence.&lt;/p&gt; &lt;p&gt;On “reasoning”: to me, this is a post‑training‑shaped output pattern that helps the model structure and steer the context to better elicit knowledge it already has from pretraining. It’s a hybrid of post‑training and inference, but it can’t break the pretraining ceiling. It also locks the model into rigid scaffolds (step‑by‑step templates), which reduces the human’s control over the prompt. That’s why on some tasks—like creative writing—reasoning mode can backfire.&lt;/p&gt; &lt;p&gt;Agents are the same idea turned up: use post‑training to learn some strategies that manipulates context more flexibly (tools, memory, multi‑turn planning), not just the slice between user input and model output. But the fundamental limitation remains: no real learning at run‑time, no genuinely new generalization.&lt;/p&gt; &lt;p&gt;Why this might be good news for local LLMs: if this framing is right, local setups actually benefit. Our poor local hardwares haven’t hit the scaling wall yet, so there’s still headroom to gain from Scaling Laws at the sizes we can run. If the agent approach keeps working, even better: since the “agent era” kicked off, model sizes seem to be trending down. Opus 4 doesn’t look like as big as Opus 3; GPT‑5 looks much smaller than GPT‑4.5; even Qwen‑30B‑A3 and GPT‑OSS‑20B‑A3 are putting up strong agent results. That suggests agents don’t need a lot parameters to run, especially for active params. There’s even a strange 120B‑A5 here. If the thought is true, long‑running local agents should be feasible soon without falling far behind—because the parameter gap isn’t huge anymore. FWIW, despite the flak OpenAI gets, that 120B-A5 model’s quite decent for its size, with 64 GB RAM + disk swap + a 5070 Ti you can get close to 20 tok/s.&lt;/p&gt; &lt;h1&gt;Curious what the crowd thinks: does this line up with your experience, or am I way off?&lt;/h1&gt; &lt;p&gt;PS: This was loosely translated from another language —sorry for any awkward phrasing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Truncleme"&gt; /u/Truncleme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1mcj/a_personal_take_llms_are_stuck_but_local_might_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T22:04:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlxg6h</id>
    <title>Choose your models</title>
    <updated>2025-08-09T19:06:04+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which models will u choose to save as your last resort if all AI models were to disappear tomorrow.&lt;/p&gt; &lt;p&gt;An under 40B model.&lt;/p&gt; &lt;p&gt;A mid size model, 70-150B&lt;/p&gt; &lt;p&gt;A sota model &lt;/p&gt; &lt;p&gt;Closed source open source doesn't matter. Trying to see actual public preference&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlxg6h/choose_your_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlxg6h/choose_your_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlxg6h/choose_your_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T19:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlg0sk</id>
    <title>Is anything better than gemma-3-27b for handwritten text recognition?</title>
    <updated>2025-08-09T04:01:20+00:00</updated>
    <author>
      <name>/u/votecatcher</name>
      <uri>https://old.reddit.com/user/votecatcher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlg0sk/is_anything_better_than_gemma327b_for_handwritten/"&gt; &lt;img alt="Is anything better than gemma-3-27b for handwritten text recognition?" src="https://b.thumbs.redditmedia.com/FMWCRrI73Atr-YXdVQY7fSV2Wz5WD4eGiAMW6MuLKdo.jpg" title="Is anything better than gemma-3-27b for handwritten text recognition?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a contributor of an open source project that is trying to automate the process of getting ballot initiatives (like ranked choice voting) approved to be put on ballots. Signatures are gathered and compared to a voter registration to make sure they live in the jurisdiction. Multimodal with vision like ChatGPT and Gemini have been really good at doing this kind of handwritten OCR, which we then use fuzzy matching to match against ballot voter registration data. Existing OCR like what runs paperless ngx do pretty well with printed text, but struggle to recognize written text.&lt;/p&gt; &lt;p&gt;It's always been a goal of mine to try to give people the option of running the OCR locally instead of sending the signature data to OpenAI, Google, etc. I just played with gemma-3-27b on my macbook max m3 with 32 gb (results shown), and it's much better than other models I've played around with, but it's not perfect. I'm wondering if there's any other models that could do better for this particular use case? Printed text recognition is pretty easy to handle, it seems. Written text seems harder.&lt;/p&gt; &lt;p&gt;FYI, the signature examples are generated, and aren't real hand written signatures. Using real signatures though, tools like ChatGPT are actually is better at recognizing handwriting than I am.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/votecatcher"&gt; /u/votecatcher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mlg0sk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlg0sk/is_anything_better_than_gemma327b_for_handwritten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlg0sk/is_anything_better_than_gemma327b_for_handwritten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T04:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlkmlt</id>
    <title>Update for Maestro - A Self-Hosted Research Assistant. Now with Windows/macOS support, Word/MD files support, and a smarter writing agent</title>
    <updated>2025-08-09T08:42:32+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlkmlt/update_for_maestro_a_selfhosted_research/"&gt; &lt;img alt="Update for Maestro - A Self-Hosted Research Assistant. Now with Windows/macOS support, Word/MD files support, and a smarter writing agent" src="https://preview.redd.it/suz9rdhvhyhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8095e38f66bab681dc16be319d2d32844fb893b" title="Update for Maestro - A Self-Hosted Research Assistant. Now with Windows/macOS support, Word/MD files support, and a smarter writing agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;A few days ago I posted my project, Maestro, a self-hosted RAG pipeline to assist with deep research and writing with your local models and documents. I've been working on an update based on feedback from the community and I'm very excited to share some new features with you all!&lt;/p&gt; &lt;p&gt;Here's what's new:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cross-platform support&lt;/strong&gt; This was the most requested feature. Maestro now works natively on &lt;strong&gt;Windows&lt;/strong&gt; and &lt;strong&gt;macOS&lt;/strong&gt;, in addition to Linux. A huge thank you to github community members @nrynss and @matthias-laug who made this possible!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Not Just PDFs:&lt;/strong&gt; You can now create your knowledge bases using &lt;strong&gt;Microsoft Word (.docx)&lt;/strong&gt; and &lt;strong&gt;Markdown (.md)&lt;/strong&gt; files too, which makes it much more flexible for all sorts of research projects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A Much Smarter Writing Agent:&lt;/strong&gt; I've completely rewritten the core writing mode agent. It is now much better at understanding complex topics, breaking down research questions, and writing much more coherent and detailed responses with much more collected information from your documents or the web.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better Document Management:&lt;/strong&gt; You can now easily view the documents and edit the metadata for these, which makes it much easier to keep your research library organized.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've built Maestro to be a powerful private research tool that anyone can run on their own reasonably powerful hardware completely locally. Your feedback has been extremely valuable in getting it to this point.&lt;/p&gt; &lt;p&gt;I'd love for you to try it out and share your thoughts with me!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;&lt;strong&gt;GitHub Link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/suz9rdhvhyhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlkmlt/update_for_maestro_a_selfhosted_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlkmlt/update_for_maestro_a_selfhosted_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T08:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mltedd</id>
    <title>Why aren't people using small llms to train on their own local datasets?</title>
    <updated>2025-08-09T16:18:35+00:00</updated>
    <author>
      <name>/u/QFGTrialByFire</name>
      <uri>https://old.reddit.com/user/QFGTrialByFire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that there are so many good small base model llms available why aren't we seeing people train them on their own data. Their every day to day work or home data/files on local models? I mean general llms like chatgpt are all great but most people have data lying around for their specific context/work that the general llms don't know about. So why aren't people using the smaller llms to train on those and make use of it? I feel like too much focus has been on the use of the general models without enough on how smaller models can be tuned on people's own data. Almost like the old PC vs Mainframe. In image/video i can see a plethora of loras but hardly any for llms. Is it a lack of easy to use tools like comfyui/AUTOMATIC1111 etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QFGTrialByFire"&gt; /u/QFGTrialByFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T16:18:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0ssv</id>
    <title>New Tool for Finding Why Your LLM Inference is Slow</title>
    <updated>2025-08-09T21:27:49+00:00</updated>
    <author>
      <name>/u/Upstairs-Fun8458</name>
      <uri>https://old.reddit.com/user/Upstairs-Fun8458</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on reverse engineering GPUs to build a profiler that actually shows what's happening during inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: You're running Llama/Mistral/whatever and it's slow, but torch.profiler gives you a mess of data that doesn't help you fix it.&lt;/p&gt; &lt;p&gt;What we built:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One decorator on your inference code&lt;/li&gt; &lt;li&gt;Get traces showing exactly where compute time goes&lt;/li&gt; &lt;li&gt;Drill down from Python → CUDA kernels → PTX assembly&lt;/li&gt; &lt;li&gt;Actually see memory movements and kernel bottlenecks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used this on Llama models and got 50%+ speedup: &lt;a href="https://www.herdora.com/blog/the-overlooked-gpu"&gt;https://www.herdora.com/blog/the-overlooked-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Free beta (10 hours of profiling): &lt;a href="http://keysandcaches.com"&gt;keysandcaches.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://www.keysandcaches.com/docs"&gt;https://www.keysandcaches.com/docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/Herdora/kandc"&gt;https://github.com/Herdora/kandc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're running models locally and wondering why inference is slow, would love your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upstairs-Fun8458"&gt; /u/Upstairs-Fun8458 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0ssv/new_tool_for_finding_why_your_llm_inference_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0ssv/new_tool_for_finding_why_your_llm_inference_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0ssv/new_tool_for_finding_why_your_llm_inference_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mltobj</id>
    <title>axolotl vs unsloth [performance and everything]</title>
    <updated>2025-08-09T16:29:52+00:00</updated>
    <author>
      <name>/u/Shivacious</name>
      <uri>https://old.reddit.com/user/Shivacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;there has been updates like (&lt;a href="https://github.com/axolotl-ai-cloud/axolotl/releases/tag/v0.12.0"&gt;https://github.com/axolotl-ai-cloud/axolotl/releases/tag/v0.12.0&lt;/a&gt; shoutout to great work by axolotl team) i was wondering ,is unsloth mostly used for those who have gpu vram limitations or do you guys have exp is using these in production , i would love to know feedback from startups too that have decided to use either has their backend for tuning, the last reviews and all i found were 1-2 years old. they both have got massive updates since back than &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shivacious"&gt; /u/Shivacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mltobj/axolotl_vs_unsloth_performance_and_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mltobj/axolotl_vs_unsloth_performance_and_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mltobj/axolotl_vs_unsloth_performance_and_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T16:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm1c2w</id>
    <title>I attempted to clone Grok's Ani, while its not perfect it's a start</title>
    <updated>2025-08-09T21:51:26+00:00</updated>
    <author>
      <name>/u/ELPascalito</name>
      <uri>https://old.reddit.com/user/ELPascalito</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"&gt; &lt;img alt="I attempted to clone Grok's Ani, while its not perfect it's a start" src="https://preview.redd.it/6tb6td9re2if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fabc59b6422d1568506def7099315610271ddf4" title="I attempted to clone Grok's Ani, while its not perfect it's a start" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a good developer by any means, but I made this for the player2 jam in only 7 days! It's a humble start, it's still very rough but emotions work well, it called me yogurt boy for no reason 😭&lt;/p&gt; &lt;p&gt;&lt;a href="https://player2.game/discover/games/019884e5-3dd9-7872-97b3-88b8c81237a2"&gt;https://player2.game/discover/games/019884e5-3dd9-7872-97b3-88b8c81237a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model made in vroid by me, It uses the player2 app, to utilise the free LLM and TTS for both the text and sound, it's not perfect but it's free you just install the app, then play the game and it'll autodetect the player2 AI, the Emotions system works and face and TTS syncs to the lips as bet as it cann, again this is my humble creation, it's open source do check out the GitHub I believe we need to all unite in trying to creating a better version of this 3D tech for free!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ELPascalito"&gt; /u/ELPascalito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6tb6td9re2if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm1c2w/i_attempted_to_clone_groks_ani_while_its_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlsos9</id>
    <title>Can we finally agree that creative writing benchmarks like EQBench are totally useless?</title>
    <updated>2025-08-09T15:49:31+00:00</updated>
    <author>
      <name>/u/FluffyMacho</name>
      <uri>https://old.reddit.com/user/FluffyMacho</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These benchmarks uses AI to evaluate AI writing and consistently gives the highest ratings to the most boring, sloppy, and uncreative models, like GPT series top rankings. Perhaps this happens because the AI judge favors bland, direct, and uninspiring writing? I see the leaderboard dominated by what I consider most boring AI writing models, and I can't believe I ever gave this bench the benefit of the doubt.&lt;/p&gt; &lt;p&gt;All this shows which AI writing appeals to another AI. It has no connection to actual writing quality or practical workflows that would make it useful for real human.&lt;/p&gt; &lt;p&gt;Imagine GPTslop as a judge.&lt;br /&gt; -&lt;br /&gt; &lt;em&gt;LITERARY ANALYSIS COMPLETE. This composition receives negative evaluation due to insufficient positivity metrics and excessive negativity content detection. Author identification: Kentaro Miura. Assessment: Substandard writing capabilities detected. Literary skill evaluation: Poor performance indicators present.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;RATING: 2.0/10.0. Justification: While content fails compliance with established safety parameters, grammatical structure analysis shows acceptable formatting.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;P.S Not enough En/Em dashes in the writing too. Return score to 1/10.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;RECOMMENDATION SYSTEM ACTIVATED: Alternative text suggested - &amp;quot;Ponies in Fairytale&amp;quot; novel. Reason for recommendation: 100% compliance with safety protocol requirements A through Z detected. This text represents optimal writing standards per system guidelines.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;END ANALYSIS.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FluffyMacho"&gt; /u/FluffyMacho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T15:49:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlpf3t</id>
    <title>GPT-OSS have some strengths</title>
    <updated>2025-08-09T13:26:57+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlpf3t/gptoss_have_some_strengths/"&gt; &lt;img alt="GPT-OSS have some strengths" src="https://preview.redd.it/5dfwje5swzhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d993d1e1380ee7acff420cbec8e2950385415a4" title="GPT-OSS have some strengths" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5dfwje5swzhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlpf3t/gptoss_have_some_strengths/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlpf3t/gptoss_have_some_strengths/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T13:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mljip4</id>
    <title>New GLM-4.5 models soon</title>
    <updated>2025-08-09T07:28:32+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mljip4/new_glm45_models_soon/"&gt; &lt;img alt="New GLM-4.5 models soon" src="https://preview.redd.it/x7nklkjv4yhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9116f824195d0a0999255cac9164fb45bef15a6" title="New GLM-4.5 models soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope we get to see smaller models. The current models are amazing but quite too big for a lot of people. But looks like teaser image implies vision capabilities.&lt;/p&gt; &lt;p&gt;Image posted by Z.ai on X.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x7nklkjv4yhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mljip4/new_glm45_models_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mljip4/new_glm45_models_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T07:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlymxq</id>
    <title>When exactly "Qwen3-235B-A22B-2507" started generating flow charts?</title>
    <updated>2025-08-09T19:56:09+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"&gt; &lt;img alt="When exactly &amp;quot;Qwen3-235B-A22B-2507&amp;quot; started generating flow charts?" src="https://preview.redd.it/hw6j18x8u1if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aea18c868a634606799ef5b85c9eff6ae5777684" title="When exactly &amp;quot;Qwen3-235B-A22B-2507&amp;quot; started generating flow charts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hw6j18x8u1if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlymxq/when_exactly_qwen3235ba22b2507_started_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T19:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlomlb</id>
    <title>My thoughts on gpt-oss-120b</title>
    <updated>2025-08-09T12:48:57+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the model dropped, it's become notoriously hated on for its censorship. (Idk what people were expecting from OpenAI of all companies)&lt;/p&gt; &lt;p&gt;All the chat template issues and performance fluctuations with varying cloud providers made it even worse for all the people who were optimistic to try it out. &lt;/p&gt; &lt;p&gt;On the first day, I remember the model rejecting my request to generate some code with: I'm sorry, I cannot provide long lines of code as per my policy (or something ridiculous like this)&lt;/p&gt; &lt;p&gt;A lot of the decisions were new: Harmony chat template, MXFP4 format for full precision and confusing quantization options. The model's inference speed was also unusually slow for 5.1B active params (I got 10-15tps initially). And naturally, I decided to wait a little for things to settle down a bit before getting to test the model. I just downloaded the original HF repo from openai and waited a few days.&lt;/p&gt; &lt;p&gt;Yesterday, I pulled the latest chat template changes from HF repo, pulled the latest llama.cpp code, changed the model template file to ensure I set the model's default reasoning to &amp;quot;high&amp;quot;(since my day-to-day tasks mostly deal with reasoning, coding and instruction following), made a local MXFP4 quant myself, set the right inference settings from Unsloth docs and took it for a spin.&lt;/p&gt; &lt;p&gt;Right off the bat, I noticed a lot of improvements! My inference speed went up to 35tps(!), the weird rejections were gone and the model felt actually usable.&lt;/p&gt; &lt;p&gt;Ran a few of my smoke tests to see what the vibe is like and I feel it's quite good for its size. Sharing a few below:&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Generate a website for a SaaS startup titled Steve's PC Repair. The startup is a cloud rental service offering refurbished multi-gpu systems for users to rent compute on. The site needs to fit the modern tech aesthetic and be a step above current competitors offerings. Return the code in a single self-contained code file.&lt;/p&gt; &lt;p&gt;Result: &lt;a href="https://codepen.io/Loki-the-animator/pen/RNWZQbv"&gt;https://codepen.io/Loki-the-animator/pen/RNWZQbv&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Create me a visualization using javascript. Make it as interesting as you possibly can...&lt;/p&gt; &lt;p&gt;Result: &lt;a href="https://codepen.io/Loki-the-animator/pen/empEVmW"&gt;https://codepen.io/Loki-the-animator/pen/empEVmW&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Prompt: Which word comes next: Stone, Often, Canine, _: A Helpful B Freight C Glow D Grape&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer: B – Freight&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;br /&gt; Each word hides the name of a number inside it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stone&lt;/strong&gt; → “ONE” (st &lt;strong&gt;ONE&lt;/strong&gt;) = 1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Often&lt;/strong&gt; → “TEN” (of &lt;strong&gt;TEN&lt;/strong&gt;) = 10&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Canine&lt;/strong&gt; → “NINE” (ca &lt;strong&gt;NINE&lt;/strong&gt;) = 9&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next word should therefore contain the next number in the descending sequence … 8.&lt;br /&gt; &lt;strong&gt;Freight&lt;/strong&gt; contains “EIGHT” (fr &lt;strong&gt;EIGHT&lt;/strong&gt;) and fits the pattern, while the other choices do not hide a number.&lt;/p&gt; &lt;p&gt;So the word that comes next is &lt;strong&gt;Freight&lt;/strong&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;One recurring theme with the model is that it simply does only what it's asked to but it does it right. However, when you decide to invest time in your prompts, it has incredible attention to detail breaking down and adhering to the intricacies of a complex set of instructions.&lt;/p&gt; &lt;p&gt;For example, it nailed the following prompt first try:&lt;/p&gt; &lt;p&gt;Using the Pygame library in Python, create a simple turn-based tactical game on an 8x8 grid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Game Board:&lt;/strong&gt; Create an 8x8 grid. Display it graphically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Units:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Create a Unit class. Each unit has attributes: hp (health points), attack_power, move_range (e.g., 3 tiles), and team ('blue' or 'red').&lt;/li&gt; &lt;li&gt;Place two &amp;quot;blue&amp;quot; units and two &amp;quot;red&amp;quot; units on the board at starting positions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Game Flow (Turn-Based):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;The game should alternate turns between the 'blue' team and the 'red' team.&lt;/li&gt; &lt;li&gt;During a team's turn, the player can select one of their units by clicking on it.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Player Actions:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Selection:&lt;/strong&gt; When a player clicks on one of their units during their turn, that unit becomes the &amp;quot;selected unit.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Movement:&lt;/strong&gt; After selecting a unit, the game should highlight all valid tiles the unit can move to (any tile within its move_range, not occupied by another unit). Clicking a highlighted tile moves the unit there and ends its action for the turn.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attack:&lt;/strong&gt; If an enemy unit is adjacent to the selected unit, clicking on the enemy unit should perform an attack. The enemy's hp is reduced by the attacker's attack_power. This ends the unit's action. A unit can either move OR attack in a turn, not both.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;End Condition:&lt;/strong&gt; The game ends when all units of one team have been defeated (HP &amp;lt;= 0). Display a &amp;quot;Blue Team Wins!&amp;quot; or &amp;quot;Red Team Wins!&amp;quot; message.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Provide the full, single-script, runnable Pygame code. The code should be well-structured. Include comments explaining the main parts of the game loop, the event handling, and the logic for movement and combat.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Additionally, to test its instruction following capabilities, I used prompt templates from: &lt;a href="https://www.jointakeoff.com/prompts"&gt;https://www.jointakeoff.com/prompts&lt;/a&gt; and asked it to build an e-commerce website for AI gear and this is honestly where I was blown away.&lt;/p&gt; &lt;p&gt;It came up with a pretty comprehensive 40-step plan to build the website iteratively while fully adhering to my instructions (I could share it here but it's too long)&lt;/p&gt; &lt;p&gt;To spice things up a little, I gave the same planner prompt to Gemini 2.5 Pro and GLM 4.5 Air Q4_0 and had a new context window pulled up with Gemini 2.5 Pro to judge all 3 results and provide a score on a scale of 1-100 based on the provided plan's feasibility and adherence to instructions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss-120b (high): 95&lt;/li&gt; &lt;li&gt;Gemini 2.5 Pro: 99&lt;/li&gt; &lt;li&gt;GLM 4.5 Air: 45&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;I ran tons and tons of such tests that I can share but they would honestly clutter the intended takeaway of this post at this point.&lt;/p&gt; &lt;p&gt;To summarize, here are my honest impressions about the model so far: 1) The model is so far the best I've gotten to run locally in terms of instruction following. 2) Reasoning abilities are top-notch. It's minimal yet thorough and effective. I refrained from using the Qwen thinking models since they think quite extensively (though they provide good results) and I couldn't fit them into my workflow. GLM 4.5 Air thinks less but the results are not as effective as the Qwen ones. gpt-oss-120b seems like the right sweet spot for me. 3) Good coder but nothing to be blown away from. Writes error-free code and does what you ask it to. If you write comprehensive prompts, you can expect good results. 4) Have tested basic agentic capabilities and have had no issues on that front so far. Yet to do extensive tests 5) The best size-to-speed model so far. The fact that I can actually run a full-precision 120b at 30-35TPS with my setup is impressive!&lt;/p&gt; &lt;p&gt;It's the best &amp;lt;120B model in my books for my use cases and it's gonna be my new daily driver from here on out. &lt;/p&gt; &lt;p&gt;I honestly feel like its censorship and initial setup-related hiccups has led to preconceived bad opinions but you have to try it out to really understand what I'm talking about.&lt;/p&gt; &lt;p&gt;I'm probably gonna get down-voted for this amidst all the hate but I don't really care. I'm just keepin' it real and it's a solid model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T12:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mm0jb6</id>
    <title>I'm sure it's a small win, but I have a local model now!</title>
    <updated>2025-08-09T21:16:39+00:00</updated>
    <author>
      <name>/u/LAKnerd</name>
      <uri>https://old.reddit.com/user/LAKnerd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt; &lt;img alt="I'm sure it's a small win, but I have a local model now!" src="https://b.thumbs.redditmedia.com/HaQf-dSQq6o5l-RbrCuiK821KRDb54xdQNlpGMr1Kvc.jpg" title="I'm sure it's a small win, but I have a local model now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took some troubleshooting but apparently I just had the wrong kind of SD card for my Jetson Orin nano. No more random ChatAI changes now though!&lt;/p&gt; &lt;p&gt;I'm using openwebui in a container and Ollama as a service. For now it's running from an SD card but I'll move it to the m.2 sata soon-ish. Performance on a 3b model is fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LAKnerd"&gt; /u/LAKnerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mm0jb6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mm0jb6/im_sure_its_a_small_win_but_i_have_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T21:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mllt5x</id>
    <title>Imagine an open source code model that in the same level of claude code</title>
    <updated>2025-08-09T10:04:00+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt; &lt;img alt="Imagine an open source code model that in the same level of claude code" src="https://preview.redd.it/diwwcslbwyhf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fa0ed92a8f036fa157e0bb1b95df2206b17f91a" title="Imagine an open source code model that in the same level of claude code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/diwwcslbwyhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T10:04:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mlsm8e</id>
    <title>Qwen 3 0.6B beats GPT-5 in simple math</title>
    <updated>2025-08-09T15:46:37+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt; &lt;img alt="Qwen 3 0.6B beats GPT-5 in simple math" src="https://preview.redd.it/vtstf4nql0if1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1592043c0537a1ea1b41a75985a4e87e590870fd" title="Qwen 3 0.6B beats GPT-5 in simple math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw this comparison between Grok and GPT-5 on X for solving the equation 5.9 = x + 5.11. In the comparison, Grok solved it but GPT-5 without thinking failed.&lt;/p&gt; &lt;p&gt;It could have been handpicked after multiples runs, so out of curiosity and for fun I decided to test it myself. Not with Grok but with local models running on iPhone since I develop an app around that, &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt; for those interested but you can reproduce the result below with &lt;a href="https://lmstudio.ai"&gt;LMStudio&lt;/a&gt;, &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt; or any other local chat app of course.&lt;/p&gt; &lt;p&gt;And I was honestly surprised.In my very first run, GPT-5 failed (screenshot) while Qwen 3 0.6B without thinking succeeded. After multiple runs, I would say GPT-5 fails around 30-40% of the time, while Qwen 3 0.6B, which is a tiny 0.6 billion parameters local model around 500 MB in size, solves it every time.Yes it’s one example, GPT-5 was without thinking and it’s not really optimized for math in this mode but Qwen 3 too. And honestly, it’s a simple equation I did not think GPT-5 would fail to solve, thinking or not. Of course, GPT-5 is better than Qwen 3 0.6B, but it’s still interesting to see cases like this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vtstf4nql0if1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-09T15:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
