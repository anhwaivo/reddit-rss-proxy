<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-09T12:07:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ma8i</id>
    <title>Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!</title>
    <updated>2025-03-08T17:35:45+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt; &lt;img alt="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" src="https://external-preview.redd.it/OERDGiS518l9lA6nng9dhSyETZuedB7NMNyJJW94EgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e69639f639d057ebece140432310ca7d2b192b1" title="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab"&gt;https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re part of the open-source project &lt;a href="https://github.com/anemll/anemll"&gt;&lt;strong&gt;ANEMLL&lt;/strong&gt;&lt;/a&gt;, which is working to bring large language models (LLMs) to the Apple Neural Engine. This hardware has incredible potential, but there’s a catch—Apple hasn’t shared much about its inner workings, like memory speeds or detailed performance specs. That’s where you come in!&lt;/p&gt; &lt;p&gt;To help us understand the Neural Engine better, we’ve launched a new benchmark tool: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;&lt;strong&gt;anemll-bench&lt;/strong&gt;&lt;/a&gt;. It measures the Neural Engine’s bandwidth, which is key for optimizing LLMs on Apple’s chips.&lt;/p&gt; &lt;p&gt;We’re especially eager to see results from &lt;strong&gt;Ultra&lt;/strong&gt; models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;M1 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;And, if you’re one of the lucky few, &lt;strong&gt;M3 Ultra&lt;/strong&gt;!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(Max models like M2 Max, M3 Max, and M4 Max are also super helpful!)&lt;/p&gt; &lt;p&gt;If you’ve got one of these Macs, here’s how you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Clone the repo&lt;/strong&gt;: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;https://github.com/Anemll/anemll-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run the benchmark&lt;/strong&gt;: Just follow the README—it’s straightforward!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Share your results&lt;/strong&gt;: Submit your JSON result via a &amp;quot;issues&amp;quot; or email&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why contribute?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll help an open-source project make real progress.&lt;/li&gt; &lt;li&gt;You’ll get to see how your device stacks up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious about the bigger picture? Check out the main ANEMLL project: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks for considering this—every contribution helps us unlock the Neural Engine’s potential&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6toz5</id>
    <title>Livrbench - Tomorrow qwq32b will be updated to score?</title>
    <updated>2025-03-08T23:11:53+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"&gt; &lt;img alt="Livrbench - Tomorrow qwq32b will be updated to score?" src="https://preview.redd.it/o87gf0yssjne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1519b5c3aee1feb44ba4d538614f2b37db684610" title="Livrbench - Tomorrow qwq32b will be updated to score?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o87gf0yssjne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6noh8</id>
    <title>Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1...</title>
    <updated>2025-03-08T18:36:50+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt; &lt;img alt="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." src="https://external-preview.redd.it/7RbGl7PqZ_sGLzEC-up6MH5b7zJrrofblGxxk0WxBC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132e3f4ed63e0faee796888e40ee542ef9d2a07c" title="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/FlappyAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72wlt</id>
    <title>Memory for Story writing project using LLM</title>
    <updated>2025-03-09T08:02:08+00:00</updated>
    <author>
      <name>/u/Secret_Scale_492</name>
      <uri>https://old.reddit.com/user/Secret_Scale_492</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey currently I am working on a personal project to build a story writing application using LLMs but I'm still figuring out what is the best way to catergorized each chapter and store to feed the LLM again so it knows from where to continue again currently I am feeding the whole chapter content to the LLM which I dont think will be suitable cause the token limit can be passed .if anyone has worked on a project like this or have faced a likewise issue please leave a comment on how I can achieve a better memory and also any features to have or improve in a story writing app with LLM&lt;/p&gt; &lt;p&gt;note: Currently using - eva-qwen2.5-32b-v0.0 from LM Studio&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Scale_492"&gt; /u/Secret_Scale_492 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72wlt/memory_for_story_writing_project_using_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72wlt/memory_for_story_writing_project_using_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j72wlt/memory_for_story_writing_project_using_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nct7</id>
    <title>Estimating how much the new NVIDIA RTX PRO 6000 Blackwell GPU should cost</title>
    <updated>2025-03-08T18:22:47+00:00</updated>
    <author>
      <name>/u/asssuber</name>
      <uri>https://old.reddit.com/user/asssuber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No price released yet, so let's figure out how much that card should cost:&lt;/p&gt; &lt;p&gt;Extra GDDR6 costs less than $8 per GB for the end consumer &lt;a href="https://arstechnica.com/gadgets/2024/01/review-radeon-7600-xt-offers-peace-of-mind-via-lots-of-ram-remains-a-midrange-gpu/"&gt;when installed in a GPU clamshell style&lt;/a&gt; like Nvidia is using here. GDDR7 chips seems to carry a &lt;a href="https://www.trendforce.com/presscenter/news/20240627-12207.html"&gt;20-30% premium&lt;/a&gt; over GDDR6 which I'm going to generalize to all other costs and margins related to putting it in a card, so we get less than $10 per GB.&lt;/p&gt; &lt;p&gt;Using the $2000 MSRP of the 32GB RTX 5090 as basis, the NVIDIA RTX PRO 6000 Blackwell with 96GB &lt;strong&gt;should cost less than $2700&lt;/strong&gt; *(see EDIT2) to the end consumer. Oh, the wonders of a competitive capitalistic market, free of monopolistic practices!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; It seems my sarcasm above, the &amp;quot;Funny&amp;quot; flair and my comment bellow weren't sufficient, so I will try to repeat here:&lt;/p&gt; &lt;p&gt;I'm estimating how much it SHOULD cost, because everyone over here seems to be keen on normalizing the exorbitant prices for extra VRAM at the top end cards, and this is wrong. I know nvidia will price it much higher, but that was not the point of my post.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT2:&lt;/strong&gt; The RTX PRO 6000 Blackwell will reportedly feature an almost fully enabled GB202 chip, with a bit more than 10% more CUDA cores than the RTX 5090, so using it's MSRP as base isn't sufficient. Think of the price as the fair price for an hypothetical RTX 5090 96GB instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asssuber"&gt; /u/asssuber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dryj</id>
    <title>Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes</title>
    <updated>2025-03-08T09:41:19+00:00</updated>
    <author>
      <name>/u/2TierKeir</name>
      <uri>https://old.reddit.com/user/2TierKeir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt; &lt;img alt="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" src="https://external-preview.redd.it/aGlvZGVrdDZzZm5lMc_Az5p3qLdEN__5qSL7XTQoE-2LI7eWZo3yGOsqXnkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7745bb1240348e2c2b8426f85b17a2fe6e2edeed" title="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2TierKeir"&gt; /u/2TierKeir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xkdwav2sfne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j73620</id>
    <title>Framework desktop</title>
    <updated>2025-03-09T08:21:34+00:00</updated>
    <author>
      <name>/u/Flowrome</name>
      <uri>https://old.reddit.com/user/Flowrome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok… i may have rushed a bit, I’ve bought the maxed desktop from framework… So now my question is, with that apu and that ram, is it possible to run these things?&lt;/p&gt; &lt;p&gt;1 istance of qwq with ollama (yeah i know llama.cpp is better but i prefer the simplicity of ollama) or any other 32b llm 1 istance of comfyui + flux.dev&lt;/p&gt; &lt;p&gt;All together without hassle? &lt;/p&gt; &lt;p&gt;I’m currently using my desktop as wake on request ollama and comfyui backend, then i use openwebui as frontend and due to hw limitations (3090+32gb ddr4) i can run 7b + schnell and it’s not on 24h/7d for energy consumption (i mean it’s a private usage only but I’m already running two proxmox nodes 24h/7d)&lt;/p&gt; &lt;p&gt;Do you think it’s worth for this usage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flowrome"&gt; /u/Flowrome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j73620/framework_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j73620/framework_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j73620/framework_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T08:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6rngt</id>
    <title>Simple inference speed comparison of Deepseek-R1 between llama.cpp and ik_llama.cpp for CPU-only inference.</title>
    <updated>2025-03-08T21:36:58+00:00</updated>
    <author>
      <name>/u/U_A_beringianus</name>
      <uri>https://old.reddit.com/user/U_A_beringianus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a simple inference speed comparison of DeepSeek-R1 between &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; for CPU-only inference. The latter is a fork of an old version of llama.cpp, but includes various recent optimizations and options that the original does not (yet?).&lt;br /&gt; Comparison is on linux, with a 16 core Ryzen 7 with 96GB RAM, using Q3 quants that are mem-mapped from nvme (~319GB). Initial context consists of merely one one-liner prompt.&lt;br /&gt; Options in &lt;strong&gt;bold&lt;/strong&gt; are exclusive to ik_llama.cpp, as of today.&lt;br /&gt; The quants in the mla/ directory are made with the fork, to support its use of the &amp;quot;-mla 1&amp;quot; command line option, which yields a significantly smaller requirement for KV-Cache space. &lt;/p&gt; &lt;p&gt;llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 0.8 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 1.1 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 55632.00 MiB&lt;br /&gt; Token rate: 1.2 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 556.63 MiB (Yes, really, no typo. This would allow the use of much larger context.)&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; (no KV cache quantization)&lt;br /&gt; KV-Cache: 1098.00 MiB&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;Quants that work with MLA can be found there: &lt;a href="https://huggingface.co/daydream-org/DeepSeek-R1-GGUF-11446/tree/main/DeepSeek-R1-Q3_K_M"&gt;Q3&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q2_K/tree/main"&gt;Q2&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q4_K/tree/main"&gt;Q4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U_A_beringianus"&gt; /u/U_A_beringianus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T21:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6iuyf</id>
    <title>NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp; 600W TBP</title>
    <updated>2025-03-08T14:56:53+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" src="https://external-preview.redd.it/ipqoihUxtH0AdjsoCf5u0QWlmwf7QkIL9jnTAAb3HTw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6333201abf11bb51d15493e0484c12b4cafa2d16" title="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-gpu-more-cores-than-rtx-5090-24064-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6f61q</id>
    <title>QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7</title>
    <updated>2025-03-08T11:24:32+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt; &lt;img alt="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" src="https://preview.redd.it/opow8do3agne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00cbc87c29904f9341ccf656e804f32edd07064a" title="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/opow8do3agne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j75xpm</id>
    <title>Dumb question - I use Claude 3.5 A LOT, what setup would I need to create a comparable local solution?</title>
    <updated>2025-03-09T11:45:31+00:00</updated>
    <author>
      <name>/u/Friendly_Signature</name>
      <uri>https://old.reddit.com/user/Friendly_Signature</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a hobbyist coder that is now working on bigger personal builds. (I was Product guy and Scrum master for AGES, now I am trying putting the policies I saw around me enforced on my own personal build projects). &lt;/p&gt; &lt;p&gt;Loving that I am learning by DOING my own CI/CD, GitHub with apps and Actions, using Rust instead of python, sticking to DDD architecture, TD development, etc&lt;/p&gt; &lt;p&gt;I spend a lot on Claude, maybe enough that I could justify a decent hardware purchase. &lt;/p&gt; &lt;p&gt;Any feedback welcome :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly_Signature"&gt; /u/Friendly_Signature &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T11:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6vmke</id>
    <title>Is RTX 3090 still the only king of price/performance for running local LLMs and diffusion models? (plus some rant)</title>
    <updated>2025-03-09T00:47:11+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a used MSI SUPRIM X RTX 3090 for 820EUR in a local store. I am so tempted to buy it. And also doubtful. Essentially, looking for an excuse to buy it.&lt;/p&gt; &lt;p&gt;Do I understand correctly that there seems to be no chance of having better (and not more expensive) alternatives with at least 24 GB RAM during the next months? Intel's rumored 24GB GPU might not even come out this year or ever.&lt;/p&gt; &lt;p&gt;Does MSI SUPRIM X RTX 3090 have good quality or are there any caveats?&lt;/p&gt; &lt;p&gt;I will power-limit it for sure. I have a mATX case that might not have that good airflow because of where it's located, and also I want the GPU to last as long as possible, being such an anxious person who upgrades rarely. Not yet sure what would be the right approach to limiting it for LLM use - powerlimit, undervolting, something else?&lt;/p&gt; &lt;p&gt;The specs of my other components:&lt;/p&gt; &lt;p&gt;Mobo: ASUS TUF Gaming B760M-Plus D4&lt;/p&gt; &lt;p&gt;RAM: 64 GB DDR4&lt;/p&gt; &lt;p&gt;CPU: i7 14700 (please don't degrade, knocking on wood, updated BIOS)&lt;/p&gt; &lt;p&gt;PSU: Seasonic Focus GX-850&lt;/p&gt; &lt;p&gt;Current GPU: 4060 Ti 16 GB&lt;/p&gt; &lt;p&gt;Case: Fractal Design Define Mini (should fit the 33cm SUPRIM, if I rearrange my hard drives).&lt;/p&gt; &lt;p&gt;Using Windows 11.&lt;/p&gt; &lt;p&gt;I know there are Macs with even more unified memory and the new AMD AI CPU with their &amp;quot;coming soon&amp;quot; devices, but the performance seems to be worse than 3090 and the price is so much higher (add 21% VAT in Europe).&lt;/p&gt; &lt;p&gt;Some personal rant follows, feel free to ignore it.&lt;/p&gt; &lt;p&gt;It's not a financial issue. I could afford even a Mac. I just cannot justify it psychologically. That's the consequence of growing up in a poor family where I could not afford even a cassette player and had to build one myself from parts that people threw out. Now I can afford everything I want but I need really good justification, otherwise, I always feel guilty for months because I spent so much.&lt;/p&gt; &lt;p&gt;I already went through similar anxious doubts when I bought a 4060 Ti 16GB some time ago naively thinking that &amp;quot;16GB is good enough&amp;quot;. Then 32B LLMs came, and then Flux, and now Wan video, and I want to &amp;quot;try it all&amp;quot; and have fun generating some content for my friends and relatives. I can run it on 4060 but I spend too much time tweaking settings and choosing the right quants to avoid outofmemory errors, and waiting too long for video generation to complete, just to find that it did not follow the prompt well enough and I need to regenerate.&lt;/p&gt; &lt;p&gt;Now about excuses. I can lie to myself that it is an investment in my work education. I'm a software developer (visually impaired since birth, BTW), but I'm working on boring ERP system integrations and not on AI. Still, I have already built my own LLM frontend for KoboldCpp/OpenRouter/Gemini. That was a development experience that might be useful in work someday... or most likely not. Also, I have experimented a bit in UnrealEngine and had an idea to create a 3D assistant avatar for LLM, but let's be real - I don't have enough time for everything. So, to be totally honest with myself, it is just a hobby.&lt;/p&gt; &lt;p&gt;How do you guys justify spending that much on GPUs? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T00:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6i1ma</id>
    <title>Can't believe it, but the RTX 4090 actually exists and it runs!!!</title>
    <updated>2025-03-08T14:15:34+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt; &lt;img alt="Can't believe it, but the RTX 4090 actually exists and it runs!!!" src="https://b.thumbs.redditmedia.com/Jyu8XHnVjN10hhVg2LdYg8XX4xiDcuuwK_tm4Uimz_g.jpg" title="Can't believe it, but the RTX 4090 actually exists and it runs!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX 4090 96G version&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2"&gt;https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1"&gt;https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6u731</id>
    <title>NVIDIA RTX PRO 6000 Blackwell leaked: 24064 cores, 96GB G7 memory and 600W Double Flow Through cooler</title>
    <updated>2025-03-08T23:35:33+00:00</updated>
    <author>
      <name>/u/mbolaris</name>
      <uri>https://old.reddit.com/user/mbolaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler"&gt;https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mbolaris"&gt; /u/mbolaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j7505c</id>
    <title>simple-computer-use: a lightweight open source Computer Use implementation for Windows and Linux</title>
    <updated>2025-03-09T10:38:44+00:00</updated>
    <author>
      <name>/u/nava_7777</name>
      <uri>https://old.reddit.com/user/nava_7777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. I made &lt;a href="https://github.com/pnmartinez/simple-computer-use"&gt;https://github.com/pnmartinez/simple-computer-use&lt;/a&gt; to solve a&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Nowawadays we can &lt;strong&gt;code with Natural Language with Cursor, Windsurf, or other tools.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;However, &lt;strong&gt;ideas often come up while away from the PC&lt;/strong&gt;, and I find myself &lt;strong&gt;putting my hardware to work for me through TeamViewer&lt;/strong&gt; or similar (which is uncomfortable),&lt;/li&gt; &lt;li&gt;I consider that &lt;strong&gt;voice support&lt;/strong&gt; for these apps like Cursor would be absolutely awesome (some issues already opened in their repo),&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Solution&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I made myself (yet another) &lt;strong&gt;tool to control a desktop GUI with natural language&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Adding a layer for &lt;strong&gt;voice control&lt;/strong&gt; is just the next step.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TODO&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Voice processing layer&lt;/strong&gt; to send the task comfortably from e.g. a phone to the desktop hardware,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;increase robustness&lt;/strong&gt;: the current implementation is too-heavily realiant on OCR (vision capabilities for icons can be greatly improved with vLLMs, this is just a POC).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to use, give feedback, open issues and PRs. etc.&lt;/p&gt; &lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/29891887/420660898-bdd5bc25-fe88-4105-a3ed-f435f98e4f18.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE1MTY5NTUsIm5iZiI6MTc0MTUxNjY1NSwicGF0aCI6Ii8yOTg5MTg4Ny80MjA2NjA4OTgtYmRkNWJjMjUtZmU4OC00MTA1LWEzZWQtZjQzNWY5OGU0ZjE4LndlYm0_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzA5JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMwOVQxMDM3MzVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00Y2ZjZWYwZDM5N2M1MTI4ZGY4YjdmZTVkNTkxMDJhMGY3MjFkYzk0NjQ1ZDk1OGQ5MGVjMjE2YTU0NTAxMjQ2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.fL-o54DMITQUFiS_Er4QtTp-Dy_N6I_ooYLQE-VzG_E"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nava_7777"&gt; /u/nava_7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j7505c/simplecomputeruse_a_lightweight_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T10:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1j707rk</id>
    <title>I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude</title>
    <updated>2025-03-09T05:02:27+00:00</updated>
    <author>
      <name>/u/SamchonFramework</name>
      <uri>https://old.reddit.com/user/SamchonFramework</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt; &lt;img alt="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" src="https://external-preview.redd.it/r6RwKjQxXMDR-e5d7t4YxF5tZC1G1HG5fUFlHIeaPcI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57049fdbdeab552ae7b0d02c18658d895181b100" title="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SamchonFramework"&gt; /u/SamchonFramework &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nestia.io/articles/llm-function-calling/i-made-mcp-alternative-solution.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T05:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6z9aq</id>
    <title>Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA.</title>
    <updated>2025-03-09T04:05:30+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"&gt; &lt;img alt="Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA." src="https://external-preview.redd.it/anhtenllMHA4bG5lMV78OOspL0cK_E8n1WAZwKPHs9OMmQEsahJ1oWnR6qQ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f289895f4efbc2ce043570e85108d77fa3ef3b60" title="Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a20gd80p8lne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T04:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dzai</id>
    <title>Real-time token graph in Open WebUI</title>
    <updated>2025-03-08T09:56:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt; &lt;img alt="Real-time token graph in Open WebUI" src="https://external-preview.redd.it/dm1rY2E3dWl1Zm5lMeNo1g2VbIy6NNGx_1T_ctYYVLkaFt3bwpFyaChfDLc3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c66a72211f8ad427c81d09e427101d7638dfd38" title="Real-time token graph in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zscr76uiufne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6xpvt</id>
    <title>How large is your local LLM context?</title>
    <updated>2025-03-09T02:38:39+00:00</updated>
    <author>
      <name>/u/iwinux</name>
      <uri>https://old.reddit.com/user/iwinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm new to this rabbit hole. Never realized context is such a VRAM hog until I loaded my first model (Qwen2.5 Coder 14B Instruct &lt;code&gt;Q4_K_M&lt;/code&gt; GGUF) with LM Studio. On my Mac mini M2 Pro (32GB RAM), increasing context size from 32K to 64K almost eats up all RAM.&lt;/p&gt; &lt;p&gt;So I wonder, do you run LLMs with max context size by default? Or keep it as low as possible?&lt;/p&gt; &lt;p&gt;For my use case (coding, as suggested by the model), I'm already spoiled by Claude / Gemini's huge context size :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iwinux"&gt; /u/iwinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T02:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nzrk</id>
    <title>New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s</title>
    <updated>2025-03-08T18:51:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt; &lt;img alt="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" src="https://preview.redd.it/wfkxh0q5iine1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63ab35ce1aa6589c56196d048a5e4231e07749f" title="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfkxh0q5iine1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j72gw5</id>
    <title>AMD May Bring ROCm Support On Windows Operating System As AMD’s Vice President Nods For It</title>
    <updated>2025-03-09T07:30:47+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/"&gt;https://wccftech.com/amd-may-bring-rocm-support-on-windows-operating-system/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j72gw5/amd_may_bring_rocm_support_on_windows_operating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T07:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6w3qq</id>
    <title>PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters.</title>
    <updated>2025-03-09T01:11:29+00:00</updated>
    <author>
      <name>/u/computemachines</name>
      <uri>https://old.reddit.com/user/computemachines</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt; &lt;img alt="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." src="https://preview.redd.it/4k4rbdxjdkne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1088de160301ef1df0ee665bb6dbee41c324644" title="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computemachines"&gt; /u/computemachines &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k4rbdxjdkne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T01:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6yxdr</id>
    <title>Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf (and Thinking/Reasoning MOES...) ... 34+ new models (Lllamas, Qwen - MOES and not Moes..)</title>
    <updated>2025-03-09T03:46:25+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From David_AU ;&lt;/p&gt; &lt;p&gt;First two models based on Qwen's off the charts &amp;quot;QwQ 32B&amp;quot; model just released, with some extra power. Detailed instructions, and examples at each repo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Model, Free thinker, Extra Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular, Not so Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AND Qwen/Llama Thinking/Reasoning MOES - all sizes, shapes ...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;34&lt;/strong&gt; reasoning/thinking models (example generations, notes, instructions etc):&lt;/p&gt; &lt;p&gt;Includes Llama 3,3.1,3.2 and Qwens, DeepSeek/QwQ/DeepHermes in MOE and NON MOE config plus others:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is an interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf"&gt;https://huggingface.co/DavidAU/DeepThought-MOE-8X3B-R1-Llama-3.2-Reasoning-18B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For Qwens (12 models) only (Moes and/or Enhanced):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde"&gt;https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another interesting one:&lt;br /&gt; &lt;a href="https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-MOE-2X1.5B-DeepSeek-Uncensored-Censored-4B-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Separate source / full precision sections/collections at main repo here:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;656 Models, in 27 collections:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU"&gt;https://huggingface.co/DavidAU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Special service note for Lmstudio users:&lt;/p&gt; &lt;p&gt;The issue with QwQs (32B from Qwen and mine 35B) re: Templates/Jinja templates has been fixed. Make sure you update to build 0.3.12 ; otherwise manually select CHATML template to work with the new QwQ models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T03:46:25+00:00</published>
  </entry>
</feed>
