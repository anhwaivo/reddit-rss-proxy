<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-27T11:05:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k8ivb5</id>
    <title>Split MoE GGUFs for modular quants?</title>
    <updated>2025-04-26T17:52:15+00:00</updated>
    <author>
      <name>/u/Aerikh</name>
      <uri>https://old.reddit.com/user/Aerikh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the optimizations happening around MoE models such as in Ktransformers and Llama.cpp with custom layer offloading overrides, I was thinking that it would be nice if there were GGUFs where the static parts of the model (the layers that are active every token, which for Llama 4 would be the dense layers and the 1 &amp;quot;shared&amp;quot; expert) are stored in a different file from the non-static parts (the routed experts). This would allow a user to mix and match to optimize for their hardware. Someone with a 12 GB GPU and 96 GB RAM for instance would be able to get a big quant of the static layers, while someone else with a 8 GB GPU but the same RAM could choose a smaller quant of the static, but still get the benefit of the big quant for the non-static layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aerikh"&gt; /u/Aerikh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ivb5/split_moe_ggufs_for_modular_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T17:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8601g</id>
    <title>Qwen AI - My most used LLM!</title>
    <updated>2025-04-26T05:56:59+00:00</updated>
    <author>
      <name>/u/Glittering-Cancel-25</name>
      <uri>https://old.reddit.com/user/Glittering-Cancel-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Qwen, DeepSeek, paid ChatGPT, and paid Claude. I must say, i find myself using Qwen the most often. It's great, especially for a free model! &lt;/p&gt; &lt;p&gt;I use all of the LLMs for general and professional work. E.g., writing, planning, management, self-help, idea generation, etc. For most of those things, i just find that Qwen produces the best results and requires the least rework, follow ups, etc. I've tested all of the LLMs by putting in the exact same prompt (i've probably done this a couple dozen times) and overall (but not always), Qwen produces the best result for me. I absolutely can't wait until they release Qwen3 Max! I also have a feeling DeepSeek is gonna go with with R2... &lt;/p&gt; &lt;p&gt;Id love to know what LLM you find yourself using the most, what you use them for (that makes a big difference), and why you think that one is the best. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Cancel-25"&gt; /u/Glittering-Cancel-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8601g/qwen_ai_my_most_used_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T05:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8jymm</id>
    <title>End-to-end conversation projects? Dia, Sesame, etc</title>
    <updated>2025-04-26T18:39:34+00:00</updated>
    <author>
      <name>/u/Kep0a</name>
      <uri>https://old.reddit.com/user/Kep0a</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past month we've had some pretty amazing voice models. After talking with the Sesame demo, I'm wondering, has anyone made an easy streaming end-to-end, conversation project yet? I want to run these but combining things seamlessly is outside my skillset. I need my 'Her' moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kep0a"&gt; /u/Kep0a &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8jymm/endtoend_conversation_projects_dia_sesame_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T18:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8aput</id>
    <title>LangoTango - A local language model powered language learning partner</title>
    <updated>2025-04-26T11:23:55+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"&gt; &lt;img alt="LangoTango - A local language model powered language learning partner" src="https://b.thumbs.redditmedia.com/lyZDij5G9j7z2Yk-cHHcn18tC7IFPbWsmBtOUr5HdYs.jpg" title="LangoTango - A local language model powered language learning partner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Put this together over the week. It's a fork of another app I made called Dillon, but in this case I optimised it for language learning. It can be forked for all sorts of different hobbies. You could make a fork for personal recipe books or exercise diaries for example.&lt;/p&gt; &lt;p&gt;Here's the repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/shokuninstudio/LangoTango"&gt;https://github.com/shokuninstudio/LangoTango&lt;/a&gt;&lt;/p&gt; &lt;p&gt;macOS and Windows binaries are ready to download.&lt;/p&gt; &lt;p&gt;If you want to build it for Linux it's easy with pyinstaller and should work. I have not been able to test on Linux as I only have VMs at the moment. I need some drivers (not available) to run Linux native on my laptop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k8aput"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8aput/langotango_a_local_language_model_powered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T11:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8dqa7</id>
    <title>It's really cool now to have an idea, and few hours later you have a working app</title>
    <updated>2025-04-26T14:05:42+00:00</updated>
    <author>
      <name>/u/Nyao</name>
      <uri>https://old.reddit.com/user/Nyao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"&gt; &lt;img alt="It's really cool now to have an idea, and few hours later you have a working app" src="https://external-preview.redd.it/MzQ1cGE3N2FxNnhlMe_3z2US61k9w-e99dI3sh4KfvfwKeGQ6lAD-f6G97nN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d39f3e4229afc94ed424a14e9084252b056a258" title="It's really cool now to have an idea, and few hours later you have a working app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I rarely do web development, and without the help of LLMs it would have taken me days to build the frontend and these animations. But after one morning, I already have a cool result.&lt;/p&gt; &lt;p&gt;The idea and the app themselves aren't very original or complex, but here's the source code in case anyone is interested: &lt;a href="https://github.com/YofarDev/chapitre"&gt;https://github.com/YofarDev/chapitre&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nyao"&gt; /u/Nyao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vgz7967aq6xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8dqa7/its_really_cool_now_to_have_an_idea_and_few_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T14:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8wvop</id>
    <title>Runtime Identity Drift in LLMs — Can We Stabilize Without Memory?</title>
    <updated>2025-04-27T05:47:32+00:00</updated>
    <author>
      <name>/u/Robin898989</name>
      <uri>https://old.reddit.com/user/Robin898989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on stabilizing role identity in LLM outputs over long interactions — without relying on memory, logs, or retraining.&lt;/p&gt; &lt;p&gt;Problem: Most multi-agent chains and LLM workflows suffer from role drift and behavioral collapse after a few hundred turns. Context windowing and prompt engineering only delay the inevitable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/2jd1j8kecbxe1.gif"&gt;https://i.redd.it/2jd1j8kecbxe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experiment: I built a runtime coherence layer (called SAGE) that maintains behavioral identity using real-time feedback signals (Cr, ∆Cr, RTR) — without storing past interactions.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wp5z7ysfcbxe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22e7eb38d9d9bd0fe0cfe5a344d95656596c7d5f"&gt;https://preview.redd.it/wp5z7ysfcbxe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22e7eb38d9d9bd0fe0cfe5a344d95656596c7d5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Actually now, I feel a bit like the early creators of LoRA — trying to push an idea that doesn’t yet have “official” academic traction.&lt;/p&gt; &lt;p&gt;I’ve also recorded a couple of &lt;strong&gt;live test runs&lt;/strong&gt; (posted on YouTube) where you can see the behavior under drift pressure — happy to share links if you’re curious.&lt;/p&gt; &lt;p&gt;P.S: I am currently seeking &lt;strong&gt;academic validation&lt;/strong&gt; of the runtime model through collaboration with university research labs.&lt;/p&gt; &lt;p&gt;If any research teams, lab members, or independent researchers are interested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I can provide a &lt;strong&gt;secure demo version&lt;/strong&gt; of the system for evaluation purposes.&lt;/li&gt; &lt;li&gt;In exchange, I would request a &lt;strong&gt;brief written technical assessment&lt;/strong&gt; (positive or critical) from the lab or research group.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can drop links to videos, reports, and demos in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robin898989"&gt; /u/Robin898989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8wvop/runtime_identity_drift_in_llms_can_we_stabilize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8wvop/runtime_identity_drift_in_llms_can_we_stabilize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8wvop/runtime_identity_drift_in_llms_can_we_stabilize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T05:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8tqm3</id>
    <title>Trying to understand chunked prefill scheduling policy for vLLM</title>
    <updated>2025-04-27T02:35:59+00:00</updated>
    <author>
      <name>/u/lechatonnoir</name>
      <uri>https://old.reddit.com/user/lechatonnoir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've already perused &lt;a href="https://docs.vllm.ai/en/latest/performance/optimization.html"&gt;https://docs.vllm.ai/en/latest/performance/optimization.html&lt;/a&gt; and I believe I understand the basic concepts of what prefill and decoding are, plus the general concept of pipelining inference and dynamic batching.&lt;/p&gt; &lt;p&gt;Nevertheless, I have the following questions: - Suppose that my prefills are usually small, say 256 tokens. What does it mean for me to set a max num_batched_tokens as high as 4096? Will the scheduler wait for 16 prefills to be scheduled, and then compute them all at once? &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;As I understand it the output of a prefill operation is the KV cache for the tokens in the prefill, so consider what happens after those prefills are computed, and suppose you don't have enough memory to hold 16 KV caches at once for the whole decode operation. Since for every prefill operation you also need to do a decode operation, and the decode operations may take way more space, don't we have to evacuate the prefilled operations? If so, what was the point of computing them? If we can evacuate them to something like CPU memory, then does that really save any time at all (since as I understand it, inference is typically bound by I/O between the GPU memory bus and the compute cores, let alone the presumably much longer I/O time between the CPU and GPU)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If my output sequences are on the order of thousands of tokens (as they would be for a reasoning model), will the difference in performance due to the changed scheduling policy then be effectively negligible? Is there any situation in which it is actually worse (e.g due to movement of memory)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Finally, and a bit unrelatedly, suppose that I want to run inference on ten copies of the same prompt. So, I can benefit from the fact that all ten prefills are the same, but from there there will not be any benefits to the runtime of the decode stage, right? (Also, how do I benefit from the fact that all ten prefills are the same with vLLM?)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lechatonnoir"&gt; /u/lechatonnoir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8tqm3/trying_to_understand_chunked_prefill_scheduling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8tqm3/trying_to_understand_chunked_prefill_scheduling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8tqm3/trying_to_understand_chunked_prefill_scheduling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T02:35:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8kh94</id>
    <title>[Open Source] QA for cursor - Make sure it only gives you correct code.</title>
    <updated>2025-04-26T19:02:30+00:00</updated>
    <author>
      <name>/u/Cheap_Concert168no</name>
      <uri>https://old.reddit.com/user/Cheap_Concert168no</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"&gt; &lt;img alt="[Open Source] QA for cursor - Make sure it only gives you correct code." src="https://external-preview.redd.it/dWp1NWc2em42OHhlMUtddb3oYk3YHCqLnAUEgID_4mfN2VhQqz8ywXIoHXVz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b47ad65b6d6631fd64a9f0fc219c95cbec0702f" title="[Open Source] QA for cursor - Make sure it only gives you correct code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a MCP server that allows cursor(,etc) to test out the code before delivering it to you. If test fails it gets the exact logical error/console errors/screenshots directly resulting in a feedback loop until it gets it right. This makes the agent get as close to your requirements as possible before delivering it to you. Particularly, improving the coding experience with smaller/open coding models&lt;/p&gt; &lt;p&gt;It also tests in regression (test old features) so that new developments don't break working features which is a very common problem with these agents. It also has a mode to discover new test flows just by crawling a website, but that is trash for now. &lt;/p&gt; &lt;p&gt;You can use any LLM for this but I am using free gemini-2.0-flash and it works like a charm. It works a looot faster on gemini-2.0-flash-lite but I am happy to trade off time for accuracy (demo is sped up, check github for full length demo). A testing integration is inevitable for cursor/windsurf so until then I will keep working on this. Any feedback is welcome :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ilikepizza2/QA-MCP"&gt;QA-MCP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Concert168no"&gt; /u/Cheap_Concert168no &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1p5tt1zn68xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8kh94/open_source_qa_for_cursor_make_sure_it_only_gives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T19:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8f38v</id>
    <title>Dia-1.6B in Jax to generate audio from text from any machine</title>
    <updated>2025-04-26T15:07:47+00:00</updated>
    <author>
      <name>/u/Due-Yoghurt2093</name>
      <uri>https://old.reddit.com/user/Due-Yoghurt2093</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"&gt; &lt;img alt="Dia-1.6B in Jax to generate audio from text from any machine" src="https://external-preview.redd.it/OUXDxyqECR3Zwafh-pl-0Sio26N3Ne4UW5G-7VkXL1o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=868d202201bf917cbb597d31bb847a1ff2f6b446" title="Dia-1.6B in Jax to generate audio from text from any machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a JAX port of Dia, the 1.6B parameter text-to-speech model to generate voice from any machine, and would love to get any feedback. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due-Yoghurt2093"&gt; /u/Due-Yoghurt2093 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jaco-bro/diajax"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8f38v/dia16b_in_jax_to_generate_audio_from_text_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T15:07:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k916b3</id>
    <title>What UI is he using? Looks like ComfyUI but for text?</title>
    <updated>2025-04-27T10:49:38+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt; &lt;img alt="What UI is he using? Looks like ComfyUI but for text?" src="https://b.thumbs.redditmedia.com/cjo_Dt8ykCBx-xq5FTCMC7_jS5yL02p60je3K7j8ovo.jpg" title="What UI is he using? Looks like ComfyUI but for text?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if it's not just a mockup workflow. Found that on someone's page where he offers LLM services such as building AI agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m9k29nnpxcxe1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2fa0c0ea31d622e87a29141d91b6b51f03593c4c"&gt;https://preview.redd.it/m9k29nnpxcxe1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2fa0c0ea31d622e87a29141d91b6b51f03593c4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if it doesn't exist as an UI, it should.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T10:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8qaov</id>
    <title>Jamba support for llamacpp in the works!!</title>
    <updated>2025-04-26T23:34:17+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8qaov/jamba_support_for_llamacpp_in_the_works/"&gt; &lt;img alt="Jamba support for llamacpp in the works!!" src="https://preview.redd.it/v5yruwael9xe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2b01a9cc554ffe0cbc60264038440f8eaa242f8" title="Jamba support for llamacpp in the works!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v5yruwael9xe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8qaov/jamba_support_for_llamacpp_in_the_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8qaov/jamba_support_for_llamacpp_in_the_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T23:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k903ov</id>
    <title>Llama.cpp CUDA Setup - Running into Issues - Is it Worth the Effort?</title>
    <updated>2025-04-27T09:34:27+00:00</updated>
    <author>
      <name>/u/Brandu33</name>
      <uri>https://old.reddit.com/user/Brandu33</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm exploring alternatives to Ollama and have been reading good things about Llama.cpp. I'm trying to get it set up on Ubuntu 22.04 with driver version 550.120 and CUDA 12.4 installed.&lt;/p&gt; &lt;p&gt;I've cloned the repo and tried running:&lt;/p&gt; &lt;p&gt;cmake -B build -DGGML_CUDA=ON&lt;/p&gt; &lt;p&gt;However, CMake is unable to find the CUDA toolkit, even though it's installed and `nvcc` and `nvidia-smi` are working correctly. I've found a lot of potential solutions online, but the complexity seems high.&lt;/p&gt; &lt;p&gt;For those who have successfully set up Llama.cpp with CUDA, is it *significantly* better than alternatives like Ollama to justify the setup hassle? Is the performance gain substantial?&lt;/p&gt; &lt;p&gt;Any straightforward advice or pointers would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brandu33"&gt; /u/Brandu33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k903ov/llamacpp_cuda_setup_running_into_issues_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k903ov/llamacpp_cuda_setup_running_into_issues_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k903ov/llamacpp_cuda_setup_running_into_issues_is_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T09:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8sycx</id>
    <title>I built a Chrome Extension (WebAI) to Chat with Webpages Using Your Local LLMs</title>
    <updated>2025-04-27T01:52:55+00:00</updated>
    <author>
      <name>/u/solidavocadorock</name>
      <uri>https://old.reddit.com/user/solidavocadorock</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8sycx/i_built_a_chrome_extension_webai_to_chat_with/"&gt; &lt;img alt="I built a Chrome Extension (WebAI) to Chat with Webpages Using Your Local LLMs" src="https://external-preview.redd.it/GzsnSX_znc2F3IG5HYUyhLg93Pt10YnH5n0RlHy-BHs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c80daa19b9a99c501def3a67c7c50ba1e7d0d810" title="I built a Chrome Extension (WebAI) to Chat with Webpages Using Your Local LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; folks!&lt;/p&gt; &lt;p&gt;I wanted to share a Chrome extension I've been working on called &lt;strong&gt;WebAI&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The idea is simple: browse to any webpage, pop open the extension, and you can get an AI-powered summary or start asking questions about the content, or listen spoken answer, all using &lt;strong&gt;your own local LLM&lt;/strong&gt; (like Ollama) and local Kokoro voice generation.&lt;/p&gt; &lt;p&gt;Demo (watch with audio):&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k8sycx/video/juzws2qp9axe1/player"&gt;https://reddit.com/link/1k8sycx/video/juzws2qp9axe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's what it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Summarize &amp;amp; Chat:&lt;/strong&gt; Quickly understand articles or documentation, then dive deeper by asking questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local:&lt;/strong&gt; Connects directly to your self-hosted LLM (Ollama API compatible) and TTS services. No data goes to external clouds unless you configure it that way. Your prompts and page content stay between your browser and your local services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Selection:&lt;/strong&gt; Choose which of your downloaded Ollama models you want to use for the chat.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local TTS:&lt;/strong&gt; Has an option to read answers aloud using a local TTS engine (compatible with the OpenAI TTS API format, like piper via kokoro-fastapi).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation History:&lt;/strong&gt; Remembers your chat for each specific webpage URL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's designed for those of us who love tinkering with local models and want practical ways to use them daily. Since it relies on your local setup, &lt;em&gt;you&lt;/em&gt; control the models, the data, and the privacy (&lt;a href="https://github.com/miolini/webai/blob/main/PRIVACY.md"&gt;Privacy Policy&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to get started:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You'll need your local LLM service running (like Ollama) and optionally a local TTS service. The &lt;a href="https://github.com/miolini/webai/blob/main/README.md"&gt;README&lt;/a&gt; has Docker examples to get these running quickly.&lt;/li&gt; &lt;li&gt;Grab the code from GitHub: [&lt;a href="https://github.com/miolini/webai%5D(vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)"&gt;https://github.com/miolini/webai](vscode-file://vscode-app/Applications/Visual%20Studio%20Code.app/Contents/Resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Load it as an unpacked extension in Chrome/Chromium (&lt;code&gt;chrome://extensions/&lt;/code&gt; -&amp;gt; Developer Mode -&amp;gt; Load unpacked).&lt;/li&gt; &lt;li&gt;Configure the endpoints for your LLM/TTS services in the extension options.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Call for Feedback!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is still evolving, and I'd absolutely love it if you could give it a try and let me know what you think!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does it work with your setup?&lt;/li&gt; &lt;li&gt;Are there any features you'd like to see?&lt;/li&gt; &lt;li&gt;Did you run into any bugs?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can drop feedback here in the comments or open an issue on GitHub.&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solidavocadorock"&gt; /u/solidavocadorock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8sycx/i_built_a_chrome_extension_webai_to_chat_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8sycx/i_built_a_chrome_extension_webai_to_chat_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8sycx/i_built_a_chrome_extension_webai_to_chat_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T01:52:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8fe14</id>
    <title>Hot Take: Gemini 2.5 Pro Makes Too Many Assumptions About Your Code</title>
    <updated>2025-04-26T15:21:19+00:00</updated>
    <author>
      <name>/u/HideLord</name>
      <uri>https://old.reddit.com/user/HideLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemini 2.5 Pro is probably the smartest model that is publicly available at the moment. But it makes TOO fucking many assumptions about your code that often outright break functionality. Not only that, but it's overly verbose and boilerplate-y. Google really needs to tone it down. &lt;/p&gt; &lt;p&gt;I'll give an example: I had a function which extracts a score from a given string. The correct format is 1-10/10. Gemini randomly decides that this is a bug and modifies the regex to also accept 0/10. &lt;/p&gt; &lt;p&gt;The query was to use the result from the function to calculate the MSE. Nowhere did I specify it to modify the get_score function. Sonnet/DeepSeek do not have that issue by the way. &lt;/p&gt; &lt;p&gt;Thanks for coming to my TED talk. I just needed to vent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HideLord"&gt; /u/HideLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8fe14/hot_take_gemini_25_pro_makes_too_many_assumptions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T15:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8n0de</id>
    <title>NotebookLM-Style Dia – Imperfect but Getting Close</title>
    <updated>2025-04-26T20:56:50+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"&gt; &lt;img alt="NotebookLM-Style Dia – Imperfect but Getting Close" src="https://external-preview.redd.it/eTJ3bWZsbWJxOHhlMdo2OU2R9yA1Xpyg2FE52Rzb3D8ISdcjzfkEj4iTIyws.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04dc1bdee3bd7ada23838e805242ef81ebdd3f08" title="NotebookLM-Style Dia – Imperfect but Getting Close" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/dia"&gt;https://github.com/PasiKoodaa/dia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is not yet stable enough to produce 100% perfect results, and this app is also far from flawless. It’s often unclear whether generation failures are due to limitations in the model, issues in the app's code, or incorrect app settings. For instance, there are occasional instances where the last word of a speaker's output might be missing. But it's getting closer to NoteBookLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ak3jq9mbq8xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T20:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8hob9</id>
    <title>My AI dev prompt playbook that actually works (saves me 10+ hrs/week)</title>
    <updated>2025-04-26T17:00:42+00:00</updated>
    <author>
      <name>/u/namanyayg</name>
      <uri>https://old.reddit.com/user/namanyayg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been using AI tools to speed up my dev workflow for about 2 years now, and I've finally got a system that doesn't suck. Thought I'd share my prompt playbook since it's helped me ship way faster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix the root cause&lt;/strong&gt;: when debugging, AI usually tries to patch the end result instead of understanding the root cause. Use this prompt for that case:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyze this error: [bug details] Don't just fix the immediate issue. Identify the underlying root cause by: - Examining potential architectural problems - Considering edge cases - Suggesting a comprehensive solution that prevents similar issues &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ask for explanations:&lt;/strong&gt; Here's another one that's saved my ass repeatedly - the &amp;quot;explain what you just generated&amp;quot; prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Can you explain what you generated in detail: 1. What is the purpose of this section? 2. How does it work step-by-step? 3. What alternatives did you consider and why did you choose this one? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Forcing myself to understand ALL code before implementation has eliminated so many headaches down the road.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My personal favorite:&lt;/strong&gt; what I call the &amp;quot;rage prompt&amp;quot; (I usually have more swear words lol):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This code is DRIVING ME CRAZY. It should be doing [expected] but instead it's [actual]. PLEASE help me figure out what's wrong with it: [code] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works way better than it should! Sometimes being direct cuts through the BS and gets you answers faster.&lt;/p&gt; &lt;p&gt;The main thing I've learned is that AI is like any other tool - it's all about HOW you use it.&lt;/p&gt; &lt;p&gt;Good prompts = good results. Bad prompts = garbage.&lt;/p&gt; &lt;p&gt;What prompts have y'all found useful? I'm always looking to improve my workflow.&lt;/p&gt; &lt;p&gt;EDIT: This is blowing up! I added some more details + included some more prompts on my blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://nmn.gl/blog/ai-prompt-engineering"&gt;https://nmn.gl/blog/ai-prompt-engineering&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/namanyayg"&gt; /u/namanyayg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T17:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xu4d</id>
    <title>🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)</title>
    <updated>2025-04-27T06:53:42+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"&gt; &lt;img alt="🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)" src="https://external-preview.redd.it/fEQ0sRkP5Cc9pEvs5-UwG3ZTsuSDMCRcakJgt0TeA4k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=984dc75cd6fc1d4afcf245873710ed59ecf086db" title="🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;After a lot of work, I'm excited to share a &lt;strong&gt;prebuilt CUDA 12.8 wheel&lt;/strong&gt; for &lt;strong&gt;llama-cpp-python (version 0.3.8)&lt;/strong&gt; — built specifically for &lt;strong&gt;Windows 10/11 (x64)&lt;/strong&gt; systems!&lt;/p&gt; &lt;h1&gt;✅ Highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CUDA 12.8 GPU acceleration&lt;/strong&gt; fully enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Gemma 3 model support&lt;/strong&gt; (1B, 4B, 12B, 27B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built against llama.cpp b5192&lt;/strong&gt; (April 26, 2025)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tested and verified&lt;/strong&gt; on a dual-GPU setup (3090 + 4060 Ti)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working production inference&lt;/strong&gt; at &lt;strong&gt;16k context length&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No manual compilation&lt;/strong&gt; needed — just &lt;code&gt;pip install&lt;/code&gt; and you're running!&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔥 Why This Matters&lt;/h1&gt; &lt;p&gt;Building &lt;code&gt;llama-cpp-python&lt;/code&gt; with CUDA on Windows is notoriously painful —&lt;br /&gt; CMake configs, Visual Studio toolchains, CUDA paths... it’s a nightmare.&lt;/p&gt; &lt;p&gt;This wheel &lt;strong&gt;eliminates all of that&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No CMake.&lt;/li&gt; &lt;li&gt;No Visual Studio setup.&lt;/li&gt; &lt;li&gt;No manual CUDA environment tuning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Just download the&lt;/strong&gt; &lt;code&gt;.whl&lt;/code&gt;&lt;strong&gt;, install with pip, and you're ready to run Gemma 3 models on GPU immediately.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;✨ Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I haven't been able to find &lt;strong&gt;any other prebuilt llama-cpp-python wheel&lt;/strong&gt; supporting &lt;strong&gt;Gemma 3 + CUDA 12.8&lt;/strong&gt; on Windows — so I thought I'd post this ASAP.&lt;/li&gt; &lt;li&gt;I know you Linux folks are way ahead of me — but hey, now Windows users can play too! 😄&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/boneylizard/llama-cpp-python-cu128-gemma3/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T06:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8ncco</id>
    <title>Introducing Kimi Audio 7B, a SOTA audio foundation model</title>
    <updated>2025-04-26T21:11:34+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"&gt; &lt;img alt="Introducing Kimi Audio 7B, a SOTA audio foundation model" src="https://external-preview.redd.it/p4q_Zkpix3p8TiVMmz6bqei1OGSeQFuMhiONWJiDPGQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db35b2daa9cfe12cd1fa69d51172fee3172edc92" title="Introducing Kimi Audio 7B, a SOTA audio foundation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on Qwen 2.5 btw&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Audio-7B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T21:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8t8z9</id>
    <title>New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)</title>
    <updated>2025-04-27T02:08:41+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"&gt; &lt;img alt="New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)" src="https://external-preview.redd.it/6doJTf1GdAI5T-9SwoTSuPFBmq9PsTM3q-ChSWlGb_o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cd45bd18ce212c176a456f3dc0390fe542b9c06" title="New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(disclaimer, it's just a qwen2.5 32b fine tune)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/OpenMath-Nemotron-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T02:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k90u7f</id>
    <title>I'm building "Gemini Coder" enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI</title>
    <updated>2025-04-27T10:26:37+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"&gt; &lt;img alt="I'm building &amp;quot;Gemini Coder&amp;quot; enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI" src="https://external-preview.redd.it/M29waGh5eDFzY3hlMQztlUbXx4HmVNCbcZJT8RQ5gRkadeJUcw9XZgxVzRZr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47ff30d434ebd8acbfcd91bb9ccddfa2df9bc357" title="I'm building &amp;quot;Gemini Coder&amp;quot; enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some web chats come with extended support with automatically set model, system instructions and temperature (AI Studio, OpenRouter Chat, Open WebUI) while integration with others (ChatGPT, Claude, Gemini, Mistral, etc.) is limited to just initializations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder"&gt;https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tool is 100% free and open source (MIT licensed).&lt;br /&gt; I hope it will be received by the community as a helpful resource supporting everyday coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n2iwzxx1scxe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T10:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xb3k</id>
    <title>Overwhelmed by the number of Gemma 3 27B QAT variants</title>
    <updated>2025-04-27T06:16:58+00:00</updated>
    <author>
      <name>/u/iwinux</name>
      <uri>https://old.reddit.com/user/iwinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the Q4 quantization alone, I found 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;google/gemma-3-27b-it-qat-q4_0-gguf&lt;/code&gt;, official release, 17.2GB, seems to have some token-related issues according to this &lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3"&gt;discussion&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/code&gt;, requantized, 15.6GB, states to fix the issues mentioned above.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;jaxchang/google-gemma-3-27b-it-qat-q4_0-gguf-fix&lt;/code&gt;, further derived from stduhpf's variant, 15.6GB, states to fix some more issues?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even more variants that are derived from &lt;code&gt;google/gemma-3-27b-it-qat-q4_0-unquantized&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/code&gt; offers llama.cpp-specific quantizations from Q2 to Q8.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;unsloth/gemma-3-27b-it-qat-GGUF&lt;/code&gt; also offers Q2 to Q8 quantizations, and I can't figure what they have changed because the model description looks like copy-pasta.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How am I supposed to know which one to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iwinux"&gt; /u/iwinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T06:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8yrem</id>
    <title>Made Mistral 24B code like a senior dev by making it recursively argue with itself</title>
    <updated>2025-04-27T07:59:40+00:00</updated>
    <author>
      <name>/u/HearMeOut-13</name>
      <uri>https://old.reddit.com/user/HearMeOut-13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"&gt; &lt;img alt="Made Mistral 24B code like a senior dev by making it recursively argue with itself" src="https://b.thumbs.redditmedia.com/I1eroeMkhMj9dvaQstK2YCuhXrEczxhGjXs1EiERsCU.jpg" title="Made Mistral 24B code like a senior dev by making it recursively argue with itself" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with local models lately and built something that dramatically improves their output quality without fine-tuning or fancy prompting.&lt;/p&gt; &lt;p&gt;I call it CoRT (Chain of Recursive Thoughts). The idea is simple: make the model generate multiple responses, evaluate them, and iteratively improve. Like giving it the ability to second-guess itself. With Mistral 24B Tic-tac-toe game went from basic CLI(Non CoRT) to full OOP with AI opponent(CoRT)&lt;/p&gt; &lt;p&gt;What's interesting is that smaller models benefit even more from this approach. It's like giving them time to &amp;quot;think harder&amp;quot; actually works, but i also imagine itd be possible with some prompt tweaking to get it to heavily improve big ones too.&lt;/p&gt; &lt;p&gt;GitHub: [&lt;a href="https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts"&gt;https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Technical details: - Written in Python - Wayyyyy slower but way better output - Adjustable thinking rounds (1-5) + dynamic - Works with any OpenRouter-compatible model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HearMeOut-13"&gt; /u/HearMeOut-13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k8yrem"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:59:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xyvp</id>
    <title>Finally got ~10t/s DeepSeek V3-0324 hybrid (FP8+Q4_K_M) running locally on my RTX 4090 + Xeon with with 512GB RAM, KTransformers and 32K context</title>
    <updated>2025-04-27T07:02:36+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just wanted to share a fun project I have been working on. I managed to get DeepSeek V3-0324 onto my single RTX 4090 + Xeon box running 512 GB RAM using KTransformers and a clever FP8+GGUF hybrid trick from KTransformers. &lt;/p&gt; &lt;p&gt;Attention &amp;amp; FF layers on GPU (FP8): Cuts VRAM down to ~24 GB, so your 4090 can handle the critical parts lightning fast.&lt;/p&gt; &lt;p&gt;Expert weights on CPU (4-bit GGUF): All the huge MoE banks live in system RAM and load as needed. &lt;/p&gt; &lt;p&gt;End result: I’m seeing about ~10 tokens/sec with a 32K context window—pretty smooth for local tinkering.&lt;/p&gt; &lt;p&gt;KTransformers made it so easy with its Docker image. It handles the FP8 kernels under the hood and shuffles data between CPU/GPU token by token.&lt;/p&gt; &lt;p&gt;I posted a llama-4 maverick run on KTransformers a couple of days back and got good feedback on here. So I am sharing this build as well, in case it helps anyone out!&lt;/p&gt; &lt;p&gt;My Build:&lt;br /&gt; Motherboard: ASUS Pro WS W790E-SAGE SE. Why This Board? 8-channel DDR5 ECC RAM, I have 8x64 GB ECC DDR5 RAM 4800MHz&lt;br /&gt; CPU with AI &amp;amp; ML Boost: Engineering Sample QYFS (56C/112T!)&lt;br /&gt; I get consistently 9.5-10.5 tokens per second with this for decode. And I get 40-50 prefill speed.&lt;/p&gt; &lt;p&gt;If you would like to checkout the youtube video of the run: &lt;a href="https://www.youtube.com/watch?v=oLvkBZHU23Y"&gt;https://www.youtube.com/watch?v=oLvkBZHU23Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My Hardware Build and reasoning for picking up this board: &lt;a href="https://www.youtube.com/watch?v=r7gVGIwkZDc"&gt;https://www.youtube.com/watch?v=r7gVGIwkZDc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8yk8w</id>
    <title>TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324</title>
    <updated>2025-04-27T07:44:51+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"&gt; &lt;img alt="TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324" src="https://external-preview.redd.it/1No-ofpBVwLtrVVqwYujJ6PJAf7A4c3ZgbbSJrDaop0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fa5483c63f6faf71fe54d107797d498abbdd369" title="TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Today we release DeepSeek-R1T-Chimera, an open weights model adding R1 reasoning to &lt;a href="https://x.com/deepseek_ai"&gt;@deepseek_ai&lt;/a&gt; V3-0324 with a novel construction method. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In benchmarks, it appears to be as smart as R1 but much faster, using 40% fewer output tokens. &lt;/p&gt; &lt;p&gt;The Chimera is a child LLM, using V3s shared experts augmented with a custom merge of R1s and V3s routed experts. It is not a finetune or distillation, but constructed from neural network parts of both parent MoE models. &lt;/p&gt; &lt;p&gt;A bit surprisingly, we did not detect defects of the hybrid child model. Instead, its reasoning and thinking processes appear to be more compact and orderly than the sometimes very long and wandering thoughts of the R1 parent model. &lt;/p&gt; &lt;p&gt;Model weights are on &lt;a href="https://x.com/huggingface"&gt;@huggingface&lt;/a&gt;, just a little late for &lt;a href="https://x.com/hashtag/ICLR2025?src=hashtag_click"&gt;#ICLR2025&lt;/a&gt;. Kudos to &lt;a href="https://x.com/deepseek_ai"&gt;@deepseek_ai&lt;/a&gt; for V3 and R1!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/tngtech/status/1916284566127444468"&gt;https://x.com/tngtech/status/1916284566127444468&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/DeepSeek-R1T-Chimera"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8pbkz</id>
    <title>Rumors of DeepSeek R2 leaked!</title>
    <updated>2025-04-26T22:46:10+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"&gt; &lt;img alt="Rumors of DeepSeek R2 leaked!" src="https://external-preview.redd.it/p9U-cs9GtQZIPfmxhgS9u_IU8nn0dCQTBamMDocU7cQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=142d863bd878aeef678a295eb3f985d05fc875a7" title="Rumors of DeepSeek R2 leaked!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;—1.2T param, 78B active, hybrid MoE —97.3% cheaper than GPT 4o ($0.07/M in, $0.27/M out) —5.2PB training data. 89.7% on C-Eval2.0 —Better vision. 92.4% on COCO —82% utilization in Huawei Ascend 910B&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/deedydas/status/1916160465958539480?s=46"&gt;https://x.com/deedydas/status/1916160465958539480?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/deedydas/status/1916160465958539480?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T22:46:10+00:00</published>
  </entry>
</feed>
