<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T14:55:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mhl5yo</id>
    <title>Gemini 3 is coming?..</title>
    <updated>2025-08-04T18:15:40+00:00</updated>
    <author>
      <name>/u/SlerpE</name>
      <uri>https://old.reddit.com/user/SlerpE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt; &lt;img alt="Gemini 3 is coming?.." src="https://preview.redd.it/59joqndkn1hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e89c768daaac9653e4f2ad00c6a0ee5f6412107" title="Gemini 3 is coming?.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/OfficialLoganK/status/1952430214375493808"&gt;https://x.com/OfficialLoganK/status/1952430214375493808&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlerpE"&gt; /u/SlerpE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59joqndkn1hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhl5yo/gemini_3_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhrx3m</id>
    <title>How to use your Local Models to watch your screen. Open Source and Completely Free!!</title>
    <updated>2025-08-04T22:30:05+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt; &lt;img alt="How to use your Local Models to watch your screen. Open Source and Completely Free!!" src="https://external-preview.redd.it/czVxbDlzeWx3MmhmMTeXrb7fw6xx0BNL_5u8ms92EIrAoiEjzO7YOwhlTBj3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba0ed7244b693bf15ef5f04c6f2d7cddc4f49c67" title="How to use your Local Models to watch your screen. Open Source and Completely Free!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I built this &lt;strong&gt;open source&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; app that lets your local models &lt;strong&gt;watch your screen&lt;/strong&gt; and do stuff! It is now &lt;strong&gt;suuuper easy to install&lt;/strong&gt; and use, to make local AI &lt;strong&gt;accessible to&lt;/strong&gt; &lt;strong&gt;everybody&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;Hey r/LocalLLaMA! I'm back with some Observer updates c: first of all &lt;strong&gt;Thank You&lt;/strong&gt; so much for all of your support and feedback, i've been working hard to take this project to this current state. I added the app installation which is a significant QOL improvement for ease of use for first time users!! The docker-compose option is still supported and viable for people wanting a more specific and custom install.&lt;/p&gt; &lt;p&gt;The new app tools are a &lt;strong&gt;game-changer&lt;/strong&gt;!! You can now have direct system-level pop ups or notifications that come up right &lt;strong&gt;up to your face&lt;/strong&gt; hahaha. And sorry to everyone who tried out SMS and WhatsApp and were frustrated because you weren't getting notifications, Meta started blocking my account thinking i was just spamming messages to you guys.&lt;/p&gt; &lt;p&gt;But the pushover and discord notifications work perfectly well!&lt;/p&gt; &lt;p&gt;If you have any feedback please reach out through the discord, i'm really open to suggestions.&lt;/p&gt; &lt;p&gt;This is the projects &lt;a href="https://github.com/Roy3838/Observer"&gt;Github&lt;/a&gt; (completely open source)&lt;br /&gt; And the discord: &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have any questions i'll be hanging out here for a while!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g3pod2zlw2hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhrx3m/how_to_use_your_local_models_to_watch_your_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T22:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7pei</id>
    <title>DeepSeek R1 vs. V3 - Going Head-To-Head In AI Roleplay</title>
    <updated>2025-08-05T12:22:52+00:00</updated>
    <author>
      <name>/u/RPWithAI</name>
      <uri>https://old.reddit.com/user/RPWithAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7pei/deepseek_r1_vs_v3_going_headtohead_in_ai_roleplay/"&gt; &lt;img alt="DeepSeek R1 vs. V3 - Going Head-To-Head In AI Roleplay" src="https://external-preview.redd.it/078KX2JOJbgzKwAGsWu6xPvajl1VjjFOwNmdxWSzZUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=959c9afd9f6202323ed8c3a296d7f74c135ccc04" title="DeepSeek R1 vs. V3 - Going Head-To-Head In AI Roleplay" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When it comes to AI Roleplay, people have had both good and bad experiences with DeepSeek R1 and DeepSeek V3. We wanted to examine how DeepSeek R1 vs. V3 perform in roleplay when they go head-to-head against each other under different scenarios.&lt;/p&gt; &lt;p&gt;This little deep-dive will help you figure out which model will give you the experience you are looking for without wasting your time, request limits/tokens, or money.&lt;/p&gt; &lt;h2&gt;5 Different Characters, Several Themes, And Complete Conversation Logs&lt;/h2&gt; &lt;p&gt;We tested both the models with &lt;strong&gt;5 different characters&lt;/strong&gt;. We explored each scenario up to a satisfactory depth. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knight Araeth Ruene by Yoiiru (Themes: Medieval, Politics, Morality)&lt;/li&gt; &lt;li&gt;Harumi – Your Traitorous Daughter from Jgag2 (Themes: Drama, Angst, Battle)&lt;/li&gt; &lt;li&gt;Time Looping Friend Amara Schwartz by Sleep Deprived (Themes: Sci-fi, Psychological Drama)&lt;/li&gt; &lt;li&gt;You’re A Ghost! Irish by Calrston (Themes: Paranormal, Comedy)&lt;/li&gt; &lt;li&gt;Royal Mess, Astrid by KornyPony (Themes: Fantasy, Magic, Fluff)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Complete conversation logs for both models with each character is available for you to read through and understand how the models perform.&lt;/p&gt; &lt;h2&gt;In-Depth Observations, Character Creator’s Opinions, And Conclusions.&lt;/h2&gt; &lt;p&gt;We provide our in-depth observation along with the character creator's opinion on how the models portrayed their creation. If you want a TLDR, each scenario has a condensed conclusion!&lt;/p&gt; &lt;h2&gt;Read The Article&lt;/h2&gt; &lt;p&gt;You can read the article here: &lt;a href="https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/"&gt;&lt;strong&gt;DeepSeek R1 vs. V3 – Which Is Better For AI Roleplay?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h1&gt;The Final Conclusion&lt;/h1&gt; &lt;p&gt;Across our five head-to-head roleplay tests, neither model claims dominance. Each excels in its own area.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt; won three scenarios (Knight Araeth, Time-Looping Friend Amara, You’re a Ghost! Irish) by staying focused on character traits, providing deeper hypotheticals, and maintaining emotionally rich, dialogue-driven exchanges. Its strength is in consistent meta-reasoning and faithful, restrained portrayal, even if it sometimes feels heavy or needs more user guidance to push the action forward.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt; took the lead in two scenarios (Traitorous Daughter Harumi, Royal Mess Astrid) by adding expressive flourishes, dynamic actions, and cinematic details that made characters feel more alive. It performs well when you want vivid, action-oriented storytelling, although it can sometimes lead to chaos or cut emotional beats short.&lt;/p&gt; &lt;p&gt;If you crave in-depth conversation, logical consistency, and true-to-character dialogue, DeepSeek R1 is your go-to. If you prefer a more visual, emotionally expressive, and fast-paced narrative, DeepSeek V3 will serve you better. Both models bring unique strengths; your choice should match the roleplay style you want to create.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thank you for taking your time to check this out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RPWithAI"&gt; /u/RPWithAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rpwithai.com/deepseek-r1-vs-v3-for-roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7pei/deepseek_r1_vs_v3_going_headtohead_in_ai_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7pei/deepseek_r1_vs_v3_going_headtohead_in_ai_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:22:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi5s6w</id>
    <title>Mi50 32gb (Working config, weirdness and performance)</title>
    <updated>2025-08-05T10:43:56+00:00</updated>
    <author>
      <name>/u/Danternas</name>
      <uri>https://old.reddit.com/user/Danternas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thought I'd share some knowledge after a week with an Mi50 32gb bought from Ebay. Was originally supposed to be a response but hyper-focus took over and this is more suited as a post.&lt;/p&gt; &lt;p&gt;It arrived new-looking. Anti-static bag, not a spec of dust and plastic peel still on the AMD Instinct branded shroud. Mine came with an extra radial fan which can be mounted on the back and connected to a 12v header. Some tape was necessary to direct the air into the heat-sink. I was sceptical about the capability of this small radial fan but it seem to keep the GPU edge under 80C under heavy use, though I have not stress tested it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weirdness&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One weird thing is how it is listed in lspci:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;0a:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [1002:66a3]&lt;/p&gt; &lt;p&gt;Subsystem: Apple Inc. Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [106b:0201]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which suggests it is not an Mi50 at all? Or some weird Chinese shifting of components. Note the Apple subsystem. In rocm-smi it does boost over 1700mhz and pull near 300w, which is consistent with Mi50 specs. However, Mi50 seem to be a cut down Radeon Pro Vega II. So maybe it is a Radeon Pro Vega II put on a Mi50 board and flashed with Mi50 BIOS? Could it be flashed back to a Radeon Pro Vega II. I have no idea, even less why that would make any sense. Maybe I'm just overthinking it.&lt;/p&gt; &lt;p&gt;Another curious thing is that the card lacks a fan or even fan header but reports fan speed in rocm-smi.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Working configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I got it to work on the following configuration&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;GPU: AMD Instinct MI50 (32 GB, gfx906)&lt;/p&gt; &lt;p&gt;Proxmox: 8.4.6&lt;/p&gt; &lt;p&gt;Kernel: 6.8.12-4-pve (downgraded from 6.8.12-13-pve, though I am unsure if this mattered)&lt;/p&gt; &lt;p&gt;OS in the Proxmox host: Debian 12 (Bookworm) + Ubuntu 24.04 (&amp;quot;Noble&amp;quot;) repositories for ROCm&lt;/p&gt; &lt;p&gt;ROCm-version: 6.4.2&lt;/p&gt; &lt;p&gt;Driver: amdgpu-dkms installed after headers&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;My method was as stupid as it sounds. But it worked after hours if trial and error. Right now I am just happy it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"&gt;https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run the commands for ROCm Ubuntu 24.04, then AMDGPU driver commands for Ubuntu 24.04, and then the commands for ROCm Ubuntu 24.04 again. There's probably some way simpler way and maybe something else I did contributed. But right now I am happy it works without installing a 5.15 Ubuntu kernel and I can still use Proxmox.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pass-through not working, LXC working fine&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once it register in rocm-smi it was easy to use the OpenWebUI LXC community script to make an LXC container. Then I manually installed Ollama inside of it. I did not get it to work pass-through and I have not seen any example where this works. AMD also lists it as not compatible with pass-through. Use it bare metal. Make sure to give the LXC the resources /dev/kfd, /dev/dri/card0, and /dev/dri/renderD128 with the right GID.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Idle power draw is 25w according to rocm-smi, which seems accurate compared to measure usage from the wall and UPS. During benchmarking it reached 220-260w and 68c.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The card is in a server with a Ryzen 5 3600 and 64gb of ram, where the LXC container is limited to 8 cores and 8gb of ram. This seem to be overkill as basically all computation is done in the GPU and usage is under 20% of the 8 logical cores/4gb. The Mi50 boosts all the way to 1730mhz/&amp;gt;95% usage and remains there.&lt;/p&gt; &lt;p&gt;llm_benchmark:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;mistral:7b Median run average of eval rate: 63.754 tokens/s&lt;/p&gt; &lt;p&gt;llama3.1:8b Median run average of eval rate: 56.772 tokens/s&lt;/p&gt; &lt;p&gt;gemma2:9b Median run average of eval rate: 43.736 tokens/s&lt;/p&gt; &lt;p&gt;llava:7b Median run average of eval rate: 74.874 tokens/s&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It had a dip in performance on the 2nd run of 5 prompts and for some reason couldn't finish deepseek-r1:8b. Not sure why as I have been able to do deepseek-r1:32b just fine in OpenWebUI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VRAM is absolutely fantastic of course and the main reason to consider the Mi50 in my opinion. If not for the VRAM you may as well get an RTX 3060 12gb or similar from Nvidia to save you from some AMD driver headaches. 30b models doesn't seem to be any issues at all with vram to spare.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Mi50 right now gives you big GPU capability for a cheap price. In my opinion it is mainly for you who want the 32gb. I see less point in the 16gb, but it is even cheaper I suppose. Be aware though that AMD considers the Mi50 unsupported and depending on your use-case you may encounter a poor experience getting the drivers to work properly. Not to mention I don't think it works at all in Windows. It is not a card for someone who just want things to work, but it is cheap 32gb of HBM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danternas"&gt; /u/Danternas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi5s6w/mi50_32gb_working_config_weirdness_and_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T10:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbpmo</id>
    <title>New Qwen Models Today!!!</title>
    <updated>2025-08-04T12:12:00+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt; &lt;img alt="New Qwen Models Today!!!" src="https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639" title="New Qwen Models Today!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qemmgysvuzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi8bkb</id>
    <title>Try Chatgpt agent and GLM4.5 on slides making</title>
    <updated>2025-08-05T12:50:33+00:00</updated>
    <author>
      <name>/u/Apart-River475</name>
      <uri>https://old.reddit.com/user/Apart-River475</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8bkb/try_chatgpt_agent_and_glm45_on_slides_making/"&gt; &lt;img alt="Try Chatgpt agent and GLM4.5 on slides making" src="https://b.thumbs.redditmedia.com/QuG4JkTqiB_g2B50S0s9s8BF0LZYqPmq6SJWW46_WqE.jpg" title="Try Chatgpt agent and GLM4.5 on slides making" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to make poster for Yellowstone park using both Chatgpt agent(plus mode) and glm4.5(on z.ai), The prompt is like: &amp;quot;Make me a four-season poster of Yellowstone Park, four sheets, to attract everyone to visit.，A sophisticated design style&amp;quot;&lt;br /&gt; Chatgpt agent use the browser for 10 minutes and return the result &lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/q5piakwo57hf1.gif"&gt;https://i.redd.it/q5piakwo57hf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Glm 4.5 use two minutes and get me a better result :&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/i85pz9iv57hf1.gif"&gt;https://i.redd.it/i85pz9iv57hf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It seems like glm4.5 is better and faster !!! but seems like I can't directly use my own picture in ppt or poster at the time I test.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-River475"&gt; /u/Apart-River475 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8bkb/try_chatgpt_agent_and_glm45_on_slides_making/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8bkb/try_chatgpt_agent_and_glm45_on_slides_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8bkb/try_chatgpt_agent_and_glm45_on_slides_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhlkyx</id>
    <title>support for GLM 4.5 family of models has been merged into llama.cpp</title>
    <updated>2025-08-04T18:30:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt; &lt;img alt="support for GLM 4.5 family of models has been merged into llama.cpp" src="https://external-preview.redd.it/c5JLWNvDayy9hBNWlkTcKlG0BX-MgLkUBV-jJh9mTeo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e04ffa9cbd0a435f87d74eaf876a5853c1e06023" title="support for GLM 4.5 family of models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhlkyx/support_for_glm_45_family_of_models_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T18:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhe1rl</id>
    <title>r/LocalLLaMA right now</title>
    <updated>2025-08-04T13:52:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt; &lt;img alt="r/LocalLLaMA right now" src="https://preview.redd.it/f0xr7mshc0hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d701cbdf33b1ec6ea47dd4b4874202dbea647e" title="r/LocalLLaMA right now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0xr7mshc0hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T13:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1miaugk</id>
    <title>II-Search-4B: model tuned for reasoning with search tools</title>
    <updated>2025-08-05T14:33:02+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt; &lt;img alt="II-Search-4B: model tuned for reasoning with search tools" src="https://preview.redd.it/w6vtnupyo7hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=592d2a50ce20e6ddff73d1324a8bdda6dc148b14" title="II-Search-4B: model tuned for reasoning with search tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most search models need the cloud.&lt;/p&gt; &lt;p&gt;II-Search-4B doesn’t.&lt;/p&gt; &lt;p&gt;4B model tuned for reasoning with search tools, built for local use.&lt;/p&gt; &lt;p&gt;Performance of models 10x its size.&lt;/p&gt; &lt;p&gt;Search that is small, smart, and open.&lt;/p&gt; &lt;p&gt;II-Search-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;II-Search-CIR-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://ii.inc/web/blog/post/ii-search"&gt;https://ii.inc/web/blog/post/ii-search&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w6vtnupyo7hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mht910</id>
    <title>GLM 4.5 GGUFs are coming</title>
    <updated>2025-08-04T23:26:08+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt; &lt;img alt="GLM 4.5 GGUFs are coming" src="https://external-preview.redd.it/mPuzW0dQMIeYzrva9cFzmx9vYSbfW4-X3nbfzwnmTUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbaa78bb76999536a7337e9b0c9e2f578691b200" title="GLM 4.5 GGUFs are coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FINALLY&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/GLM-4.5-Air-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mht910/glm_45_ggufs_are_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T23:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1miagcn</id>
    <title>Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments.</title>
    <updated>2025-08-05T14:18:17+00:00</updated>
    <author>
      <name>/u/Loud_Possibility_148</name>
      <uri>https://old.reddit.com/user/Loud_Possibility_148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"&gt; &lt;img alt="Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments." src="https://preview.redd.it/ew3ww50cm7hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4453fb44558bfa1a595869b6a3ef9f9cdc4bc1de" title="Kaggle is considering further opening its evaluation platform to allow the community to add their own game environments." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loud_Possibility_148"&gt; /u/Loud_Possibility_148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew3ww50cm7hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miagcn/kaggle_is_considering_further_opening_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhctd</id>
    <title>🚀 Meet Qwen-Image</title>
    <updated>2025-08-04T15:58:11+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt; &lt;img alt="🚀 Meet Qwen-Image" src="https://preview.redd.it/7a463it8z0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a9bc46837ad4e1dac08bb6879d131c650ac6476" title="🚀 Meet Qwen-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Meet Qwen-Image — a 20B MMDiT model for next-gen text-to-image generation. Especially strong at creating stunning graphic posters with native text. Now open-source.&lt;/p&gt; &lt;p&gt;🔍 Key Highlights:&lt;/p&gt; &lt;p&gt;🔹 SOTA text rendering — rivals GPT-4o in English, best-in-class for Chinese&lt;/p&gt; &lt;p&gt;🔹 In-pixel text generation — no overlays, fully integrated&lt;/p&gt; &lt;p&gt;🔹 Bilingual support, diverse fonts, complex layouts&lt;/p&gt; &lt;p&gt;🎨 Also excels at general image generation — from photorealistic to anime, impressionist to minimalist. A true creative powerhouse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7a463it8z0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhctd/meet_qwenimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhgu6t</id>
    <title>Sam Altman watching Qwen drop model after model</title>
    <updated>2025-08-04T15:38:39+00:00</updated>
    <author>
      <name>/u/TheRealSerdra</name>
      <uri>https://old.reddit.com/user/TheRealSerdra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt; &lt;img alt="Sam Altman watching Qwen drop model after model" src="https://preview.redd.it/g7t8cmgrv0hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=460e01fe2091b6bc2347e41a918d258022a40353" title="Sam Altman watching Qwen drop model after model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealSerdra"&gt; /u/TheRealSerdra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g7t8cmgrv0hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhgu6t/sam_altman_watching_qwen_drop_model_after_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhiqqn</id>
    <title>Qwen-Image is out</title>
    <updated>2025-08-04T16:49:14+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt; &lt;img alt="Qwen-Image is out" src="https://external-preview.redd.it/ZXc5MXNwZzA4MWhmMcYwrNxnpQZAm2APaO7BYeGkDJuDLh9yjPxalcFVQ96q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2392cbc2ee2f68a83914b65971bd3a688fc24739" title="Qwen-Image is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1952398250121756992"&gt;https://x.com/Alibaba_Qwen/status/1952398250121756992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better than Flux Kontext, gpt-image level&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4077mfg081hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhiqqn/qwenimage_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhhdig</id>
    <title>QWEN-IMAGE is released!</title>
    <updated>2025-08-04T15:58:55+00:00</updated>
    <author>
      <name>/u/TheIncredibleHem</name>
      <uri>https://old.reddit.com/user/TheIncredibleHem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt; &lt;img alt="QWEN-IMAGE is released!" src="https://external-preview.redd.it/qvxd1_x1PaBd3IEw-2xS9ifjngcFBwLHvsX1ihQDi64.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a65aaca919e956009709dd069f70c0c907403912" title="QWEN-IMAGE is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's better than Flux Kontext Pro (according to their benchmarks). That's insane. Really looking forward to it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheIncredibleHem"&gt; /u/TheIncredibleHem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhhdig/qwenimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T15:58:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1vov</id>
    <title>Qwen-image now supported in ComfyUI</title>
    <updated>2025-08-05T06:37:16+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At last after wait of few hours, ComfyUI now has support for Qwen-Image. Its from their &lt;a href="https://github.com/comfyanonymous/ComfyUI/pull/9179"&gt;git repo&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1vov/qwenimage_now_supported_in_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi45h1</id>
    <title>Kitten TTS Web Demo</title>
    <updated>2025-08-05T09:04:22+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick web demo of the new &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt;. Loads the model up using transformers.js in the browser, running fully locally client-side: &lt;a href="https://clowerweb.github.io/kitten-tts-web-demo/"&gt;https://clowerweb.github.io/kitten-tts-web-demo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/clowerweb/kitten-tts-web-demo"&gt;https://github.com/clowerweb/kitten-tts-web-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Only uses CPU for now, but I'm going to add WebGPU support for it later today, plus maybe a Whisper implementation also in transformers.js for a nice little local STS pipeline, if anyone is interested in something like that.&lt;/p&gt; &lt;p&gt;I also have a little open-source chat interface in progress that I might plop the STS pipeline into here: &lt;a href="https://github.com/clowerweb/Simple-AI"&gt;https://github.com/clowerweb/Simple-AI&lt;/a&gt; (built with Nuxt 3 &amp;amp; Tailwind 4) -- supports chat tabs &amp;amp; history, markdown, code highlighting, and LaTeX, and also lets you run Qwen3 4B via transformers.js or add your own custom API endpoints, with settings for temperature, top_p, top_k, etc. Only supports OpenAI-compatible endpoints currently. You can add custom API providers (including your own llama.cpp servers and whatnot), custom models with their own settings, custom system prompts, etc. If you're interested in seeing an STS pipeline added to that though with Kitten &amp;amp; Whisper, lemme know what the interest levels are for something like that. I'll probably toss this project into Electron when it's ready and make it into a desktop app for Mac, Windows, and Linux as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi45h1/kitten_tts_web_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T09:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi1fdc</id>
    <title>DFLoat11 Quantization for Qwen-Image Drops – Run It on 17GB VRAM with CPU Offloading!</title>
    <updated>2025-08-05T06:09:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt; &lt;img alt="DFLoat11 Quantization for Qwen-Image Drops – Run It on 17GB VRAM with CPU Offloading!" src="https://preview.redd.it/sv779zmy65hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=935b56fc5932d42d92b2f9647262c1937a3e7446" title="DFLoat11 Quantization for Qwen-Image Drops – Run It on 17GB VRAM with CPU Offloading!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sv779zmy65hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi1fdc/dfloat11_quantization_for_qwenimage_drops_run_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T06:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0luy</id>
    <title>generated using Qwen</title>
    <updated>2025-08-05T05:20:23+00:00</updated>
    <author>
      <name>/u/Vision--SuperAI</name>
      <uri>https://old.reddit.com/user/Vision--SuperAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt; &lt;img alt="generated using Qwen" src="https://b.thumbs.redditmedia.com/xCXpPKDOF28RU5xW4gyR6Ubwr1ZXuMCJDS0Yp62MVtI.jpg" title="generated using Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vision--SuperAI"&gt; /u/Vision--SuperAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mi0luy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0luy/generated_using_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6brm</id>
    <title>Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices.</title>
    <updated>2025-08-05T11:13:52+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt; &lt;img alt="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." src="https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02f020e4a288dbe74ee37d8070bfab6a8b6d543" title="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.&lt;/p&gt; &lt;p&gt;Github Link: &lt;a href="https://github.com/OHF-Voice/piper1-gpl"&gt;https://github.com/OHF-Voice/piper1-gpl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4f9mf37ap6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6bkf</id>
    <title>The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!</title>
    <updated>2025-08-05T11:13:33+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt; &lt;img alt="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" src="https://preview.redd.it/h2p8ceo4p6hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ddb7dfeb9a7a66e33196efe1d60a4acb157f2a" title="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h2p8ceo4p6hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi8lbl</id>
    <title>Qwen3 Coder vs. Kimi K2 vs. Sonnet 4 Coding Comparison (Tested on Qwen CLI)</title>
    <updated>2025-08-05T13:02:08+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba released Qwen3‑Coder (480B → 35B active) alongside Qwen Code CLI, a complete fork of Gemini CLI for agentic coding workflows specifically adapted for Qwen3 Coder. I tested it head-to-head with Kimi K2 and Claude Sonnet 4 in practical coding tasks using the same CLI via &lt;strong&gt;OpenRouter&lt;/strong&gt; to keep things consistent for all models. The results surprised me.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ℹ️ &lt;strong&gt;Note:&lt;/strong&gt; All test timings are based on the OpenRouter providers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've done some real-world coding tests for all three, not just regular prompts. Here are the three questions I asked all three models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLI Chat MCP Client in Python:&lt;/strong&gt; Build a CLI chat MCP client in Python. More like a chat room. Integrate Composio integration for tool calls (Gmail, Slack, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geometry Dash WebApp Simulation:&lt;/strong&gt; Build a web version of Geometry Dash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Typing Test WebApp:&lt;/strong&gt; Build a monkeytype-like typing test app with a theme switcher (Catppuccin theme) and animations (typing trail).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4 was the most reliable across all tasks, with complete, production-ready outputs. It was also the fastest, usually taking 5–7 minutes.&lt;/li&gt; &lt;li&gt;Qwen3-Coder surprised me with solid results, much faster than Kimi, though not quite on Claude’s level.&lt;/li&gt; &lt;li&gt;Kimi K2 writes good UI and follows standards well, but it is slow (20+ minutes on some tasks) and sometimes non-functional.&lt;/li&gt; &lt;li&gt;On tool-heavy prompts like MCP + Composio, Claude was the only one to get it right in one try.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Verdict&lt;/h1&gt; &lt;p&gt;Honestly, Qwen3-Coder feels like the best middle ground if you want budget-friendly coding without massive compromises. But for real coding speed, Claude still dominates all these recent models.&lt;/p&gt; &lt;p&gt;I can't see much hype around Kimi K2, to be honest. It's just painfully slow and not really as great as they say it is in coding. It's mid! (Keep in mind, timings are noted based on the OpenRouter providers.)&lt;/p&gt; &lt;p&gt;Here's a complete blog post with timings for all the tasks for each model and a nice demo here: &lt;a href="https://composio.dev/blog/qwen-3-coder-vs-kimi-k2-vs-claude-4-sonnet-coding-comparison"&gt;Qwen 3 Coder vs. Kimi K2 vs. Claude 4 Sonnet: Coding comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has benchmarked these models with real coding projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T13:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0co2</id>
    <title>Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!</title>
    <updated>2025-08-05T05:06:08+00:00</updated>
    <author>
      <name>/u/MrJiks</name>
      <uri>https://old.reddit.com/user/MrJiks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt; &lt;img alt="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" src="https://preview.redd.it/9z1vbpnsu4hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb282a6194808b25822f60262b2b9d1dd1570e" title="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Dario Amodei's recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/jikkujose/status/1952588432280051930"&gt;https://x.com/jikkujose/status/1952588432280051930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJiks"&gt; /u/MrJiks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z1vbpnsu4hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:06:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7bem</id>
    <title>New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`</title>
    <updated>2025-08-05T12:04:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt; &lt;img alt="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" src="https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b" title="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15077"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyzp7</id>
    <title>Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2025-08-05T03:52:26+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt; &lt;img alt="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc51726f166f8072e9a0767410a3b105af874a6d" title="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kitten ML has released open source code and weights of their new TTS model's preview.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt; &lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt; &lt;li&gt;Can literally run anywhere lol : Forget “No gpu required.” - this thing can even run on raspberry pi’s and phones. Great news for gpu-poor folks like me.&lt;/li&gt; &lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdfv5uihi4hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T03:52:26+00:00</published>
  </entry>
</feed>
