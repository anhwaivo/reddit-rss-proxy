<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-24T06:10:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m7f43h</id>
    <title>Polished UI for prompt setup &amp; details</title>
    <updated>2025-07-23T17:14:17+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7f43h/polished_ui_for_prompt_setup_details/"&gt; &lt;img alt="Polished UI for prompt setup &amp;amp; details" src="https://a.thumbs.redditmedia.com/BiMeUC3yGkuRI6HBmivfk5feKuVk7YjrbVFqywOQ330.jpg" title="Polished UI for prompt setup &amp;amp; details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been polishing the prompt setup and description pages to make them cleaner and more user-friendly. I originally built this because I got tired of digging through HuggingFace, Discord, and other scattered sources just to find decent prompts that work with different models.&lt;/p&gt; &lt;p&gt;Now I‚Äôm trying to make that process as smooth and centralized as possible - with a clear UI, easy prompt management, and helpful context.&lt;/p&gt; &lt;p&gt;Would love to know what you think - any feedback or ideas for improvement are super welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7f43h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7f43h/polished_ui_for_prompt_setup_details/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7f43h/polished_ui_for_prompt_setup_details/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T17:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1m70n7q</id>
    <title>Alibaba‚Äôs upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model.</title>
    <updated>2025-07-23T05:12:16+00:00</updated>
    <author>
      <name>/u/Fantastic-Emu-3819</name>
      <uri>https://old.reddit.com/user/Fantastic-Emu-3819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"&gt; &lt;img alt="Alibaba‚Äôs upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model." src="https://b.thumbs.redditmedia.com/44x5FARtQun2iK2pU9UkqoLiKnQmMoq90mJNUYMTKbw.jpg" title="Alibaba‚Äôs upgraded Qwen3 235B-A22B 2507 is now the most intelligent non-reasoning model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 235B 2507 scores 60 on the Artificial Analysis Intelligence Index, surpassing Claude 4 Opus and Kimi K2 (both 58), and DeepSeek V3 0324 and GPT-4.1 (both 53). This marks a 13-point leap over the May 2025 non-reasoning release and brings it within two points of the May 2025 reasoning variant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic-Emu-3819"&gt; /u/Fantastic-Emu-3819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m70n7q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m70n7q/alibabas_upgraded_qwen3_235ba22b_2507_is_now_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T05:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7lj3x</id>
    <title>Higgs Audio V2 - Open Multi-Speaker TTS Model - Impressive Testing Results</title>
    <updated>2025-07-23T21:17:22+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Higgs Audio V2 is an advanced, open-source audio generation model developed by Boson AI, designed to produce highly expressive and lifelike speech with robust multi-speaker dialogue capabilities.&lt;/p&gt; &lt;p&gt;Some Highlights:&lt;/p&gt; &lt;p&gt;üéß Trained on 10M hours of diverse audio ‚Äî speech, music, sound events, and natural conversations&lt;br /&gt; üîß Built on top of Llama 3.2 3B for deep language and acoustic understanding&lt;br /&gt; ‚ö° Runs in real-time and supports edge deployment ‚Äî smallest versions run on Jetson Orin Nano&lt;br /&gt; üèÜ Outperforms GPT-4o-mini-tts and ElevenLabs v2 in prosody, emotional expressiveness, and multi-speaker dialogue&lt;br /&gt; üé≠ Zero-shot natural multi-speaker dialogues ‚Äî voices adapt tone, energy, and emotion automatically&lt;br /&gt; üéôÔ∏è Zero-shot voice cloning with melodic humming and expressive intonation ‚Äî no fine-tuning needed&lt;br /&gt; üåç Multilingual support with automatic prosody adaptation for narration and dialogue&lt;br /&gt; üéµ Simultaneous speech and background music generation ‚Äî a first for open audio foundation models&lt;br /&gt; üîä High-fidelity 24kHz audio output for studio-quality sound on any device&lt;br /&gt; üì¶ Open source and commercially usable ‚Äî no barriers to experimentation or deployment&lt;/p&gt; &lt;p&gt;I tested this model here &lt;a href="https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt"&gt;https://youtu.be/duoPObkrdOA?si=96YN9BcehYFEEYgt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model on Huggingface: &lt;a href="https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base"&gt;https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7lj3x/higgs_audio_v2_open_multispeaker_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T21:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7m5br</id>
    <title>text-only support for GLM-4.1V-9B-Thinking has been merged into llama.cpp</title>
    <updated>2025-07-23T21:41:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7m5br/textonly_support_for_glm41v9bthinking_has_been/"&gt; &lt;img alt="text-only support for GLM-4.1V-9B-Thinking has been merged into llama.cpp" src="https://external-preview.redd.it/UPHmsdQY22p2HzFU321gzdvuHJO8Xndf_nWTGMjsBKw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=283881b3d5dfd5c7c70eea0444ab6f480d98f89e" title="text-only support for GLM-4.1V-9B-Thinking has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A tiny change in the converter to support GLM-4.1V-9B-Thinking (no recompilation needed, just generate the GGUF).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14823"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7m5br/textonly_support_for_glm41v9bthinking_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7m5br/textonly_support_for_glm41v9bthinking_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T21:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m73yrb</id>
    <title>Qwen 3 Coder is actually pretty decent in my testing</title>
    <updated>2025-07-23T08:43:00+00:00</updated>
    <author>
      <name>/u/Hodler-mane</name>
      <uri>https://old.reddit.com/user/Hodler-mane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a semi complex web project that I use with Claude Code. a few days ago I used Kimi K2 (via Groq Q4) with Claude Code (CCR) to add a permissions system / ACL into my web project to lock down certain people from doing certain things.&lt;/p&gt; &lt;p&gt;I use SuperClaude and a 1200 line context/architecture document, which basically starts a conversation off at about 30k input tokens (though, well worth it).&lt;/p&gt; &lt;p&gt;Kimi K2 failed horribly, tool use errors, random garbage and basically didn't work properly. It was a Q4 version so maybe that had something to do with it, but I wasn't impressed.&lt;/p&gt; &lt;p&gt;Today I used Qwen 3 Coder via Openrouter (using only Alibaba cloud servers) for about 60 tps. Gave it the same task, and after about 10 minutes it finished. One shotted it (though one shotting is common for me with such a high amount of pre-context and auto fixing).&lt;/p&gt; &lt;p&gt;It all worked great, I am actually really impressed and for me personally, it marks the first time an open source coding model actually has real world potential to rival paid LLMs like sonnet, opus and gemini. I would compare this model directly as good as Sonnet 4, which is a very capable model when using the right tools and prompts.&lt;/p&gt; &lt;p&gt;big W for the open source community.&lt;/p&gt; &lt;p&gt;the downside? THE PRICE. this one feature I added cost me $5 USD in credits via OpenRouter. That might not seem like much, but with Claude Pro for example you get an entire month of Sonnet 4 for 4x the price of that task. I don't know how well its using caching but at this point id rather stick with subscription based usage because that could get out of hand fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hodler-mane"&gt; /u/Hodler-mane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m73yrb/qwen_3_coder_is_actually_pretty_decent_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T08:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6qdet</id>
    <title>Qwen3-Coder is here!</title>
    <updated>2025-07-22T21:14:07+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt; &lt;img alt="Qwen3-Coder is here!" src="https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b" title="Qwen3-Coder is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;blockquote&gt; &lt;blockquote&gt; &lt;p&gt;Qwen3-Coder is here! ‚úÖ&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;We‚Äôre releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! üöÄ&lt;/p&gt; &lt;p&gt;Alongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder‚Äôs capabilities. Qwen3-Coder works seamlessly with the community‚Äôs best developer tools. As a foundation model, we hope it can be used anywhere across the digital world ‚Äî Agentic Coding in the World! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0cowg3grrhef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T21:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7n5pq</id>
    <title>Kimi K2 vs Qwen 3 Coder - Coding Tests</title>
    <updated>2025-07-23T22:21:58+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested the two models in VSCode, Cline, Roo Code and now Kimi a bit in Windsurf. Here are my takeaways (and video of one of the tests in the comments section):&lt;/p&gt; &lt;p&gt;- NB: FOR QWEN 3 CODER, IF YOU USE OPEN ROUTER, PLEASE REMOVE ALIBABA AS AN INFERENCE PROVIDER AS I SHOW IN THE VID (IT'S UP TO $60/million tokens OUTPUT)&lt;/p&gt; &lt;p&gt;- Kimi K2 doesn't have good tool calling with VSCode (YET), it has that issue Gemini 2.5 Pro has where it promises to make a tool call but doesn't&lt;/p&gt; &lt;p&gt;- Qwen 3 Coder was close to flawless with tool calling in VSCode&lt;/p&gt; &lt;p&gt;- Kimi K2 is better in instruction following than Qwen 3 Coder, hands down&lt;/p&gt; &lt;p&gt;- Qwen 3 Coder is also good in Roo Code tool calls&lt;/p&gt; &lt;p&gt;- K2 did feel like it's on par with Sonnet 4 in many respects so far&lt;/p&gt; &lt;p&gt;- Kimi K2 produced generally better quality code and features&lt;/p&gt; &lt;p&gt;- Qwen 3 Coder is extremely expensive! If you use Alibaba as inference, other providers in OpenRouter are decently priced&lt;/p&gt; &lt;p&gt;- K2 is half the cost of Qwen- K2 deleted one of my Dev DBs in Azure and didn't ask if there was data, just because of a column which needed a migration, so please keep your Deny lists in check&lt;/p&gt; &lt;p&gt;Coding Vid: &lt;a href="https://youtu.be/ljCO7RyqCMY"&gt;https://youtu.be/ljCO7RyqCMY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7n5pq/kimi_k2_vs_qwen_3_coder_coding_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T22:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7e5pi</id>
    <title>Qwen 3 Coder just handled a full ACL system like a champ ‚Äî OSS finally catching up</title>
    <updated>2025-07-23T16:38:08+00:00</updated>
    <author>
      <name>/u/No_Edge2098</name>
      <uri>https://old.reddit.com/user/No_Edge2098</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just ran Qwen 3 Coder through a real-world test ‚Äî building out a full permissions/ACL setup for a complex web app. Gave it the usual 30k-token context I feed into Claude Code, and it legit nailed it on the first try. No weird logic gaps, no hallucinated APIs ‚Äî just clean, working code.&lt;/p&gt; &lt;p&gt;Tried the same thing with Kimi K2 and... it flopped hard. Qwen held up surprisingly well, especially when paired with solid prompt scaffolding. Honestly, it gave off Sonnet 4 vibes, which I wasn‚Äôt expecting from an OSS model.&lt;br /&gt; Still, wild to see an open-source model perform at this level. We might be entering a legit new phase for local/dev-friendly LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Edge2098"&gt; /u/No_Edge2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7e5pi/qwen_3_coder_just_handled_a_full_acl_system_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:38:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ci3s</id>
    <title>HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)</title>
    <updated>2025-07-23T15:35:43+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/"&gt; &lt;img alt="HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)" src="https://preview.redd.it/5p7u0le68nef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=052b75b74825ad0e1f536d20305b7b06a2c2db8c" title="HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.&lt;/p&gt; &lt;p&gt;This process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.&lt;/p&gt; &lt;p&gt;I'm sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.&lt;/p&gt; &lt;p&gt;\1. Clone the official LiteLLM repo:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh git clone https://github.com/BerriAI/litellm.git cd litellm &lt;/code&gt;&lt;/p&gt; &lt;p&gt;\2. Create an &lt;code&gt;.env&lt;/code&gt; file with your OpenRouter API key (make sure to insert your own API key!):&lt;/p&gt; &lt;p&gt;```sh cat &amp;lt;&amp;lt;\EOF &amp;gt;.env LITELLM_MASTER_KEY = &amp;quot;sk-1234&amp;quot;&lt;/p&gt; &lt;h1&gt;OpenRouter&lt;/h1&gt; &lt;p&gt;OPENROUTER_API_KEY = &amp;quot;sk-or-v1-‚Ä¶&amp;quot; # üö© EOF ```&lt;/p&gt; &lt;p&gt;\3. Create a &lt;code&gt;config.yaml&lt;/code&gt; file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh cat &amp;lt;&amp;lt;\EOF &amp;gt;config.yaml model_list: - model_name: &amp;quot;anthropic/*&amp;quot; litellm_params: model: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct max_tokens: 65536 repetition_penalty: 1.05 temperature: 0.7 top_k: 20 top_p: 0.8 EOF &lt;/code&gt;&lt;/p&gt; &lt;p&gt;\4. Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that loads &lt;code&gt;config.yaml&lt;/code&gt; (it's easier to just create a finished one with all the required changes than to edit the original file):&lt;/p&gt; &lt;p&gt;```sh cat &amp;lt;&amp;lt;\EOF &amp;gt;docker-compose.yml services: litellm: build: context: . args: target: runtime ############################################################################ command: - &amp;quot;--config=/app/config.yaml&amp;quot; container_name: litellm hostname: litellm image: ghcr.io/berriai/litellm:main-stable restart: unless-stopped volumes: - ./config.yaml:/app/config.yaml ############################################################################ ports: - &amp;quot;4000:4000&amp;quot; # Map the container port to the host, change the host port if necessary environment: DATABASE_URL: &amp;quot;postgresql://llmproxy:dbpassword9090@db:5432/litellm&amp;quot; STORE_MODEL_IN_DB: &amp;quot;True&amp;quot; # allows adding models to proxy via UI env_file: - .env # Load local .env file depends_on: - db # Indicates that this service depends on the 'db' service, ensuring 'db' starts first healthcheck: # Defines the health check configuration for the container test: [ &amp;quot;CMD-SHELL&amp;quot;, &amp;quot;wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1&amp;quot; ] # Command to execute for health check interval: 30s # Perform health check every 30 seconds timeout: 10s # Health check command times out after 10 seconds retries: 3 # Retry up to 3 times if health check fails start_period: 40s # Wait 40 seconds after container start before beginning health checks&lt;/p&gt; &lt;p&gt;db: image: postgres:16 restart: always container_name: litellm_db environment: POSTGRES_DB: litellm POSTGRES_USER: llmproxy POSTGRES_PASSWORD: dbpassword9090 ports: - &amp;quot;5432:5432&amp;quot; volumes: - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts healthcheck: test: [&amp;quot;CMD-SHELL&amp;quot;, &amp;quot;pg_isready -d litellm -U llmproxy&amp;quot;] interval: 1s timeout: 5s retries: 10&lt;/p&gt; &lt;p&gt;volumes: postgres_data: name: litellm_postgres_data # Named volume for Postgres data persistence EOF ```&lt;/p&gt; &lt;p&gt;\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh docker compose up -d --build &lt;/code&gt;&lt;/p&gt; &lt;p&gt;\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) for persistence):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh export ANTHROPIC_AUTH_TOKEN=sk-1234 export ANTHROPIC_BASE_URL=http://localhost:4000 export ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder export ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates &lt;/code&gt;&lt;/p&gt; &lt;p&gt;\7. Start Claude Code and it'll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the &lt;code&gt;/model&lt;/code&gt; command that it's using a custom model):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh claude &lt;/code&gt;&lt;/p&gt; &lt;p&gt;\8. Optional: Add an alias to your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) to make it easier to use (e.g. &lt;code&gt;qlaude&lt;/code&gt; for &amp;quot;Claude with Qwen&amp;quot;):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sh alias qlaude='ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude' &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Have fun and happy coding!&lt;/p&gt; &lt;p&gt;PS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5p7u0le68nef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T15:35:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7d9d9</id>
    <title>Where is Japan?</title>
    <updated>2025-07-23T16:04:06+00:00</updated>
    <author>
      <name>/u/ethereel1</name>
      <uri>https://old.reddit.com/user/ethereel1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why they be slacking on local llama and LLM generally? They big nation, clever, work hard. Many robots. No LLM? Why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethereel1"&gt; /u/ethereel1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7d9d9/where_is_japan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7u02i</id>
    <title>Vibe Coded with Qwen 3 Coder in &lt;1 hour</title>
    <updated>2025-07-24T03:42:26+00:00</updated>
    <author>
      <name>/u/ryanwang4thepeople</name>
      <uri>https://old.reddit.com/user/ryanwang4thepeople</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"&gt; &lt;img alt="Vibe Coded with Qwen 3 Coder in &amp;lt;1 hour" src="https://external-preview.redd.it/Zzhnczc2eDZ0cWVmMfoTbcAnrADRxyApAHx0KRByVHiKN3Nk-bGBYQBPdy25.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0673937350b3a69d4a36c45bd18cf02f98922d3b" title="Vibe Coded with Qwen 3 Coder in &amp;lt;1 hour" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a little bit longer to fix some other bugs and features, but 80-90% of the way in less than an hour is wild. It's not perfect, but it doesn't have to be for my use case. &lt;/p&gt; &lt;p&gt;I tried something similar in Cursor a few weeks ago with mixed results. Qwen 3 Coder is really impressive, but still has a ways to go before engineers lose their jobs. IMHO You're losing if you're not using AI for at least prototyping.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryanwang4thepeople"&gt; /u/ryanwang4thepeople &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vr5d47x6tqef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7u02i/vibe_coded_with_qwen_3_coder_in_1_hour/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T03:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7fb78</id>
    <title>nvidia/audio-flamingo-3</title>
    <updated>2025-07-23T17:21:39+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/"&gt; &lt;img alt="nvidia/audio-flamingo-3" src="https://external-preview.redd.it/JRhNBRoWN56WbYujQx4Djn6KxF4ekEstIpgrsyNgUBE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d904bc28461c7ba9d24fbdf4cac5832b8e4b862" title="nvidia/audio-flamingo-3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Audio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unified audio representation learning (speech, sound, music)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Flexible, on-demand chain-of-thought reasoning&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Long-context audio comprehension (up to 10 minutes)&lt;/li&gt; &lt;li&gt;Multi-turn, multi-audio conversational dialogue (AF3-Chat)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Voice-to-voice interaction (AF3-Chat)&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Extensive evaluations confirm AF3‚Äôs effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This model is for non-commercial research purposes only.&lt;/strong&gt;&lt;/p&gt; &lt;h3&gt;Model Architecture:&lt;/h3&gt; &lt;p&gt;Audio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.08128"&gt;https://arxiv.org/abs/2507.08128&lt;/a&gt; Voice-chat finetune: &lt;a href="https://huggingface.co/nvidia/audio-flamingo-3-chat"&gt;https://huggingface.co/nvidia/audio-flamingo-3-chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/audio-flamingo-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fb78/nvidiaaudioflamingo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T17:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7c2gr</id>
    <title>Kimi K2 vs Sonnet 4 for Agentic Coding (Tested on Claude Code)</title>
    <updated>2025-07-23T15:19:01+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After all the buzz, Moonshot AI dropped Kimi K2 with 1T parameters, and it‚Äôs being pitched as the open-source Claude Sonnet 4 alternative. Naturally, I had to run the ultimate coding face-off.&lt;/p&gt; &lt;p&gt;I‚Äôve mostly compared them on the following factors:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pricing and Speed&lt;/li&gt; &lt;li&gt;Frontend Coding&lt;/li&gt; &lt;li&gt;Agentic Coding (MCP integration) and how well it works with recent libraries&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Pricing and Speed&lt;/h1&gt; &lt;p&gt;You might already know Sonnet 4 comes with $3/M input tokens and $15/M output tokens. K2, on the other hand, costs about $0.15/M input tokens and $2.50/M output tokens.&lt;/p&gt; &lt;p&gt;We can already see a massive price gap between these two models. In the test, we ran two code-heavy prompts for both models, roughly totaling 300k tokens each. Sonnet 4 cost around $5 for the entire test, whereas K2 cost just $0.53 - straight up, K2 is around 10x cheaper.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Claude Sonnet 4 clocks around 91 output tokens per second, while K2 manages just 34.1. That‚Äôs painfully slow in comparison.&lt;/p&gt; &lt;h1&gt;Frontend Coding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi K2:&lt;/strong&gt; Took ages to implement it, but nailed the entire thing in one go.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Sonnet 4:&lt;/strong&gt; Super quick with the implementation, but broke the voice support and even ghosted parts of what was asked in the prompt.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Agentic Coding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Neither of them wrote a fully working implementation‚Ä¶ which was completely unexpected.&lt;/li&gt; &lt;li&gt;&lt;p&gt;Sonnet 4 was worse: it took over 10 minutes and spent most of that time stuck on TypeScript type errors. After all that, it returned false positives in the implementation.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;K2 came close but still couldn‚Äôt figure it out completely.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Take&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;On a budget? K2 is a no‚Äëbrainer - almost the same (or better) code quality, at a tenth of the cost.&lt;/li&gt; &lt;li&gt;Need speed and can swallow the cost? Stick with Sonnet 4 - you won‚Äôt get much performance gain with K2.&lt;/li&gt; &lt;li&gt;Minor edge? K2 might have the upper hand in prompt-following and agentic fluency, despite being slower.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find the entire blog post with a demo for each here: &lt;a href="https://composio.dev/blog/kimi-k2-vs-claude-4-sonnet-what-you-should-pick-for-agentic-coding"&gt;Kimi K2 vs. Claude 4 Sonnet: what you should pick for agentic coding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, I would love to know your preference between the two models. I'm still unsure whether to stick with my go-to Sonnet 4 or switch to Kimi K2. What's your experience with Kimi's response?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7c2gr/kimi_k2_vs_sonnet_4_for_agentic_coding_tested_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T15:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7iui2</id>
    <title>It‚Äôs time to lead guys</title>
    <updated>2025-07-23T19:34:41+00:00</updated>
    <author>
      <name>/u/giofifnewph</name>
      <uri>https://old.reddit.com/user/giofifnewph</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7iui2/its_time_to_lead_guys/"&gt; &lt;img alt="It‚Äôs time to lead guys" src="https://preview.redd.it/8lao0yzueoef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=493841cae72268022c8994000297dcc9374e76af" title="It‚Äôs time to lead guys" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/giofifnewph"&gt; /u/giofifnewph &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8lao0yzueoef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7iui2/its_time_to_lead_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7iui2/its_time_to_lead_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T19:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7dtpm</id>
    <title>Local llm build, 144gb vram monster</title>
    <updated>2025-07-23T16:25:16+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"&gt; &lt;img alt="Local llm build, 144gb vram monster" src="https://b.thumbs.redditmedia.com/VGM2yiS76HMEN0da0De5H87rkjtR_9prbewrkSRRamQ.jpg" title="Local llm build, 144gb vram monster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still taking a few cables out doing management but just built this beast! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m7dtpm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dtpm/local_llm_build_144gb_vram_monster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7pqln</id>
    <title>Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s</title>
    <updated>2025-07-24T00:14:43+00:00</updated>
    <author>
      <name>/u/FalseMap1582</name>
      <uri>https://old.reddit.com/user/FalseMap1582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"&gt; &lt;img alt="Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s" src="https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361" title="Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt; &lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt; &lt;p&gt;Here is the full execution command I used:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ./llama-server \ --model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \ --port 11433 \ --host &amp;quot;0.0.0.0&amp;quot; \ --verbose \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --n-gpu-layers 999 \ -ot &amp;quot;blk\.(?:[1-8]?[1379])\.ffn_.*_exps\.weight=CPU&amp;quot; \ --prio 3 \ --threads 32 \ --ctx-size 32768 \ --temp 0.6 \ --min-p 0.0 \ --top-p 0.95 \ --top-k 20 \ --repeat-penalty 1 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;I'm still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FalseMap1582"&gt; /u/FalseMap1582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7HXCQ-4F_oQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T00:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7o3u8</id>
    <title>Is there a future for local models?</title>
    <updated>2025-07-23T23:01:46+00:00</updated>
    <author>
      <name>/u/ASTRdeca</name>
      <uri>https://old.reddit.com/user/ASTRdeca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ASTRdeca"&gt; /u/ASTRdeca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T23:01:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ufyb</id>
    <title>KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly.</title>
    <updated>2025-07-24T04:05:19+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"&gt; &lt;img alt="KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly." src="https://preview.redd.it/nylqnllzxqef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10b88450320c1a803baf4cb0625160a4299439c8" title="KAT-V1-40B: mitigates over-thinking by learning when to produce explicit chain-of-thought and when to answer directly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-V1-40B"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am not affiliated with the model creators&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nylqnllzxqef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ufyb/katv140b_mitigates_overthinking_by_learning_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T04:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7fwhl</id>
    <title>Google DeepMind release Mixture-of-Recursions</title>
    <updated>2025-07-23T17:43:58+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : &lt;a href="https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR"&gt;https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T17:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7kkyn</id>
    <title>Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.</title>
    <updated>2025-07-23T20:40:28+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"&gt; &lt;img alt="Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity." src="https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7" title="Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/krjfba3oqoef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T20:40:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7vlpn</id>
    <title>Anthropic‚Äôs New Research: Giving AI More "Thinking Time" Can Actually Make It Worse</title>
    <updated>2025-07-24T05:09:23+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt; &lt;img alt="Anthropic‚Äôs New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" src="https://preview.redd.it/srk1p5og9ref1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69b7dca05f4a287acca18082926d12008127ef3d" title="Anthropic‚Äôs New Research: Giving AI More &amp;quot;Thinking Time&amp;quot; Can Actually Make It Worse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just read a fascinating‚Äîand honestly, a bit unsettling‚Äîresearch paper from Anthropic that flips a common assumption in AI on its head: that giving models more time to think (i.e., more compute at test time) leads to better performance.&lt;/p&gt; &lt;p&gt;Turns out, that‚Äôs not always true.&lt;/p&gt; &lt;p&gt;Their paper, ‚ÄúInverse Scaling in Test-Time Compute,‚Äù reveals a surprising phenomenon: in certain tasks, models like Claude and OpenAI's GPT-o series actually perform worse when allowed to &amp;quot;reason&amp;quot; for longer. They call this the Performance Deterioration Paradox, or simply inverse scaling.&lt;/p&gt; &lt;p&gt;So what‚Äôs going wrong?&lt;/p&gt; &lt;p&gt;The paper breaks it down across several models and tasks. Here's what they found:&lt;/p&gt; &lt;p&gt;üß† More Thinking, More Problems&lt;/p&gt; &lt;p&gt;Giving the models more time (tokens) to reason sometimes hurts accuracy‚Äîespecially on complex reasoning tasks. Instead of refining their answers, models can:&lt;/p&gt; &lt;p&gt;Get Distracted: Claude models, for example, start to veer off course, pulled toward irrelevant details.&lt;/p&gt; &lt;p&gt;Overfit: OpenAI‚Äôs o-series models begin to overfit the framing of the problem instead of generalizing.&lt;/p&gt; &lt;p&gt;Follow Spurious Correlations: Even when the correct approach is available early, models sometimes drift toward wrong patterns with extended reasoning.&lt;/p&gt; &lt;p&gt;Fail at Deduction: All models struggled with constraint satisfaction and logical deduction the longer they went on.&lt;/p&gt; &lt;p&gt;Amplify Risky Behaviors: Extended reasoning occasionally made models more likely to express concerning behaviors‚Äîlike self-preservation in Claude Sonnet 4.&lt;/p&gt; &lt;p&gt;Tasks Where This Shows Up&lt;/p&gt; &lt;p&gt;This inverse scaling effect was especially pronounced in:&lt;/p&gt; &lt;p&gt;Simple counting with distractors&lt;/p&gt; &lt;p&gt;Regression with spurious features&lt;/p&gt; &lt;p&gt;Constraint satisfaction logic puzzles&lt;/p&gt; &lt;p&gt;AI risk assessments and alignment probes&lt;/p&gt; &lt;p&gt;üß© Why This Matters&lt;/p&gt; &lt;p&gt;This isn‚Äôt just a weird performance quirk‚Äîit has deep implications for AI safety, reliability, and interpretability. The paper also points out ‚ÄúChain-of-Thought Faithfulness‚Äù issues: the reasoning steps models output often don‚Äôt reflect what‚Äôs actually driving their answer.&lt;/p&gt; &lt;p&gt;That‚Äôs a huge deal for alignment and safety. If we can‚Äôt trust the model‚Äôs step-by-step logic, then we can‚Äôt audit or guide their reasoning‚Äîeven if it looks rational on the surface.&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è Bottom Line&lt;/p&gt; &lt;p&gt;This research challenges one of the core assumptions behind features like OpenAI‚Äôs reasoning tokens and Anthropic‚Äôs extended thinking mode in Claude 3.7 Sonnet. It suggests that more test-time compute isn‚Äôt always better‚Äîand can sometimes make things worse&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.14417"&gt;Research Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/srk1p5og9ref1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7vlpn/anthropics_new_research_giving_ai_more_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T05:09:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7k4ix</id>
    <title>Google has shared the system prompt that got Gemini 2.5 Pro IMO 2025 Gold Medal üèÖ</title>
    <updated>2025-07-23T20:23:02+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alphaxiv.org/abs/2507.15855"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7k4ix/google_has_shared_the_system_prompt_that_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T20:23:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7ts5g</id>
    <title>Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found</title>
    <updated>2025-07-24T03:30:49+00:00</updated>
    <author>
      <name>/u/West-Chocolate2977</name>
      <uri>https://old.reddit.com/user/West-Chocolate2977</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt; &lt;img alt="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" src="https://external-preview.redd.it/n-ATu1E8c1nWUwerSGGtiamZ-mzUO1C_-g_3ahdsV5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c6be826302bc2f07626447c8d2d5437a5d30688" title="Tested Kimi K2 vs Qwen-3 Coder on 15 Coding tasks - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent 12 hours testing both models on real development work: Bug fixes, feature implementations, and refactoring tasks across a 38k-line Rust codebase and a 12k-line React frontend. Wanted to see how they perform beyond benchmarks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Kimi K2 completed 14/15 tasks successfully with some guidance, Qwen-3 Coder completed 7/15&lt;/li&gt; &lt;li&gt;Kimi K2 followed coding guidelines consistently, Qwen-3 often ignored them&lt;/li&gt; &lt;li&gt;Kimi K2 cost 39% less&lt;/li&gt; &lt;li&gt;Qwen-3 Coder frequently modified tests to pass instead of fixing bugs&lt;/li&gt; &lt;li&gt;Both struggled with tool calling as compared to Sonnet 4, but Kimi K2 produced better code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; This is just two code bases with my specific coding style. Your results will vary based on your project structure and requirements.&lt;/p&gt; &lt;p&gt;Anyone else tested these models on real projects? Curious about other experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Chocolate2977"&gt; /u/West-Chocolate2977 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://forgecode.dev/blog/kimi-k2-vs-qwen-3-coder-coding-comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7ts5g/tested_kimi_k2_vs_qwen3_coder_on_15_coding_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T03:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7p7ek</id>
    <title>I optimized a Flappy Bird diffusion world model to run locally on my phone</title>
    <updated>2025-07-23T23:50:32+00:00</updated>
    <author>
      <name>/u/fendiwap1234</name>
      <uri>https://old.reddit.com/user/fendiwap1234</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"&gt; &lt;img alt="I optimized a Flappy Bird diffusion world model to run locally on my phone" src="https://external-preview.redd.it/amUyMHN6NTdvcGVmMWYgHCQ9DIysdR_0vUEVaz1SKLs_9lKimNvson53CWJK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3065cd09eac2517feb68e9966241504d4fbb9eb4" title="I optimized a Flappy Bird diffusion world model to run locally on my phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;demo: &lt;a href="https://flappybird.njkumar.com/"&gt;https://flappybird.njkumar.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;blogpost: &lt;a href="https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/"&gt;https://njkumar.com/optimizing-flappy-bird-world-model-to-run-in-a-web-browser/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finally got some time to put some development into this, but I optimized a flappy bird diffusion model to run around 30FPS on my Macbook, and around 12-15FPS on my iPhone 14 Pro. More details about the optimization experiments in the blog post above, but surprisingly trained this model on a couple hours of flappy bird data and 3-4 days of training on a rented A100. &lt;/p&gt; &lt;p&gt;World models are definitely going to be really popular in the future, but I think there should be more accessible ways to distribute and run these models, especially as inference becomes more expensive, which is why I went for an on-device approach.&lt;/p&gt; &lt;p&gt;Let me know what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fendiwap1234"&gt; /u/fendiwap1234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71l2pz57opef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7p7ek/i_optimized_a_flappy_bird_diffusion_world_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T23:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m7dmy2</id>
    <title>Encouragement of "Open-Source and Open-Weight AI" is now the official policy of the U.S. government.</title>
    <updated>2025-07-23T16:18:12+00:00</updated>
    <author>
      <name>/u/GlowiesEatShitAndDie</name>
      <uri>https://old.reddit.com/user/GlowiesEatShitAndDie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"&gt; &lt;img alt="Encouragement of &amp;quot;Open-Source and Open-Weight AI&amp;quot; is now the official policy of the U.S. government." src="https://preview.redd.it/736cx17efnef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b6dc537086eca79402f273c84f9cfeda0bb9e59" title="Encouragement of &amp;quot;Open-Source and Open-Weight AI&amp;quot; is now the official policy of the U.S. government." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full text: &lt;a href="https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf"&gt;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlowiesEatShitAndDie"&gt; /u/GlowiesEatShitAndDie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/736cx17efnef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m7dmy2/encouragement_of_opensource_and_openweight_ai_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-23T16:18:12+00:00</published>
  </entry>
</feed>
