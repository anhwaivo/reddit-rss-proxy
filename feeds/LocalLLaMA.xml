<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-14T17:05:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i15wd7</id>
    <title>NVidia APUs for notebooks also just around the corner (May 2025 release!)</title>
    <updated>2025-01-14T13:20:42+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"&gt; &lt;img alt="NVidia APUs for notebooks also just around the corner (May 2025 release!)" src="https://external-preview.redd.it/wC4DxBzn0bkUQbBwl3DM46XFZ5rIkcH8ELtQY-BnjB4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15075443ade86cd753617579ea2aaf16bd20f7b6" title="NVidia APUs for notebooks also just around the corner (May 2025 release!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/D7rR69tMAxs?si=CVkW_ZvqFGwVZjbQ&amp;amp;t=370"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i15wd7/nvidia_apus_for_notebooks_also_just_around_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T13:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i10xzl</id>
    <title>GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation</title>
    <updated>2025-01-14T07:40:55+00:00</updated>
    <author>
      <name>/u/mazen160</name>
      <uri>https://old.reddit.com/user/mazen160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"&gt; &lt;img alt="GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation" src="https://external-preview.redd.it/kUJ8ldjMwlu5j4ZyXRJNdi0V4qdDcgF3b5_dwBmElCg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9a8d0aa1145c94a6dd8e92a4170570cdefa3d7" title="GitHub - mazen160/llmquery: Powerful LLM Query Framework with YAML Prompt Templates. Made for Automation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mazen160"&gt; /u/mazen160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/mazen160/llmquery/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i10xzl/github_mazen160llmquery_powerful_llm_query/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T07:40:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0u8su</id>
    <title>Understanding LLMs from Scratch Using Middle School Math</title>
    <updated>2025-01-14T01:14:48+00:00</updated>
    <author>
      <name>/u/reddit_kwr</name>
      <uri>https://old.reddit.com/user/reddit_kwr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt; &lt;img alt="Understanding LLMs from Scratch Using Middle School Math" src="https://external-preview.redd.it/Rm8Vpve3R611qZhYKG5oG1MiMgKY95Fsco7MI76keJM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=709246061419aa2c09e1e20d5606675288d8debf" title="Understanding LLMs from Scratch Using Middle School Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reddit_kwr"&gt; /u/reddit_kwr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0u8su/understanding_llms_from_scratch_using_middle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T01:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0rdx1</id>
    <title>RTX Titan Ada 48GB Prototype</title>
    <updated>2025-01-13T23:03:07+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like more exciting than 5090 if it is real and sold for $3k. Essentially it is a L40 with all its 144 SM enabled. It will not have its FP16 with FP32 accumulate halved compare to non-TITAN, so it will have double the performance in mixed precision training. It is also likely to have the transformer engine in L40 which 4090 doesn't have (most likely 5090 also doesn't have).&lt;/p&gt; &lt;p&gt;While memory bandwidth is significantly slower, I think it is fast enough for 48GB. TDP is estimated by comparing TITAN V to V100. If it is 300W to 350W, a simple 3xTitan Ada setup can be easily setup.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;RTX Titan Ada&lt;/th&gt; &lt;th align="left"&gt;5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;367.17&lt;/td&gt; &lt;td align="left"&gt;419.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory&lt;/td&gt; &lt;td align="left"&gt;48GB&lt;/td&gt; &lt;td align="left"&gt;32GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Memory Bandwidth&lt;/td&gt; &lt;td align="left"&gt;864GB/s&lt;/td&gt; &lt;td align="left"&gt;1792GB/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;300W&lt;/td&gt; &lt;td align="left"&gt;575W&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;1223.88&lt;/td&gt; &lt;td align="left"&gt;728.71&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype"&gt;https://videocardz.com/newz/alleged-nvidia-rtx-titan-ada-surfaces-with-18432-cuda-cores-and-48gb-gddr6-memory-alongside-gtx-2080-ti-prototype&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0rdx1/rtx_titan_ada_48gb_prototype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0b289</id>
    <title>Hugging Face released a free course on agents.</title>
    <updated>2025-01-13T10:26:28+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added a chapter to smol course on agents. Naturally, using smolagents! The course cover these topics:&lt;/p&gt; &lt;p&gt;- Code agents that solve problem with code&lt;br /&gt; - Retrieval agents that supply grounded context&lt;br /&gt; - Custom functional agents that do whatever you need!&lt;/p&gt; &lt;p&gt;If you're building agent applications, this course should help.&lt;/p&gt; &lt;p&gt;Course in smol course &lt;a href="https://github.com/huggingface/smol-course/tree/main/8_agents"&gt;https://github.com/huggingface/smol-course/tree/main/8_agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T10:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0eio5</id>
    <title>NVidia's official statement on the Biden Administration's Ai Diffusion Rule</title>
    <updated>2025-01-13T13:57:44+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt; &lt;img alt="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" src="https://external-preview.redd.it/mWL0PsxkiUDwuew99qIBVAVxp2i94of5Kngrra_8DKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f30abf9568aba3ecc9b3830767ca6d7b17d79785" title="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/ai-policy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T13:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0mb67</id>
    <title>16GB Raspberry Pi 5 on sale now at $120</title>
    <updated>2025-01-13T19:30:07+00:00</updated>
    <author>
      <name>/u/barefoot_twig</name>
      <uri>https://old.reddit.com/user/barefoot_twig</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barefoot_twig"&gt; /u/barefoot_twig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.raspberrypi.com/news/16gb-raspberry-pi-5-on-sale-now-at-120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0mb67/16gb_raspberry_pi_5_on_sale_now_at_120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T19:30:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i15r15</id>
    <title>gptme v0.26.0 released (terminal agent): now with local TTS support thanks to Kokoro!</title>
    <updated>2025-01-14T13:12:51+00:00</updated>
    <author>
      <name>/u/ErikBjare</name>
      <uri>https://old.reddit.com/user/ErikBjare</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15r15/gptme_v0260_released_terminal_agent_now_with/"&gt; &lt;img alt="gptme v0.26.0 released (terminal agent): now with local TTS support thanks to Kokoro!" src="https://external-preview.redd.it/nVrrnWPE9PNaZxi0hikKCRe5FivIKQCQYSuViCRtN4E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f71b1d99417f0a767821b8c5b465f03022407f4" title="gptme v0.26.0 released (terminal agent): now with local TTS support thanks to Kokoro!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ErikBjare"&gt; /u/ErikBjare &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ErikBjare/gptme/releases/tag/v0.26.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i15r15/gptme_v0260_released_terminal_agent_now_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i15r15/gptme_v0260_released_terminal_agent_now_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0q8nw</id>
    <title>Titans: Learning to Memorize at Test Time</title>
    <updated>2025-01-13T22:13:41+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2501.00663v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0q8nw/titans_learning_to_memorize_at_test_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T22:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0sj1f</id>
    <title>Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!</title>
    <updated>2025-01-13T23:54:49+00:00</updated>
    <author>
      <name>/u/Any_Praline_8178</name>
      <uri>https://old.reddit.com/user/Any_Praline_8178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt; &lt;img alt="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" src="https://external-preview.redd.it/MnRsc25hdDJudWNlMbxhQbonukNuLCehUVr37R_wGdLDix2HfauICRmOLuhO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d7d4c265d5b0d9acef345297c97de7eb356b23f" title="Testing vLLM with Open-WebUI - Llama 3 70B Tulu - 4x AMD Instinct Mi60 Rig - 26 tok/s!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Praline_8178"&gt; /u/Any_Praline_8178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oq7fwat2nuce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0sj1f/testing_vllm_with_openwebui_llama_3_70b_tulu_4x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T23:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1i19fu5</id>
    <title>Llama.cpp server locks up randomly serving Llama-3.2-3B-Instruct-Q8_0.gguf</title>
    <updated>2025-01-14T16:07:37+00:00</updated>
    <author>
      <name>/u/lurkalotter</name>
      <uri>https://old.reddit.com/user/lurkalotter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone come across something like this? It looks like the context window is getting &amp;quot;clogged up&amp;quot; as it were, but unsure how to make it fail the request if that were to happen, as opposed to just locking up and rendering the server useless?&lt;/p&gt; &lt;p&gt;This is how this server is started in Docker:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama1:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;image: llama-cpp-docker&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;container_name: llama1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;restart: unless-stopped&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;environment:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- GGML_CUDA_NO_PINNED=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_CTX_SIZE=8192&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_MODEL=/models/Llama-3.2-3B-Instruct-Q8_0.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_N_GPU_LAYERS=99&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_BATCH_SIZE=512&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_UBATCH_SIZE=1024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_THREADS=3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;- LLAMA_LOG_FILE=llama&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Below is what the log of the failed request looks like. Any nudge in the right direction will be greatly appreciated!&lt;/p&gt; &lt;p&gt;&lt;code&gt;srv update_slots: all slots are idle&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot launch_slot_: id 0 | task 1649 | processing task&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | new prompt, n_ctx_slot = 8192, n_keep = 0, n_prompt_tokens = 3866&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [0, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 512, n_tokens = 512, progress = 0.132437&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [512, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 1024, n_tokens = 512, progress = 0.264873&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [1024, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 1536, n_tokens = 512, progress = 0.397310&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [1536, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 2048, n_tokens = 512, progress = 0.529747&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [2048, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 2560, n_tokens = 512, progress = 0.662183&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [2560, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 3072, n_tokens = 512, progress = 0.794620&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [3072, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 3584, n_tokens = 512, progress = 0.927056&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | kv cache rm [3584, end)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt processing progress, n_past = 3866, n_tokens = 282, progress = 1.000000&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | prompt done, n_past = 3866, n_tokens = 282&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;slot update_slots: id 0 | task 1649 | slot context shift, n_keep = 1, n_left = 8190, n_discard = 4095&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lurkalotter"&gt; /u/lurkalotter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19fu5/llamacpp_server_locks_up_randomly_serving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19fu5/llamacpp_server_locks_up_randomly_serving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i19fu5/llamacpp_server_locks_up_randomly_serving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i16s5q</id>
    <title>Llama 3 8b or Mistral Nemo 12b for 12gb Vram?</title>
    <updated>2025-01-14T14:05:31+00:00</updated>
    <author>
      <name>/u/NaviGray</name>
      <uri>https://old.reddit.com/user/NaviGray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a ryzen 5 5500 and an rtx 3060 12gb. I'm new to LLM stuff but I want to start learning to fine-tune one. Which one should I use. I found online that both are fantastic but Llama might be too much with 12gb?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaviGray"&gt; /u/NaviGray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i16s5q/llama_3_8b_or_mistral_nemo_12b_for_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T14:05:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i12g9x</id>
    <title>Android voice input method based on Whisper</title>
    <updated>2025-01-14T09:35:42+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://f-droid.org/de/packages/org.woheller69.whisper/"&gt;https://f-droid.org/de/packages/org.woheller69.whisper/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i12g9x/android_voice_input_method_based_on_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T09:35:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i19e8u</id>
    <title>Agentic setups beat vanilla LLMs by a huge margin 📈</title>
    <updated>2025-01-14T16:05:40+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt; &lt;img alt="Agentic setups beat vanilla LLMs by a huge margin 📈" src="https://b.thumbs.redditmedia.com/VaHY2thgF4XeViAsdm5g4KaSQqjvdrVHVe17FR8kgTs.jpg" title="Agentic setups beat vanilla LLMs by a huge margin 📈" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks 👋🏻 I'm Merve, I work on Hugging Face's new agents library smolagents. &lt;/p&gt; &lt;p&gt;We recently observed that many people are sceptic of agentic systems, so we benchmarked our CodeAgents (agents that write their actions/tool calls in python blobs) against vanilla LLM calls.&lt;/p&gt; &lt;p&gt;Plot twist: agentic setups easily bring 40 percentage point improvements compared to vanilla LLMs This crazy score increase makes sense, let's take this SimpleQA question:&lt;br /&gt; &amp;quot;Which Dutch player scored an open-play goal in the 2022 Netherlands vs Argentina game in the men’s FIFA World Cup?&amp;quot;&lt;/p&gt; &lt;p&gt;If I had to answer that myself, I certainly would do better with access to a web search tool than with my vanilla knowledge. (argument put forward by Andrew Ng in a great talk at Sequoia)&lt;br /&gt; Here each benchmark is a subsample of ~50 questions from the original benchmarks. Find the whole benchmark here: &lt;a href="https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb"&gt;https://github.com/huggingface/smolagents/blob/main/examples/benchmark.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5"&gt;https://preview.redd.it/7p6lbz7fgzce1.png?width=1467&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30d91e22b32e572e8824b08b4d95a52aeb82c5d5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i19e8u/agentic_setups_beat_vanilla_llms_by_a_huge_margin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:05:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0vrm5</id>
    <title>Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source.</title>
    <updated>2025-01-14T02:30:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt; &lt;img alt="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." src="https://external-preview.redd.it/IqG-6k7nq3XlqnkLxN4BETD1asSgHQ6DDeSuHbGLcoE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ead37f624cbf68ece5069b4337ea6165f2619b57" title="Here is our new reranker model, which we trained on over 95 languages and it achieves better performance than comparable rerankers on our eval benchmarks. Weights, data, and training code are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lightblue/lb-reranker-0.5B-v1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T02:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1i16lvy</id>
    <title>openbmb/MiniCPM-o-2_6 · Hugging Face</title>
    <updated>2025-01-14T13:57:25+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"&gt; &lt;img alt="openbmb/MiniCPM-o-2_6 · Hugging Face" src="https://external-preview.redd.it/HfgLF9MP4qFrrT-J0Ft0hKLbfWPg5ZEDF194P91AP-U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf06c624cc0dcbf599f5edea7b4be7e420f634b7" title="openbmb/MiniCPM-o-2_6 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for realtime speech conversation and multimodal live streaming.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i16lvy/openbmbminicpmo2_6_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T13:57:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i1a88y</id>
    <title>MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)</title>
    <updated>2025-01-14T16:41:20+00:00</updated>
    <author>
      <name>/u/Many_SuchCases</name>
      <uri>https://old.reddit.com/user/Many_SuchCases</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt; &lt;img alt="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" src="https://external-preview.redd.it/HbANHzNsjzvIfVaIHJm0DQnyVjkhdwH7FoXz3GLoR3k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ceab60c72e05525604b9367fa7915922146839a5" title="MiniMax-Text-01 - A powerful new MoE language model with 456B total parameters (45.9 billion activated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91"&gt;https://preview.redd.it/8os84sl2mzce1.png?width=3320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4f6f93b8a0965d65139ba727de29c55880f1b91&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; MiniMax-Text-01 is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax-Text-01 adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methods—such as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax-Text-01's training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax-Text-01 also demonstrates the performance of a top-tier model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total Parameters: 456B&lt;/li&gt; &lt;li&gt;Activated Parameters per Token: 45.9B&lt;/li&gt; &lt;li&gt;Number Layers: 80&lt;/li&gt; &lt;li&gt;Hybrid Attention: a softmax attention is positioned after every 7 lightning attention. &lt;ul&gt; &lt;li&gt;Number of attention heads: 64&lt;/li&gt; &lt;li&gt;Attention head dimension: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of experts: 32&lt;/li&gt; &lt;li&gt;Expert hidden dimension: 9216&lt;/li&gt; &lt;li&gt;Top-2 routing strategy&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000&lt;/li&gt; &lt;li&gt;Hidden Size: 6144&lt;/li&gt; &lt;li&gt;Vocab Size: 200,064&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Blog post:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en/news/minimax-01-series-2"&gt;https://www.minimaxi.com/en/news/minimax-01-series-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-Text-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try online:&lt;/strong&gt; &lt;a href="https://www.hailuo.ai/"&gt;https://www.hailuo.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/MiniMax-AI/MiniMax-01"&gt;https://github.com/MiniMax-AI/MiniMax-01&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Homepage:&lt;/strong&gt; &lt;a href="https://www.minimaxi.com/en"&gt;https://www.minimaxi.com/en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PDF paper:&lt;/strong&gt; &lt;a href="https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf"&gt;https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I am not affiliated&lt;/p&gt; &lt;p&gt;GGUF quants might take a while because the architecture is new (MiniMaxText01ForCausalLM)&lt;/p&gt; &lt;p&gt;A Vision model was also released: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-VL-01"&gt;https://huggingface.co/MiniMaxAI/MiniMax-VL-01&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Many_SuchCases"&gt; /u/Many_SuchCases &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i1a88y/minimaxtext01_a_powerful_new_moe_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T16:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0ysa7</id>
    <title>Qwen released a 72B and a 7B process reward models (PRM) on their recent math models</title>
    <updated>2025-01-14T05:12:52+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt; &lt;img alt="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" src="https://external-preview.redd.it/VHyrFnoulzKOpumKfTYgd3z5gbzActL7OlrPPgPm9KM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c713b68d12275996f8c476345dd6d21eae4d0b75" title="Qwen released a 72B and a 7B process reward models (PRM) on their recent math models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;In addition to the mathematical Outcome Reward Model (ORM) Qwen2.5-Math-RM-72B, we release the Process Reward Model (PRM), namely Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B. PRMs emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), aiming to identify and mitigate intermediate errors in the reasoning processes. Our trained PRMs exhibit both impressive performance in the Best-of-N (BoN) evaluation and stronger error identification performance in ProcessBench.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f"&gt;https://preview.redd.it/ekfsi99i7wce1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=412d38180444dc55ea80687762ea2ed8d0cb3e8f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper: The Lessons of Developing Process Reward Models in Mathematical Reasoning&lt;br /&gt; arXiv:2501.07301 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.07301"&gt;https://arxiv.org/abs/2501.07301&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0ysa7/qwen_released_a_72b_and_a_7b_process_reward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T05:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i192xf</id>
    <title>DDR6 RAM and a reasonable GPU should be able to run 70b models with good speed</title>
    <updated>2025-01-14T15:51:58+00:00</updated>
    <author>
      <name>/u/itsnottme</name>
      <uri>https://old.reddit.com/user/itsnottme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now low VRAM GPUs are the bottleneck in running bigger models, but DDR6 ram should somewhat fix this issue. The ram can supplement GPUs to run LLMs at pretty good speed. &lt;/p&gt; &lt;p&gt;Running bigger models on CPU alone is not ideal, a reasonable speed GPU will still be needed to calculate the context. Let's use a RTX 4080 for example but a slower one is fine as well.&lt;/p&gt; &lt;p&gt;A 70b Q4 KM model is ~40 GB&lt;/p&gt; &lt;p&gt;8192 context is around 3.55 GB&lt;/p&gt; &lt;p&gt;RTX 4080 can hold around 12 GB of the model + 3.55 GB context + leaving 0.45 GB for system memory.&lt;/p&gt; &lt;p&gt;RTX 4080 Memory Bandwidth is 716.8 GB/s x 0.7 for efficiency = ~502 GB/s&lt;/p&gt; &lt;p&gt;For DDR6 ram, it's hard to say for sure but should be around twice the speed of DDR5 and supports Quad Channel so should be close to 360 GB/s * 0.7 = 252 GB/s&lt;/p&gt; &lt;p&gt;(0.3×502) + (0.7×252) = 327 GB/s&lt;/p&gt; &lt;p&gt;So the model should run at around 8.2 tokens/s&lt;/p&gt; &lt;p&gt;It should be a pretty reasonable speed for the average user. Even a slower GPU should be fine as well.&lt;/p&gt; &lt;p&gt;If I made a mistake in the calculation, feel free to let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnottme"&gt; /u/itsnottme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i192xf/ddr6_ram_and_a_reasonable_gpu_should_be_able_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T15:51:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i10okg</id>
    <title>The more you buy...</title>
    <updated>2025-01-14T07:21:39+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt; &lt;img alt="The more you buy..." src="https://preview.redd.it/eprbpw3puwce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee912b8940316504b27f1914cf643a624a21541a" title="The more you buy..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eprbpw3puwce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i10okg/the_more_you_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T07:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11961</id>
    <title>MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device</title>
    <updated>2025-01-14T08:03:44+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt; &lt;img alt="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " src="https://external-preview.redd.it/kijGNUkOqzC0sy8QxKC6uApBn38O_fAQyil0gLR68s8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2741dd1ca03bff7cebb63910f7d365982fb4506e" title="MiniCPM-o 2.6: An 8B size, GPT-4o level Omni Model runs on device " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OpenBMB/status/1879074895113621907"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11961/minicpmo_26_an_8b_size_gpt4o_level_omni_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i142iy</id>
    <title>What % of these do you think will be here by 2026?</title>
    <updated>2025-01-14T11:31:28+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt; &lt;img alt="What % of these do you think will be here by 2026?" src="https://preview.redd.it/bk6yk62f3yce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98a42c3a350d3d500729e61e160e27f838a59bd" title="What % of these do you think will be here by 2026?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6yk62f3yce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i142iy/what_of_these_do_you_think_will_be_here_by_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i148es</id>
    <title>Today I start my very own org 100% devoted to open-source - and it's all thanks to LLMs</title>
    <updated>2025-01-14T11:43:10+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Big thank you to every single one of you here!! My background is in biology - not software dev. This huge milestone in my life could never have happened if it wasn't for LLMs, the fantastic open source ecosystem around them, and of course all the awesome folks here in r /LocalLlama!&lt;/p&gt; &lt;p&gt;Also this post was originally a lot longer but I keep getting autofiltered lol - will put the rest in comments 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i148es/today_i_start_my_very_own_org_100_devoted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T11:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i11hre</id>
    <title>Why are they releasing open source models for free?</title>
    <updated>2025-01-14T08:21:50+00:00</updated>
    <author>
      <name>/u/wochiramen</name>
      <uri>https://old.reddit.com/user/wochiramen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are getting several quite good AI models. It takes money to train them, yet they are being released for free.&lt;/p&gt; &lt;p&gt;Why? What’s the incentive to release a model for free?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wochiramen"&gt; /u/wochiramen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i11hre/why_are_they_releasing_open_source_models_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T08:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i17k5e</id>
    <title>OASIS: Open social media stimulator that uses up to 1 million agents.</title>
    <updated>2025-01-14T14:43:05+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt; &lt;img alt="OASIS: Open social media stimulator that uses up to 1 million agents." src="https://preview.redd.it/rgfjjzbf1zce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad7c4c95213e6848f1fad91fc11eacb2cb18e3b8" title="OASIS: Open social media stimulator that uses up to 1 million agents." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rgfjjzbf1zce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i17k5e/oasis_open_social_media_stimulator_that_uses_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-14T14:43:05+00:00</published>
  </entry>
</feed>
