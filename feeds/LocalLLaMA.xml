<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-26T15:24:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lkxevd</id>
    <title>Any hardware hints for inference that I can get shopping in China?</title>
    <updated>2025-06-26T11:11:07+00:00</updated>
    <author>
      <name>/u/Chris8080</name>
      <uri>https://old.reddit.com/user/Chris8080</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm going to China soon for a few weeks and I was wondering, whether there is any hardware alternative to NVIDIA that I can get there with somewhat decent inference speed?&lt;/p&gt; &lt;p&gt;Currently, I've got a ca. 3 year old Lenovo Laptop:&lt;/p&gt; &lt;p&gt;Processors: 16 × AMD Ryzen 7 PRO 6850U with Radeon Graphics&lt;br /&gt; Memory: 30,1 GiB of RAM&lt;br /&gt; Graphics Processor: AMD Radeon Graphics&lt;/p&gt; &lt;p&gt;and I'd be happy to have something external / additional standing close by for demo / inference testing.&lt;br /&gt; It doesn't have to be faster than the laptop, but it should be able to load bigger models (3 - 8b seems to be the max reasonable on my laptop).&lt;/p&gt; &lt;p&gt;Is there anything feasible for ca. 500 - 2000US$ available?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chris8080"&gt; /u/Chris8080 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkxevd/any_hardware_hints_for_inference_that_i_can_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkxevd/any_hardware_hints_for_inference_that_i_can_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkxevd/any_hardware_hints_for_inference_that_i_can_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T11:11:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkixss</id>
    <title>Getting an LLM to set its own temperature: OpenAI-compatible one-liner</title>
    <updated>2025-06-25T22:01:59+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"&gt; &lt;img alt="Getting an LLM to set its own temperature: OpenAI-compatible one-liner" src="https://external-preview.redd.it/eGFpenhxOTlhNTlmMTzexiqj7MHOyelTArwBqWdVto7F0MAAs0_5qkS8tdr3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5712d62002d73b1e2e2644d94f511d87e132b75f" title="Getting an LLM to set its own temperature: OpenAI-compatible one-liner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure many seen the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt;ThermoAsk: getting an LLM to set its own temperature&lt;/a&gt; by u/&lt;a href="https://www.reddit.com/user/tycho_brahes_nose_/"&gt;tycho_brahes_nose_&lt;/a&gt; from earlier today. &lt;/p&gt; &lt;p&gt;So did I and the idea sounded very intriguing (thanks to OP!), so I spent some time to make it work with any OpenAI-compatible UI/LLM.&lt;/p&gt; &lt;p&gt;You can run it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run \ -e &amp;quot;HARBOR_BOOST_OPENAI_URLS=http://172.17.0.1:11434/v1&amp;quot; \ -e &amp;quot;HARBOR_BOOST_OPENAI_KEYS=sk-ollama&amp;quot; \ -e &amp;quot;HARBOR_BOOST_MODULES=autotemp&amp;quot; \ -p 8004:8000 \ ghcr.io/av/harbor-boost:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you don't use Ollama or have configured an auth for it - adjust the &lt;code&gt;URLS&lt;/code&gt; and &lt;code&gt;KEYS&lt;/code&gt; env vars as needed.&lt;/p&gt; &lt;p&gt;This service has OpenAI-compatible API on its own, so you can connect to it from any compatible client via URL/Key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://localhost:8004/v1 sk-boost &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kjxowr99a59f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkixss/getting_an_llm_to_set_its_own_temperature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T22:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1a1o</id>
    <title>Feeding it text messages</title>
    <updated>2025-06-26T14:17:05+00:00</updated>
    <author>
      <name>/u/eRetArDeD</name>
      <uri>https://old.reddit.com/user/eRetArDeD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone fed Khoj (or another local LLM) a huge amount of personal chat history, like say, years of iMessages?&lt;/p&gt; &lt;p&gt;I’m wondering if there’s some recommended pre-processing or any other tips people may have from personal experience? I’m building an app to help me &lt;del&gt;argue&lt;/del&gt; text better with my partner. It’s working well, but I’m wondering if it can work even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eRetArDeD"&gt; /u/eRetArDeD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1a1o/feeding_it_text_messages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1a1o/feeding_it_text_messages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1a1o/feeding_it_text_messages/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:17:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1eeh</id>
    <title>9070XT Rocm ollama</title>
    <updated>2025-06-26T14:22:05+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys do you know if 9070xt supports ollama now? I’ve been waiting for some time and if it works then I’ll get it set up today&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1eeh/9070xt_rocm_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1eeh/9070xt_rocm_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1eeh/9070xt_rocm_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1xdj</id>
    <title>2 GPU's: Cuda + Vulkan - llama.cpp build setup</title>
    <updated>2025-06-26T14:43:32+00:00</updated>
    <author>
      <name>/u/Ok-Panda-78</name>
      <uri>https://old.reddit.com/user/Ok-Panda-78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What the best approach to build llama.cpp to support 2 GPUs simultaneously? &lt;/p&gt; &lt;p&gt;Should I use Vulkan for both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Panda-78"&gt; /u/Ok-Panda-78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lktqz9</id>
    <title>Unusual use cases of local LLMs that don't require programming</title>
    <updated>2025-06-26T07:17:02+00:00</updated>
    <author>
      <name>/u/leuchtetgruen</name>
      <uri>https://old.reddit.com/user/leuchtetgruen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you use your local llms for that is not a standard use case (chatting, code generation, [E]RP)?&lt;/p&gt; &lt;p&gt;What I'm looking for is something like this: I use OpenWebUIs RAG feature in combination with Ollama to automatically generate cover letters for job applications. It has my CV as knowledge and I just paste the job description. It will generate a cover letter for me, that I then can continue to work on. But it saves me 80% of the time that I'd usually need to write a cover letter. &lt;/p&gt; &lt;p&gt;I created a &amp;quot;model&amp;quot; in OpenWebUI that has in it's system prompt the instruction to create a cover letter for the job description it's given. I gave this model access to the CV via RAG. I use Gemma3:12b as the model and it works quite well. I do all of this in German. &lt;/p&gt; &lt;p&gt;I think that's not something that comes to your mind immediately but it also didn't require any programming using LangChain or other things.&lt;/p&gt; &lt;p&gt;So my question is: Do you use any combination of standard tools in a use case that is a bit &amp;quot;out of the box&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leuchtetgruen"&gt; /u/leuchtetgruen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lktqz9/unusual_use_cases_of_local_llms_that_dont_require/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T07:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkmp5s</id>
    <title>Open source has a similar tool like google cli released today?</title>
    <updated>2025-06-26T00:50:17+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source has a similar tool like google cli released today? ... because just tested that and OMG that is REALLY SOMETHING.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkmp5s/open_source_has_a_similar_tool_like_google_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T00:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyo2p</id>
    <title>Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)</title>
    <updated>2025-06-25T06:44:26+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt; &lt;img alt="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" src="https://external-preview.redd.it/MDRyeGJ6bmJvMDlmMdx7LrexgFcEoZTqX8Yp_PzSREeGDqUB-Qd2XY93v_7d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71ebd7c03a7ccb476c3ff52d6b9e5cc00e65722" title="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I'd like to introduce our latest model: &lt;strong&gt;Jan-nano-128k&lt;/strong&gt; - this model is fine-tuned on &lt;strong&gt;Jan-nano&lt;/strong&gt; (which is a qwen3 finetune), improve performance when enable YaRN scaling &lt;strong&gt;(instead of having degraded performance)&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can uses tools continuously, repeatedly. &lt;/li&gt; &lt;li&gt;It can perform deep research &lt;strong&gt;VERY VERY DEEP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extremely persistence (please pick the right MCP as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, we are not trying to beat Deepseek-671B models, we just want to see how far this current model can go. To our surprise, &lt;strong&gt;it is going very very far.&lt;/strong&gt; Another thing, we have spent all the resource on this version of Jan-nano so.... &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We pushed back the technical report release! But it's coming ...sooon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-128k"&gt;https://huggingface.co/Menlo/Jan-nano-128k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;strong&gt;We are converting the GGUF check in comment section&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model will require &lt;strong&gt;YaRN Scaling&lt;/strong&gt; supported from inference engine, we already configure it in the model, but your inference engine will need to be able to handle YaRN scaling. Please run the model in l&lt;strong&gt;lama.server or Jan app&lt;/strong&gt; (these are from our team, we tested them, just it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; - jan-nano-v0.4-with-MCP: 80.7&lt;br /&gt; &lt;strong&gt;- jan-nano-128k-with-MCP: 83.2&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/909kwwnbo09f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkht3t</id>
    <title>Typos in the prompt lead to worse results</title>
    <updated>2025-06-25T21:15:53+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone knows that LLMs are great at ignoring all of your typos and still respond correctly - mostly. It &lt;a href="https://news.mit.edu/2025/llms-factor-unrelated-information-when-recommending-medical-treatments-0623"&gt;was now discovered&lt;/a&gt; that the response accuracy drops by around 8% when there are typos, upper/lower-case usage, or even extra white spaces in the prompt. There's also some degradation when not using precise language. (&lt;a href="https://dl.acm.org/doi/pdf/10.1145/3715275.3732121"&gt;paper&lt;/a&gt;, &lt;a href="https://github.com/abinithago/medium-is-message"&gt;code&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;A while ago it was found that &lt;a href="https://www.reddit.com/r/ChatGPTPro/comments/18xxyr8/comment/kg8nvjq/?context=3"&gt;tipping $50&lt;/a&gt; lead to better answers. The LLMs apparently generalized that people who offered a monetary incentive got higher quality results. Maybe the LLMs also generalized, that lower quality texts get lower-effort responses. Or those prompts simply didn't sufficiently match the high-quality medical training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkht3t/typos_in_the_prompt_lead_to_worse_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll2fyh</id>
    <title>Deepseek V3 0324 vs R1 0528 for coding tasks.</title>
    <updated>2025-06-26T15:03:50+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested with java and js coding tasks both locally, both with the largest version i can accommodate on my system, unsloth Q3-XL-UD (almost 300GB) following the recomended settings for coding, temp 0 for V3 and 0.6 for R1 and, to my surprise I find the V3 to make less mistakes and to generate better code for me. I have for both a context size of 74k, Q8 cache. I was expecting that with all the thinking, R1 will create better code than V3. I am usually using large context prompts, 10k-20k cause I paste the relevant code files together with my question. Is this caused by the temperature? R1 needs larger temp for thinking process and this can lead to more errors in the generation? What is your experience with these two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lku0lo</id>
    <title>Is there any dedicated subreddits for neural network audio/voice/music generation?</title>
    <updated>2025-06-26T07:34:42+00:00</updated>
    <author>
      <name>/u/wh33t</name>
      <uri>https://old.reddit.com/user/wh33t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought I'd ask here for recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wh33t"&gt; /u/wh33t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lku0lo/is_there_any_dedicated_subreddits_for_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T07:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkohrx</id>
    <title>With Unsloth's model's, what do the things like K, K_M, XL, etc mean?</title>
    <updated>2025-06-26T02:17:54+00:00</updated>
    <author>
      <name>/u/StartupTim</name>
      <uri>https://old.reddit.com/user/StartupTim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking here: &lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"&gt;https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I understand the quant parts, but what do the differences in these specifically mean:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4bit: &lt;/li&gt; &lt;li&gt;IQ4_XS&lt;/li&gt; &lt;li&gt;IQ4_NL&lt;/li&gt; &lt;li&gt;Q4_K_S&lt;/li&gt; &lt;li&gt;Q4_0&lt;/li&gt; &lt;li&gt;Q4_1&lt;/li&gt; &lt;li&gt;Q4_K_M&lt;/li&gt; &lt;li&gt;Q4_K_XL&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Could somebody please break down each, what it means? I'm a bit lost on this. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StartupTim"&gt; /u/StartupTim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkohrx/with_unsloths_models_what_do_the_things_like_k_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T02:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkifu8</id>
    <title>Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B.</title>
    <updated>2025-06-25T21:41:21+00:00</updated>
    <author>
      <name>/u/nero10578</name>
      <uri>https://old.reddit.com/user/nero10578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"&gt; &lt;img alt="Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B." src="https://external-preview.redd.it/bSYUJ_kisf3lxijdNPv6SmJ0R61X4277NoocNI2k1XI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6154edc4c489a4618af2112c22325225277cb6c9" title="Full range of RpR-v4 reasoning models. Small-8B, Fast-30B-A3B, OG-32B, Large-70B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nero10578"&gt; /u/nero10578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/DS-R1-Distill-70B-ArliAI-RpR-v4-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkifu8/full_range_of_rprv4_reasoning_models_small8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkijb5</id>
    <title>Open-source realtime 3D manipulator (minority report style)</title>
    <updated>2025-06-25T21:45:19+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"&gt; &lt;img alt="Open-source realtime 3D manipulator (minority report style)" src="https://external-preview.redd.it/aDdxYnZ0NmE4NTlmMfJKDYQsVfkIjJ_s4x_6JULCYI76ypQLK241aQ2pa_y3.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62a3edde8273227155f03fb45297589b6d41c361" title="Open-source realtime 3D manipulator (minority report style)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;demo link: &lt;a href="https://huggingface.co/spaces/stereoDrift/3d-model-playground"&gt;https://huggingface.co/spaces/stereoDrift/3d-model-playground&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b03bkt6a859f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkijb5/opensource_realtime_3d_manipulator_minority/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T21:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzpdc</id>
    <title>I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor</title>
    <updated>2025-06-26T13:08:22+00:00</updated>
    <author>
      <name>/u/zuluana</name>
      <uri>https://old.reddit.com/user/zuluana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"&gt; &lt;img alt="I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor" src="https://external-preview.redd.it/dHdxbjd0Z2R0OTlmMYJTg58zegrAzYwLDecY21tQ6Q7YMhgJ9y6C6hMRxDnx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d49d6c96df81c4c30a597d62ceb6718b3eb822e6" title="I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an iPhone, and holding the side button always activates Siri... which I'm not crazy about.&lt;/p&gt; &lt;p&gt;I tried using back-tap to open ChatGPT, but it takes too long, and it's inconsistent.&lt;/p&gt; &lt;p&gt;Wired up a quick circuit to immediately interact with language models of my choice (along with my data / integrations)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zuluana"&gt; /u/zuluana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kkt198rdt99f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkc5mr</id>
    <title>LM Studio now supports MCP!</title>
    <updated>2025-06-25T17:37:55+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read the announcement: &lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/mcp"&gt;lmstudio.ai/blog/mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkh3og</id>
    <title>Introducing: The New BS Benchmark</title>
    <updated>2025-06-25T20:48:12+00:00</updated>
    <author>
      <name>/u/Turdbender3k</name>
      <uri>https://old.reddit.com/user/Turdbender3k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt; &lt;img alt="Introducing: The New BS Benchmark" src="https://preview.redd.it/4b2ufnhcy49f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfd8525d5b8c8bc0411893fe54cdd82fd4431a59" title="Introducing: The New BS Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a bs detector benchmark?^^ what if we can create questions that defy any logic just to bait the llm into a bs answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turdbender3k"&gt; /u/Turdbender3k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b2ufnhcy49f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll0e5d</id>
    <title>Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer</title>
    <updated>2025-06-26T13:38:53+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt; &lt;img alt="Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer" src="https://external-preview.redd.it/eMGOFT-dCyqrcGU8o4sNWdjVcmCnEWFc2iYXpXWsCCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9014417b8d198c19f07252830c07d7c077d974f" title="Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yars4a5sy99f1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f41d24b120ac3004968352bd549653db24140944"&gt;https://preview.redd.it/yars4a5sy99f1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f41d24b120ac3004968352bd549653db24140944&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;So far, we’ve explored what a tokenizer is and even built our own from scratch. However, one of the key limitations of building a custom tokenizer is handling unknown or rare words. This is where advanced tokenizers like OpenAI’s tiktoken, which uses Byte Pair Encoding (BPE), really shine.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We also understood, Language models don’t read or understand in the same way humans do. Before any text can be processed by a model, it needs to be tokenized, that is, broken into smaller chunks called tokens. One of the most efficient and widely adopted techniques to perform this is called Byte Pair Encoding (BPE).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s dive deep into how it works, why it’s important, and how to use it in practice.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;What Is Byte Pair Encoding?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Byte Pair Encoding is a data compression algorithm adapted for tokenization. Instead of treating words as whole units, it breaks them down into smaller, more frequent subword units. This allows it to:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Handle unknown words gracefully&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Strike a balance between character-level and word-level tokenization&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Reduce the overall vocabulary size&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How BPE Works (Step-by-Step)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Let’s understand this with a simplified example.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 1: Start with Characters&lt;/h1&gt; &lt;p&gt;&lt;em&gt;We begin by breaking all words in our corpus into characters:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;low&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;newest&amp;quot;, &amp;quot;widest&amp;quot; → [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;], [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;r&amp;quot;], ... &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Count Pair Frequencies&lt;/h1&gt; &lt;p&gt;&lt;em&gt;We count the frequency of adjacent character pairs (bigrams). For example:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;l o&amp;quot;: 2, &amp;quot;o w&amp;quot;: 2, &amp;quot;w e&amp;quot;: 2, &amp;quot;e s&amp;quot;: 2, ... &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 3: Merge the Most Frequent Pair&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Merge the most frequent pair into a new token:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Merge &amp;quot;e s&amp;quot; → &amp;quot;es&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Now “newest” becomes:&lt;/em&gt; &lt;code&gt;[&amp;quot;n&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;t&amp;quot;]&lt;/code&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 4: Repeat Until Vocabulary Limit&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Continue this process until you reach the desired vocabulary size or until no more merges are possible.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Why Is BPE Powerful?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficient&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: It reuses frequent subwords to reduce redundancy.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Flexible&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles rare and compound words better than word-level tokenizers.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Compact vocabulary&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Essential for performance in large models.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;It solves a key problem: how to tokenize unknown or rare words without bloating the vocabulary.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Where Is BPE Used?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;OpenAI’s GPT (e.g., GPT-2, GPT-3, GPT-4)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Hugging Face’s RoBERTa&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;EleutherAI’s GPT-NeoX&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Most transformer models before newer techniques like Unigram or SentencePiece came in&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example: Using tiktoken for BPE Tokenization&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Now let’s see how to use the&lt;/em&gt; &lt;a href="https://github.com/openai/tiktoken"&gt;&lt;em&gt;tiktoken&lt;/em&gt;&lt;/a&gt; &lt;em&gt;library by OpenAI, which implements BPE for GPT models.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Installation&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install tiktoken &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;🧑‍💻 Code Example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;import tiktoken # Load GPT-4 tokenizer (you can also try &amp;quot;gpt2&amp;quot;, &amp;quot;cl100k_base&amp;quot;, etc.) encoding = tiktoken.get_encoding(&amp;quot;cl100k_base&amp;quot;) # Input text text = &amp;quot;IdeaWeaver is building a tokenizer using BPE&amp;quot; # Tokenize token_ids = encoding.encode(text) print(&amp;quot;Token IDs:&amp;quot;, token_ids) # Decode back to text decoded_text = encoding.decode(token_ids) print(&amp;quot;Decoded Text:&amp;quot;, decoded_text) # Optional: Show individual tokens tokens = [encoding.decode([id]) for id in token_ids] print(&amp;quot;Tokens:&amp;quot;, tokens) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Token IDs: [10123, 91234, ...] Decoded Text: IdeaWeaver is building a tokenizer using BPE Tokens: ['Idea', 'Weaver', ' is', ' building', ' a', ' tokenizer', ' using', ' BPE'] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;You can see that even compound or rare words are split into manageable subword units, which is the strength of BPE.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Byte Pair Encoding may sound simple, but it’s one of the key innovations that made today’s large language models possible. It strikes a balance between efficiency, flexibility, and robustness in handling diverse language input.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Next time you ask a question to GPT, remember, BPE made sure your words were understood!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkv8vd</id>
    <title>MUVERA: Making multi-vector retrieval as fast as single-vector search</title>
    <updated>2025-06-26T08:57:50+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt; &lt;img alt="MUVERA: Making multi-vector retrieval as fast as single-vector search" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="MUVERA: Making multi-vector retrieval as fast as single-vector search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkr9k7</id>
    <title>AMD can't be THAT bad at LLMs, can it?</title>
    <updated>2025-06-26T04:44:33+00:00</updated>
    <author>
      <name>/u/tojiro67445</name>
      <uri>https://old.reddit.com/user/tojiro67445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I recently upgraded from a Nvidia 3060 (12GB) to a AMD 9060XT (16GB) and running local models with the new GPU is effectively unusable. I knew Nvidia/CUDA dominate this space, but the difference is so shockingly bad that I feel like I must be doing something wrong. AMD can't possibly be THAT bad at this, right?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; I actually don't really use LLMs for anything, but they are adjacent to my work on GPU APIs so I like to keep tabs on how things evolve in that space. Call it academic curiosity. In any case, I usually dip in every few months, try a couple of newer local models, and get a feel for what they can and can't do.&lt;/p&gt; &lt;p&gt;I had a pretty good sense for the limits of my previous Nvidia GPU, and would get maybe ~10T/s with quantized 12B models running with koboldcpp. Nothing spectacular but it was fine for my needs.&lt;/p&gt; &lt;p&gt;This time around I decided to switch teams and get an AMD GPU, and I've been genuinely happy with it! Runs the games I throw at it great (because 1440p at 60FPS is perfectly fine IMO). But I was kind of shocked when I spun up koboldcpp with a model I had run earlier and was getting... ~1T/s??? A literal order of magnitude slower than with a GPU nearly 5 years older.&lt;/p&gt; &lt;p&gt;For context, I tried it with kobaldcpp_nocuda on Windows 11, Vulkan backend, gemma-3-12b-it-q4_0 as the model. Seems to load OK:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = false) load_tensors: relocated tensors: 0 of 627 load_tensors: offloading 48 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 49/49 layers to GPU load_tensors: Vulkan0 model buffer size = 7694.17 MiB load_tensors: Vulkan_Host model buffer size = 1920.00 MiB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the output is dreadful.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Processing Prompt [BLAS] (1024 / 1024 tokens) Generating (227 / 300 tokens) (EOS token triggered! ID:106) [20:50:09] CtxLimit:1251/4096, Amt:227/300, Init:0.00s, Process:21.43s (47.79T/s), Generate:171.62s (1.32T/s), Total:193.05s ====== Note: Your generation speed appears rather slow. You can try relaunching KoboldCpp with the high priority toggle (or --highpriority) to see if it helps. ====== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Spoiler alert: &lt;code&gt;--highpriority&lt;/code&gt; does not help.&lt;/p&gt; &lt;p&gt;So my question is am I just doing something wrong, or is AMD just really truly this terrible at the whole AI space? I know that most development in this space is done with CUDA and I'm certain that accounts for some of it, but in my experience devs porting CUDA code over to another GPU environment like Vulkan tend to come back with things like &amp;quot;initial release is 15% slower than the CUDA version because we haven't implemented these 20 vendor-specific extensions yet&amp;quot;, not 10x slower implementations. I also don't think that using a ROCm backend (should it ever get around to supporting the 9000 series on Windows) is magically going to give me a 10x boost. Vulkan is hard, y'all, but it's not THAT hard.&lt;/p&gt; &lt;p&gt;Anyone else have experience with the newer AMD cards that either confirms what I'm seeing or indicates I'm doing something wrong?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wow! This got more of a response than I was anticipating! Thanks all! At least it's abundantly clear that it's a problem with my setup and not the GPU.&lt;/p&gt; &lt;p&gt;For what it's worth I tried LM Studio this morning and I'm getting the same thing. It reported 1.5T/s. Looking at resource manager when using LM Studio or Kobold I can see that it's using the GPU's compute capabilities at near 100%, so it's not trying to do the inference on the CPU. I did notice in the AMD software that it said only about a gig of VRAM was being used. The windows performance panel shows that 11Gb of &amp;quot;Shared GPU Memory&amp;quot; is being used, but only 1.8 Gb of &amp;quot;Dedicated GPU Memory&amp;quot; was utilized. So my working theory is that somehow the wrong Vulkan memory heap is being used?&lt;/p&gt; &lt;p&gt;In any case, I'll investigate more tonight but thank you again for all the feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tojiro67445"&gt; /u/tojiro67445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T04:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkbiva</id>
    <title>Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge.</title>
    <updated>2025-06-25T17:13:56+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt; &lt;img alt="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." src="https://preview.redd.it/11rgwmzvv39f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7039783722436b51c07b3fedff7d641b7b004cd" title="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/11rgwmzvv39f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1yjh</id>
    <title>LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀</title>
    <updated>2025-06-26T14:44:50+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt; &lt;img alt="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀" src="https://external-preview.redd.it/GPs8oonK03Al4q6HtUFhFxh4J-39nPu_HZOBEQOCcn8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faddbc4424a43d6c2043b2d74892e39170e98392" title="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://huggingface.co/papers/2506.16406"&gt;https://huggingface.co/papers/2506.16406&lt;/a&gt; Project Link: &lt;a href="https://jerryliang24.github.io/DnD/"&gt;https://jerryliang24.github.io/DnD/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ll1yjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lko09j</id>
    <title>Google's CLI DOES use your prompting data</title>
    <updated>2025-06-26T01:54:24+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt; &lt;img alt="Google's CLI DOES use your prompting data" src="https://preview.redd.it/j1km6ff1h69f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=183f6cf57cbd408bb1e17247c8aba72d8086d1a3" title="Google's CLI DOES use your prompting data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1km6ff1h69f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T01:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzynl</id>
    <title>The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)</title>
    <updated>2025-06-26T13:19:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt; &lt;img alt="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" src="https://external-preview.redd.it/1wJhDztWCANroswcLW3p5i3oMCiTskJ82JKTdTfiCRM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6977975d5861c60901c746f5374dd709bf8cb89" title="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running GPUs in virtual machines for AI workloads is quickly becoming the golden standard - especially for isolation, orchestration, and multi-tenant setups. So I decided to measure the actual performance penalty of this approach.&lt;/p&gt; &lt;p&gt;I benchmarked some LLMs (via ollama-benchmark) on an AMD RX 9060 XT 16GB - first on bare metal Ubuntu 24.04, then in a VM (Ubuntu 24.04) running under AI Linux (Sbnb Linux) with GPU passthrough via &lt;code&gt;vfio-pci&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Models tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral:7b&lt;/li&gt; &lt;li&gt;gemma2:9b&lt;/li&gt; &lt;li&gt;phi4:14b&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VM performance was just &lt;strong&gt;1–2% slower&lt;/strong&gt; than bare metal. That’s it. Practically a rounding error.&lt;/p&gt; &lt;p&gt;So… yeah. Turns out GPU passthrough isn’t the scary performance killer.&lt;/p&gt; &lt;p&gt;👉 I put together the full setup, AMD ROCm install steps, benchmark commands, results, and even a diagram - all in this README: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or help if you’re setting up something similar!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lkzynl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkz0hg</id>
    <title>Meta wins AI copyright lawsuit as US judge rules against authors | Meta</title>
    <updated>2025-06-26T12:35:26+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt; &lt;img alt="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" src="https://external-preview.redd.it/P24oFDRu9fwfx1j87kht5i8PPJV3CyEIC0aLVuyN_0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5020fe75b422d099598cd47f46c61ccb4e8bea63" title="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/jun/26/meta-wins-ai-copyright-lawsuit-as-us-judge-rules-against-authors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T12:35:26+00:00</published>
  </entry>
</feed>
