<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-21T07:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kqu7dv</id>
    <title>Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery.</title>
    <updated>2025-05-20T02:35:05+00:00</updated>
    <author>
      <name>/u/cjsalva</name>
      <uri>https://old.reddit.com/user/cjsalva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"&gt; &lt;img alt="Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery." src="https://external-preview.redd.it/dHQ1MWk0aGltdTFmMag1LLoTdbDTHM6ta6WYNiJEU-q2NTMmBmX376-kobql.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=806dd199a12b6b4480b4f6523c191c4d63a67943" title="Mindblowing demo: John Link led a team of AI agents to discover a forever-chemical-free immersion coolant using Microsoft Discovery." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cjsalva"&gt; /u/cjsalva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9b7qevfimu1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqu7dv/mindblowing_demo_john_link_led_a_team_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T02:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1krciqv</id>
    <title>AI Mini-PC updates from Computex-2025</title>
    <updated>2025-05-20T18:36:38+00:00</updated>
    <author>
      <name>/u/kkb294</name>
      <uri>https://old.reddit.com/user/kkb294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;br /&gt; I am attending &lt;strong&gt;Computex-2025&lt;/strong&gt; and really interested in looking at prospective AI mini pc's based on Nvidia DGX platform. Was able to visit Mediatek, MSI, and Asus exhibits and these are the updates I got: &lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Key Takeaways:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Everyone’s aiming at the AI PC market&lt;/strong&gt;, and the target is clear: &lt;strong&gt;compete head-on with Apple’s Mac Mini lineup&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;This launch phase is being treated like a &lt;strong&gt;“Founders Edition” release&lt;/strong&gt;. No customizations or tweaks — just Nvidia’s &lt;strong&gt;bare-bone reference architecture&lt;/strong&gt; being brought to market by system integrators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;MSI and Asus&lt;/strong&gt; both confirmed that &lt;strong&gt;early access units will go out to tech influencers by end of July&lt;/strong&gt;, with general availability expected by &lt;strong&gt;end of August&lt;/strong&gt;. From the discussions, &lt;strong&gt;MSI seems on track to hit the market first&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A more refined version — with &lt;strong&gt;BIOS, driver optimizations, and I/O customizations&lt;/strong&gt; — is expected by &lt;strong&gt;Q1 2026&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pricing for now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;1TB model:&lt;/strong&gt; ~$2,999&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;4TB model:&lt;/strong&gt; ~$3,999&lt;br /&gt; When asked about the $1,000 difference for storage alone, they pointed to &lt;strong&gt;Apple’s pricing philosophy&lt;/strong&gt; as their benchmark.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;What’s Next?&lt;/h3&gt; &lt;p&gt;I still need to check out: - &lt;strong&gt;AMD’s AI PC lineup&lt;/strong&gt; - &lt;strong&gt;Intel Arc variants&lt;/strong&gt; (24GB and 48GB)&lt;/p&gt; &lt;p&gt;Also, tentatively planning to attend the &lt;a href="https://www.gaie.com.cn/Default.html"&gt;&lt;strong&gt;GAI Expo in China&lt;/strong&gt;&lt;/a&gt; if time permits.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If there’s anything specific you’d like me to check out or ask the vendors about — &lt;strong&gt;drop your questions or suggestions here&lt;/strong&gt;. Happy to help bring more insights back!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kkb294"&gt; /u/kkb294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krciqv/ai_minipc_updates_from_computex2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krciqv/ai_minipc_updates_from_computex2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krciqv/ai_minipc_updates_from_computex2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T18:36:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqw9xn</id>
    <title>Now that I converted my N64 to Linux, what is the best NSFW model to run on it?</title>
    <updated>2025-05-20T04:29:28+00:00</updated>
    <author>
      <name>/u/DeepWisdomGuy</name>
      <uri>https://old.reddit.com/user/DeepWisdomGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need the model in the 4.5MB range.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeepWisdomGuy"&gt; /u/DeepWisdomGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqw9xn/now_that_i_converted_my_n64_to_linux_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T04:29:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1krmgld</id>
    <title>RL algorithms like GRPO are not effective when paried with LoRA on complex reasoning tasks</title>
    <updated>2025-05-21T02:00:14+00:00</updated>
    <author>
      <name>/u/VBQL</name>
      <uri>https://old.reddit.com/user/VBQL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krmgld/rl_algorithms_like_grpo_are_not_effective_when/"&gt; &lt;img alt="RL algorithms like GRPO are not effective when paried with LoRA on complex reasoning tasks" src="https://external-preview.redd.it/m_cCtyX88pvEEjBKG1e4xZruJRILCtqhhamGgPvME80.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9eb46d38a1fe44fd67f1f9fe39e8952e6a6a28e" title="RL algorithms like GRPO are not effective when paried with LoRA on complex reasoning tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VBQL"&gt; /u/VBQL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://osmosis.ai/blog/lora-comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krmgld/rl_algorithms_like_grpo_are_not_effective_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krmgld/rl_algorithms_like_grpo_are_not_effective_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T02:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqxa25</id>
    <title>Microsoft unveils “USB-C for AI apps.” I open-sourced the same concept 3 days earlier—proof inside.</title>
    <updated>2025-05-20T05:32:20+00:00</updated>
    <author>
      <name>/u/iluxu</name>
      <uri>https://old.reddit.com/user/iluxu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"&gt; &lt;img alt="Microsoft unveils “USB-C for AI apps.” I open-sourced the same concept 3 days earlier—proof inside." src="https://external-preview.redd.it/bzRdKyansO1kJ-qEk0diKPCKD02A4z1C6vyWkV3u2bE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2167bf2f062636489b5eac5bdc773d33eb543d7f" title="Microsoft unveils “USB-C for AI apps.” I open-sourced the same concept 3 days earlier—proof inside." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• I released &lt;em&gt;llmbasedos&lt;/em&gt; on 16 May.&lt;br /&gt; • Microsoft showed an almost identical “USB-C for AI” pitch on 19 May.&lt;br /&gt; • Same idea, mine is already running and Apache-2.0.&lt;/p&gt; &lt;p&gt;16 May 09:14 UTC GitHub tag v0.1 16 May 14:27 UTC Launch post on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;br /&gt; 19 May 16:00 UTC Verge headline “Windows gets the USB-C of AI apps”&lt;/p&gt; &lt;h2&gt;What llmbasedos does today&lt;/h2&gt; &lt;p&gt;• Boots from USB/VM in under a minute&lt;br /&gt; • FastAPI gateway speaks JSON-RPC to tiny Python daemons&lt;br /&gt; • 2-line cap.json → your script is callable by ChatGPT / Claude / VS Code&lt;br /&gt; • Offline llama.cpp by default; flip a flag to GPT-4o or Claude 3&lt;br /&gt; • Runs on Linux, Windows (VM), even Raspberry Pi&lt;/p&gt; &lt;h2&gt;Why I’m posting&lt;/h2&gt; &lt;p&gt;Not shouting “theft” — just proving prior art and inviting collab so this stays truly open.&lt;/p&gt; &lt;h2&gt;Try or help&lt;/h2&gt; &lt;p&gt;Code: see the link USB image + quick-start docs coming this week.&lt;br /&gt; Pre-flashed sticks soon to fund development—feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iluxu"&gt; /u/iluxu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iluxu/llmbasedos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqxa25/microsoft_unveils_usbc_for_ai_apps_i_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T05:32:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kre5zr</id>
    <title>Red Hat open-sources llm-d project for distributed AI inference</title>
    <updated>2025-05-20T19:42:28+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5zr/red_hat_opensources_llmd_project_for_distributed/"&gt; &lt;img alt="Red Hat open-sources llm-d project for distributed AI inference" src="https://external-preview.redd.it/eOnGLf0BTZTIEmCMeuDQt-wO8kPOnXVIUvomwjOC5_o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d04765de396d77eda2dbb54f85e86ee409a3f0c3" title="Red Hat open-sources llm-d project for distributed AI inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;This Red Hat press release announces the launch of llm-d, a new open source project targeting distributed generative AI inference at scale. Built on Kubernetes architecture with vLLM-based distributed inference and AI-aware network routing, llm-d aims to overcome single-server limitations for production inference workloads. Key technological innovations include prefill and decode disaggregation to distribute AI operations across multiple servers, KV cache offloading based on LMCache to shift memory burdens to more cost-efficient storage, Kubernetes-powered resource scheduling, and high-performance communication APIs with NVIDIA Inference Xfer Library support. The project is backed by founding contributors CoreWeave, Google Cloud, IBM Research and NVIDIA, along with partners AMD, Cisco, Hugging Face, Intel, Lambda and Mistral AI, plus academic supporters from UC Berkeley and the University of Chicago. Red Hat positions llm-d as the foundation for a &amp;quot;any model, any accelerator, any cloud&amp;quot; vision, aiming to standardize generative AI inference similar to how Linux standardized enterprise IT.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Announcement: &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-launches-llm-d-community-powering-distributed-gen-ai-inference-scale"&gt;https://www.redhat.com/en/about/press-releases/red-hat-launches-llm-d-community-powering-distributed-gen-ai-inference-scale&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Google Cloud: &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/enhancing-vllm-for-distributed-inference-with-llm-d"&gt;https://cloud.google.com/blog/products/ai-machine-learning/enhancing-vllm-for-distributed-inference-with-llm-d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/llm-d"&gt;https://github.com/llm-d&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-launches-llm-d-community-powering-distributed-gen-ai-inference-scale"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5zr/red_hat_opensources_llmd_project_for_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5zr/red_hat_opensources_llmd_project_for_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T19:42:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1krlxoe</id>
    <title>Best local creative writing model and how to set it up?</title>
    <updated>2025-05-21T01:33:14+00:00</updated>
    <author>
      <name>/u/BenefitOfTheDoubt_01</name>
      <uri>https://old.reddit.com/user/BenefitOfTheDoubt_01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a TITAN XP (12GB), 32GB ram and 8700K. What would the best creative writing model be? &lt;/p&gt; &lt;p&gt;I like to try out different stories and scenarios to incorporate into UE5 game dev. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenefitOfTheDoubt_01"&gt; /u/BenefitOfTheDoubt_01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krlxoe/best_local_creative_writing_model_and_how_to_set/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krlxoe/best_local_creative_writing_model_and_how_to_set/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krlxoe/best_local_creative_writing_model_and_how_to_set/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T01:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1krkjhv</id>
    <title>Parking Analysis with Object Detection and Ollama models for Report Generation</title>
    <updated>2025-05-21T00:21:43+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krkjhv/parking_analysis_with_object_detection_and_ollama/"&gt; &lt;img alt="Parking Analysis with Object Detection and Ollama models for Report Generation" src="https://external-preview.redd.it/bmVkZHV1d3AzMTJmMeMfnSo893myclMRvg1dOF4kmROzcG9sBbtv4hMJoM_m.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e16a9184a60bb3efc441b625037d97edd720717" title="Parking Analysis with Object Detection and Ollama models for Report Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit!&lt;/p&gt; &lt;p&gt;Been tinkering with a fun project combining computer vision and LLMs, and wanted to share the progress.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The gist:&lt;/strong&gt;&lt;br /&gt; It uses a YOLO model (via Roboflow) to do real-time object detection on a video feed of a parking lot, figuring out which spots are taken and which are free. You can see the little red/green boxes doing their thing in the video.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But here's the (IMO) coolest part:&lt;/strong&gt; The system then takes that occupancy data and feeds it to an open-source LLM (running locally with Ollama, tried models like Phi-3 for this). The LLM then generates a surprisingly detailed &amp;quot;Parking Lot Analysis Report&amp;quot; in Markdown.&lt;/p&gt; &lt;p&gt;This report isn't just &amp;quot;X spots free.&amp;quot; It calculates occupancy percentages, assesses current demand (e.g., &amp;quot;moderately utilized&amp;quot;), flags potential risks (like overcrowding if it gets too full), and even suggests actionable improvements like dynamic pricing strategies or better signage.&lt;/p&gt; &lt;p&gt;It's all automated – from seeing the car park to getting a mini-management consultant report.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Stack Snippets:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CV:&lt;/strong&gt; YOLO model from Roboflow for spot detection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; Ollama for local LLM inference (e.g., Phi-3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; Markdown reports.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The video shows it in action, including the report being generated.&lt;/p&gt; &lt;p&gt;Github Code: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/ollama/parking_analysis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also if in this code you have to draw the polygons manually I built a separate app for it you can check that code here: &lt;a href="https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app"&gt;https://github.com/Pavankunchala/LLM-Learn-PK/tree/main/polygon-zone-app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Self-promo note: If you find the code useful, a star on GitHub would be awesome!)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm thinking next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time alerts for lot managers.&lt;/li&gt; &lt;li&gt;Predictive analysis for peak hours.&lt;/li&gt; &lt;li&gt;Maybe a simple web dashboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; On a related note, I'm actively looking for new opportunities in Computer Vision and LLM engineering. If your team is hiring or you know of any openings, I'd be grateful if you'd reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; [&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:pavankunchalaofficial@gmail.com"&gt;pavankunchalaofficial@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My other projects on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resume:&lt;/strong&gt; &lt;a href="https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view"&gt;https://drive.google.com/file/d/1ODtF3Q2uc0krJskE_F12uNALoXdgLtgp/view&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uu7z8vwp312f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krkjhv/parking_analysis_with_object_detection_and_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krkjhv/parking_analysis_with_object_detection_and_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T00:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr7p6k</id>
    <title>nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 · Hugging Face</title>
    <updated>2025-05-20T15:26:57+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 · Hugging Face" src="https://external-preview.redd.it/0tCB7CHNBDQpzdV-8tDcf6X2YJH1390tDmRQSvFRDCc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98cc9c4e0b2c297be6e2403cacbb46e4f6bd2221" title="nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr7p6k/nvidiallama31nemotronnano4bv11_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T15:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1krryjx</id>
    <title>Are there any recent 14b or less MoE models?</title>
    <updated>2025-05-21T07:30:52+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are quite a few from 2024 but was wondering if there are any more recent ones. Qwen3 30b a3d but a bit large and requires a lot of vram. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krryjx/are_there_any_recent_14b_or_less_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1krr7hn</id>
    <title>How to get the most from llama.cpp's iSWA support</title>
    <updated>2025-05-21T06:38:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;https://github.com/ggml-org/llama.cpp/pull/13194&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to our gguf god ggerganov, we finally have iSWA support for gemma 3 models that significantly reduces KV cache usage. Since I participated in the pull discussion, I would like to offer tips to get the most out of this update.&lt;/p&gt; &lt;p&gt;Previously, by default fp16 KV cache for 27b model at 64k context is 31744MiB. Now by default batch_size=2048, fp16 KV cache becomes 6368MiB. This is 79.9% reduction.&lt;/p&gt; &lt;p&gt;Group Query Attention KV cache: (ie original implementation)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;1984MB&lt;/td&gt; &lt;td align="left"&gt;3968MB&lt;/td&gt; &lt;td align="left"&gt;7936MB&lt;/td&gt; &lt;td align="left"&gt;15872MB&lt;/td&gt; &lt;td align="left"&gt;31744MB&lt;/td&gt; &lt;td align="left"&gt;63488MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;1536MB&lt;/td&gt; &lt;td align="left"&gt;3072MB&lt;/td&gt; &lt;td align="left"&gt;6144MB&lt;/td&gt; &lt;td align="left"&gt;12288MB&lt;/td&gt; &lt;td align="left"&gt;24576MB&lt;/td&gt; &lt;td align="left"&gt;49152MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;544MB&lt;/td&gt; &lt;td align="left"&gt;1088MB&lt;/td&gt; &lt;td align="left"&gt;2176MB&lt;/td&gt; &lt;td align="left"&gt;4352MB&lt;/td&gt; &lt;td align="left"&gt;8704MB&lt;/td&gt; &lt;td align="left"&gt;17408MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The new implementation splits KV cache to Local Attention KV cache and Global Attention KV cache that are detailed in the following two tables. The overall KV cache use will be the sum of the two. Local Attn KV depends on the batch_size only while the Global attn KV depends on the context length.&lt;/p&gt; &lt;p&gt;Since the local attention KV depends on the batch_size only, you can reduce the batch_size (via the -b switch) from 2048 to 64 (setting values lower than this will just be set to 64) to further reduce KV cache. Originally, it is 5120+1248=6368MiB. Now it is 5120+442=5562MiB. Memory saving will now 82.48%. The cost of reducing batch_size is reduced prompt processing speed. Based on my llama-bench pp512 test, it is only around 20% reduction when you go from 2048 to 64.&lt;/p&gt; &lt;p&gt;Local Attention KV cache size valid at any context:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;batch&lt;/th&gt; &lt;th align="left"&gt;64&lt;/th&gt; &lt;th align="left"&gt;512&lt;/th&gt; &lt;th align="left"&gt;2048&lt;/th&gt; &lt;th align="left"&gt;8192&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;kv_size&lt;/td&gt; &lt;td align="left"&gt;1088&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;9216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;442MB&lt;/td&gt; &lt;td align="left"&gt;624MB&lt;/td&gt; &lt;td align="left"&gt;1248MB&lt;/td&gt; &lt;td align="left"&gt;3744MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;340MB&lt;/td&gt; &lt;td align="left"&gt;480MB&lt;/td&gt; &lt;td align="left"&gt;960MB&lt;/td&gt; &lt;td align="left"&gt;2880MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;123.25MB&lt;/td&gt; &lt;td align="left"&gt;174MB&lt;/td&gt; &lt;td align="left"&gt;348MB&lt;/td&gt; &lt;td align="left"&gt;1044MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Global Attention KV cache:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;context&lt;/th&gt; &lt;th align="left"&gt;4k&lt;/th&gt; &lt;th align="left"&gt;8k&lt;/th&gt; &lt;th align="left"&gt;16k&lt;/th&gt; &lt;th align="left"&gt;32k&lt;/th&gt; &lt;th align="left"&gt;64k&lt;/th&gt; &lt;th align="left"&gt;128k&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;td align="left"&gt;5120MB&lt;/td&gt; &lt;td align="left"&gt;10240MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-12b&lt;/td&gt; &lt;td align="left"&gt;256MB&lt;/td&gt; &lt;td align="left"&gt;512MB&lt;/td&gt; &lt;td align="left"&gt;1024MB&lt;/td&gt; &lt;td align="left"&gt;2048MB&lt;/td&gt; &lt;td align="left"&gt;4096MB&lt;/td&gt; &lt;td align="left"&gt;8192MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-4b&lt;/td&gt; &lt;td align="left"&gt;80MB&lt;/td&gt; &lt;td align="left"&gt;160MB&lt;/td&gt; &lt;td align="left"&gt;320MB&lt;/td&gt; &lt;td align="left"&gt;640MB&lt;/td&gt; &lt;td align="left"&gt;1280MB&lt;/td&gt; &lt;td align="left"&gt;2560MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;If you only have one 24GB card, you can use the default batch_size 2048 and run 27b qat q4_0 at 64k, then it should be 15.6GB model + 5GB global KV + 1.22GB local KV = 21.82GB. Previously, that would take 48.6GB total.&lt;/p&gt; &lt;p&gt;If you want to run it at even higher context, you can use KV quantization (lower accuracy) and/or reduce batch size (slower prompt processing). Reducing batch size to the minimum 64 should allow you to run 96k (total 23.54GB). KV quant alone at Q8_0 should allow you to run 128k at 21.57GB.&lt;/p&gt; &lt;p&gt;So we now finally have a viable long context local LLM that can run with a single card. Have fun summarizing long pdfs with llama.cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T06:38:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1krbhr1</id>
    <title>Gemma 3n blog post</title>
    <updated>2025-05-20T17:55:46+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"&gt; &lt;img alt="Gemma 3n blog post" src="https://external-preview.redd.it/6Uiw9QwCmEOV2-HLrpi2sZAHXDpSta5QPjHpcK86Z_Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac8b86ea6caf300bd46fdcfa5c35348f14e45e9c" title="Gemma 3n blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/models/gemma/gemma-3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krbhr1/gemma_3n_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqye2t</id>
    <title>Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3</title>
    <updated>2025-05-20T06:48:35+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"&gt; &lt;img alt="Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3" src="https://external-preview.redd.it/wwo-l6Lp28bzCUco8EwP9KcszHoY94gQORkIHOKSj3w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f8e6a6a9f7f8cd4578b2ec231165e18a1067cfb" title="Sliding Window Attention support merged into llama.cpp, dramatically reducing the memory requirements for running Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T06:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kre5gs</id>
    <title>Running Gemma 3n on mobile locally</title>
    <updated>2025-05-20T19:41:53+00:00</updated>
    <author>
      <name>/u/United_Dimension_46</name>
      <uri>https://old.reddit.com/user/United_Dimension_46</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"&gt; &lt;img alt="Running Gemma 3n on mobile locally" src="https://preview.redd.it/xhvtdzjvpz1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec66952f2520d8ba93f2f38c94004afc60e0854" title="Running Gemma 3n on mobile locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United_Dimension_46"&gt; /u/United_Dimension_46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhvtdzjvpz1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kre5gs/running_gemma_3n_on_mobile_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T19:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1krrp2f</id>
    <title>The P100 isn't dead yet - Qwen3 benchmarks</title>
    <updated>2025-05-21T07:12:06+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to test how fast I could run Qwen3-14B-GPTQ-Int4 on a P100 versus Qwen3-14B-GPTQ-AWQ on a 3090.&lt;/p&gt; &lt;p&gt;I found that it was quite competitive in single-stream generation with around 45 tok/s on the P100 at 150W power limit vs around 54 tok/s on the 3090 with a PL of 260W.&lt;/p&gt; &lt;p&gt;So if you're willing to eat the idle power cost (26W in my setup), a single P100 is a nice way to run a decent model at good speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krrp2f/the_p100_isnt_dead_yet_qwen3_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T07:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1krcdg5</id>
    <title>Gemini 2.5 Flash (05-20) Benchmark</title>
    <updated>2025-05-20T18:30:45+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"&gt; &lt;img alt="Gemini 2.5 Flash (05-20) Benchmark" src="https://preview.redd.it/q5m5i3c6dz1f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=367b7989c05f4f3b26a8222ba271d7a1bc61b829" title="Gemini 2.5 Flash (05-20) Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q5m5i3c6dz1f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T18:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1krpvwj</id>
    <title>Gemma 3N E4B and Gemini 2.5 Flash Tested</title>
    <updated>2025-05-21T05:10:56+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lEtLksaaos8"&gt;https://www.youtube.com/watch?v=lEtLksaaos8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared Gemma 3n e4b against Qwen 3 4b. Mixed results. Gemma does great on classification, matches Qwen 4B on Structured JSON extraction. Struggles with coding and RAG.&lt;/p&gt; &lt;p&gt;Also compared Gemini 2.5 Flash to Open AI 4.1. Altman should be worried. Cheaper than 4.1 mini, better than full 4.1.&lt;/p&gt; &lt;h1&gt;Harmful Question Detector&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;70.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Named Entity Recognition New&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;60.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Retrieval Augmented Generation Prompt&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;97.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;83.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;62.50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;SQL Query Generator&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemini-2.5-flash-preview-05-20&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-4.1&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-4b:free&lt;/td&gt; &lt;td align="left"&gt;75.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3n-e4b-it:free&lt;/td&gt; &lt;td align="left"&gt;65.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krpvwj/gemma_3n_e4b_and_gemini_25_flash_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T05:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr9rvp</id>
    <title>OpenEvolve: Open Source Implementation of DeepMind's AlphaEvolve System</title>
    <updated>2025-05-20T16:49:21+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I'm excited to share &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of Google DeepMind's AlphaEvolve system that I recently completed. For those who missed it, AlphaEvolve is an evolutionary coding agent that DeepMind announced in May that uses LLMs to discover new algorithms and optimize existing ones.&lt;/p&gt; &lt;h1&gt;What is OpenEvolve?&lt;/h1&gt; &lt;p&gt;OpenEvolve is a framework that &lt;strong&gt;evolves entire codebases&lt;/strong&gt; through an iterative process using LLMs. It orchestrates a pipeline of code generation, evaluation, and selection to continuously improve programs for a variety of tasks.&lt;/p&gt; &lt;p&gt;The system has four main components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt Sampler&lt;/strong&gt;: Creates context-rich prompts with past program history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM Ensemble&lt;/strong&gt;: Generates code modifications using multiple LLMs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluator Pool&lt;/strong&gt;: Tests generated programs and assigns scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Program Database&lt;/strong&gt;: Stores programs and guides evolution using MAP-Elites inspired algorithm&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Works with any LLM&lt;/strong&gt; via OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ensembles multiple models&lt;/strong&gt; for better results (we found Gemini-Flash-2.0-lite + Gemini-Flash-2.0 works great)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evolves entire code files&lt;/strong&gt;, not just single functions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-objective optimization&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible prompt engineering&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distributed evaluation&lt;/strong&gt; with checkpointing&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We replicated AlphaEvolve's results!&lt;/h1&gt; &lt;p&gt;We successfully replicated two examples from the AlphaEvolve paper:&lt;/p&gt; &lt;h1&gt;Circle Packing&lt;/h1&gt; &lt;p&gt;Started with a simple concentric ring approach and evolved to discover mathematical optimization with scipy.minimize. We achieved 2.634 for the sum of radii, which is 99.97% of DeepMind's reported 2.635!&lt;/p&gt; &lt;p&gt;The evolution was fascinating - early generations used geometric patterns, by gen 100 it switched to grid-based arrangements, and finally it discovered constrained optimization.&lt;/p&gt; &lt;h1&gt;Function Minimization&lt;/h1&gt; &lt;p&gt;Evolved from a basic random search to a full simulated annealing algorithm, discovering concepts like temperature schedules and adaptive step sizes without being explicitly programmed with this knowledge.&lt;/p&gt; &lt;h1&gt;LLM Performance Insights&lt;/h1&gt; &lt;p&gt;For those running their own LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low latency is critical since we need many generations&lt;/li&gt; &lt;li&gt;We found Cerebras AI's API gave us the fastest inference&lt;/li&gt; &lt;li&gt;For circle packing, an ensemble of Gemini-Flash-2.0 + Claude-Sonnet-3.7 worked best&lt;/li&gt; &lt;li&gt;The architecture allows you to use any model with an OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself!&lt;/h1&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/codelion/openevolve"&gt;https://github.com/codelion/openevolve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/circle_packing"&gt;Circle Packing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/openevolve/tree/main/examples/function_minimization"&gt;Function Minimization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what you build with it and hear your feedback. Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr9rvp/openevolve_open_source_implementation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1krl0du</id>
    <title>LLAMACPP - SWA support ..FNALLY ;-)</title>
    <updated>2025-05-21T00:45:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because of that for instance gemma 3 27b q4km with flash attention fp16 and card with 24 GB VRAM I can fit &lt;strong&gt;75k context&lt;/strong&gt; now!&lt;/p&gt; &lt;p&gt;Before I was able to fix max 15k context with those parameters.&lt;/p&gt; &lt;p&gt;Source&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13194"&gt;https://github.com/ggml-org/llama.cpp/pull/13194&lt;/a&gt;&lt;/p&gt; &lt;p&gt;download&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for CLI&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model google_gemma-3-27b-it-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 75000 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap --top_k 64 --temp 1.0 -fa &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For server ( GIU )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server.exe --model google_gemma-3-27b-it-Q4_K_M.gguf --mmproj models/new3/google_gemma-3-27b-it-bf16-mmproj.gguf --threads 30 --keep -1 --n-predict -1 --ctx-size 75000 -ngl 99 --no-mmap --min_p 0 -fa &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krl0du/llamacpp_swa_support_fnally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T00:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1krb6uu</id>
    <title>Google MedGemma</title>
    <updated>2025-05-20T17:44:16+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt; &lt;img alt="Google MedGemma" src="https://external-preview.redd.it/IkdSAGaHbYPwN7JuzggxNmmy1Ov_W_6LD8_ETnav3jw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dae3e4abe286e7ffab20fc05dd9c3c108fc0c88e" title="Google MedGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krb6uu/google_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T17:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1krc35x</id>
    <title>Announcing Gemma 3n preview: powerful, efficient, mobile-first AI</title>
    <updated>2025-05-20T18:19:09+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"&gt; &lt;img alt="Announcing Gemma 3n preview: powerful, efficient, mobile-first AI" src="https://external-preview.redd.it/0ZfqdzMMjWqMp0M38-XRODYXqi_qFGgfPApxf9tbLSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=753cbcbb9d784f9b6d8275021386984d8ac88f5a" title="Announcing Gemma 3n preview: powerful, efficient, mobile-first AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krc35x/announcing_gemma_3n_preview_powerful_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T18:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kr8s40</id>
    <title>Gemma 3n Preview</title>
    <updated>2025-05-20T16:10:01+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt; &lt;img alt="Gemma 3n Preview" src="https://external-preview.redd.it/nuTGd6nR-D7i0exzDvXeyeroWnA1sgWJyyF8GipdVWU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4092ee3492e35aa48ddc115bdbd7e2144d1d03c2" title="Gemma 3n Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-preview-682ca41097a31e5ac804d57b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kr8s40/gemma_3n_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T16:10:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1krp4hq</id>
    <title>They also released the Android app with which you can interact with the new Gemma3n</title>
    <updated>2025-05-21T04:25:10+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;This is really good&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android"&gt;https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/google-ai-edge/gallery"&gt;https://github.com/google-ai-edge/gallery&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krp4hq/they_also_released_the_android_app_with_which_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T04:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1krnk8v</id>
    <title>ByteDance Bagel 14B MOE (7B active) Multimodal with image generation (open source, apache license)</title>
    <updated>2025-05-21T02:57:30+00:00</updated>
    <author>
      <name>/u/noage</name>
      <uri>https://old.reddit.com/user/noage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights - &lt;a href="https://github.com/ByteDance-Seed/Bagel"&gt;GitHub - ByteDance-Seed/Bagel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website - &lt;a href="https://bagel-ai.org/"&gt;BAGEL: The Open-Source Unified Multimodal Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper - &lt;a href="https://arxiv.org/abs/2505.14683"&gt;[2505.14683] Emerging Properties in Unified Multimodal Pretraining&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses a mixture of experts and a mixture of transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noage"&gt; /u/noage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1krnk8v/bytedance_bagel_14b_moe_7b_active_multimodal_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kri7ik</id>
    <title>ok google, next time mention llama.cpp too!</title>
    <updated>2025-05-20T22:31:42+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt; &lt;img alt="ok google, next time mention llama.cpp too!" src="https://preview.redd.it/ml66h5yxj02f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36aba859e0c8b8e47fe122c7315b0f3ad3607ad1" title="ok google, next time mention llama.cpp too!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ml66h5yxj02f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kri7ik/ok_google_next_time_mention_llamacpp_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-20T22:31:42+00:00</published>
  </entry>
</feed>
