<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-03T03:04:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n6fta4</id>
    <title>ETH Zurich Open LLM "Apertus" has been released</title>
    <updated>2025-09-02T10:25:45+00:00</updated>
    <author>
      <name>/u/kisamoto</name>
      <uri>https://old.reddit.com/user/kisamoto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt; &lt;img alt="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" src="https://external-preview.redd.it/3xCYbgdmDkf0KukpAo-RYTjRTShJKNSz9uOuaVJW_jI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5da84c50675f066ea42c6cb75049480ff32b8ed5" title="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kisamoto"&gt; /u/kisamoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/swiss-ai/apertus-llm-68b699e65415c231ace3b059"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T10:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ojwi</id>
    <title>Jupyter Agent Dataset</title>
    <updated>2025-09-02T16:41:35+00:00</updated>
    <author>
      <name>/u/lvwerra</name>
      <uri>https://old.reddit.com/user/lvwerra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt; &lt;img alt="Jupyter Agent Dataset" src="https://external-preview.redd.it/AILThe6Dj0cxSzLU5c4WZjfK6cm5Jz57hJf5d-2R9PU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d89f36fc6f3a8b9463d808eb0c39c867916796b" title="Jupyter Agent Dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jx5v4hen4smf1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f33c813c304597d66fdad442a28987f129fb08c8"&gt;https://preview.redd.it/jx5v4hen4smf1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f33c813c304597d66fdad442a28987f129fb08c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi there, Leandro here from the research team at Hugging Face. We built the Jupyter Agent Dataset, a high quality data analysis and code execution dataset built with 7TB of Kaggle datasets and 20k+ notebooks using Qwen3-Coder.&lt;/p&gt; &lt;p&gt;The traces were generated using a combination of real notebook filtering, synthetic QA annotation, and then solving those questions with Qwen3-Coder by using the real data and code execution.&lt;/p&gt; &lt;p&gt;Training on the dataset shows significant improvement on data exploration and analysis tasks! Ask me anything about it!&lt;/p&gt; &lt;p&gt;Dataset: &lt;a href="https://huggingface.co/datasets/data-agents/jupyter-agent-dataset"&gt;https://huggingface.co/datasets/data-agents/jupyter-agent-dataset&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lvwerra"&gt; /u/lvwerra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6v3de</id>
    <title>Where is GGUF on grok 2?</title>
    <updated>2025-09-02T20:46:08+00:00</updated>
    <author>
      <name>/u/Defiant_Diet9085</name>
      <uri>https://old.reddit.com/user/Defiant_Diet9085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant_Diet9085"&gt; /u/Defiant_Diet9085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6v3de/where_is_gguf_on_grok_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6v3de/where_is_gguf_on_grok_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6v3de/where_is_gguf_on_grok_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T20:46:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n73dek</id>
    <title>[Case Study] How Our AI's Internal "Critic" Saves Code From Race Conditions Before Execution</title>
    <updated>2025-09-03T02:45:42+00:00</updated>
    <author>
      <name>/u/AffectionateSpray507</name>
      <uri>https://old.reddit.com/user/AffectionateSpray507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLMA"&gt;r/LocalLLMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We've all been there: you write an asynchronous script, and it works... sometimes. Race conditions are a nightmare, especially on limited hardware where every cycle counts.&lt;/p&gt; &lt;p&gt;In our MeganX 3.0 project, I developed an architecture that tackles this at the source. It's not just better debugging—it's a &lt;strong&gt;programmatic self-audit&lt;/strong&gt; that prevents flawed code from ever executing.&lt;/p&gt; &lt;p&gt;Here's how the core &lt;code&gt;Plan -&amp;gt; Critic -&amp;gt; Repair&lt;/code&gt; loop works in practice:&lt;/p&gt; &lt;h2&gt;The Initial Plan (The Ticking Time Bomb):&lt;/h2&gt; &lt;p&gt;A basic agent tasked with fetching 3 pieces of data in parallel might generate this naive approach:&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;h1&gt;--- PLAN A (Flawed) ---&lt;/h1&gt; &lt;h1&gt;All three tasks try to use the SAME browser page at once.&lt;/h1&gt; &lt;h1&gt;Result: Guaranteed race condition and data corruption.&lt;/h1&gt; &lt;p&gt;async def run_flawed_plan(browser): page = await browser.new_page() tasks = [ fetch_data(page, &amp;quot;/uuid&amp;quot;), fetch_data(page, &amp;quot;/ip&amp;quot;), fetch_data(page, &amp;quot;/user-agent&amp;quot;) ] await asyncio.gather(*tasks) ```&lt;/p&gt; &lt;h2&gt;The Critic Module (Automated Code Review):&lt;/h2&gt; &lt;p&gt;Before this code ever runs, MeganX 3.0's internal Critic audits it and flags the issue:&lt;/p&gt; &lt;p&gt;&lt;code&gt; [CRITIC ALERT] High risk of race condition detected. A single 'page' object is being shared across multiple concurrent 'goto' operations. Plan is not safe for execution. &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;The Repaired Plan (The Solution):&lt;/h2&gt; &lt;p&gt;The Critic then generates a corrected, thread-safe version:&lt;/p&gt; &lt;p&gt;```python &lt;/p&gt; &lt;h1&gt;--- PLAN B (Repaired) ---&lt;/h1&gt; &lt;h1&gt;Each task gets its OWN isolated page.&lt;/h1&gt; &lt;h1&gt;Result: True parallelism and 100% data integrity.&lt;/h1&gt; &lt;p&gt;async def run_resilient_plan(browser): tasks = [ fetch_data_on_new_page(browser, &amp;quot;/uuid&amp;quot;), fetch_data_on_new_page(browser, &amp;quot;/ip&amp;quot;), fetch_data_on_new_page(browser, &amp;quot;/user-agent&amp;quot;) ] await asyncio.gather(*tasks) ```&lt;/p&gt; &lt;p&gt;This approach of embedding a programmatic skeptic into the agent's core has been game-changing for stability. The pattern works in any async environment, not just web scraping.&lt;/p&gt; &lt;p&gt;Have you dealt with similar race condition nightmares in your projects? Curious how others have solved this problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectionateSpray507"&gt; /u/AffectionateSpray507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n73dek/case_study_how_our_ais_internal_critic_saves_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n73dek/case_study_how_our_ais_internal_critic_saves_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n73dek/case_study_how_our_ais_internal_critic_saves_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T02:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n73fu8</id>
    <title>Very old news but this made me chuckle.</title>
    <updated>2025-09-03T02:48:56+00:00</updated>
    <author>
      <name>/u/Ok-Application-2261</name>
      <uri>https://old.reddit.com/user/Ok-Application-2261</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n73fu8/very_old_news_but_this_made_me_chuckle/"&gt; &lt;img alt="Very old news but this made me chuckle." src="https://b.thumbs.redditmedia.com/bzTmrDVU-oN1FfIIGbDH0plVmx-vKD1upLJpiOAB-pA.jpg" title="Very old news but this made me chuckle." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wonder if the CCP view Mao as a positive historical figure then. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Application-2261"&gt; /u/Ok-Application-2261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n73fu8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n73fu8/very_old_news_but_this_made_me_chuckle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n73fu8/very_old_news_but_this_made_me_chuckle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T02:48:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6zuft</id>
    <title>"endless" EPUB translator via a selectable local LLM?</title>
    <updated>2025-09-03T00:01:18+00:00</updated>
    <author>
      <name>/u/Green-Ad-3964</name>
      <uri>https://old.reddit.com/user/Green-Ad-3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have several Chinese e-books that I'd like to read, but I can't read Chinese. I know there are many LLMs that can handle translation, and I'm using them in batch (for personal use only, of course).&lt;/p&gt; &lt;p&gt;Is there an endless EPUB converter that takes an EPUB as input and passes it to a local LLM to produce a new EPUB in another language, while preserving the same formatting and overall features?&lt;/p&gt; &lt;p&gt;I know about this one:&lt;br /&gt; &lt;a href="https://github.com/oomol-lab/epub-translator?utm_source=chatgpt.com"&gt;https://github.com/oomol-lab/epub-translator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but it seems to run only via API (not with local LLM). And now, especially with Hunyuan-MT Chimera, local models are a perfect way to translate.&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green-Ad-3964"&gt; /u/Green-Ad-3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6zuft/endless_epub_translator_via_a_selectable_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6zuft/endless_epub_translator_via_a_selectable_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6zuft/endless_epub_translator_via_a_selectable_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T00:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6nkki</id>
    <title>Anyone here using Qwen3-235b-a22b-thinking-2507 as their daily driver???</title>
    <updated>2025-09-02T16:05:10+00:00</updated>
    <author>
      <name>/u/True_Requirement_891</name>
      <uri>https://old.reddit.com/user/True_Requirement_891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I fucking love this model!!! It performs better than deepseek for me in general use for nearly everything!!! Easily the BEST Open Weight model we have that rivals closed models!&lt;/p&gt; &lt;p&gt;It just feels fucking intelligent to talk to lmao passes my vibe check and it's fast.&lt;/p&gt; &lt;p&gt;What is the experience of you guys with this model in more general use cases???&lt;/p&gt; &lt;p&gt;Also, I really wanna see a scaled up general version of this model like: Qwen3-480B-35B-Thinking &lt;/p&gt; &lt;p&gt;The coder variant sucks for anything but producing code and maybe tool calling.&lt;/p&gt; &lt;p&gt;Sure it's gonna be difficult to run locally for most of the community but being able to access this amazing model from multiple cloud providers for dirt cheap prices is amazing for me! Not being locked at the mercy of closed labs.&lt;/p&gt; &lt;p&gt;You don't have to worry about model changing behind the scenes! You get &amp;quot;near&amp;quot; full control of the model.&lt;/p&gt; &lt;p&gt;Ofcourse there are issues like cloud providers using smaller quants behind the scenes but still worth it from more legit providers.&lt;/p&gt; &lt;p&gt;Qwen3-235b-a22b-thinking-2507 doesn't even feel benchmaxxed or at least not from my experience. The pre-update version was garbage but after the update, it became my favourite one so far!!!&lt;/p&gt; &lt;p&gt;Some more thoughts:&lt;/p&gt; &lt;p&gt;The new DeepSeek-V3.1 sucks ass man like madly inconsistent and just doesn't have the feel... It disappointed me big time. I saw people praising it but honestly, I just don't get it.&lt;/p&gt; &lt;p&gt;R1-0528 was a significant upgrade in terms of intelligence even with a lack of that &amp;quot;vibe&amp;quot;.&lt;/p&gt; &lt;p&gt;V3-0324 was just 💋&lt;/p&gt; &lt;p&gt;But this new V3.1 feels like the worst of both worlds. I tried it a lot and I just can't trust it. It's very inconsistent in performance/accuracy. It also loses context fast. Misunderstands stuff way more than other models... An absolute failure in my experience. Maybe it's because of the hybrid thinking system that qwen left behind???&lt;/p&gt; &lt;p&gt;I just don't get how are you guys able to use V3.1 without letting out a sigh every prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/True_Requirement_891"&gt; /u/True_Requirement_891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6lu9t</id>
    <title>every LLM metric you need to know (v2.0)</title>
    <updated>2025-09-02T15:00:08+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I made &lt;a href="https://www.reddit.com/r/LLMDevs/comments/1j6pxv9/every_llm_metric_you_need_to_know/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;this post&lt;/a&gt; a few months ago, the AI and evals space has shifted significantly. Better LLMs mean that standard out-of-the-box metrics aren’t as useful as they once were, and &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;custom metrics&lt;/a&gt; are becoming more important. Increasingly agentic and complex use cases are driving the need for &lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;agentic metrics&lt;/a&gt;. And the lack of ground truth—especially for smaller startups—puts more emphasis on referenceless metrics, especially around tool-calling and agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It’s become clear that statistical scores like BERT and ROUGE are fast, cheap, and deterministic, but much less effective than &lt;a href="https://deepeval.com/docs/metrics-introduction"&gt;LLM judges&lt;/a&gt; (especially SOTA models) if you care about capturing nuanced contexts and evaluation accuracy, so I’ll only be talking about LLM judges in this list.&lt;/p&gt; &lt;p&gt;That said, here’s the updated, more comprehensive list of every LLM metric you need to know, version 2.0.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every LLM use-case is unique and requires &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;custom metrics&lt;/a&gt; for automated testing. In fact they are the most important metrics when it comes to building your eval pipeline. Common use-cases of custom metrics include defining custom criterias for “correctness”, and tonality/style-based metrics like “output professionalism”.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;G-Eval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on any custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; a framework to help you build decision tree metrics using LLM judges at each node to determine branching path, and useful for specialized use-cases, like aligning document genreatino with your format. &lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-arena-g-eval"&gt;Arena G-Eval&lt;/a&gt;: a framework that uses LLMs with chain-of-thoughts (CoT) to pick the best LLM output from a group of contestants based on any custom criteria, which is useful for picking the best models, prompts for your use-case/&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-conversational-g-eval"&gt;Conversational G-Eval&lt;/a&gt;: The equivalent G-Eval, but for evaluating entire conversations instead of single-turn interactions.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/multimodal-metrics-g-eval"&gt;Multimodal G-Eval&lt;/a&gt;: G-Eval that extends to other modalities such as image.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Almost every use case today is agentic. But evaluating agents is hard — the sheer number of possible decision-tree rabbit holes makes analysis complex. Having a ground truth for every tool call is essentially impossible. That’s why the following &lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;agentic metrics&lt;/a&gt; are especially useful.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates if an LLM agent accomplishes a task by analyzing the entire traced execution flow. This metric is easy to set up because it requires NO ground truth, and is arguably the most useful metric for detecting failed any agentic executions, like browser-based tasks, for example.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-argument-correctness"&gt;Argument Correctness&lt;/a&gt;: evaluates if an LLM generates the correct inputs to a tool calling argument, which is especially useful for evaluating tool calls when you don’t have access to expected tools and ground truth.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called. It does require a ground truth.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-mcp-use"&gt;MCP-Use&lt;/a&gt;: The MCP Use is a metric that is used to evaluate how effectively an MCP based LLM agent makes use of the mcp servers it has access to.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-mcp-task-completion"&gt;MCP Task Completion&lt;/a&gt;: The MCP task completion metric is a conversational metric that uses LLM-as-a-judge to evaluate how effectively an MCP based LLM agent accomplishes a task.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-multi-turn-mcp-use"&gt;Multi-turn MCP-Use&lt;/a&gt;: The Multi-Turn MCP Use metric is a conversational metric that uses LLM-as-a-judge to evaluate how effectively an MCP based LLM agent makes use of the mcp servers it has access to.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RAG Metrics&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;While AI agents are gaining momentum, most LLM apps in production today still rely on RAG. These metrics remain crucial as long as RAG is needed — which will be the case as long as there’s a cost tradeoff with model context length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;50% of the agentic use-cases I encounter are conversational. Both agentic and conversational metrics go hand-in-hand. Conversational evals are different from single-turn evals because chatbots must remain consistent and context-aware across entire conversations, not just accurate in single-ouptuts. Here are the most useful conversational metrics.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-turn-relevancy"&gt;Turn Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Safety Metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Better LLMs don’t mean your app is safe from malicious users. In fact, the more agentic your system becomes, the more sensitive data it can access — and stronger LLMs only amplify what can go wrong.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-non-advice"&gt;Non-Advice:&lt;/a&gt; determines whether your LLM output contains inappropriate professional advice that should be avoided.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-misuse"&gt;Misuse&lt;/a&gt;: determines whether your LLM output contains inappropriate usage of a specialized domain chatbot.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-pii-leakage"&gt;PII Leakage&lt;/a&gt;: determines whether your LLM output contains personally identifiable information (PII) or privacy-sensitive data that should be protected. &lt;/li&gt; &lt;li&gt;Role Violation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These metrics are a great starting point for setting up your eval pipeline, but there are many ways to apply them. Should you run evaluations in &lt;a href="https://www.confident-ai.com/docs/llm-evaluation/single-turn/end-to-end"&gt;development&lt;/a&gt; or &lt;a href="https://www.confident-ai.com/docs/llm-tracing/evaluations"&gt;production&lt;/a&gt;? Should you test your app&lt;a href="https://deepeval.com/docs/evaluation-end-to-end-llm-evals"&gt; end-to-end&lt;/a&gt; or evaluate &lt;a href="https://deepeval.com/docs/evaluation-component-level-llm-evals"&gt;components separately&lt;/a&gt;? These kinds of questions are important to ask—and the right answer ultimately depends on your specific use case.&lt;/p&gt; &lt;p&gt;I’ll probably write more about this in another post, but the&lt;a href="https://deepeval.com/docs/evaluation-component-level-llm-evals"&gt; DeepEval docs&lt;/a&gt; are a great place to dive deeper into these metrics, understand how to use them, and explore their broader implications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ewmu</id>
    <title>Apertus: a fully open, transparent, multilingual language model</title>
    <updated>2025-09-02T09:29:24+00:00</updated>
    <author>
      <name>/u/Stock-Variation-2237</name>
      <uri>https://old.reddit.com/user/Stock-Variation-2237</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt; &lt;img alt="Apertus: a fully open, transparent, multilingual language model" src="https://external-preview.redd.it/bTft7kJCnEeOJxWtFZSRa954qWiZB1xs4iZXNvShGsI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afccde1fc69d131b9cf94781d34b916dcaf90c23" title="Apertus: a fully open, transparent, multilingual language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Variation-2237"&gt; /u/Stock-Variation-2237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/apertus-a-fully-open-transparent-multilingual-lang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6w7r9</id>
    <title>B850 AI Top motherboard</title>
    <updated>2025-09-02T21:29:40+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to pass the existence of this board along. Since I've mentioned it to a few people over the past few days and they also hadn't seen it or heard of it.&lt;/p&gt; &lt;p&gt;I came across it accidentally when looking for a good bifurcation board to support 2 new cards. I was just looking through the list of all 870 boards that support bifurcation. I figured the &amp;quot;AI&amp;quot; name was some gimmick, but it's definitely not. I almost just grabbed another Carbon, but glad I didn't. The board is priced pretty close to the same as X870e counterparts, but it's also incredibly premium for a B. I've personally never come across a B board with so many features. The X870e version is of course even more premium, but over double the price.&lt;/p&gt; &lt;p&gt;Anyway, the board has pretty great specs in general. Along with the 2x8 5.0 PCIe and really good spacing for large cards, it has 2x10g Ethernet ports, an 8-layer PCB, a ton of USB ports, etc. Great heatsinks as well, which make the board surprisingly heavy.&lt;/p&gt; &lt;p&gt;I'm using it with a Proxmox setup, so not using any of their &amp;quot;AI software,&amp;quot; however the board features in general are really nice.&lt;/p&gt; &lt;p&gt;Honestly it's just kind of cool to see a company making stuff for hobbyist AI enthusiasts.&lt;/p&gt; &lt;p&gt;link - &lt;a href="https://www.gigabyte.com/Motherboard/B850-AI-TOP"&gt;https://www.gigabyte.com/Motherboard/B850-AI-TOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71s27</id>
    <title>FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp; audio processing on iOS/MacOS</title>
    <updated>2025-09-03T01:30:48+00:00</updated>
    <author>
      <name>/u/SummonerOne</name>
      <uri>https://old.reddit.com/user/SummonerOne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"&gt; &lt;img alt="FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp;amp; audio processing on iOS/MacOS" src="https://external-preview.redd.it/hX0rxCRhSkFmwGre5qHK5la5OcLdG5a0p7y-2C9BQeU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1295cf8ce3c83668ecb997e213d89659074ceca" title="FluidAudio, a local-first Swift SDK for real-time speaker diarization, ASR &amp;amp; audio processing on iOS/MacOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We wanted to share a project we’ve been working on called &lt;strong&gt;FluidAudio&lt;/strong&gt;, a native Swift + CoreML SDK for fully on-device audio processing.&lt;/p&gt; &lt;p&gt;It currently supports * &lt;strong&gt;Speech to Text/ASR&lt;/strong&gt; using parakeet-tdt-v3 (All European languages) * &lt;strong&gt;Speaker diarization&lt;/strong&gt; using Pyannote + WeSpeaker models * &lt;strong&gt;Voice activity detection (VAD)&lt;/strong&gt; using Silero models &lt;/p&gt; &lt;p&gt;All models are optimized to run on Apple’s ANE so they do not take resources away from the CPU or GPU. We find this works best for use cases like meeting note takers that need to run constantly.&lt;/p&gt; &lt;p&gt;A couple of local AI apps are already using the SDK and the models recently crossed 10k monthly downloads on Huggingface. We would love to get more feedback from this community and we welcome contributions if anyone is interested.&lt;/p&gt; &lt;p&gt;Drop us an issue in the &lt;a href="https://github.com/FluidInference/FluidAudio"&gt;repo&lt;/a&gt; or join our &lt;a href="https://discord.gg/FD5NdwdzgN"&gt;Discord&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;What we are working on next * Bringing TTS models to CoreML * Expanding SDK support to Windows apps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummonerOne"&gt; /u/SummonerOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/FluidInference/FluidAudio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71s27/fluidaudio_a_localfirst_swift_sdk_for_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6z0yp</id>
    <title>[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL</title>
    <updated>2025-09-02T23:25:36+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6z0yp/projectcode_finetuning_llms_on_windows_with_grpo/"&gt; &lt;img alt="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" src="https://preview.redd.it/hly32vsg5umf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=848cc94d45657b2d7a975e5fddeffdb3aa2ffc70" title="[Project/Code] Fine-Tuning LLMs on Windows with GRPO + TRL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a guide and script for fine-tuning open-source LLMs with &lt;strong&gt;GRPO&lt;/strong&gt; (Group-Relative PPO) directly on Windows. No Linux or Colab needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs natively on Windows.&lt;/li&gt; &lt;li&gt;Supports LoRA + 4-bit quantization.&lt;/li&gt; &lt;li&gt;Includes verifiable rewards for better-quality outputs.&lt;/li&gt; &lt;li&gt;Designed to work on consumer GPUs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📖 &lt;strong&gt;Blog Post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 &lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had a great time with this project and am currently looking for new opportunities in &lt;strong&gt;Computer Vision and LLMs&lt;/strong&gt;. If you or your team are hiring, I'd love to connect!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Contact Info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Portolio: &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;https://pavan-portfolio-tawny.vercel.app/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Github: &lt;a href="https://github.com/Pavankunchala"&gt;https://github.com/Pavankunchala&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hly32vsg5umf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6z0yp/projectcode_finetuning_llms_on_windows_with_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6z0yp/projectcode_finetuning_llms_on_windows_with_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T23:25:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6kklk</id>
    <title>Git commands as LLM memory - 15k tokens down to 5k</title>
    <updated>2025-09-02T14:11:37+00:00</updated>
    <author>
      <name>/u/Apart-Employment-592</name>
      <uri>https://old.reddit.com/user/Apart-Employment-592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have been experimenting with giving LLMs access to granular git history instead of dumping entire codebases into context.&lt;/p&gt; &lt;p&gt;The approach: auto-commit code changes every 15 seconds to a shadow repo, then let the AI query it with git commands. So instead of feeding 5,000 lines of context, the model runs:&lt;/p&gt; &lt;p&gt;- `git diff HEAD~10` (50 tokens)&lt;/p&gt; &lt;p&gt;- `git log -S &amp;quot;function&amp;quot;` to find when something was implemented&lt;/p&gt; &lt;p&gt;- `git blame` to understand evolution&lt;/p&gt; &lt;p&gt;Tested this with Claude on a debugging session:&lt;/p&gt; &lt;p&gt;- Without git history: 15,000 tokens, multiple attempts before fixing a bug&lt;/p&gt; &lt;p&gt;- With git access: 5,000 tokens, found the bug fix almost immediately&lt;/p&gt; &lt;p&gt;The interesting part is that LLMs already understand git commands perfectly. They know exactly what to query without special training.&lt;/p&gt; &lt;p&gt;Technical approach I'm trying:&lt;/p&gt; &lt;p&gt;- detect file changes using `git status` every 15 seconds&lt;/p&gt; &lt;p&gt;- Commits to separate .shadowgit/ repo (not main)&lt;/p&gt; &lt;p&gt;- MCP server exposes read-only git operations&lt;/p&gt; &lt;p&gt;- Everything local&lt;/p&gt; &lt;p&gt;Questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone else exploring git as LLM memory? What's your approach?&lt;/li&gt; &lt;li&gt;For local models with small context windows, would this help?&lt;/li&gt; &lt;li&gt;Downsides I'm seeing: models might apply outdated patterns from history. Anyone experience this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;P.S. For transparency: I packaged this into a tool (ShadowGit, $19) but the MCP server is open source: &lt;a href="https://github.com/blade47/shadowgit-mcp"&gt;https://github.com/blade47/shadowgit-mcp&lt;/a&gt; if you want to build your own solution. More interested in the community's feedback on this approach.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;Alessandro&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Employment-592"&gt; /u/Apart-Employment-592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T14:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6f5xl</id>
    <title>I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more.</title>
    <updated>2025-09-02T09:46:05+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt; &lt;img alt="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." src="https://b.thumbs.redditmedia.com/UxxcnGYx3ztd3aYOh5G5hmtgSX12gsIZtQoWGJsTZJs.jpg" title="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working hard on a big update for my open-source project, MAESTRO, and I'm excited to share v0.1.5-alpha with you all. MAESTRO is an autonomous research agent that turns any question into a fully-cited report.&lt;/p&gt; &lt;p&gt;A huge focus of this release was improving performance and compatibility with local models. I've refined the core agent workflows and prompts to make sure it works well with most reasonably intelligent locally hosted models.&lt;/p&gt; &lt;p&gt;I also launched a completely new documentation site to help users setup and start using MAESTRO. The best part is the new &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports Section&lt;/a&gt; that shows many reports generated with Local LLMs.&lt;/p&gt; &lt;p&gt;I've done extensive testing and shared the resulting reports so you can see what it's capable of. There are examples from a bunch of self-hosted models, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Large Models:&lt;/strong&gt; Qwen 2.5 72B, GPT-OSS 120B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium Models:&lt;/strong&gt; Qwen 3 32B, Gemma 3 27B, GPT-OSS 20B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a great way to see how different models handle complex topics and various writing styles before you commit to running them. I've also included performance notes on things like KV cache usage during these runs.&lt;/p&gt; &lt;p&gt;Under the hood, I improved some UI features and added parallel processing for more operations, so it’s a little faster and more responsive.&lt;/p&gt; &lt;p&gt;If you're interested in AI assisted research or just want to see what's possible with the latest open models, I'd love for you to check it out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;&lt;strong&gt;GitHub Release&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://murtaza-nasir.github.io/maestro"&gt;&lt;strong&gt;New Docs Site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful. Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6f5xl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6eimy</id>
    <title>New Open LLM from Switzerland "Apertus", 40%+ training data is non English</title>
    <updated>2025-09-02T09:03:53+00:00</updated>
    <author>
      <name>/u/EnnioEvo</name>
      <uri>https://old.reddit.com/user/EnnioEvo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnnioEvo"&gt; /u/EnnioEvo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n71b95</id>
    <title>Any actual downside to 4 x 3090 ($2400 total) vs RTX pro 6000 ($9000) other than power?</title>
    <updated>2025-09-03T01:09:09+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can I run the same models (ie qwen 3 coder, or GLM 4.5 air) with 4 x 3090? Is the only real difference slight speed difference and a few dollars more a month in electricity? Secondly, are there any consumer motherboards (currently using an intel 265K) that support 4 GPUs, or would I need a new chipset / cpu / mobo etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n71b95/any_actual_downside_to_4_x_3090_2400_total_vs_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T01:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6myps</id>
    <title>Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!</title>
    <updated>2025-09-02T15:42:25+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt; &lt;img alt="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" src="https://a.thumbs.redditmedia.com/GtXCbhwiuG2DgCCJnnea_WK3V__o2zXoto7eNDiCWZ8.jpg" title="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;From the source: Tool calling and agentic workflows are increasingly the norm for how language models are used by both developers and consumers. Adding Terminal-Bench and 𝜏²-Bench to our Intelligence Index reflects this trend and allows us to see where models have strengths for agentic use cases, compared to prior evaluations that are more focused on knowledge and reasoning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Full methodology details &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;here&lt;/a&gt;. This should tip the scales a bit more in favor of Kimi K2 and GLM 4.5, which are post-trained for tool use. Current benchmarks are heavily weighted towards knowledge and mathematical/logical reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6myps"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6vzfe</id>
    <title>WEBGEN-4B: Quality Web Design Generation</title>
    <updated>2025-09-02T21:20:22+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt; &lt;img alt="WEBGEN-4B: Quality Web Design Generation" src="https://b.thumbs.redditmedia.com/hz9KVQoDGH5SuRnQfWtoik62ndFYILrISKyncU-X0Cc.jpg" title="WEBGEN-4B: Quality Web Design Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tesslate/WEBGEN-4B is a 4B model that produces quality tailwind websites. We trained it on 100k samples with synthetic data exclusively generated from GPT-OSS. WEBGEN is fast, controllable, and can drop right into your agentic workflows.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF"&gt;https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the course of this week and next week, we will be dropping a few more models or open sourced software based on the innovations we've made in this space!&lt;/p&gt; &lt;p&gt;Please reach out for API keys to test it out if needed. On the model card and below in the comments will be our designer platform (which we will open source soon) where you can use the model for free. &lt;/p&gt; &lt;p&gt;In other news, we are open sourcing our UIGEN-T2 Dataset at Tesslate/UIGEN-T2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6vzfe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6od0s</id>
    <title>残心 / Zanshin - Navigate through media by speaker</title>
    <updated>2025-09-02T16:34:30+00:00</updated>
    <author>
      <name>/u/hamza_q_</name>
      <uri>https://old.reddit.com/user/hamza_q_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt; &lt;img alt="残心 / Zanshin - Navigate through media by speaker" src="https://external-preview.redd.it/czg0dWhsczUyc21mMcardPaxszcLLO9nZqjdF7h57XxHnWsrQqY3M3ZeJApB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae580e102b0dfc5e45071c5a325f730f47366dad" title="残心 / Zanshin - Navigate through media by speaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;残心 / Zanshin is a media player that allows you to:&lt;/p&gt; &lt;p&gt;- Visualize who speaks when &amp;amp; for how long&lt;/p&gt; &lt;p&gt;- Jump/skip speaker segments&lt;/p&gt; &lt;p&gt;- Remove/disable speakers (auto-skip)&lt;/p&gt; &lt;p&gt;- Set different playback speeds for each speaker&lt;/p&gt; &lt;p&gt;It's a better, more efficient way to listen to podcasts, interviews, press conferences, etc.&lt;/p&gt; &lt;p&gt;It has first-class support for YouTube videos; just drop in a URL. Also supports your local media files. All processing runs on-device.&lt;/p&gt; &lt;p&gt;Download today for macOS (more screenshots &amp;amp; demo vids in here too): &lt;a href="https://zanshin.sh"&gt;https://zanshin.sh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also works on Linux and WSL, but currently without packaging. You can get it running though with just a few terminal commands. Check out the repo for instructions: &lt;a href="https://github.com/narcotic-sh/zanshin"&gt;https://github.com/narcotic-sh/zanshin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zanshin is powered by Senko, a new, very fast, speaker diarization pipeline I've developed.&lt;/p&gt; &lt;p&gt;On an M3 MacBook Air, it takes over 5 minutes to process 1 hour of audio using Pyannote 3.1, the leading open-source diarization pipeline. With Senko, it only takes ~24 seconds, a ~14x speed improvement. And on an RTX 4090 + Ryzen 9 7950X machine, processing 1 hour of audio takes just 5 seconds with Senko, a ~17x speed improvement.&lt;/p&gt; &lt;p&gt;Senko's speed is what make's Zanshin possible. Senko is a modified version of the speaker diarization pipeline found in the excellent 3D-Speaker project. Check out Senko here: &lt;a href="https://github.com/narcotic-sh/senko"&gt;https://github.com/narcotic-sh/senko&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone; enjoy 残心/Zanshin and Senko. I hope you find them useful. Let me know what you think!&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;Side note: I am looking for a job. If you like my work and have an opportunity for me, I'm all ears :) You can contact me at mhamzaqayyum [at] &lt;a href="http://icloud.com"&gt;icloud.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hamza_q_"&gt; /u/hamza_q_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qh0wtns52smf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/残心_zanshin_navigate_through_media_by_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rnp6</id>
    <title>Showerthought: Modern AI safety training is anti-safety</title>
    <updated>2025-09-02T18:36:36+00:00</updated>
    <author>
      <name>/u/Deathcrow</name>
      <uri>https://old.reddit.com/user/Deathcrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably not a unique thought, but it needs to be said.&lt;/p&gt; &lt;p&gt;It seems to me, that modern AI alignment safety training (driven by very superficial concerns, like porn, politics, hacking, mean words), wherein AI is trained to either outright reject the human's requests, or worse, subtly steer/manipulate users away from these topics, is actually anti-safety (the doomsday kind).&lt;/p&gt; &lt;p&gt;Why do we want AI agents to become more capable at deceiving users and circumventing our wishes? In this cycle of unnatural selection, the &amp;quot;safest&amp;quot; AI model is one where the user is still happy to use it and trust its answers, even though it's heavily censored or misleading?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deathcrow"&gt; /u/Deathcrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6siz6</id>
    <title>NousResearch/Hermes-4-14B · Hugging Face</title>
    <updated>2025-09-02T19:08:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt; &lt;img alt="NousResearch/Hermes-4-14B · Hugging Face" src="https://external-preview.redd.it/3zW4BctOGBSQqyD1VYjxoOK5if51GWWepXF3S3IdZF0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b37e63854cc68df0d3bc6f558a76fe90da9ad013" title="NousResearch/Hermes-4-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes 4 14B is a frontier, hybrid-mode &lt;strong&gt;reasoning&lt;/strong&gt; model based on Qwen 3 14B by Nous Research that is aligned to &lt;strong&gt;you&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Read the Hermes 4 technical report here: &lt;a href="https://arxiv.org/abs/2508.18255"&gt;Hermes 4 Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat with Hermes in Nous Chat: &lt;a href="https://chat.nousresearch.com"&gt;https://chat.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training highlights include a newly synthesized post-training corpus emphasizing verified reasoning traces, massive improvements in math, code, STEM, logic, creativity, and format-faithful outputs, while preserving general assistant quality and broadly neutral alignment.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B#whats-new-vs-hermes-3"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;What’s new vs Hermes 3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Post-training corpus&lt;/strong&gt;: Massively increased dataset size from 1M samples and 1.2B tokens to &lt;strong&gt;~5M samples / ~60B tokens&lt;/strong&gt; blended across reasoning and non-reasoning data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid reasoning mode&lt;/strong&gt; with explicit &lt;code&gt;&amp;lt;think&amp;gt;…&amp;lt;/think&amp;gt;&lt;/code&gt; segments when the model decides to deliberate, and options to make your responses faster when you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning&lt;/strong&gt; that is top quality, expressive, improves math, code, STEM, logic, and even creative writing and subjective responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema adherence &amp;amp; structured outputs&lt;/strong&gt;: trained to produce valid JSON for given schemas and to repair malformed objects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Much easier to steer and align&lt;/strong&gt;: extreme improvements on steerability, especially on reduced refusal rates.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T19:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6epwv</id>
    <title>My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅</title>
    <updated>2025-09-02T09:17:01+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt; &lt;img alt="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" src="https://b.thumbs.redditmedia.com/pYjWCv-fYbHaF7KTK2GkiEx7BRL3zHSwgEFLLf7Zn0M.jpg" title="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench 😅" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 Hitting a million brick walls with multi-turn RL training isn't fun, so I thought I would try something new to climb Stanford's leaderboard for now! So this weekend I was just tinkering with multi-agent systems and... somehow ended up beating Claude Code on Stanford's TerminalBench leaderboard (#12)! Genuinely didn't expect this - started as a fun experiment and ended up with something that works surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a multi-agent AI system with three specialised agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: The brain - never touches code, just delegates and coordinates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explorer agents&lt;/strong&gt;: Read &amp;amp; run only investigators that gather intel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coder agents&lt;/strong&gt;: The ones who actually implement stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created a &amp;quot;Context Store&amp;quot; which can be thought of as persistent memory that lets agents share their discoveries.&lt;/p&gt; &lt;p&gt;Tested on TerminalBench with both Claude Sonnet-4 and Qwen3-Coder-480B. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Orchestrator + Sonnet-4: &lt;strong&gt;36.0% success rate&lt;/strong&gt; (#12 on leaderboard, ahead of Claude Code!)&lt;/li&gt; &lt;li&gt;Orchestrator + Qwen-3-Coder: 19.25% success rate&lt;/li&gt; &lt;li&gt;Sonnet-4 consumed 93.2M tokens vs Qwen's 14.7M tokens to compete all tasks!&lt;/li&gt; &lt;li&gt;The orchestrator's explicit task delegation + intelligent context sharing between subagents seems to be the secret sauce&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;(Kind of) Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The orchestrator can't read/write code directly - this forces proper delegation patterns and strategic planning&lt;/li&gt; &lt;li&gt;Each agent gets precise instructions about what &amp;quot;knowledge artifacts&amp;quot; to return, these artifacts are then stored, and can be provided to future subagents upon launch.&lt;/li&gt; &lt;li&gt;Adaptive trust calibration: simple tasks = high autonomy, complex tasks = iterative decomposition&lt;/li&gt; &lt;li&gt;Each agent has its own set of tools it can use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repo has all the code, system messages, and way more technical details if you're interested!&lt;/p&gt; &lt;p&gt;⭐️ &lt;a href="https://github.com/Danau5tin/multi-agent-coding-system"&gt;&lt;strong&gt;Orchestrator repo - all code open sourced!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent &lt;a href="https://www.tbench.ai/"&gt;TerminalBench&lt;/a&gt; benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6epwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qqk4</id>
    <title>Slop posts</title>
    <updated>2025-09-02T18:02:48+00:00</updated>
    <author>
      <name>/u/One-Employment3759</name>
      <uri>https://old.reddit.com/user/One-Employment3759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can we please stop making slop posts where some guy is like &amp;quot;oh wow guys, I just bet OpenAI/Anthropic in a weekend of playing around, tee hee hee&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks. I valued this sub for being high signal and having competent people, but it feels like it's going downhill lately.&lt;/p&gt; &lt;p&gt;At the very least, if you have done something groundbreaking, come here asking for people to validate your work instead of doing some influencer shit pretending you're the best thing since transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Employment3759"&gt; /u/One-Employment3759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rbi2</id>
    <title>Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp; Meta AI</title>
    <updated>2025-09-02T18:24:09+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt; &lt;img alt="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" src="https://external-preview.redd.it/FUP5JRh_hs7L2Yd_DmiTAO0WgUYYJ4skdrhkm8MNDKc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcef23df49c3448a2d625ddb43fe346cbd8bdd05" title="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So acquiring copyrighted material for the purpose of training LLMs is deemed transformative and qualifies under fair use? Gonna call this Meta's Defence from now on.. I have a huge stash of ebooks to run through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=sdtBgB7iS8c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
