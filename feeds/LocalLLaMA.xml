<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-19T21:05:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mum1fb</id>
    <title>gpt-oss-20b-surviveV1</title>
    <updated>2025-08-19T15:41:28+00:00</updated>
    <author>
      <name>/u/lolzinventor</name>
      <uri>https://old.reddit.com/user/lolzinventor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A fine-tuned version of GPT-OSS-20B specifically for survival-related discussions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/lolzinventor/gpt-oss-surviveV1"&gt;https://huggingface.co/lolzinventor/gpt-oss-surviveV1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key findings:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Still provides refusals on controversial topics&lt;/li&gt; &lt;li&gt;More willing to discuss survival scenarios&lt;/li&gt; &lt;li&gt;Sometimes &amp;quot;trips over its own thinking&amp;quot; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolzinventor"&gt; /u/lolzinventor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mum1fb/gptoss20bsurvivev1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mum1fb/gptoss20bsurvivev1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mum1fb/gptoss20bsurvivev1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1munr40</id>
    <title>Added Emotional Reactions to My Chatbot — Here’s How It Looks</title>
    <updated>2025-08-19T16:42:47+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munr40/added_emotional_reactions_to_my_chatbot_heres_how/"&gt; &lt;img alt="Added Emotional Reactions to My Chatbot — Here’s How It Looks" src="https://preview.redd.it/z3jjvs7t80kf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=6dd61a117d5db7d6dff604e8f5181a15a960b55e" title="Added Emotional Reactions to My Chatbot — Here’s How It Looks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building my own AI chatbot platform solo, and just added a fun new feature - prompts that can dynamically change a character’s emotions. &lt;/p&gt; &lt;p&gt;In the GIF, you’ll see a chat where the character’s avatar changes from neutral → surprised → happy depending on the flow of the conversation. &lt;/p&gt; &lt;p&gt;This opens up a lot of possibilities for roleplay, interactive stories, and more immersive chats. &lt;/p&gt; &lt;p&gt;Right now, the platform is in open testing with unlimited free tokens and local LLM support - I’m gathering feedback and ideas from early users. &lt;/p&gt; &lt;p&gt;If you want to try it out, send me a DM for an invite!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z3jjvs7t80kf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munr40/added_emotional_reactions_to_my_chatbot_heres_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munr40/added_emotional_reactions_to_my_chatbot_heres_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mutroq</id>
    <title>brute-llama - A llama.cpp llama-server testbench</title>
    <updated>2025-08-19T20:18:07+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"&gt; &lt;img alt="brute-llama - A llama.cpp llama-server testbench" src="https://external-preview.redd.it/0kAodq2vpgwlNs8NkvpJTL0EKKMaEMge_I7TkUpawwA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69c304bdff4294f7d2c5e01531d320ab09fffbe4" title="brute-llama - A llama.cpp llama-server testbench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I introduce brute-llama. I needed a tool to sweep through llama-server parameters and options to brute force good configs and find anomalies. Here it is. It does nothing special. It just runs llama-server and checks in nested loops and plots results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/crashr/brute-llama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mutroq/brutellama_a_llamacpp_llamaserver_testbench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T20:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1muso0a</id>
    <title>Follow-up: Looking for a local RAG + chatbot solution for our machine manual</title>
    <updated>2025-08-19T19:38:25+00:00</updated>
    <author>
      <name>/u/ReserveOdd1984</name>
      <uri>https://old.reddit.com/user/ReserveOdd1984</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqwrry/how_can_we_train_an_opensource_ai_model_with_a/"&gt;previous post&lt;/a&gt;, many suggested that I should use a &lt;strong&gt;RAG agent&lt;/strong&gt; for our machine manual AI project. After doing some research, I couldn’t find a good &lt;strong&gt;prebuilt solution&lt;/strong&gt; that fits our requirements.&lt;br /&gt; &lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent should be able to &lt;strong&gt;read text, tables, images/layouts&lt;/strong&gt; from a PDF(760 pages pdf / 70 MB file).&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;chatbot UI&lt;/strong&gt; (not just a CLI).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completely local deployment&lt;/strong&gt; (no cloud).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Currently testing on my PC (i5 12400f, 32GB RAM, RX6500 — AMD GPU so no AI acceleration).&lt;/li&gt; &lt;li&gt;Field PCs will have better specs (likely i5/i7 12th gen + RTX 3070 or similar).&lt;/li&gt; &lt;li&gt;So we want a solution that allows us to change/upgrade the LLM model later depending on the hardware.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’ve tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AnythingLLM&lt;/strong&gt; → couldn’t get proper answers from the document.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NotebookLM&lt;/strong&gt; → works well, but can’t deploy locally.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;About me:&lt;/strong&gt;&lt;br /&gt; I’m a full stack developer (mostly Node.js). I don’t have much prior experience with Python, but I can adapt if needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are there any &lt;strong&gt;prebuilt/local RAG + chatbot frameworks&lt;/strong&gt; you’d recommend that meet the requirements?&lt;/li&gt; &lt;li&gt;If not, what’s the easiest starting point for someone coming from a full-stack (Node.js) background?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is &lt;a href="https://github.com/MIbnEKhalid/Field-RagAgent/blob/main/manual_sample.pdf"&gt;PDF Sample&lt;/a&gt; with some images/layouts and tables.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReserveOdd1984"&gt; /u/ReserveOdd1984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muso0a/followup_looking_for_a_local_rag_chatbot_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf6ry</id>
    <title>NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</title>
    <updated>2025-08-19T11:00:19+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt; &lt;img alt="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" src="https://external-preview.redd.it/ojIYaD1O8xYRW9Q-A7BHJQx5N3b1m-3M6OVRuLj2lzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfc0130326f6be10fdb5d6657d21b292210a99b2" title="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/NextStep-1-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu15vr</id>
    <title>bilbo.high.reasoning.medium.mini.3lightbulbs.ultra</title>
    <updated>2025-08-18T22:47:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt; &lt;img alt="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" src="https://preview.redd.it/bfdlovjpvujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b521d9b5416a36368655d8645cd92560559169e" title="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfdlovjpvujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvgjx</id>
    <title>NVIDIA Releases Nemotron Nano 2 AI Models</title>
    <updated>2025-08-18T19:12:01+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt; &lt;img alt="NVIDIA Releases Nemotron Nano 2 AI Models" src="https://preview.redd.it/pzrpnuykutjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61af91b9dcdda4649c24e581ac3941490ab82c0" title="NVIDIA Releases Nemotron Nano 2 AI Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• 6X faster than similarly sized models, while also being more accurate&lt;/p&gt; &lt;p&gt;• NVIDIA is also releasing most of the data they used to create it, including the pretraining corpus&lt;/p&gt; &lt;p&gt;• The hybrid Mamba-Transformer architecture supports 128K context length on single GPU.&lt;/p&gt; &lt;p&gt;Full research paper here: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzrpnuykutjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua1k4</id>
    <title>GPT OSS quality on Nebius - fixed (update)</title>
    <updated>2025-08-19T05:47:34+00:00</updated>
    <author>
      <name>/u/ai_devrel_eng</name>
      <uri>https://old.reddit.com/user/ai_devrel_eng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt; &lt;img alt="GPT OSS quality on Nebius - fixed (update)" src="https://b.thumbs.redditmedia.com/sK7Bm1tG5gwQQ5s7xz2h10LEJrqGwQrDc2Udh4wNqoE.jpg" title="GPT OSS quality on Nebius - fixed (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_devrel_eng"&gt; /u/ai_devrel_eng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mua1k4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1munrls</id>
    <title>azzurra-voice is a new State-of-the-Art Italian Text-to-Speech model</title>
    <updated>2025-08-19T16:43:16+00:00</updated>
    <author>
      <name>/u/poppear</name>
      <uri>https://old.reddit.com/user/poppear</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poppear"&gt; /u/poppear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.cartesia.one/posts/introducing-azzurra-voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munrls/azzurravoice_is_a_new_stateoftheart_italian/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munrls/azzurravoice_is_a_new_stateoftheart_italian/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>🚀 Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="🚀 Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="🚀 Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;✨ Key Features&lt;/p&gt; &lt;p&gt;✅ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;✅ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;✅ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mulnzj</id>
    <title>Google is also untrustworthy</title>
    <updated>2025-08-19T15:28:08+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt; &lt;img alt="Google is also untrustworthy" src="https://preview.redd.it/exvjbuqdvzjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62aedddee4f39d51757d9a1dfbf650ced3057d7b" title="Google is also untrustworthy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/exvjbuqdvzjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1muraxw</id>
    <title>GPT-oss performs like Llama 4 Maverick on Fiction.liveBench</title>
    <updated>2025-08-19T18:49:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"&gt; &lt;img alt="GPT-oss performs like Llama 4 Maverick on Fiction.liveBench" src="https://preview.redd.it/r6tk8zj6v0kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cca6225ef2c5dac7e1fad67f6f973645d78b197c" title="GPT-oss performs like Llama 4 Maverick on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6tk8zj6v0kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muraxw/gptoss_performs_like_llama_4_maverick_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:49:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq5bv</id>
    <title>With the rising trends of finetuning small language model, data engineering will be needed even more.</title>
    <updated>2025-08-19T18:08:10+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're seeing a flood of compact language models hitting the market weekly - Gemma3 270M, LFM2 1.2B, SmolLM3 3B, and many others. The pattern is always the same: organizations release these models with a disclaimer essentially saying &amp;quot;this performs poorly out-of-the-box, but fine-tune it for your specific use case and watch it shine.&amp;quot;&lt;/p&gt; &lt;p&gt;I believe we're witnessing the beginning of a major shift in AI adoption. Instead of relying on massive general-purpose models, companies will increasingly fine-tune these lightweight models into specialized agents for their particular needs. The economics are compelling - these small models are significantly cheaper to train, deploy, and operate compared to their larger counterparts, making AI accessible to businesses with tighter budgets.&lt;/p&gt; &lt;p&gt;This creates a huge opportunity for data engineers, who will become crucial in curating the right training datasets for each domain. The lower operational costs mean more companies can afford to experiment with custom AI solutions.&lt;/p&gt; &lt;p&gt;This got me thinking: what does high-quality training data actually look like for different industries when building these task-specific AI agents? Let's break down what effective agentic training data might contain across various sectors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discussion starter:&lt;/strong&gt; What industries do you think will benefit most from this approach, and what unique data challenges might each sector face?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq5bv/with_the_rising_trends_of_finetuning_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1muslis</id>
    <title>I tried to get 600 dollars "deep think" for local models by making them argue with each other for hours. It's slow, but it's interesting</title>
    <updated>2025-08-19T19:35:53+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking a lot about how we, as people, develop ideas. It's rarely a single, brilliant flash of insight. Our minds are shaped by the countless small interactions we have throughout the day—a conversation here, an article there. This environment of constant, varied input seems just as important as the act of thinking itself.&lt;/p&gt; &lt;p&gt;I wanted to see if I could recreate a small-scale version of that &amp;quot;soup&amp;quot; required for true insight, for local LLMs. The result is a project I'm calling &lt;strong&gt;Network of Agents (NoA)&lt;/strong&gt;, and I wanted to share it with you all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/NoA"&gt;Full README &lt;/a&gt;&lt;/p&gt; &lt;p&gt;The core idea is to treat AI agents like individual neurons in a larger network. You give the network a difficult problem, and a whole team of agents, each with a different &amp;quot;personality&amp;quot; and skillset, starts working on it. They pass their ideas from one layer to the next, building on each other's work to come up with a final, combined solution.&lt;/p&gt; &lt;p&gt;Here’s the part that I'm most curious about. I was inspired by the concept of backpropagation in neural networks. It's a numerical algorithm, of course, but I wondered if the core principle could be applied qualitatively. What if, instead of sending back a numerical error signal, you sent back a &amp;quot;reflection&amp;quot;? &lt;/p&gt; &lt;p&gt;After the network produces a solution, a &amp;quot;critique&amp;quot; agent reviews it and provides criticism. This feedback is then used to automatically re-write the core system prompts of the agents that contributed. The goal is for the network to &amp;quot;learn&amp;quot; from its mistakes over multiple cycles, refining not just its answers, but its own internal structure and approach.&lt;/p&gt; &lt;p&gt;The whole thing is designed to run locally on modest hardware. I've been running it on my laptop with the streets local legend (qwen 30b a3b 2507 instruct lol). It allows the machine to just sit and &amp;quot;think&amp;quot; about a problem for a very long time. The algorithm does really well in problems where creativity and insight override pure precision. It can come up with new frameworks for the social sciences for instance. Physics and math not so much. Looking into opensource gemini-cli to give each &amp;quot;neuron&amp;quot; an execution environment so it can code, but that would be future tense. It certainly adds a lot more complexity.&lt;/p&gt; &lt;p&gt;The obvious trade-off here is speed. It’s the opposite of instantaneous. A 6-layer network with 6 agents per layer, running for 20 cycles, can easily take 12 hours to complete. You're trading quick computation for a slow, iterative process of refinement. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is where I'd love to get some community input.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My long-term vision is to go beyond a single machine. I dream of building a P2P networking layer that would allow multiple users to connect their instances of the micro-app. We could create a shared, distributed network where our machines could collaborate to tackle truly massive problems.&lt;/p&gt; &lt;p&gt;However, my background is in Python and AI, and I'm not an expert in distributed systems. &lt;strong&gt;If you're someone who knows about peer-to-peer networking and this idea sounds at all interesting to you, I would genuinely love to hear from you and potentially collaborate.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It’s an open-source experiment, and I’d be grateful for any thoughts, feedback, or ideas you might have.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muslis/i_tried_to_get_600_dollars_deep_think_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T19:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mueqhs</id>
    <title>When will low-cost Chinese GPUs hit the market?</title>
    <updated>2025-08-19T10:35:22+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard of some Chinese GPUs, but I'm curious when they'll release low-cost alternatives that can seriously challenge NVIDIA 50xx dominance. Are there any indications that this will happen anytime soon? I'd love the hardware equivalent of a &amp;quot;deepseek moment&amp;quot; for OpenAI earlier this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumpub</id>
    <title>Generating code with gpt-oss-120b on Strix Halo with ROCm</title>
    <updated>2025-08-19T16:05:34+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt; &lt;img alt="Generating code with gpt-oss-120b on Strix Halo with ROCm" src="https://external-preview.redd.it/MTBtNjc4d2sxMGtmMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae351c6a5432420dd53119df551944d7d45bc802" title="Generating code with gpt-oss-120b on Strix Halo with ROCm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen a few posts asking about how to get gpt-oss models running on AMD devices. This guide gives a quick 3-minute overview of how it works on Strix Halo (Ryzen AI MAX 395).&lt;/p&gt; &lt;p&gt;The same steps work for gpt-oss-20b, and many other models, on Radeon 7000/9000 GPUs as well.&lt;/p&gt; &lt;h2&gt;Detailed Instructions&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install and run Lemonade from the GitHub &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; in your browser and open the Model Manager&lt;/li&gt; &lt;li&gt;Click the download button on gpt-oss-120b. Go find something else to do while it downloads ~60 GB.&lt;/li&gt; &lt;li&gt;Launch Lemonade Server in ROCm mode &lt;ul&gt; &lt;li&gt;&lt;code&gt;lemonade-server server --llamacpp rocm&lt;/code&gt; (Windows GUI installation)&lt;/li&gt; &lt;li&gt;&lt;code&gt;lemonade-server-dev server --llamacpp rocm&lt;/code&gt; (Linux/Windows pypi/source installation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the steps in the Continue + Lemonade setup guide to start generating code: &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;https://lemonade-server.ai/docs/server/apps/continue/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Need help? Find the team on Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for checking this out, hope it was helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pnap0vvk10kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mul4sx</id>
    <title>Deepseek-V3.1-Base released</title>
    <updated>2025-08-19T15:09:09+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt; &lt;img alt="Deepseek-V3.1-Base released" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="Deepseek-V3.1-Base released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1muqvcj</id>
    <title>Nvidia charged with patent infringement for DGX technology.</title>
    <updated>2025-08-19T18:34:16+00:00</updated>
    <author>
      <name>/u/Red_Phoenix_69</name>
      <uri>https://old.reddit.com/user/Red_Phoenix_69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt; &lt;img alt="Nvidia charged with patent infringement for DGX technology." src="https://external-preview.redd.it/0ADfQU54uUeiTQ2u5_D5DC4J8Vcm-QU6r2UvKAucrBk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91170ebd7534ade69e4c05a4e9a92ae6429143ff" title="Nvidia charged with patent infringement for DGX technology." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will the DGX spark ever launch? Maybe Nvidia can buy out this company. Is it time to just buy a AMD AI395 clone or Apple M5 chip mac mini for desktop development running LLM's locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Phoenix_69"&gt; /u/Red_Phoenix_69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techzine.eu/news/infrastructure/133818/nvidia-under-fire-german-patent-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muqvcj/nvidia_charged_with_patent_infringement_for_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1munvj6</id>
    <title>The new design in DeepSeek V3.1</title>
    <updated>2025-08-19T16:47:11+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just pulled the &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;V3.1-Base&lt;/a&gt; configs and compared to V3-Base&lt;br /&gt; They add four new special tokens&lt;br /&gt; &amp;lt;｜search▁begin｜&amp;gt; (id: 128796)&lt;br /&gt; &amp;lt;｜search▁end｜&amp;gt; (id: 128797)&lt;br /&gt; &amp;lt;think&amp;gt; (id: 128798)&lt;br /&gt; &amp;lt;/think&amp;gt; (id: 128799)&lt;br /&gt; And I noticed that V3.1 on the web version actively searches even when the search button is turned off, unless explicitly instructed &amp;quot;do not search&amp;quot; in the prompt.&lt;br /&gt; would this be related to the design of the special tokens mentioned above?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1munvj6/the_new_design_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1muq72y</id>
    <title>Deepseek v3.1 scores 71.6% on aider – non-reasoning sota</title>
    <updated>2025-08-19T18:09:56+00:00</updated>
    <author>
      <name>/u/Similar-Cycle8413</name>
      <uri>https://old.reddit.com/user/Similar-Cycle8413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` - dirname: 2025-08-19-17-08-33--deepseek-v3.1 test_cases: 225 model: deepseek/deepseek-chat edit_format: diff commit_hash: 32faf82 pass_rate_1: 41.3 pass_rate_2: 71.6 pass_num_1: 93 pass_num_2: 161 percent_cases_well_formed: 95.6 error_outputs: 13 num_malformed_responses: 11 num_with_malformed_responses: 10 user_asks: 63 lazy_comments: 0 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 1 prompt_tokens: 2239930 completion_tokens: 551692 test_timeouts: 8 total_tests: 225 command: aider --model deepseek/deepseek-chat date: 2025-08-19 versions: 0.86.2.dev seconds_per_case: 134.0 total_cost: 1.0112&lt;/p&gt; &lt;p&gt;costs: $0.0045/test-case, $1.01 total, $1.01 projected ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Cycle8413"&gt; /u/Similar-Cycle8413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muq72y/deepseek_v31_scores_716_on_aider_nonreasoning_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukwq6</id>
    <title>🤗 DeepSeek-V3.1-Base</title>
    <updated>2025-08-19T15:01:07+00:00</updated>
    <author>
      <name>/u/newsletternew</name>
      <uri>https://old.reddit.com/user/newsletternew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v3.1 base model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsletternew"&gt; /u/newsletternew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumext</id>
    <title>Tried mixing local LLM + face recognition just for fun (wild results)</title>
    <updated>2025-08-19T15:54:45+00:00</updated>
    <author>
      <name>/u/yeahiiiiiii</name>
      <uri>https://old.reddit.com/user/yeahiiiiiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’ve been tinkering a lot with running models locally (mostly LLaMA variants + some vision stuff). I like keeping things offline when possible, just feels better knowing my data isn’t flying around random servers.&lt;/p&gt; &lt;p&gt;Over the weekend I got curious… what if I combine face matching with a local LLM? Like, have the LLM explain what it “thinks” about a person from a pic (just descriptive, nothing deep), and then use a tool to see if that face exists online.&lt;/p&gt; &lt;p&gt;I played around with this app I came across called Faceseek – it’s basically a face search tool. Not local tho, so I only tested it with some older selfies + public pics I already had floating around. Honestly the results shocked me. It matched one of my 2016 Facebook photos to some random forum post I forgot even existed. Crazy how well it pulled that up.&lt;/p&gt; &lt;p&gt;I didn’t hook it fully with my local LLM setup yet, but the idea of pairing recognition with reasoning feels like where a lot of this stuff is headed. Imagine running all that fully offline though – no cloud, no leaks, just you + your box.&lt;/p&gt; &lt;p&gt;Anyway, this got me thinking… has anyone here tried doing something similar? Like combining local models with external recognition tools? Do you think we’ll eventually get a fully local version of this (face search + reasoning) or is that still years away?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeahiiiiiii"&gt; /u/yeahiiiiiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1murf0u</id>
    <title>Can't be the only one who finds this funny</title>
    <updated>2025-08-19T18:53:25+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1murf0u/cant_be_the_only_one_who_finds_this_funny/"&gt; &lt;img alt="Can't be the only one who finds this funny" src="https://preview.redd.it/m98jn9stu0kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e4404c90cc2104b0440982b2727dcc9f71fd35e" title="Can't be the only one who finds this funny" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m98jn9stu0kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1murf0u/cant_be_the_only_one_who_finds_this_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1murf0u/cant_be_the_only_one_who_finds_this_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T18:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base · Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
