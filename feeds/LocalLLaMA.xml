<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-23T06:49:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jgp6sw</id>
    <title>China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd</title>
    <updated>2025-03-21T19:25:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt; &lt;img alt="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" src="https://b.thumbs.redditmedia.com/Uv9b5l37Z3HDbLOdsI_RvWZEDLPnFUNe8L0-bY4NmCE.jpg" title="China modified 4090s with 48gb sold cheaper than RTX 5090 - water cooled around 3400 usd" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jgp6sw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgp6sw/china_modified_4090s_with_48gb_sold_cheaper_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T19:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh0ovc</id>
    <title>MoshiVis by kyutai - first open-source real-time speech model that can talk about images</title>
    <updated>2025-03-22T04:40:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt; &lt;img alt="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" src="https://external-preview.redd.it/Y3ptd2t6NGE3NnFlMax15wl1W2mX3SQ6hWixr4c-XUrnbjt3Ig1vm4pgUatm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53210f67e2cb9507383ccec4a8ff094869da2fcb" title="MoshiVis by kyutai - first open-source real-time speech model that can talk about images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v86w8w4a76qe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh0ovc/moshivis_by_kyutai_first_opensource_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhg5yl</id>
    <title>PyChat</title>
    <updated>2025-03-22T19:13:47+00:00</updated>
    <author>
      <name>/u/mspamnamem</name>
      <uri>https://old.reddit.com/user/mspamnamem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve seen a few posts recently about chat clients that people have been building. They’re great! &lt;/p&gt; &lt;p&gt;I’ve been working on one of my own context aware chat clients. It is written in python and has a few unique things:&lt;/p&gt; &lt;p&gt;(1) can import and export chats. I think this so I can export a “starter” chat. I sort of think of this like a sourdough starter. Share it with your friends. Can be useful for coding if you don’t want to start from scratch every time.&lt;/p&gt; &lt;p&gt;(2) context aware and can switch provider and model in the chat window. &lt;/p&gt; &lt;p&gt;(3) search and archive threads. &lt;/p&gt; &lt;p&gt;(4) allow two AIs to communicate with one another. Also useful for coding: make one strong coding model the developer and a strong language model the manager. Can also simulate debates and stuff. &lt;/p&gt; &lt;p&gt;(5) attempts to highlight code into code blocks and allows you to easily copy them. &lt;/p&gt; &lt;p&gt;I have this working at home with a Mac on my network hosting ollama and running this client on a PC. I haven’t tested it with localhost ollama running on the same machine but it should still work. Just make sure that ollama is listening on 0.0.0.0 not just html server. &lt;/p&gt; &lt;p&gt;Note: - API keys are optional to OpenAI and Anthropic. They are stored locally but not encrypted. Same with the chat database. Maybe in the future I’ll work to encrypt these. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are probably some bugs because I’m just one person. Willing to fix. Let me know! &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/Magnetron85/PyChat"&gt;https://github.com/Magnetron85/PyChat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mspamnamem"&gt; /u/mspamnamem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhg5yl/pychat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhg5yl/pychat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhg5yl/pychat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:13:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhiail</id>
    <title>Uncensored Image Generator?</title>
    <updated>2025-03-22T20:47:32+00:00</updated>
    <author>
      <name>/u/Key_Appointment_7582</name>
      <uri>https://old.reddit.com/user/Key_Appointment_7582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to get around my own school charging me hundreds for MY OWN grad photos. Does anyone know a local model that I can upload my images and have the model remove watermarks and resize the image so it can return a png or jpeg I can have for myself?&lt;/p&gt; &lt;p&gt;I only have 8g vram and 32g ram laptop 4070 so a smaller model Is preferred thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Key_Appointment_7582"&gt; /u/Key_Appointment_7582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhiail/uncensored_image_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhiail/uncensored_image_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhiail/uncensored_image_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T20:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhjbgj</id>
    <title>Best LLM for code? Through api with Aider</title>
    <updated>2025-03-22T21:35:05+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I want to know how the payment process for the API works. I always try for free, so I want to know if I can just put, for example, 5 dollars, and that’s it. I mean, I don't want to enter my credit card information only to later receive a bill I can't pay. Does a good LLM for what I want have that possibility? Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhjbgj/best_llm_for_code_through_api_with_aider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhjbgj/best_llm_for_code_through_api_with_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhjbgj/best_llm_for_code_through_api_with_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T21:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh3i7k</id>
    <title>1.5B surprises o1-preview math benchmarks with this new finding</title>
    <updated>2025-03-22T07:59:05+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt; &lt;img alt="1.5B surprises o1-preview math benchmarks with this new finding" src="https://external-preview.redd.it/v81uCWR00P0A7u3BP_mTIasdD33pY9M4769VjQUIiSw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff86f87a266a03096a160a58098c8ced38e6b00c" title="1.5B surprises o1-preview math benchmarks with this new finding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2503.16219"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh3i7k/15b_surprises_o1preview_math_benchmarks_with_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T07:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhix6i</id>
    <title>Both my PC and Mac make a hissing sound as local LLMs generate tokens</title>
    <updated>2025-03-22T21:17:03+00:00</updated>
    <author>
      <name>/u/s3bastienb</name>
      <uri>https://old.reddit.com/user/s3bastienb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a desktop PC with an rx7900xtx and a Macbook pro m1 Max that is powered by a thunderbolt dock (cal digit ts3) and they are both plugged into my UPS (Probably the source of the problem).&lt;/p&gt; &lt;p&gt;I'm running Ollama and LM studio and I use them as LLM servers when working on my iOS LLM client and as I watch the tokens stream in I can hear the PC or Mac making a small hissing sound and its funny how it matches each token generated. It kinda reminds me of how computer terminals in movies seem to beep when streaming in text.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s3bastienb"&gt; /u/s3bastienb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhix6i/both_my_pc_and_mac_make_a_hissing_sound_as_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhix6i/both_my_pc_and_mac_make_a_hissing_sound_as_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhix6i/both_my_pc_and_mac_make_a_hissing_sound_as_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T21:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4r72</id>
    <title>Deepseek (the website) now has a optout like the others, earlier they didn't have.</title>
    <updated>2025-03-22T09:34:02+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt; &lt;img alt="Deepseek (the website) now has a optout like the others, earlier they didn't have." src="https://b.thumbs.redditmedia.com/EtrziXc9-Pm757HjCIYTYkRO79iwGVPKkOldgIO-yAM.jpg" title="Deepseek (the website) now has a optout like the others, earlier they didn't have." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7"&gt;https://preview.redd.it/vooy8j4gn7qe1.png?width=1042&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a82a68facaa4f88b976ef720f67abb57811b25b7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4r72/deepseek_the_website_now_has_a_optout_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhfz4n</id>
    <title>What's the status of using a local LLM for software development?</title>
    <updated>2025-03-22T19:05:30+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please help an old programmer navigate the maze that is the current LLM-enabled SW stacks.&lt;/p&gt; &lt;p&gt;I'm sure that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;I won't use Claude or any online LLM&lt;/strong&gt;. Just a local model that is small enough to leave enough room for context (eg Qwen2.5 Coder 14B).&lt;/li&gt; &lt;li&gt;I need a tool that can feed an entire project to an LLM as context.&lt;/li&gt; &lt;li&gt;I know how to code but want to use an LLM to do the boilerplate stuff, not to take full control of a project.&lt;/li&gt; &lt;li&gt;Preferably FOSS.&lt;/li&gt; &lt;li&gt;Preferably integrated into a solid IDE, rather then being standalone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhfz4n/whats_the_status_of_using_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhe3lq</id>
    <title>gemma3 vision</title>
    <updated>2025-03-22T17:44:22+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok im gonna write in all lower case because the post keeps getting auto modded. its almost like local llama encourage low effort post. super annoying. imagine there was a fully compliant gemma3 vision model, wouldn't that be nice?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha"&gt;https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhe3lq/gemma3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jgqmlr</id>
    <title>"If we confuse users enough, they will overpay"</title>
    <updated>2025-03-21T20:26:10+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt; &lt;img alt="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" src="https://preview.redd.it/epfkc4xxq3qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f18b9505527bc8ed40557544a084be28952fd9b" title="&amp;quot;If we confuse users enough, they will overpay&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/epfkc4xxq3qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jgqmlr/if_we_confuse_users_enough_they_will_overpay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T20:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6j47</id>
    <title>🚀 Running vLLM with 2 GPUs on my home server - automated in minutes!</title>
    <updated>2025-03-22T11:39:27+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt; &lt;img alt="🚀 Running vLLM with 2 GPUs on my home server - automated in minutes!" src="https://b.thumbs.redditmedia.com/ScRkwidkM9zx6RUmuOk3MXdlYXgdVEQFsbGekaQlRYs.jpg" title="🚀 Running vLLM with 2 GPUs on my home server - automated in minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got vLLM running on a dual-GPU home server, complete with my Sbnb Linux distro tailored for AI, Grafana GPU utilization dashboards, and automated benchmarking - all set up in just a few minutes thanks to Ansible.&lt;/p&gt; &lt;p&gt;If you’re into LLMs, home labs, or automation, I put together a detailed how-to here: 🔗 &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to help if anyone wants to get started!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh6j47"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6j47/running_vllm_with_2_gpus_on_my_home_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh4s2h</id>
    <title>LLama.cpp smillar speed but in pure Rust, local LLM inference alternatives.</title>
    <updated>2025-03-22T09:35:49+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, every time I want to run a LLM locally, the only choice is llama.cpp or other tools with magical optimization. However, llama.cpp is not always easy to set up especially when it comes to a new model and new architecture. Without help from the community, you can hardly convert a new model into GGUF. Even if you can, it is still very hard to make it work in llama.cpp.&lt;/p&gt; &lt;p&gt;Now, we can have an alternative way to infer LLM locally with maximum speed. And it's in pure Rust! No C++ needed. With pyo3 you can still call it with python, but Rust is easy enough, right?&lt;/p&gt; &lt;p&gt;I made a minimal example the same as llama.cpp chat cli. It runs 6 times faster than using pytorch, based on the Candle framework.Check it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lucasjinreal/Crane"&gt;https://github.com/lucasjinreal/Crane&lt;/a&gt;&lt;/p&gt; &lt;p&gt;next I would adding Spark-TTS and &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus-TTS&lt;/a&gt; support, if you interested in Rust and fast inference, please join to develop with rust!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh4s2h/llamacpp_smillar_speed_but_in_pure_rust_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T09:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhbxr9</id>
    <title>Token impact by long-Chain-of-Thought Reasoning Models</title>
    <updated>2025-03-22T16:10:20+00:00</updated>
    <author>
      <name>/u/dubesor86</name>
      <uri>https://old.reddit.com/user/dubesor86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt; &lt;img alt="Token impact by long-Chain-of-Thought Reasoning Models" src="https://preview.redd.it/hxrz73n2l9qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be6c35234bc628ab1e2c263ab5a9a084397d8793" title="Token impact by long-Chain-of-Thought Reasoning Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dubesor86"&gt; /u/dubesor86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hxrz73n2l9qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhbxr9/token_impact_by_longchainofthought_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T16:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhsqlr</id>
    <title>How does Groq.com do it? (Groq not Elon's grok)</title>
    <updated>2025-03-23T06:05:49+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does groq run llms so fast? Is it just very high power or they use some technique?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhsqlr/how_does_groqcom_do_it_groq_not_elons_grok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T06:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh7c6e</id>
    <title>My 4x3090 eGPU collection</title>
    <updated>2025-03-22T12:28:25+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt; &lt;img alt="My 4x3090 eGPU collection" src="https://b.thumbs.redditmedia.com/tuwbOdIfpLg-K_qo2ArzC4oMvVIIdECI4tmNxUjTuKA.jpg" title="My 4x3090 eGPU collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 more 3090s ready to hook up to the 2nd Thunderbolt port in the back when I get the UT4g docks in. &lt;/p&gt; &lt;p&gt;Will need to find an area with more room though 😅&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jh7c6e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh7c6e/my_4x3090_egpu_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T12:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhpgum</id>
    <title>Llama 3.3 70B vs Nemotron Super 49B (Based on Lllama 3.3)</title>
    <updated>2025-03-23T02:43:49+00:00</updated>
    <author>
      <name>/u/Prestigious-Use5483</name>
      <uri>https://old.reddit.com/user/Prestigious-Use5483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you guys like using better? I haven't tested Nemotron Super 49B much, but I absolute loved llama 3.3 70B. Please share the reason you prefer one over the other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Use5483"&gt; /u/Prestigious-Use5483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T02:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhrqov</id>
    <title>Here's another AMD Strix Halo Mini PC announcement with video of it running a 70B Q8 model.</title>
    <updated>2025-03-23T04:58:43+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the Sixunited 395+ Mini PC. It's also supposed to come out in May. It's all in Chinese. I do see what appears to be 3 token scroll across the screen. Which I assume means it's 3tk/s. Considering it's a 70GB model, that makes sense considering the memory bandwidth of Strix Halo.&lt;/p&gt; &lt;p&gt;The LLM stuff starts at about the 4 min mark.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.bilibili.com/video/BV1xhKsenE4T"&gt;https://www.bilibili.com/video/BV1xhKsenE4T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhrqov/heres_another_amd_strix_halo_mini_pc_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-23T04:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhf6x3</id>
    <title>Has anyone switched from remote models (claude, etc.) models to local? Meaning did your investment pay off?</title>
    <updated>2025-03-22T18:31:34+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously a 70b or 32b model won't be as good as Claude API, on the other hand, many are spending $10 to $30+ per day on the API, so it could be a lot cheaper.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhf6x3/has_anyone_switched_from_remote_models_claude_etc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T18:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhdpjk</id>
    <title>Fallen Gemma3 4B 12B 27B - An unholy trinity with no positivity! For users, mergers and cooks!</title>
    <updated>2025-03-22T17:27:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a complete decensor tune, but it should be absent of positivity.&lt;/p&gt; &lt;p&gt;Vision works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-4B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Gemma3-27B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhdpjk/fallen_gemma3_4b_12b_27b_an_unholy_trinity_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T17:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6y0</id>
    <title>Are any of the big API providers (OpenAI, Anthropic, etc) actually making money, or are all of them operating at a loss and burning through investment cash?</title>
    <updated>2025-03-22T23:03:34+00:00</updated>
    <author>
      <name>/u/AnticitizenPrime</name>
      <uri>https://old.reddit.com/user/AnticitizenPrime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a consensus right now that local LLMs are not cheaper to run than the myriad of APIs out there at this time, when you consider the initial investment in hardware, the cost of energy, etc. The reasons for going local are for privacy, independence, hobbyism, tinkering/training your own stuff, working offline, or just the wow factor of being able to hold a conversation with your GPU.&lt;/p&gt; &lt;p&gt;But is that necessarily the case? Is it possible that these low API costs are unsustainable in the long term?&lt;/p&gt; &lt;p&gt;Genuinely curious. As far as I know, no LLM provider has turned a profit thus far, but I'd welcome a correction if I'm wrong.&lt;/p&gt; &lt;p&gt;I'm just wondering if the conception that 'local isn't as cheap as APIs' might not hold true anymore after all the investment money dries up and these companies need to actually price their API usage in a way that keeps the lights on and the GPUs going brrr.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnticitizenPrime"&gt; /u/AnticitizenPrime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6y0/are_any_of_the_big_api_providers_openai_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jh6lsx</id>
    <title>OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision.</title>
    <updated>2025-03-22T11:44:18+00:00</updated>
    <author>
      <name>/u/lessis_amess</name>
      <uri>https://old.reddit.com/user/lessis_amess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt; &lt;img alt="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." src="https://preview.redd.it/x942twbra8qe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79683f47809a02571ff90500acb5d28a046d6940" title="OpenAI released GPT-4.5 and O1 Pro via their API and it looks like a weird decision." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;O1 Pro costs 33 times more than Claude 3.7 Sonnet, yet in many cases delivers less capability. GPT-4.5 costs 25 times more and it’s an old model with a cut-off date from November.&lt;/p&gt; &lt;p&gt;Why release old, overpriced models to developers who care most about cost efficiency?&lt;/p&gt; &lt;p&gt;This isn't an accident.&lt;/p&gt; &lt;p&gt;It's anchoring.&lt;/p&gt; &lt;p&gt;Anchoring works by establishing an initial reference point. Once that reference exists, subsequent judgments revolve around it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Show something expensive.&lt;/li&gt; &lt;li&gt;Show something less expensive.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The second thing seems like a bargain.&lt;/p&gt; &lt;p&gt;The expensive API models reset our expectations. For years, AI got cheaper while getting smarter. OpenAI wants to break that pattern. They're saying high intelligence costs money. Big models cost money. They're claiming they don't even profit from these prices.&lt;/p&gt; &lt;p&gt;When they release their next frontier model at a &amp;quot;lower&amp;quot; price, you'll think it's reasonable. But it will still cost more than what we paid before this reset. The new &amp;quot;cheap&amp;quot; will be expensive by last year's standards.&lt;/p&gt; &lt;p&gt;OpenAI claims these models lose money. Maybe. But they're conditioning the market to accept higher prices for whatever comes next. The API release is just the first move in a longer game.&lt;/p&gt; &lt;p&gt;This was not a confused move. It’s smart business. (i'm VERY happy we have open-source)&lt;/p&gt; &lt;p&gt;&lt;a href="https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro"&gt;https://ivelinkozarev.substack.com/p/the-pricing-of-gpt-45-and-o1-pro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lessis_amess"&gt; /u/lessis_amess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x942twbra8qe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jh6lsx/openai_released_gpt45_and_o1_pro_via_their_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T11:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhhsgv</id>
    <title>Qwen2.5-Omni Incoming? Huggingface Transformers PR 36752</title>
    <updated>2025-03-22T20:25:00+00:00</updated>
    <author>
      <name>/u/Inevitable_Sea8804</name>
      <uri>https://old.reddit.com/user/Inevitable_Sea8804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Haven't seen anyone bring this up, so making a post here...&lt;/p&gt; &lt;p&gt;Using DeepSeek-R1 to summarize the features of this model based on PR commits:&lt;/p&gt; &lt;hr /&gt; &lt;h1&gt;&lt;strong&gt;Qwen2.5-Omni Technical Summary&lt;/strong&gt;&lt;/h1&gt; &lt;h2&gt;&lt;strong&gt;1. Basic Information&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Scale&lt;/strong&gt;: 7B parameter version (&amp;quot;Qwen/Qwen2.5-Omni-7B&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Fully open-sourced under Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;2. Input/Output Modalities&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language instructions&lt;/li&gt; &lt;li&gt;&lt;em&gt;Images&lt;/em&gt;: Common formats (JPEG/PNG)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Audio&lt;/em&gt;: WAV/MP3 (requires FFmpeg)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Video&lt;/em&gt;: MP4 with audio track extraction&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Capabilities&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Text&lt;/em&gt;: Natural language responses&lt;/li&gt; &lt;li&gt;&lt;em&gt;Speech&lt;/em&gt;: 24kHz natural speech (streaming supported)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;3. Architectural Design&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Encoder&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Block-wise Processing&lt;/em&gt;: Decouples long-sequence handling between encoder (perception) and LLM (sequence modeling)&lt;/li&gt; &lt;li&gt;&lt;em&gt;TMRoPE&lt;/em&gt;: Time-aligned Multimodal Rotary Positional Encoding for audio-video synchronization&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual-path Generation&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;em&gt;Thinker&lt;/em&gt;: Text-generating LLM backbone&lt;/li&gt; &lt;li&gt;&lt;em&gt;Talker&lt;/em&gt;: Dual-track AR model for audio token generation using Thinker's hidden states&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming Optimization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Sliding-window Diffusion Transformer (DiT) reduces audio latency&lt;/li&gt; &lt;li&gt;Simultaneous text/speech streaming output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;4. Technical Highlights&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified Multimodal Processing&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;End-to-end joint training without intermediate representations&lt;/li&gt; &lt;li&gt;Supports arbitrary modality combinations (single/mixed)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Attention&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Native FlashAttention 2 support&lt;/li&gt; &lt;li&gt;Compatible with PyTorch SDPA&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Customization&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Prebuilt voices: &lt;code&gt;Cherry&lt;/code&gt; (female) &amp;amp; &lt;code&gt;Ethan&lt;/code&gt; (male)&lt;/li&gt; &lt;li&gt;Dynamic voice switching via &lt;code&gt;spk&lt;/code&gt; parameter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment Flexibility&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Disable speech output to save VRAM (~2GB)&lt;/li&gt; &lt;li&gt;Text-only mode (&lt;code&gt;return_audio=False&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;5. Performance&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Benchmarks&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;SOTA on Omni-Bench&lt;/li&gt; &lt;li&gt;Outperforms same-scale Qwen2-VL/Qwen2-Audio in vision/audio tasks&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Understanding&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;First open-source model with text-level E2E speech instruction following&lt;/li&gt; &lt;li&gt;Matches text-input performance on MMLU/GSM8K with speech inputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;6. Implementation Details&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware Support&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Auto device mapping (&lt;code&gt;device_map=&amp;quot;auto&amp;quot;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Mixed precision (&lt;code&gt;bfloat16/float16&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processing Pipeline&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Unified &lt;code&gt;Qwen2_5OmniProcessor&lt;/code&gt; handles multimodal inputs&lt;/li&gt; &lt;li&gt;Batch processing of mixed media combinations&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;7. Requirements&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;System Prompt&lt;/strong&gt;: Mandatory for full functionality:&lt;br /&gt; &lt;code&gt; &amp;quot;You are Qwen... capable of generating text and speech.&amp;quot; &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;FlashAttention 2 (optional acceleration)&lt;/li&gt; &lt;li&gt;FFmpeg (video/non-WAV audio processing)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;This architecture achieves deep multimodal fusion through innovative designs while maintaining strong text capabilities, significantly advancing audiovisual understanding/generation for multimodal agent development.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Also from the PR:&lt;/p&gt; &lt;p&gt;&lt;em&gt;We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can the community help confirm whether this PR is legit?&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(Original PR: &lt;a href="https://github.com/huggingface/transformers/pull/36752"&gt;https://github.com/huggingface/transformers/pull/36752&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Sea8804"&gt; /u/Inevitable_Sea8804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhhsgv/qwen25omni_incoming_huggingface_transformers_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T20:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhgpew</id>
    <title>nsfw orpheus tts?</title>
    <updated>2025-03-22T19:37:43+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im currently in the data curation / filtering / cleaning phase&lt;/p&gt; &lt;p&gt;but i would like to see how many local guys would be interested in a tts for there anime waifus that can make &amp;quot;interesting&amp;quot; emotional noises&lt;/p&gt; &lt;p&gt;Total audio events found: 181218&lt;/p&gt; &lt;p&gt;(sighs): 8594&lt;/p&gt; &lt;p&gt;(laughs): 68590&lt;/p&gt; &lt;p&gt;(gasps): 14113&lt;/p&gt; &lt;p&gt;(moans): 20576&lt;/p&gt; &lt;p&gt;(whimpers): 418&lt;/p&gt; &lt;p&gt;(breathing): 114&lt;/p&gt; &lt;p&gt;(pants): 776&lt;/p&gt; &lt;p&gt;and many more ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T19:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jhl6jp</id>
    <title>Gemma3 is outperforming a ton of models on fine-tuning / world knowledge</title>
    <updated>2025-03-22T23:03:02+00:00</updated>
    <author>
      <name>/u/fluxwave</name>
      <uri>https://old.reddit.com/user/fluxwave</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt; &lt;img alt="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" src="https://external-preview.redd.it/XifaOkXuUsJa3iGAsHuitV7h5kD9H1PhRfgSOnPUnbc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1663259cbca9e5b87b6c2d99b3d23f12f5a2118" title="Gemma3 is outperforming a ton of models on fine-tuning / world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea"&gt;https://preview.redd.it/yid0t6cxmbqe1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2cacf5b4f6c0d99f9902bf7e3a5e4da5c50d41ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At fine-tuning they seem to be smashing evals -- see this tweet above from OpenPipe. &lt;/p&gt; &lt;p&gt;Then in world-knowledge (or at least this smaller task of identifying the gender of scholars across history) a 12B model beat OpenAI's gpt-4o-mini. This is using no fine-tuning. &lt;a href="https://thedataquarry.com/blog/using-llms-to-enrich-datasets/"&gt;https://thedataquarry.com/blog/using-llms-to-enrich-datasets/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p11ujen8nbqe1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=897f8506ee01cffcbad459d11da436a2e1521501"&gt;Written by Prashanth Rao&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(disclaimer: Prashanth is a member of the BAML community -- our prompting DSL / toolchain &lt;a href="https://github.com/BoundaryML/baml"&gt;https://github.com/BoundaryML/baml&lt;/a&gt; , but he works at KuzuDB).&lt;/p&gt; &lt;p&gt;Has anyone else seen amazing results with Gemma3? Curious to see if people have tried it more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fluxwave"&gt; /u/fluxwave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jhl6jp/gemma3_is_outperforming_a_ton_of_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-22T23:03:02+00:00</published>
  </entry>
</feed>
