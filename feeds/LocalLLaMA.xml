<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-12T15:24:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1inrd65</id>
    <title>Looking for basics of LLM, AI, ML texts</title>
    <updated>2025-02-12T13:57:20+00:00</updated>
    <author>
      <name>/u/NotFallacyBuffet</name>
      <uri>https://old.reddit.com/user/NotFallacyBuffet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been reading &lt;em&gt;Steve Ph0enix's Blog&lt;/em&gt; that gets linked here. &lt;/p&gt; &lt;p&gt;That got me to wondering if there's something more basic for LLM, like &amp;quot;the Dragon book&amp;quot; was for compilers. &lt;/p&gt; &lt;p&gt;'Cause, after I read the Dragon Book and wrote a toy Pascal (lol) compiler, my coding and understanding was greatly improved. &lt;/p&gt; &lt;p&gt;Looking for something similar in this domain. Because a lot of the lingo used here is just whoosh to me. &lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotFallacyBuffet"&gt; /u/NotFallacyBuffet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrd65/looking_for_basics_of_llm_ai_ml_texts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrd65/looking_for_basics_of_llm_ai_ml_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inrd65/looking_for_basics_of_llm_ai_ml_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T13:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1inrlip</id>
    <title>Knowladge base and PDFs</title>
    <updated>2025-02-12T14:07:46+00:00</updated>
    <author>
      <name>/u/GVT84</name>
      <uri>https://old.reddit.com/user/GVT84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using openwebUI to try to do a knowledge base of scientific papers through its feature but seems that is not working correctly. Some models cant understand the question about docs. &lt;/p&gt; &lt;p&gt;As alternative Im thinking in comercial options. Someone know if for local option can achieve something like a good knowledge base about 500 scientific papers to get data from this without errors? &lt;/p&gt; &lt;p&gt;For example one question that Im chasing is: “ find in the papers a similar sentence to justify this concept”.&lt;/p&gt; &lt;p&gt;And get the sentences and the correct lines qhere find it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GVT84"&gt; /u/GVT84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrlip/knowladge_base_and_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrlip/knowladge_base_and_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inrlip/knowladge_base_and_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T14:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1inf39f</id>
    <title>Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit</title>
    <updated>2025-02-12T01:16:26+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt; &lt;img alt="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" src="https://external-preview.redd.it/6TXevgYpufzYpfTjezpGg52UERA8ZIyKmvoa8ynszgs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1ee4f4645a585d0b344f74ef45c2a3af79f5ac" title="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.sky.com/story/politics-latest-immigration-labour-starmer-badenoch-farage-live-news-12593360?postid=9087865#liveblog-body"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T01:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1inahmj</id>
    <title>AI-RP GUI, thoughts?</title>
    <updated>2025-02-11T21:50:23+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt; &lt;img alt="AI-RP GUI, thoughts?" src="https://external-preview.redd.it/anUxbWh5MXF3a2llMZDcyaEte4yY5OTnn_MraO3a1mLbSyQuEc8JUukRHPMy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffabf7e7e829026793cb0c9fe2931fd6dab7c685" title="AI-RP GUI, thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9ej41y1qwkie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1in4b81</id>
    <title>Why AMD or Intel doesn't sell card with huge amount of Vram ?</title>
    <updated>2025-02-11T17:39:14+00:00</updated>
    <author>
      <name>/u/Euphoric_Tutor_5054</name>
      <uri>https://old.reddit.com/user/Euphoric_Tutor_5054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, we saw that even with an epyc processor and 512 gb of ram you can run deepseek pretty fast, but compared to a graphic card it's pretty slow. But the problem is that you need a lot of vram on your graphic card so why AMD and intel doesn't sell such card with enormous amount of vram ? especially since 8gb of gddr6 is super cheap now ! like 3$ I believe, look here : &lt;a href="https://www.dramexchange.com/"&gt;https://www.dramexchange.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would be a killer for inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Tutor_5054"&gt; /u/Euphoric_Tutor_5054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T17:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1in0938</id>
    <title>I made Iris: A fully-local realtime voice chatbot!</title>
    <updated>2025-02-11T14:49:16+00:00</updated>
    <author>
      <name>/u/Born_Search2534</name>
      <uri>https://old.reddit.com/user/Born_Search2534</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt; &lt;img alt="I made Iris: A fully-local realtime voice chatbot!" src="https://external-preview.redd.it/stLjrcu85AgTrwW4zDhH8loF7eayJD3_hD2XyXmIgUw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65bd986bcf0e0f5ab0f244100b8cb83d8e1266a9" title="I made Iris: A fully-local realtime voice chatbot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Search2534"&gt; /u/Born_Search2534 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=XK-37m-p11k&amp;amp;feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbili</id>
    <title>UK and US refuse to sign international AI declaration</title>
    <updated>2025-02-11T22:32:41+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt; &lt;img alt="UK and US refuse to sign international AI declaration" src="https://external-preview.redd.it/4F7TAGCOz8Rfg3WqXnrQP8MWV7RA9T-cLoTH3Je-EA8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa17295cbd542f368737b59db21a9f78ea0823" title="UK and US refuse to sign international AI declaration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bbc.com/news/articles/c8edn0n58gwo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoewc</id>
    <title>OLMoE-0125 &amp; iOS App from allenai</title>
    <updated>2025-02-12T11:03:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"&gt; &lt;img alt="OLMoE-0125 &amp;amp; iOS App from allenai" src="https://b.thumbs.redditmedia.com/2W3p9PcVbwGzJeMR11PCMqP_IHoAoR2nsQAUErncanY.jpg" title="OLMoE-0125 &amp;amp; iOS App from allenai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OLMoE-0125:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmoe-january-2025-67992134f9ebea0a941706ca"&gt;https://huggingface.co/collections/allenai/olmoe-january-2025-67992134f9ebea0a941706ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;iOS App:&lt;/p&gt; &lt;p&gt;&lt;a href="https://allenai.org/blog/olmoe-app"&gt;https://allenai.org/blog/olmoe-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inoewc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1injegi</id>
    <title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title>
    <updated>2025-02-12T05:05:27+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.07374"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T05:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1in83vw</id>
    <title>Chonky Boi has arrived</title>
    <updated>2025-02-11T20:12:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt; &lt;img alt="Chonky Boi has arrived" src="https://external-preview.redd.it/YKjiYoZG5DJKHF8InRyOIM7iTEJQimQj1eCDwySpqqE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69fcf80dcc8f1be1b6b900fa7ba68bf7b62ee469" title="Chonky Boi has arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/kh64WJy.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1in69s3</id>
    <title>4x3090 in a 4U case, don't recommend it</title>
    <updated>2025-02-11T18:58:29+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt; &lt;img alt="4x3090 in a 4U case, don't recommend it" src="https://a.thumbs.redditmedia.com/78a1YQ1sn0cHJ3q5f1VyWRySQ3DMLohcp9folBrA-j0.jpg" title="4x3090 in a 4U case, don't recommend it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1in69s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inieoe</id>
    <title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title>
    <updated>2025-02-12T04:07:22+00:00</updated>
    <author>
      <name>/u/ekaesmem</name>
      <uri>https://old.reddit.com/user/ekaesmem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt; &lt;img alt="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" src="https://b.thumbs.redditmedia.com/xPf7D_ti2_9HmqPFloVt-vKCywazKffYdOe7zEWXDAg.jpg" title="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca"&gt;https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.06703"&gt;[2502.06703] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekaesmem"&gt; /u/ekaesmem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T04:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9vsb</id>
    <title>NYT: Vance speech at EU AI summit</title>
    <updated>2025-02-11T21:24:49+00:00</updated>
    <author>
      <name>/u/Mediocre_Tree_5690</name>
      <uri>https://old.reddit.com/user/Mediocre_Tree_5690</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt; &lt;img alt="NYT: Vance speech at EU AI summit" src="https://preview.redd.it/vjltv4twukie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ab17775dd480a87f8fde7e76f52035c4a8b6cc" title="NYT: Vance speech at EU AI summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archive.is/eWNry"&gt;https://archive.is/eWNry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's an archive link in case anyone wants to read the article. Macron spoke about lighter regulation at the AI summit as well. Are we thinking safetyism is finally on its way out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Tree_5690"&gt; /u/Mediocre_Tree_5690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjltv4twukie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1inrhd9</id>
    <title>The light based computer that supports pytorch</title>
    <updated>2025-02-12T14:02:29+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey a funny one today! A pick at light-based computer with a startup called Q.ANT They refactor 90's CMOS foundery to make ai chips using light. Their chips are already on there ways to datacenter. &lt;a href="https://youtu.be/2xE4bopeXhw?feature=shared"&gt;https://youtu.be/2xE4bopeXhw?feature=shared&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T14:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inqb6n</id>
    <title>Letting LLMs using an IDE’s debugger</title>
    <updated>2025-02-12T13:04:42+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just built an experimental VSCode extension called LLM Debugger. It’s a proof-of-concept that lets a large language model take charge of debugging. Instead of only looking at the static code, the LLM also gets to see the live runtime state—actual variable values, function calls, branch decisions, and more. The idea is to give it enough context to help diagnose issues faster and even generate synthetic data from running programs.&lt;/p&gt; &lt;p&gt;Here’s what it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Active Debugging: It integrates with Node.js debug sessions to gather runtime info (like variable states and stack traces).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Automated Breakpoints: It automatically sets and manages breakpoints based on both code analysis and LLM suggestions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM Guidance: With live debugging context, the LLM can suggest actions like stepping through code or adjusting breakpoints in real time.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this out of curiosity to see if combining static code with runtime data could help LLMs solve bugs more effectively. It’s rough around the edges and definitely not production-ready&lt;/p&gt; &lt;p&gt;I’m not planning on maintaining it further. But I thought it was a fun experiment and wanted to share it with you all.&lt;/p&gt; &lt;p&gt;Check out the attached video demo to see it in action. Would love to hear your thoughts and any feedback you might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T13:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1imyp19</id>
    <title>If you want my IT department to block HF, just say so.</title>
    <updated>2025-02-11T13:35:20+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt; &lt;img alt="If you want my IT department to block HF, just say so." src="https://preview.redd.it/h1dbbwhxiiie1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1bafacbc3514f10c5b939ade16c607722c1d9b0" title="If you want my IT department to block HF, just say so." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1dbbwhxiiie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1in8vya</id>
    <title>EU mobilizes $200 billion in AI race against US and China</title>
    <updated>2025-02-11T20:44:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt; &lt;img alt="EU mobilizes $200 billion in AI race against US and China" src="https://external-preview.redd.it/X_db72AfOkvUPacuwPMCLwkrfkqSycSFXdfcaogRMnw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6dff9e983d7903f62f20823b94ae4fd34bac0f8" title="EU mobilizes $200 billion in AI race against US and China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1in7nka</id>
    <title>ChatGPT 4o feels straight up stupid after using o1 and DeepSeek for awhile</title>
    <updated>2025-02-11T19:54:09+00:00</updated>
    <author>
      <name>/u/Getabock_</name>
      <uri>https://old.reddit.com/user/Getabock_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And to think I used to be really impressed with 4o. Crazy. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Getabock_"&gt; /u/Getabock_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoxu6</id>
    <title>Tested lot of small models for coding and i was surprised how good is (nvidia/AceInstruct-7B), idk why no one talking about it</title>
    <updated>2025-02-12T11:49:06+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It feels like this one flew over the radar, idk if its just a fintune or not, but usually what i do when testing small models for coding i start with : (Make a single html modern calculator)&lt;/p&gt; &lt;p&gt;Just to see if its gonna give me a good looking one, most models struggle to make each button in its own place, layout usualy bad, AceInstruct-7B does good job&lt;/p&gt; &lt;p&gt;After that i use my second prompt:&lt;/p&gt; &lt;p&gt;(Make a windows app using python that has a simple interface, With three buttons, when you click the first button it turns green When you click second button it turns blue When you click third button, it turns red, buttons its self change color)&lt;/p&gt; &lt;p&gt;Again simple but most small models struggle, AceInstruct-7B does it and it follow changes pretty well, like if you ask it to make changes, it will do so and give you the updated code without making weird changes that cause errors,&lt;/p&gt; &lt;p&gt;Just wanted to share this, and there is a 72B version too, ill try to find a way to test it for coding, but i think it will be insane &lt;/p&gt; &lt;p&gt;Edit :&lt;/p&gt; &lt;p&gt;The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is Improved using Qwen. These models are fine-tuned on Qwen2.5-Base using general SFT datasets. These same datasets are also used in the training of AceMath-Instruct. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:49:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1inn034</id>
    <title>Phi-4, but pruned and unsafe</title>
    <updated>2025-02-12T09:13:06+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some things just start on a &lt;strong&gt;whim&lt;/strong&gt;. This is the story of &lt;strong&gt;Phi-Lthy4&lt;/strong&gt;, pretty much:&lt;/p&gt; &lt;p&gt;&amp;gt; yo sicarius can you make phi-4 smarter?&lt;br /&gt; nope. but i can still make it better.&lt;br /&gt; &amp;gt; wdym??&lt;br /&gt; well, i can yeet a couple of layers out of its math brain, and teach it about the wonders of love and intimate relations. maybe. idk if its worth it.&lt;br /&gt; &amp;gt; lol its all synth data in the pretrain. many before you tried.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;fine. ill do it.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;But... why?&lt;/h1&gt; &lt;p&gt;The trend it seems, is to make AI models more &lt;strong&gt;assistant-oriented&lt;/strong&gt;, use as much &lt;strong&gt;synthetic data&lt;/strong&gt; as possible, be more &lt;strong&gt;'safe'&lt;/strong&gt;, and be more &lt;strong&gt;benchmaxxed&lt;/strong&gt; (hi qwen). Sure, this makes great assistants, but &lt;strong&gt;sanitized&lt;/strong&gt; data (like in the &lt;strong&gt;Phi&lt;/strong&gt; model series case) butchers &lt;strong&gt;creativity&lt;/strong&gt;. Not to mention that the previous &lt;strong&gt;Phi 3.5&lt;/strong&gt; wouldn't even tell you how to &lt;strong&gt;kill a process&lt;/strong&gt; and so on and so forth...&lt;/p&gt; &lt;p&gt;This little side project took about &lt;strong&gt;two weeks&lt;/strong&gt; of on-and-off fine-tuning. After about &lt;strong&gt;1B tokens&lt;/strong&gt; or so, I lost track of how much I trained it. The idea? A &lt;strong&gt;proof of concept&lt;/strong&gt; of sorts to see if sheer will (and 2xA6000) will be enough to shape a model to &lt;strong&gt;any&lt;/strong&gt; parameter size, behavior or form.&lt;/p&gt; &lt;p&gt;So I used mergekit to perform a crude &lt;strong&gt;LLM brain surgery&lt;/strong&gt;— and yeeted some &lt;strong&gt;useless&lt;/strong&gt; neurons that dealt with math. How do I know that these exact neurons dealt with math? Because &lt;strong&gt;ALL&lt;/strong&gt; of Phi's neurons dealt with math. Success was guaranteed.&lt;/p&gt; &lt;p&gt;Is this the best Phi-4 &lt;strong&gt;11.9B&lt;/strong&gt; RP model in the &lt;strong&gt;world&lt;/strong&gt;? It's quite possible, simply because tuning &lt;strong&gt;Phi-4&lt;/strong&gt; for RP is a completely stupid idea, both due to its pretraining data, &amp;quot;limited&amp;quot; context size of &lt;strong&gt;16k&lt;/strong&gt;, and the model's MIT license.&lt;/p&gt; &lt;p&gt;Surprisingly, it's &lt;strong&gt;quite good at RP&lt;/strong&gt;, turns out it didn't need those 8 layers after all. It could probably still solve a basic math question, but I would strongly recommend using a calculator for such tasks. Why do we want LLMs to do basic math anyway?&lt;/p&gt; &lt;p&gt;Oh, regarding &lt;strong&gt;censorship&lt;/strong&gt;... Let's just say it's... &lt;strong&gt;Phi-lthy&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The BEST Phi-4 Roleplay&lt;/strong&gt; finetune in the &lt;strong&gt;world&lt;/strong&gt; (Not that much of an achievement here, Phi roleplay finetunes can probably be counted on a &lt;strong&gt;single hand&lt;/strong&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compact size &amp;amp; fully healed from the brain surgery&lt;/strong&gt; Only &lt;strong&gt;11.9B&lt;/strong&gt; parameters. &lt;strong&gt;Phi-4&lt;/strong&gt; wasn't that hard to run even at &lt;strong&gt;14B&lt;/strong&gt;, now with even fewer brain cells, your new phone could probably run it easily. (&lt;strong&gt;SD8Gen3&lt;/strong&gt; and above recommended).&lt;/li&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities. This really surprised me. &lt;strong&gt;Actually good&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Writes and roleplays &lt;strong&gt;quite uniquely&lt;/strong&gt;, probably because of lack of RP\writing slop in the &lt;strong&gt;pretrain&lt;/strong&gt;. Who would have thought?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt; - It kept some of the smarts, and our little Phi-Lthy here will be quite eager to answer your naughty questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quite good&lt;/strong&gt; at following the &lt;strong&gt;character card&lt;/strong&gt;. Finally, it puts its math brain to some productive tasks. Gooner technology is becoming more popular by the day.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T09:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ino284</id>
    <title>Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)</title>
    <updated>2025-02-12T10:33:29+00:00</updated>
    <author>
      <name>/u/uti24</name>
      <uri>https://old.reddit.com/user/uti24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"&gt; &lt;img alt="Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)" src="https://preview.redd.it/cte975mgroie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f1eb752eac584c3cdc9b974d150c70db7f42d16" title="Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uti24"&gt; /u/uti24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cte975mgroie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T10:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inos01</id>
    <title>Some details on Project Digits from PNY presentation</title>
    <updated>2025-02-12T11:32:04+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt; &lt;img alt="Some details on Project Digits from PNY presentation" src="https://b.thumbs.redditmedia.com/pzMMeiqVpng-Evo7PS_VS5BolvYAdkUVtZ-ex05okEA.jpg" title="Some details on Project Digits from PNY presentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are my meeting notes, unedited:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Only 19 people attended the presentation?!!! Some left mid-way.. • Presentation by PNY DGX EMEA lead • PNY takes Nvidia DGX ecosystemto market • Memory is DDR5x, 128GB &amp;quot;initially&amp;quot; ○ No comment on memory speed or bandwidth. ○ The memory is on the same fabric, connected to CPU and GPU. ○ &amp;quot;we don't have the specific bandwidth specification&amp;quot; • Also include a dual port QSFP networking, includes a Mellanox chip, supports infiniband and ethernet. Expetced at least 100gb/port, not yet confirmed by Nvidia. • Brand new ARM processor built for the Digits, never released before product (processor, not core). • Real product pictures, not rendering. • &amp;quot;what makes it special is the software stack&amp;quot; • Will run a Ubuntu based OS. Software stack shared with the rest of the nvidia ecosystem. • Digits is to be the first product of a new line within nvidia. • No dedicated power connector could be seen, USB-C powered? ○ &amp;quot;I would assume it is USB-C powered&amp;quot; • Nvidia indicated two maximum can be stacked. There is a possibility to cluster more. ○ The idea is to use it as a developer kit, not or production workloads. • &amp;quot;hopefully May timeframe to market&amp;quot;. • Cost: circa $3k RRP. Can be more depending on software features required, some will be paid. • &amp;quot;significantly more powerful than what we've seen on Jetson products&amp;quot; ○ &amp;quot;exponentially faster than Jetson&amp;quot; ○ &amp;quot;everything you can run on DGX, you can run on this, obviously slower&amp;quot; ○ Targeting universities and researchers. • &amp;quot;set expectations:&amp;quot; ○ It's a workstation ○ It can work standalone, or can be connected to another device to offload processing. ○ Not a replacement for a &amp;quot;full-fledged&amp;quot; multi-GPU workstation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few of us pushed on how the performance compares to a RTX 5090. No clear answer given beyond talking about 5090 not designed for enterprise workload, and power consumption&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inos01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
</feed>
