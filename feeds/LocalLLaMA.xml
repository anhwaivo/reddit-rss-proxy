<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-31T18:07:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kzcc3f</id>
    <title>Noob question: Why did Deepseek distill Qwen3?</title>
    <updated>2025-05-30T18:56:24+00:00</updated>
    <author>
      <name>/u/Turbulent-Week1136</name>
      <uri>https://old.reddit.com/user/Turbulent-Week1136</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In unsloth's &lt;a href="https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally"&gt;documentation&lt;/a&gt;, it says &amp;quot;DeepSeek also released a R1-0528 distilled version by fine-tuning Qwen3 (8B).&amp;quot;&lt;/p&gt; &lt;p&gt;Being a noob, I don't understand why they would use Qwen3 as the base and then distill from there and then call it Deepseek-R1-0528. Isn't it mostly Qwen3 and they are taking Qwen3's work and then doing a little bit extra and then calling it DeepSeek? What advantage is there to using Qwen3's as the base? Are they allowed to do that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent-Week1136"&gt; /u/Turbulent-Week1136 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcc3f/noob_question_why_did_deepseek_distill_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzjhfd</id>
    <title>Ollama 0.9.0 Supports ability to enable or disable thinking</title>
    <updated>2025-05-31T00:02:35+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"&gt; &lt;img alt="Ollama 0.9.0 Supports ability to enable or disable thinking" src="https://external-preview.redd.it/siebatzGRswDgDaN-1zNrvtsYo1Ar9xfV07jYfmXMSI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae2b9d5abfab7fd77887d83caad614ad77503d57" title="Ollama 0.9.0 Supports ability to enable or disable thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.9.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjhfd/ollama_090_supports_ability_to_enable_or_disable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T00:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzzvzt</id>
    <title>[Update] Rensa: added full CMinHash + OptDensMinHash support (fast MinHash in Rust for dataset deduplication / LLM fine-tuning)</title>
    <updated>2025-05-31T15:33:13+00:00</updated>
    <author>
      <name>/u/BeowulfBR</name>
      <uri>https://old.reddit.com/user/BeowulfBR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzvzt/update_rensa_added_full_cminhash_optdensminhash/"&gt; &lt;img alt="[Update] Rensa: added full CMinHash + OptDensMinHash support (fast MinHash in Rust for dataset deduplication / LLM fine-tuning)" src="https://external-preview.redd.it/5-lt2FyUCjdvV--0DsoyDNNXFgYjxAoICz1QQO0XTVc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5505689fd81e257fb9e8b4a1756ec434868c1664" title="[Update] Rensa: added full CMinHash + OptDensMinHash support (fast MinHash in Rust for dataset deduplication / LLM fine-tuning)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all — quick update on &lt;a href="https://github.com/beowolx/rensa"&gt;Rensa&lt;/a&gt;, a MinHash library I’ve been building in Rust with Python bindings. It’s focused on speed and works well for deduplicating large text datasets — especially stuff like LLM fine-tuning where near duplicates are a problem.&lt;/p&gt; &lt;p&gt;Originally, I built a custom algorithm called &lt;strong&gt;RMinHash&lt;/strong&gt; because existing tools (like &lt;code&gt;datasketch&lt;/code&gt;) were way too slow for my use cases. RMinHash is a fast, simple alternative to classic MinHash and gave me much better performance on big datasets.&lt;/p&gt; &lt;p&gt;Since I last posted, I’ve added:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CMinHash&lt;/strong&gt; – full implementation based on the paper (“C-MinHash: reducing K permutations to two”). It’s highly optimized, uses batching + vectorization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OptDensMinHash&lt;/strong&gt; – handles densification for sparse data, fills in missing values in a principled way.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran benchmarks on a 100K-row dataset (&lt;code&gt;gretelai/synthetic_text_to_sql&lt;/code&gt;) with 256 permutations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;CMinHash&lt;/code&gt;: 5.47s&lt;/li&gt; &lt;li&gt;&lt;code&gt;RMinHash&lt;/code&gt;: 5.58s&lt;/li&gt; &lt;li&gt;&lt;code&gt;OptDensMinHash&lt;/code&gt;: 12.36s&lt;/li&gt; &lt;li&gt;&lt;code&gt;datasketch&lt;/code&gt;: 92.45s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So yeah, still ~10-17x faster than datasketch, depending on variant.&lt;/p&gt; &lt;p&gt;Accuracy-wise, all Rensa variants produce very similar (sometimes identical) results to &lt;code&gt;datasketch&lt;/code&gt; in terms of deduplicated examples.&lt;/p&gt; &lt;p&gt;It’s a side project I built out of necessity and I'd love to get some feedback from the community :)&lt;br /&gt; The Python API is simple and should feel familiar if you’ve used datasketch before.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/yourusername/rensa"&gt;https://github.com/beowolx/rensa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeowulfBR"&gt; /u/BeowulfBR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/beowolx/rensa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzvzt/update_rensa_added_full_cminhash_optdensminhash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzvzt/update_rensa_added_full_cminhash_optdensminhash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzrbuv</id>
    <title>Do you think we'll get the r1 distill for the other qwen3 models?</title>
    <updated>2025-05-31T07:29:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been quite a few days now and im losing hope. I don't remember how long it took last time though. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrbuv/do_you_think_well_get_the_r1_distill_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrbuv/do_you_think_well_get_the_r1_distill_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrbuv/do_you_think_well_get_the_r1_distill_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T07:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz0kqi</id>
    <title>Ollama continues tradition of misnaming models</title>
    <updated>2025-05-30T10:13:30+00:00</updated>
    <author>
      <name>/u/profcuck</name>
      <uri>https://old.reddit.com/user/profcuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't really get the hate that Ollama gets around here sometimes, because much of it strikes me as unfair. Yes, they rely on llama.cpp, and have made a great wrapper around it and a very useful setup.&lt;/p&gt; &lt;p&gt;However, their propensity to misname models is very aggravating.&lt;/p&gt; &lt;p&gt;I'm very excited about DeepSeek-R1-Distill-Qwen-32B. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But to run it from Ollama, it's: ollama run deepseek-r1:32b&lt;/p&gt; &lt;p&gt;This is nonsense. It confuses newbies all the time, who think they are running Deepseek and have no idea that it's a distillation of Qwen. It's inconsistent with HuggingFace for absolutely no valid reason.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/profcuck"&gt; /u/profcuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz0kqi/ollama_continues_tradition_of_misnaming_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T10:13:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzf9nl</id>
    <title>Deepseek is cool, but is there an alternative to Claude Code I can use with it?</title>
    <updated>2025-05-30T20:56:55+00:00</updated>
    <author>
      <name>/u/BITE_AU_CHOCOLAT</name>
      <uri>https://old.reddit.com/user/BITE_AU_CHOCOLAT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for an AI coding framework that can help me with training diffusion models. Take existing quasi-abandonned spaguetti codebases and update them to latest packages, implement papers, add features like inpainting, autonomously experiment using different architectures, do hyperparameter searches, preprocess my data and train for me etc... It wouldn't even require THAT much intelligence I think. Sonnet could probably do it. But after trying the API I found its tendency to deceive and take shortcuts a bit frustrating so I'm still on the fence for the €110 subscription (although the auto-compact feature is pretty neat). Is there an open-source version that would get me more for my money?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BITE_AU_CHOCOLAT"&gt; /u/BITE_AU_CHOCOLAT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzf9nl/deepseek_is_cool_but_is_there_an_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:56:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzjcdf</id>
    <title>Built an open source desktop app to easily play with local LLMs and MCP</title>
    <updated>2025-05-30T23:56:00+00:00</updated>
    <author>
      <name>/u/WalrusVegetable4506</name>
      <uri>https://old.reddit.com/user/WalrusVegetable4506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"&gt; &lt;img alt="Built an open source desktop app to easily play with local LLMs and MCP" src="https://preview.redd.it/i4tcl9p5c04f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80ceafd5cc7131bca4a2e6423a8fbe6fe7ed3d14" title="Built an open source desktop app to easily play with local LLMs and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tome is an open source desktop app for Windows or MacOS that lets you chat with an MCP-powered model without having to fuss with Docker, npm, uvx or json config files. Install the app, connect it to a local or remote LLM, one-click install some MCP servers and chat away.&lt;/p&gt; &lt;p&gt;GitHub link here: &lt;a href="https://github.com/runebookai/tome"&gt;https://github.com/runebookai/tome&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on scheduled tasks and other app concepts that should be released in the coming weeks to enable new powerful ways of interacting with LLMs.&lt;/p&gt; &lt;p&gt;We created this because we wanted an easy way to play with LLMs and MCP servers. We wanted to streamline the user experience to make it easy for beginners to get started. You're not going to see a lot of power user features from the more mature projects, but we're open to any feedback and have only been around for a few weeks so there's a lot of improvements we can make. :)&lt;/p&gt; &lt;p&gt;Here's what you can do today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;connect to Ollama, Gemini, OpenAI, or any OpenAI compatible API&lt;/li&gt; &lt;li&gt;add an MCP server, you can either paste something like &amp;quot;uvx mcp-server-fetch&amp;quot; or you can use the Smithery registry integration to one-click install a local MCP server - Tome manages uv/npm and starts up/shuts down your MCP servers so you don't have to worry about it&lt;/li&gt; &lt;li&gt;chat with your model and watch it make tool calls!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you get a chance to try it out we would love any feedback (good or bad!), thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WalrusVegetable4506"&gt; /u/WalrusVegetable4506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i4tcl9p5c04f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzjcdf/built_an_open_source_desktop_app_to_easily_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T23:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzfrdt</id>
    <title>ubergarm/DeepSeek-R1-0528-GGUF</title>
    <updated>2025-05-30T21:17:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt; &lt;img alt="ubergarm/DeepSeek-R1-0528-GGUF" src="https://external-preview.redd.it/_ie2E-L6KKHmnErLSoR3DbJuxwXvA6bw-mpTR5JchI8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97c275cc69bd7ce1a4060b1155a9689d94d05bdc" title="ubergarm/DeepSeek-R1-0528-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all just cooked up some ik_llama.cpp exclusive quants for the recently updated DeepSeek-R1-0528 671B. New recipes are looking pretty good (lower perplexity is &amp;quot;better&amp;quot;):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-Q8_0&lt;/code&gt; 666GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt; &lt;li&gt;I didn't upload this, it is for baseline reference only.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ3_K_R4&lt;/code&gt; 301GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.2730 +/- 0.01738&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 24GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;DeepSeek-R1-0528-IQ2_K_R4&lt;/code&gt; 220GiB &lt;ul&gt; &lt;li&gt;&lt;code&gt;Final estimate: PPL = 3.5069 +/- 0.01893&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Fits 32k context in under 16GiB VRAM&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still might release one or two more e.g. one bigger and one smaller if there is enough interest.&lt;/p&gt; &lt;p&gt;As usual big thanks to Wendell and the whole Level1Techs crew for providing hardware expertise and access to release these quants!&lt;/p&gt; &lt;p&gt;Cheers and happy weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzfrdt/ubergarmdeepseekr10528gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T21:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kz48qx</id>
    <title>Even DeepSeek switched from OpenAI to Google</title>
    <updated>2025-05-30T13:29:07+00:00</updated>
    <author>
      <name>/u/Utoko</name>
      <uri>https://old.reddit.com/user/Utoko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt; &lt;img alt="Even DeepSeek switched from OpenAI to Google" src="https://preview.redd.it/uy7wbaj17x3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e5d766354c0e76341191c5702f69924996f4b0e" title="Even DeepSeek switched from OpenAI to Google" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Similar in text Style analyses from &lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt; shows that R1 is now much closer to Google. &lt;/p&gt; &lt;p&gt;So they probably used more synthetic gemini outputs for training. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Utoko"&gt; /u/Utoko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uy7wbaj17x3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kz48qx/even_deepseek_switched_from_openai_to_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T13:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1l033vh</id>
    <title>Best models to try on 96gb gpu?</title>
    <updated>2025-05-31T17:49:48+00:00</updated>
    <author>
      <name>/u/sc166</name>
      <uri>https://old.reddit.com/user/sc166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX pro 6000 Blackwell arriving next week. What are the top local coding and image/video generation models I can try? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sc166"&gt; /u/sc166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l033vh/best_models_to_try_on_96gb_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzz7t4</id>
    <title>Use MCP to run computer use in a VM.</title>
    <updated>2025-05-31T15:04:34+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"&gt; &lt;img alt="Use MCP to run computer use in a VM." src="https://external-preview.redd.it/MHJhMDJ6dWV1NDRmMSBTlOtFiw3CN60nCKAl7ym9Md7o0mszJARyFHwBNilc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f04b4f66e0cfeee4830916f3a623e4070f0018c" title="Use MCP to run computer use in a VM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MCP Server with Computer Use Agent runs through Claude Desktop, Cursor, and other MCP clients.&lt;/p&gt; &lt;p&gt;An example use case lets try using Claude as a tutor to learn how to use Tableau.&lt;/p&gt; &lt;p&gt;The MCP Server implementation exposes CUA's full functionality through standardized tool calls. It supports single-task commands and multi-task sequences, giving Claude Desktop direct access to all of Cua's computer control capabilities.&lt;/p&gt; &lt;p&gt;This is the first MCP-compatible computer control solution that works directly with Claude Desktop's and Cursor's built-in MCP implementation. Simple configuration in your claude_desktop_config.json or cursor_config.json connects Claude or Cursor directly to your desktop environment.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p51trp5fu44f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzz7t4/use_mcp_to_run_computer_use_in_a_vm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kztjgp</id>
    <title>How are Intel gpus for local models</title>
    <updated>2025-05-31T10:02:49+00:00</updated>
    <author>
      <name>/u/Unusual_Pride_6480</name>
      <uri>https://old.reddit.com/user/Unusual_Pride_6480</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Say the b580 plus ryzen cpu and lots of ram&lt;/p&gt; &lt;p&gt;Does anyone have experience with this and what are your thoughts especially on Linux say fedora &lt;/p&gt; &lt;p&gt;I hope this makes sense I'm a bit out of my depth &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unusual_Pride_6480"&gt; /u/Unusual_Pride_6480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kztjgp/how_are_intel_gpus_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzcalh</id>
    <title>llama-server is cooking! gemma3 27b, 100K context, vision on one 24GB GPU.</title>
    <updated>2025-05-30T18:54:42+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama-server has really improved a lot recently. With vision support, SWA (sliding window attention) and performance improvements I've got 35tok/sec on a 3090. P40 gets 11.8 tok/sec. Multi-gpu performance has improved. Dual 3090s performance goes up to 38.6 tok/sec (600W power limit). Dual P40 gets 15.8 tok/sec (320W power max)! Rejoice P40 crew. &lt;/p&gt; &lt;p&gt;I've been writing more guides for the llama-swap wiki and was very surprised with the results. Especially how usable the P40 still are!&lt;/p&gt; &lt;p&gt;llama-swap config (&lt;a href="https://github.com/mostlygeek/llama-swap/wiki/gemma3-27b-100k-context"&gt;source wiki page&lt;/a&gt;): &lt;/p&gt; &lt;p&gt;```yaml macros: &amp;quot;server-latest&amp;quot;: /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --flash-attn -ngl 999 -ngld 999 --no-mmap&lt;/p&gt; &lt;p&gt;# quantize KV cache to Q8, increases context but # has a small effect on perplexity # &lt;a href="https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt; &amp;quot;q8-kv&amp;quot;: &amp;quot;--cache-type-k q8_0 --cache-type-v q8_0&amp;quot;&lt;/p&gt; &lt;p&gt;models: # fits on a single 24GB GPU w/ 100K context # requires Q8 KV quantization &amp;quot;gemma&amp;quot;: env: # 3090 - 35 tok/sec - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40 - 11.8 tok/sec #- &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1&amp;quot; cmd: | ${server-latest} ${q8-kv} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;# Requires 30GB VRAM # - Dual 3090s, 38.6 tok/sec # - Dual P40s, 15.8 tok/sec &amp;quot;gemma-full&amp;quot;: env: # 3090s - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # P40s # - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-eb1,GPU-ea4&amp;quot; cmd: | ${server-latest} --ctx-size 102400 --model /path/to/models/google_gemma-3-27b-it-Q4_K_L.gguf --mmproj /path/to/models/gemma-mmproj-model-f16-27B.gguf --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 # uncomment if using P40s # -sm row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzcalh/llamaserver_is_cooking_gemma3_27b_100k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T18:54:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzn4ix</id>
    <title>Running Deepseek R1 0528 q4_K_M and mlx 4-bit on a Mac Studio M3</title>
    <updated>2025-05-31T03:12:50+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Mac Model:&lt;/strong&gt; M3 Ultra Mac Studio 512GB, 80 core GPU&lt;/p&gt; &lt;p&gt;First- this model has a shockingly small KV Cache. If any of you saw my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;post about running Deepseek V3 q4_K_M&lt;/a&gt;, you'd have seen that the KV cache buffer in llama.cpp/koboldcpp was 157GB for 32k of context. I expected to see similar here.&lt;/p&gt; &lt;p&gt;Not even close.&lt;/p&gt; &lt;p&gt;64k context on this model is barely 8GB. Below is the buffer loading this model directly in llama.cpp with no special options; just specifying 65536 context, a port and a host. That's it. &lt;del&gt;No MLA&lt;/del&gt;, no quantized cache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Llama.cpp runs MLA be default.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;65536 context:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama_kv_cache_unified: Metal KV buffer size = 8296.00 MiB&lt;/p&gt; &lt;p&gt;llama_kv_cache_unified: KV self size = 8296.00 MiB, K (f16): 4392.00 MiB, V (f16): 3904.00 MiB&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;131072k context:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama_kv_cache_unified: Metal KV buffer size = 16592.00 MiB&lt;/p&gt; &lt;p&gt;llama_kv_cache_unified: KV self size = 16592.00 MiB, K (f16): 8784.00 MiB, V (f16): 7808.00 MiB&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Speed wise- it's a fair bit on the slow side, but if this model is as good as they say it is, I really don't mind.&lt;/p&gt; &lt;p&gt;Example: ~11,000 token prompt:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp server&lt;/strong&gt; (no flash attention) &lt;strong&gt;(~9 minutes)&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 144330.20 ms / 11090 tokens (13.01 ms per token, &lt;strong&gt;76.84 tokens per second&lt;/strong&gt;)&lt;br /&gt; eval time = 390034.81 ms / 1662 tokens (234.68 ms per token, &lt;strong&gt;4.26 tokens per second&lt;/strong&gt;)&lt;br /&gt; total time = 534365.01 ms / 12752 tokens&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;MLX 4-bit&lt;/strong&gt; for the same prompt (~2.5x speed) &lt;strong&gt;(245sec or ~4 minutes)&lt;/strong&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;2025-05-30 23:06:16,815 - DEBUG - Prompt: &lt;strong&gt;189.462 tokens-per-sec&lt;/strong&gt;&lt;br /&gt; 2025-05-30 23:06:16,815 - DEBUG - Generation: &lt;strong&gt;11.154 tokens-per-sec&lt;/strong&gt;&lt;br /&gt; 2025-05-30 23:06:16,815 - DEBUG - Peak memory: 422.248 GB&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Note- Tried flash attention in llama.cpp, and that went horribly. The prompt processing slowed to an absolute crawl. It would have taken longer to process the prompt than the non -fa run took for the whole prompt + response.&lt;/p&gt; &lt;p&gt;Another important note- &lt;strong&gt;when they say not to use System Prompts, they mean it&lt;/strong&gt;. I struggled with this model at first, until I finally completely stripped the system prompt out and jammed all my instructions into the user prompt instead. The model became far more intelligent after that. Specifically, if I passed in a system prompt, it would NEVER output the initial &amp;lt;think&amp;gt; tag no matter what I said or did. But if I don't use a system prompt, it always outputs the initial &amp;lt;think&amp;gt; tag appropriately.&lt;/p&gt; &lt;p&gt;I haven't had a chance to deep dive into this thing yet to see if running a 4bit version really harms the output quality or not, but I at least wanted to give a sneak peak into what it looks like running it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzn4ix/running_deepseek_r1_0528_q4_k_m_and_mlx_4bit_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T03:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l02hmq</id>
    <title>deepseek/deepseek-r1-0528-qwen3-8b stuck on infinite tool loop. Any ideas?</title>
    <updated>2025-05-31T17:23:44+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've downloaded the official Deepseek distillation from their official sources and it does seem a touch smarter. However, when using tools, it often gets stuck forever trying to use them. Do you know why this is going on, and if we have any workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l02hmq/deepseekdeepseekr10528qwen38b_stuck_on_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T17:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzrujd</id>
    <title>GPU-enabled Llama 3 inference in Java from scratch</title>
    <updated>2025-05-31T08:05:09+00:00</updated>
    <author>
      <name>/u/mikebmx1</name>
      <uri>https://old.reddit.com/user/mikebmx1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"&gt; &lt;img alt="GPU-enabled Llama 3 inference in Java from scratch" src="https://external-preview.redd.it/zGi8MlTX6KdNBAxbzYKKNHv02BqKK2KcEgsGUzULDDk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=114333efebb949ba294cb4a365eba5f89e344050" title="GPU-enabled Llama 3 inference in Java from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikebmx1"&gt; /u/mikebmx1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/beehive-lab/GPULlama3.java"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrujd/gpuenabled_llama_3_inference_in_java_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:05:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l01bfe</id>
    <title>Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!</title>
    <updated>2025-05-31T16:34:06+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt; &lt;img alt="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" src="https://external-preview.redd.it/ZG81Yjhkenk2NTRmMdgqNWupVXy_ZPAevb2tTQhA9R_THDnUrLckbufzOiAz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6b4e7c264f8e0ba31013dc39fa0ffa3f2a2d820" title="Giving Qwen 3 0.6B a Toolbelt in the form of MCP Support, Running Locally in Your Browser with Adjustable Thinking!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all. I have spent a couple weekends giving the tiny Qwen3 0.6B model the ability to show off its underutilized tool calling abilities by using remote MCP servers. I am pleasantly surprised at how well it can chain tools. Additionally, I gave it the option to limit how much it can think to avoid the &amp;quot;overthinking&amp;quot; issue reasoning models (especially Qwen) can have. &lt;a href="https://muellerzr.github.io/til/end_thinking.html"&gt;This implementation was largely inspired by a great article from Zach Mueller outlining just that.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Also, this project is an adaptation of &lt;a href="https://github.com/huggingface/transformers.js-examples/tree/main/qwen3-webgpu"&gt;Xenova's Qwen3 0.6 WebGPU code in transformers.js-examples&lt;/a&gt;, it was a solid starting point to work with Qwen3 0.6B. &lt;/p&gt; &lt;p&gt;Check it out for yourselves!&lt;/p&gt; &lt;p&gt;HF Space Link: &lt;a href="https://huggingface.co/spaces/callbacked/Qwen3-MCP"&gt;https://huggingface.co/spaces/callbacked/Qwen3-MCP&lt;/a&gt;&lt;br /&gt; Repo: &lt;a href="https://github.com/callbacked/qwen3-mcp"&gt;https://github.com/callbacked/qwen3-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Footnote: With Qwen3 8B having a distillation from R1-0528, I really hope we can see that trickle down to other models including Qwen3 0.6B. Seeing how much more intelligent the other models can get off of R1-0528 would be a cool thing see in action!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r495cezy654f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l01bfe/giving_qwen_3_06b_a_toolbelt_in_the_form_of_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T16:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzlb8g</id>
    <title>Unlimited Speech to Speech using Moonshine and Kokoro, 100% local, 100% open source</title>
    <updated>2025-05-31T01:34:33+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rhulha.github.io/Speech2Speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzlb8g/unlimited_speech_to_speech_using_moonshine_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T01:34:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzq4fp</id>
    <title>M3 Ultra Binned (256GB, 60-Core) vs Unbinned (512GB, 80-Core) MLX Performance Comparison</title>
    <updated>2025-05-31T06:09:42+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I recently decided to invest in an M3 Ultra model for running LLMs, and after a &lt;em&gt;lot&lt;/em&gt; of deliberation, I wanted to share some results that might help others in the same boat.&lt;/p&gt; &lt;p&gt;One of my biggest questions was the actual performance difference between the binned and unbinned M3 Ultra models. It's pretty much impossible for a single person to own and test both machines side-by-side, so there aren't really any direct, apples-to-apples comparisons available online.&lt;/p&gt; &lt;p&gt;While there are some results out there (like on the llama.cpp GitHub, where someone compared the 8B model), they didn't really cover my use case—I'm using MLX as my backend and working with much larger models (235B and above). So the available benchmarks weren’t all that relevant for me.&lt;/p&gt; &lt;p&gt;To be clear, my main reason for getting the M3 Ultra wasn't to run Deepseek models—those are just way too large to use with long context windows, even on the Ultra. My primary goal was to run the Qwen3 235B model.&lt;/p&gt; &lt;p&gt;So I’m sharing my own benchmark results comparing 4-bit and 6-bit quantization for the Qwen3 235B model on a decently long context window (~10k tokens). Hopefully, this will help anyone else who's been stuck with the same questions I had!&lt;/p&gt; &lt;p&gt;Let me know if you have questions, or if there’s anything else you want to see tested.&lt;br /&gt; Just keep in mind that the model sizes are massive, so I might not be able to cover every possible benchmark.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Side note: In the end, I decided to return the 256GB model and stick with the 512GB one. Honestly, 256GB of memory seemed sufficient for most use cases, but since I plan to keep this machine for a while (and also want to experiment with Deepseek models), I went with 512GB. I also think it’s worth using the 80-core GPU. The pp speed difference was bigger than I expected, and for me, that’s one of the biggest weaknesses of Apple silicon. Still, thanks to the MoE architecture, the 235B models run at a pretty usable speed!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Binned (256GB, 60-Core)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 40.09&lt;br /&gt; prompt_eval_duration: 35.41&lt;br /&gt; generation_duration: 4.68&lt;br /&gt; prompt_tokens_per_second: 260.58&lt;br /&gt; generation_tokens_per_second: 22.6&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 43.23&lt;br /&gt; prompt_eval_duration: 38.9&lt;br /&gt; generation _duration: 4.33&lt;br /&gt; prompt_tokens_per_second: 237.2&lt;br /&gt; generation_tokens_per_second: 18.93&lt;/p&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra Unbinned (512GB, 80-Core)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-4bit-DWQ&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 106&lt;br /&gt; total_tokens: 9334&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 31.33&lt;br /&gt; prompt_eval_duration: 26.76&lt;br /&gt; generation_duration: 4.57&lt;br /&gt; prompt_tokens_per_second: 344.84&lt;br /&gt; generation_tokens_per_second: 23.22&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-235B-A22B-6bit-MLX&lt;/strong&gt;&lt;br /&gt; prompt_tokens: 9228&lt;br /&gt; completion_tokens: 82&lt;br /&gt; total_tokens: 9310&lt;br /&gt; cached_tokens: 0&lt;br /&gt; total_time: 32.56&lt;br /&gt; prompt_eval_duration: 28.31&lt;br /&gt; generation _duration: 4.25&lt;br /&gt; prompt_tokens_per_second: 325.96&lt;br /&gt; generation_tokens_per_second: 19.31&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzq4fp/m3_ultra_binned_256gb_60core_vs_unbinned_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T06:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzw65c</id>
    <title>AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market</title>
    <updated>2025-05-31T12:41:19+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt; &lt;img alt="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" src="https://external-preview.redd.it/bnCoi_QMP0ucYNmMDpD8YzNjydtxrrZkZROQJhXvr2s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=981e9ab36322decfefbeb6831d8e913c9f0d6692" title="AMD Octa-core Ryzen AI Max Pro 385 Processor Spotted On Geekbench: Affordable Strix Halo Chips Are About To Enter The Market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-pro-385-spotted-on-geekbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzw65c/amd_octacore_ryzen_ai_max_pro_385_processor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T12:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzzshu</id>
    <title>Google lets you run AI models locally</title>
    <updated>2025-05-31T15:29:15+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/"&gt;https://techcrunch.com/2025/05/31/google-quietly-released-an-app-that-lets-you-download-and-run-ai-models-locally/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzzshu/google_lets_you_run_ai_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T15:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzrfop</id>
    <title>Getting sick of companies cherry picking their benchmarks when they release a new model</title>
    <updated>2025-05-31T07:36:35+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get why they do it. They need to hype up their thing etc. But cmon a bit of academic integrity would go a long way. Every new model comes with the claim that it outcompetes older models that are 10x their size etc. Like, no. Maybe I'm an old man shaking my fist at clouds here I don't know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzrfop/getting_sick_of_companies_cherry_picking_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T07:36:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1kze1r6</id>
    <title>Ollama run bob</title>
    <updated>2025-05-30T20:06:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt; &lt;img alt="Ollama run bob" src="https://preview.redd.it/v4krpd9g7z3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2201e590a1b08cca19a9ca4d26c56ddf0e869e85" title="Ollama run bob" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4krpd9g7z3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kze1r6/ollama_run_bob/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-30T20:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzv322</id>
    <title>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</title>
    <updated>2025-05-31T11:41:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzv322/surprisingly_fast_aigenerated_kernels_we_didnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T11:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kzsa70</id>
    <title>China is leading open source</title>
    <updated>2025-05-31T08:35:25+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt; &lt;img alt="China is leading open source" src="https://preview.redd.it/6stw9ivzw24f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87af4f2951867765dd0c43808b34253b587103b5" title="China is leading open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6stw9ivzw24f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-31T08:35:25+00:00</published>
  </entry>
</feed>
