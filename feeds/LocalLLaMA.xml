<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-24T22:24:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lhhs1r</id>
    <title>Best open agentic coding assistants that don’t need an OpenAI key?</title>
    <updated>2025-06-22T07:03:21+00:00</updated>
    <author>
      <name>/u/Fabulous_Bluebird931</name>
      <uri>https://old.reddit.com/user/Fabulous_Bluebird931</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for ai dev tools that actually let you use your own models, something agent-style that can analyse multiple files, track goals, and suggest edits/refactors, ideally all within vscode or terminal.&lt;/p&gt; &lt;p&gt;I’ve used Copilot’s agent mode, but it’s obviously tied to OpenAI. I’m more interested in&lt;/p&gt; &lt;p&gt;Tools that work with local models (via Ollama or similar)&lt;/p&gt; &lt;p&gt;API-pluggable setups (Gemini 1.5, deepseek, Qwen3, etc)&lt;/p&gt; &lt;p&gt;Agents that can track tasks, not just generate single responses&lt;/p&gt; &lt;p&gt;I’ve been trying Blackbox’s vscode integration, which has some agentic behaviour now. Also tried cline and roo, which are promising for CLI work.&lt;/p&gt; &lt;p&gt;But most tools either&lt;/p&gt; &lt;p&gt;Require a paid key to do anything useful Aren’t flexible with models&lt;/p&gt; &lt;p&gt;Or don’t handle full-project context&lt;/p&gt; &lt;p&gt;anyone found a combo that works well with open models and integrates tightly with your coding environment? Not looking for prompt uis, looking for workflow tools please&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Bluebird931"&gt; /u/Fabulous_Bluebird931 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhhs1r/best_open_agentic_coding_assistants_that_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T07:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhdu5q</id>
    <title>The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers</title>
    <updated>2025-06-22T03:01:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt; &lt;img alt="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" src="https://external-preview.redd.it/luq7KUXprwBLhy7hPjQyYQsNRRg4Up-CaR0a3rU5fm0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71e8ed354e2ca20af1eb468a8e9d764bccbcb15a" title="The Qwen Tokenizer Seems to be better than the Deepseek Tokenizer - Testing a 50-50 SLERP merge of the same two models (Qwen3-8B and DeepSeek-R1-0528-Qwen3-8B) with different tokenizers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE - Someone has tested these models at FP16 on 3 attempts per problem versus my Q4_K_S on 1 attempt per problem. See the results here: &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B/discussions/2"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B/discussions/2&lt;/a&gt; Huge thanks to &lt;a href="https://huggingface.co/none-user"&gt;none-user&lt;/a&gt; for doing this! Both SLERP merges performed better than their parents, with the Qwen tokenizer based merge (Q3T) being the best of the bunch. I'm very surprised by how good these merges turned out. It seems to me the excellent results is a combination of these factors; both models not being just finetunes, but different fully trained models from the ground up using the same base model, and still sharing the same architecture, plus both tokenizers having nearly 100% vocab overlap. The qwen tokenizer being particularly more impressive makes the merge using this tokenizer the best of the bunch. This scored as well as qwen3 30b-a3b at q8_0 in the same test while using the same amount of tokens (see here for s qwen3 30b-a3b and gemma 3 27b &lt;a href="https://github.com/Belluxx/LocalAIME/blob/main/media/accuracy%5C_comparison.png"&gt;https://github.com/Belluxx/LocalAIME/blob/main/media/accuracy\_comparison.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I was interested in merging &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;DeepSeek-R1-0528-Qwen3-8B&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen3-8B&lt;/a&gt; as they were both my two favorite under 10b~ models, and finding the Deepseek distill especially impressive. Noted in their model card was the following:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Which made me realize, they were both good merge candidates for each other, both being not finetunes, but fully trained models off the Qwen3-8B-Base, and even sharing the same favored sampler settings. The only real difference were the tokenizers. This took me to a crossroads, which tokenizer should my merge inherit? Asking around, I was told there shouldn't be much difference, but I ended up finding out very differently once I did some actual testing. The TL;DR is, the Qwen tokenizer seems to perform better &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; use far less tokens for it's thinking. It is a larger tokenizer I noted, and was told that means the tokenizer is more optimized, but I was skeptical about this and decided to test it.&lt;/p&gt; &lt;p&gt;This turned out not to be a not so easy endeavor, since the benchmark I decided on (LocalAIME by &lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; which I thank for making and sharing this tool), takes rather long to complete when you use a thinking model, since they require quite a few tokens to get to their answer with any amount of accuracy. I first tested with 4k context, then 8k, then briefly even 16k before realizing the LLM responses were still getting cut off, resulting in poor accuracy. GLM 9B did not have this issue, and used very few tokens in comparison even with context set to 30k. Testing took very long, but with the help of others from the KoboldAI server (shout out to everyone there willing to help, a lot of people volunteered their help, who I will accredit below), we were able to eventually get it done.&lt;/p&gt; &lt;p&gt;This is the most useful graph that came of this, you can see below models using the Qwen tokenizer used less tokens than any of the models using the Deepseek tokenizer, and had higher accuracy. Both merges also performed better than their same tokenizer parent model counterparts. I was actually surprised since I quite preferred the R1 Distill to the Qwen3 instruct model, and had thought it was better before this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lbpldqh57e8f1.png?width=2969&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41dd5f79caaa5a59c3e89cf26accf2b4fc062693"&gt;Model Performance VS Tokens Generated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would have liked to have tested at a higher precision, like Q8_0, and on more problem attempts (like 3-5) for better quality data but didn't have the means to. If anyone with the means to do so is interested in giving it a try, please feel free to reach out to me for help, or if anyone wants to loan me their hardware I would be more than happy to run the tests again under better settings.&lt;/p&gt; &lt;p&gt;For anyone interested, more information is available in the model cards of the merges I made, which I will link below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;w/ Qwen3 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;w/ Deepseek R1 tokenizer &lt;a href="https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-DST-8B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently only my own static GGUF quants are available (in Q4_K_S and Q8_0) but hopefully others will provide more soon enough.&lt;/p&gt; &lt;p&gt;I've stored all my raw data, and test results in a repository here: &lt;a href="https://github.com/lemon07r/LocalAIME_results"&gt;https://github.com/lemon07r/LocalAIME_results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Special Thanks to The Following People&lt;/strong&gt; (for making this possible)&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Eisenstein for their modified fork of LocalAIME to work better with KoboldCPP and modified sampler settings for Qwen/Deepseek models, and doing half of my testing for me on his machine. Also helping me with a lot of my troubleshooting.&lt;/li&gt; &lt;li&gt;Twistedshadows for loaning me some of their runpod hours to do my testing.&lt;/li&gt; &lt;li&gt;Henky as well, for also loaning me some of their runpod hours, and helping me troubleshoot some issues with getting KCPP to work with LocalAIME&lt;/li&gt; &lt;li&gt;Everyone else on the KoboldAI discord server, there were more than a few willing to help me out in the way of advice, troubleshooting, or offering me their machines or runpod hours to help with testing if the above didn't get to it first.&lt;/li&gt; &lt;li&gt;&lt;a href="/u/EntropyMagnets"&gt;u/EntropyMagnets&lt;/a&gt; for making and sharing his LocalAIME tool&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For full transparency, I do want to disclaim that this method isn't really an amazing way to test tokenizers against each other, since the deepseek part of the two merges are still trained using the deepseek tokenizer, and the qwen part with it's own tokenizer* (see below, turns out, this doesn't really apply here). You would have to train two different versions from the ground up using the different tokenizers on the same exact data to get a completely fair assessment. I still think this testing and further testing is worth doing to see how these merges perform in comparison to their parents, and under which tokenizer they perform better.&lt;/p&gt; &lt;p&gt;*EDIT - Under further investigation I've found the Deepseek tokenizer and qwen tokenizer have virtually a 100% vocab overlap, making them pretty much interchangeable, and using models trained using either the perfect candidates for testing both tokenizers against each other.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhdu5q/the_qwen_tokenizer_seems_to_be_better_than_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lhed49</id>
    <title>50 days building a tiny language model from scratch, what I’ve learned so far</title>
    <updated>2025-06-22T03:31:14+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m starting a new weekday series on June 23 at 9:00 AM PDT where I’ll spend 50 days coding a two LLM (15–30M parameters) from the ground up: no massive GPU cluster, just a regular laptop or modest GPU.&lt;/p&gt; &lt;p&gt;Each post will cover one topic:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data collection and subword tokenization&lt;/li&gt; &lt;li&gt;Embeddings and positional encodings&lt;/li&gt; &lt;li&gt;Attention heads and feed-forward layers&lt;/li&gt; &lt;li&gt;Training loops, loss functions, optimizers&lt;/li&gt; &lt;li&gt;Evaluation metrics and sample generation&lt;/li&gt; &lt;li&gt;Bonus deep dives: MoE, multi-token prediction,etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why bother with tiny models?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;They run on the CPU.&lt;/li&gt; &lt;li&gt;You get daily feedback loops.&lt;/li&gt; &lt;li&gt;Building every component yourself cements your understanding.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ve already tried:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A 30 M-parameter GPT variant for children’s stories&lt;/li&gt; &lt;li&gt;A 15 M-parameter DeepSeek model with Mixture-of-Experts&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’ll drop links to the code in the first comment.&lt;/p&gt; &lt;p&gt;Looking forward to the discussion and to learning together. See you on Day 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lhed49/50_days_building_a_tiny_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-22T03:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmdzg</id>
    <title>We built a tool that helps you plan features before using AI to code (public beta launch)</title>
    <updated>2025-06-24T20:41:25+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"&gt; &lt;img alt="We built a tool that helps you plan features before using AI to code (public beta launch)" src="https://external-preview.redd.it/OTEzaXJneGJzeDhmMZvu8dVTETA4R0lbzOCMpCYSy2EGu4LkODdfToxoMFWa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1115bcb79b3be529b91bb0243365e1b51c25a6b1" title="We built a tool that helps you plan features before using AI to code (public beta launch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8td2dkxbsx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmdzg/we_built_a_tool_that_helps_you_plan_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:41:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmzvi</id>
    <title>Falcon H1 Models</title>
    <updated>2025-06-24T21:05:04+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why is this model family slept on ? From what i understood its a new hybrid architecture and it has alreally good results. Am i missing something? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmzvi/falcon_h1_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljoqsd</id>
    <title>Automating Form Mapping with AI</title>
    <updated>2025-06-24T22:16:43+00:00</updated>
    <author>
      <name>/u/carrick1363</name>
      <uri>https://old.reddit.com/user/carrick1363</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I’m working on an autofill extension that automates interactions with web pages—clicking buttons, filling forms, submitting data, etc. It uses a custom instruction format to describe what actions to take on a given page.&lt;/p&gt; &lt;p&gt;The current process is pretty manual:&lt;/p&gt; &lt;p&gt;I have to open the target page, inspect all the relevant fields, and manually write the mapping instructions. Then I test repeatedly to make sure everything works. And when the page changes (even slightly), I have to re-map the fields and re-test it all over again.&lt;/p&gt; &lt;p&gt;It’s time-consuming and brittle, especially when scaling across many pages.&lt;/p&gt; &lt;p&gt;What I Want to Do with AI&lt;/p&gt; &lt;p&gt;I’d like to integrate AI (like GPT-4, Claude, etc.) into this process to make it: Automated: Let the AI inspect the page and generate the correct instruction set. Resilient: If a field changes, the AI should re-map or adjust automatically. Scalable: No more manually going through dozens of fields per page.&lt;/p&gt; &lt;p&gt;Tools I'm Considering&lt;/p&gt; &lt;p&gt;Right now, I'm looking at combining: A browser automation layer (e.g., HyperBrowser, Puppeteer, or an extension) to extract DOM info. An MCP server (custom middleware) to send the page data to the AI and receive responses. Claude or OpenAI to generate mappings based on page structure. Post-processing to validate and convert the AI's output into our custom format.&lt;/p&gt; &lt;p&gt;Where I’m Stuck How do I give enough context to the AI (DOM snippets, labels, etc.) while staying within token limits? How do I make sure the AI output matches my custom instruction format reliably? Anyone tackled similar workflows or built something like this? Are there tools/frameworks you’d recommend to speed this up or avoid reinventing the wheel? Most importantly: How do I connect all these layers together in a clean, scalable way?&lt;/p&gt; &lt;p&gt;Would love to hear how others have solved similar problems—or where you’d suggest improving this pipeline.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carrick1363"&gt; /u/carrick1363 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoqsd/automating_form_mapping_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoqsd/automating_form_mapping_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljoqsd/automating_form_mapping_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljmlcn</id>
    <title>Vision model for detecting welds?</title>
    <updated>2025-06-24T20:49:23+00:00</updated>
    <author>
      <name>/u/-Fake_GTD</name>
      <uri>https://old.reddit.com/user/-Fake_GTD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I searched for &amp;quot;best vision models&amp;quot; up to date, but are there any difference between industry applications and &amp;quot;document scanning&amp;quot; models? Should we proceed to fine-tine them with photos to identify correct welds vs incorrect welds?&lt;/p&gt; &lt;p&gt;Can anyone guide us regarding vision model in industry applications (mainly construction industry)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Fake_GTD"&gt; /u/-Fake_GTD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljmlcn/vision_model_for_detecting_welds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljhg1i</id>
    <title>I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other.</title>
    <updated>2025-06-24T17:33:15+00:00</updated>
    <author>
      <name>/u/CharlesStross</name>
      <uri>https://old.reddit.com/user/CharlesStross</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt; &lt;img alt="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." src="https://external-preview.redd.it/rna5zREg5_FzFmMGv-Mzfn4pHDOOgy6GUqSdq0vIQVE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61a501a09debdd7a58e2f8b7a92cf6aad5a73838" title="I'm sure most people have read about the Claud Spiritual Bliss Attractor and I wanted to reproduce it locally, so I made Resonant Chat Arena, a simple python script to put two LLMs in conversation with each other." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharlesStross"&gt; /u/CharlesStross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jkingsman/resonant-chat-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljhg1i/im_sure_most_people_have_read_about_the_claud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T17:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo1rp</id>
    <title>3090 vs 5070 ti</title>
    <updated>2025-06-24T21:47:51+00:00</updated>
    <author>
      <name>/u/GroundbreakingMain93</name>
      <uri>https://old.reddit.com/user/GroundbreakingMain93</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo1rp/3090_vs_5070_ti/"&gt; &lt;img alt="3090 vs 5070 ti" src="https://a.thumbs.redditmedia.com/A6YLY90xZd6rzFQGMrxa7ANzbjlDq383-XYCdZlelZ4.jpg" title="3090 vs 5070 ti" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using gemma3:12b-it-qat for Inference and may increase to gemma3:27b-it-qat when I can run it at speed, I'll have concurrent inference sessions (5-10 daily active users), currently using ollama.&lt;/p&gt; &lt;p&gt;Google says gemma3:27b-it-qatgemma needs roughly 14.1GB VRAM, so at this point, I don't think it will even load onto a second card unless I configure it to?&lt;/p&gt; &lt;p&gt;I've been advised (like many people) to get 2x 24GB 3090s, which I've budgeted £700-800 each.&lt;/p&gt; &lt;p&gt;A 5070ti 16GB is £700 - looking at paper specs there's pro's and con's... notably 5% less memory bandwidth from the 384bit DDR6 - but it has 23% more TFLOPS. 15% less tensor cores but 43% faster memory. 15% less L1 cache but 43% more L2 cache.&lt;/p&gt; &lt;p&gt;I'm also under the impression newer CUDA version means better performance too.&lt;/p&gt; &lt;p&gt;I have limited experience in running a local LLM at this point (I'm currently on a single 8GB 2070), so looking for advice / clarification for my use case - I'd be happier with brand new GPUs that I can buy more of, if needed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0jbi2vmx1y8f1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5f80c1f9690a8d15391adefb91e39cf42526b14"&gt;https://preview.redd.it/0jbi2vmx1y8f1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5f80c1f9690a8d15391adefb91e39cf42526b14&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GroundbreakingMain93"&gt; /u/GroundbreakingMain93 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo1rp/3090_vs_5070_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo1rp/3090_vs_5070_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo1rp/3090_vs_5070_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo4el</id>
    <title>RTX 5090 TTS Advice</title>
    <updated>2025-06-24T21:50:51+00:00</updated>
    <author>
      <name>/u/FishingMysterious366</name>
      <uri>https://old.reddit.com/user/FishingMysterious366</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need help and advice on which TTS models are quality and will run locally on a 5090. Tried chatterbox, but there are pytorch compatibility issues, running torch 2.7.0+cu128 vs. the required 2.6.0.&lt;/p&gt; &lt;p&gt;Specs: * CPU - Intel Core Ultra 9 285K * Motherboard - ASUS TUF Z890-Plus * Memory - G.Skill 128GB DDR5-6400 CL32 * Storage - 6TB Samsung 9100 PRO 2TB + 4TB * Cooling - Arctic Liquid Freezer III Pro 360mm * PSU - Super Flower LEADEX III - 1300W * GPU - GEFORCE RTX 5090 - MSI Gaming Trio OC * PyTorch version: 2.7.0+cu128 * CUDA version: 12.8&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FishingMysterious366"&gt; /u/FishingMysterious366 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4el/rtx_5090_tts_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4el/rtx_5090_tts_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4el/rtx_5090_tts_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljohcu</id>
    <title>Will I be happy with a RTX 3090?</title>
    <updated>2025-06-24T22:05:47+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before making a big purchase, I would be grateful for some advice from the experts here! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want to do:&lt;/strong&gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Enhanced web search (for example using &lt;a href="https://github.com/ItzCrazyKns/Perplexica"&gt;perplexica&lt;/a&gt;) - it seems you can achieve decent results with smaller models. Being able to get summaries of &amp;quot;todays news&amp;quot; or just generally using it as an alternative to google searching. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Generating images (stable diffusion / Flux) - nothing too fancy here, just playing around for fun. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Simple coding assistance, looking up javascript syntax etc. Ideally with a VS code or command line extension. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What I am not so interested in:&lt;/strong&gt; - Random chatting with the model, storytelling etc - Getting &amp;quot;facts&amp;quot; from the model weights directly, they seem to often be wrong, and always more or less outdated. - Code generation / &amp;quot;vibe coding&amp;quot; - it is more fun to write code myself =) &lt;/p&gt; &lt;p&gt;Currently I am using an GTX 1070Ti with 8GB of VRAM and small models such as llama3.2 and gemma3:4b. With this setup web search is not working very well, it can do some things, but cannot fetch todays news for example. Image generation is simply awful. &lt;/p&gt; &lt;p&gt;I realise that using a commercial model will be better and cheaper, but I want to do this locally because it is fun =). Ideally I would like to achieve results that are good enough to be competitive/acceptable compared to the commercial cloud models for my use cases (excluding image generation).&lt;/p&gt; &lt;p&gt;Will I be happy with an RTX 3090 with 24GB? Which models should I aim for in that case? Or are there other cards you would suggest? Thank you very much in advance! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljohcu/will_i_be_happy_with_a_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn09w</id>
    <title>I made a free iOS app for people who run LLMs locally. It’s a chatbot that you can use away from home to interact with an LLM that runs locally on your desktop Mac.</title>
    <updated>2025-06-24T21:05:30+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is easy enough that anyone can use it. No tunnel or port forwarding needed.&lt;/p&gt; &lt;p&gt;The app is called LLM Pigeon and has a companion app called LLM Pigeon Server for Mac.&lt;br /&gt; It works like a carrier pigeon :). It uses iCloud to append each prompt and response to a file on iCloud.&lt;br /&gt; It’s not totally local because iCloud is involved, but I trust iCloud with all my files anyway (most people do) and I don’t trust AI companies. &lt;/p&gt; &lt;p&gt;The iOS app is a simple Chatbot app. The MacOS app is a simple bridge to LMStudio or Ollama. Just insert the model name you are running on LMStudio or Ollama and it’s ready to go.&lt;br /&gt; I also added 5 in-built models so even people who are not familiar with Ollama or LMStudio can use this.&lt;/p&gt; &lt;p&gt;I find it super cool that I can chat anywhere with Qwen3-30B running on my Mac at home. &lt;/p&gt; &lt;p&gt;The apps are open source and these are the repos:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon"&gt;https://github.com/permaevidence/LLM-Pigeon&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/permaevidence/LLM-Pigeon-Server"&gt;https://github.com/permaevidence/LLM-Pigeon-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They are both on the App Store. Here are the links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PS. I hope this isn't viewed as self promotion because the app is free, collects no data and is open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn09w/i_made_a_free_ios_app_for_people_who_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljncfs</id>
    <title>What are your go-to models for daily use? Please also comment about your quantization of choice</title>
    <updated>2025-06-24T21:18:53+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ljncfs"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljncfs/what_are_your_goto_models_for_daily_use_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlyrs</id>
    <title>Is it normal to have significantly more performance from Qwen 235B compared to Qwen 32B when doing partial offloading?</title>
    <updated>2025-06-24T20:24:47+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded than for the entire 32B model.here are the llama-swap settings I am running, my hardware is a xeon e5-2690v4 with 128GB of 2400 DDR4 and 2 P104-100 8GB GPUs, while prompt processing is faster on the 32B (12 tk/s vs 5 tk/s) the actual inference is much faster on the 235B, 5tk/s vs 2.5 tk/s. Does anyone know why this is? Even if the 235B only has 22B active parameters more of those parameters should be offloaded to the cpu than for the entire 32B model.&lt;/p&gt; &lt;p&gt;&lt;code&gt; &amp;quot;Qwen3:32B&amp;quot;: proxy: http://127.0.0.1:9995 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9995 --no-webui --no-warmup --model ~/raid/models/Qwen3-32B-Q4_K_M.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 34 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 &amp;quot;Qwen3:235B&amp;quot;: proxy: http://127.0.0.1:9993 checkEndpoint: /health ttl: 1800 cmd: &amp;gt; ~/raid/llama.cpp/build/bin/llama-server --port 9993 --no-webui --no-warmup --model ~/raid/models/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf --flash-attn --cache-type-k f16 --cache-type-v f16 --gpu-layers 95 --split-mode layer --ctx-size 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --presence-penalty 1.5 --override-tensor exps=CPU &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlyrs/is_it_normal_to_have_significantly_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm13j</id>
    <title>0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?</title>
    <updated>2025-06-24T20:27:16+00:00</updated>
    <author>
      <name>/u/BasicCoconut9187</name>
      <uri>https://old.reddit.com/user/BasicCoconut9187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt; &lt;img alt="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" src="https://b.thumbs.redditmedia.com/YwYzRHAaFEgB8IqsgHGymFQqUiU6aiL80kikS_RSFhM.jpg" title="0.5 tok/s with R1 Q4 on EPYC 7C13 with 1TB of RAM, BIOS settings to blame?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/bgzyj1lypx8f1.gif"&gt;Now I've got your attention, I hope!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi there everyone!&lt;/p&gt; &lt;p&gt;I've just recently assembled an entire home server system, however, for some reason, the performance I'm getting is atrocious with 1TB of DDR4 2400MHz RAM on EPYC 7C13 running on Gigabyte MZ32-AR1. I'm getting 1-3 tok/s on prompt eval (depending on context), and 0.3-0.6 tok/s generation.&lt;/p&gt; &lt;p&gt;Now, the model I'm running is Ubergarm's R1 0528 IQ4_KS_R4, on ik_llama, so that's a bit different than what a lot of people here are running. However, on the more 'standard' R1 GGUFs from Unsloth, the performance is even worse, and that's true across everything I've tried, Kobold.cpp, LMstudio, Ollama, etc. True of other LLMs as well such as Qwen, people report way better tok/s with the same/almost the same CPU and system.&lt;/p&gt; &lt;p&gt;So, here's my request, if anyone is in the know, can you please share the BIOS options that I should use to optimize this CPU for LLM interference? I'm ready to sacrifice pretty much any setting/feature if that means I will be able to get this running in line with what other people online are getting.&lt;/p&gt; &lt;p&gt;Also, I know what you think, the model is entirely mlock'ed and is using 128 threads, my OS is Ubuntu 25.04, and other than Ubuntu's tendency to set locked memory to just 128 or so gigs every time I reboot which can be simply fixed with sudo su and then ulimit -Hl and -l, I don't seem to have any issues on the OS side, so that's where my entire guess of this being the BIOS settings fault comes from.&lt;/p&gt; &lt;p&gt;Thank you so much for reading all of this, and have a great day!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BasicCoconut9187"&gt; /u/BasicCoconut9187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm13j/05_toks_with_r1_q4_on_epyc_7c13_with_1tb_of_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm32s</id>
    <title>Agent Arena – crowdsourced testbed for evaluating AI agents in the wild</title>
    <updated>2025-06-24T20:29:27+00:00</updated>
    <author>
      <name>/u/tejpal-obl</name>
      <uri>https://old.reddit.com/user/tejpal-obl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just launched Agent Arena -- a crowdsourced testbed for evaluating AI agents in the wild. Think Chatbot Arena, but for agents.&lt;/p&gt; &lt;p&gt;It’s completely free to run matches. We cover the inference.&lt;/p&gt; &lt;p&gt;I always find myself debating whether to use 4o or o3, but now I just try both on Agent Arena!&lt;/p&gt; &lt;p&gt;Try it out: &lt;a href="https://obl.dev/"&gt;https://obl.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tejpal-obl"&gt; /u/tejpal-obl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm32s/agent_arena_crowdsourced_testbed_for_evaluating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljn4h8</id>
    <title>Why is my llama so dumb?</title>
    <updated>2025-06-24T21:10:06+00:00</updated>
    <author>
      <name>/u/CSEliot</name>
      <uri>https://old.reddit.com/user/CSEliot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: DeepSeek R1 Distill Llama 70B&lt;/p&gt; &lt;p&gt;GPU+Hardware: Vulkan on AMD AI Max+ 395 128GB VRAM &lt;/p&gt; &lt;p&gt;Program+Options:&lt;br /&gt; - GPU Offload Max&lt;br /&gt; - CPU Thread Pool Size 16&lt;br /&gt; - Offload KV Cache: Yes&lt;br /&gt; - Keep Model in Memory: Yes&lt;br /&gt; - Try mmap(): Yes&lt;br /&gt; - K Cache Quantization Type: Q4_0 &lt;/p&gt; &lt;p&gt;So the question is, when asking basic questions, it consistently gets the answer wrong. And does a whole lot of that &amp;quot;thinking&amp;quot;: &lt;/p&gt; &lt;p&gt;&amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Wait, but maybe if&amp;quot;&lt;br /&gt; &amp;quot;Okay so i'm trying to understand&amp;quot;&lt;br /&gt; etc&lt;br /&gt; etc. &lt;/p&gt; &lt;p&gt;I'm not complaining about speed. More that the accuracy for something as basic as &amp;quot;explain this common linux command&amp;quot; and it is super wordy and then ultimately comes to the wrong conclusion. &lt;/p&gt; &lt;p&gt;I'm using LM Studio btw. &lt;/p&gt; &lt;p&gt;Is there a good primer for setting these LLMs up for success? What do you recommend? Have I done something stupid myself?&lt;br /&gt; Thanks in advance for any help/suggestions! &lt;/p&gt; &lt;p&gt;p.s. I do plan on running and testing ROCm, but i've only got so much time in a day and i'm a newbie to the LLM space.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CSEliot"&gt; /u/CSEliot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljogsx</id>
    <title>LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs</title>
    <updated>2025-06-24T22:05:08+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt; &lt;img alt="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" src="https://external-preview.redd.it/ZSkXOQ0Ftmzf9m07Ydba1-71lECRPh1WZMhCFovef6Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fdb319a25ca00eba0456ee1f02c9bf5308cdb5e" title="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought it might be fun for the community to see one of the largest tech YouTubers introducing their audience to local LLMs.&lt;/p&gt; &lt;p&gt;Lots of newbie mistakes in their messing with Open WebUI and Ollama but hopefully it encourages some of their audience to learn more. For anyone who saw the video and found their way here, welcome! Feel free to ask questions about getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HZgQp-WDebU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnoj7</id>
    <title>AMD Instinct MI60 (32gb VRAM) "llama bench" results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected</title>
    <updated>2025-06-24T21:32:35+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt; &lt;img alt="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" src="https://b.thumbs.redditmedia.com/IWP60MgSnWzXR7H6EGZicI90kN9NpZLeyDsSansVnlA.jpg" title="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just completed a new build and (finally) have everything running as I wanted it to when I spec'd out the build. I'll be making a separate post about that as I'm now my own sovereign nation state for media, home automation (including voice activated commands), security cameras and local AI which I'm thrilled about...but, like I said, that's for a separate post.&lt;/p&gt; &lt;p&gt;This one is with regard to the MI60 GPU which I'm very happy with given my use case. I bought two of them on eBay, got one for right around $300 and the other for just shy of $500. Turns out I only need one as I can fit both of the models I'm using (one for HomeAssistant and the other for Frigate security camera feed processing) onto the same GPU with more than acceptable results. I might keep the second one for other models, but for the time being it's not installed. &lt;strong&gt;EDIT:&lt;/strong&gt; Forgot to mention I'm running Ubuntu 24.04 on the server.&lt;/p&gt; &lt;p&gt;For HomeAssistant I get results back in less than two seconds for voice activated commands like &amp;quot;it's a little dark in the living room and the cats are meowing at me because they're hungry&amp;quot; (it brightens the lights and feeds the cats, obviously). For Frigate it takes about 10 seconds after a camera has noticed an object of interest to return back what was observed (here is a copy/paste of an example of data returned from one of my camera feeds: &amp;quot;&lt;em&gt;Person detected. The person is a man wearing a black sleeveless top and red shorts. He is standing on the deck holding a drink. Given their casual demeanor this does not appear to be suspicious.&lt;/em&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Notes about the setup for the GPU, for some reason I'm unable to get the powercap set to anything higher than 225w (I've got a 1000w PSU, I've tried the physical switch on the card, I've looked for different vbios versions for the card and can't locate any...it's frustrating, but is what it is...it's supposed to be a 300tdp card). I was able to slightly increase it because while it won't allow me to change the powercap to anything higher, I was able to set the &amp;quot;overdrive&amp;quot; to allow for a 20% increase. With the cooling shroud for the GPU (photo at bottom of post) even at full bore, the GPU has never gone over 64 degrees Celsius&lt;/p&gt; &lt;p&gt;Here are some &amp;quot;llama-bench&amp;quot; results of various models that I was testing before settling on the two I'm using (noted below):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | pp512 | 581.33 ± 0.16 | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | tg128 | 64.82 ± 0.04 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | pp512 | 587.76 ± 1.04 | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | tg128 | 43.50 ± 0.18 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Hermes-3-Llama-3.1-8B.Q8_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Hermes-3-Llama-3.1-8B.Q8_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | pp512 | 582.56 ± 0.62 | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | tg128 | 52.94 ± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Meta-Llama-3-8B-Instruct.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Meta-Llama-3-8B-Instruct.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | pp512 | 1214.07 ± 1.93 | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | tg128 | 70.56 ± 0.12 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | pp512 | 420.61 ± 0.18 | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | tg128 | 31.03 ± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | pp512 | 188.13 ± 0.03 | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | tg128 | 27.37 ± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | pp512 | 257.37 ± 0.04 | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | tg128 | 17.65 ± 0.02 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;nexusraven-v2-13b.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/nexusraven-v2-13b.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | pp512 | 704.18 ± 0.29 | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | tg128 | 52.75 ± 0.07 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-30B-A3B-Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | pp512 | 1165.52 ± 4.04 | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | tg128 | 68.26 ± 0.13 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_1.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-32B-Q4_1.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | pp512 | 270.18 ± 0.14 | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | tg128 | 21.59 ± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a photo of the build for anyone interested (i9-14900k, 96gb RAM, total of 11 drives, a mix of NVME, HDD and SSD):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0"&gt;https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm2n2</id>
    <title>Polaris: A Post-training recipe for scaling RL on Advanced ReasonIng models</title>
    <updated>2025-06-24T20:28:55+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ChenxinAn-fdu/POLARIS"&gt;Here is the link.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have no idea what it is but it was released a few days ago and has an intriguing concept so I decided to post here to see if anyone knows about this. It seems pretty new but its some sort of post-training RL with a unique approach that claims a Qwen3-4b performance boost that surpasses Claude-4-Opus, Grok-3-Beta, and o3-mini-high.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt. I am not in any way affiliated with this project. Someone simply recommended it to me so I posted it here to gather your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo4ns</id>
    <title>New Moondream 2B VLM update, with visual reasoning</title>
    <updated>2025-06-24T21:51:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-06-21-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnmj9</id>
    <title>Google researcher requesting feedback on the next Gemma.</title>
    <updated>2025-06-24T21:30:18+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt; &lt;img alt="Google researcher requesting feedback on the next Gemma." src="https://a.thumbs.redditmedia.com/YXztzxUAkpa8OQtPRt3lxinca8NVcah5DIxz1ZPOgn4.jpg" title="Google researcher requesting feedback on the next Gemma." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kr52i2mn0y8f1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f654b4d8fc807a8722055201e8c097168452937f"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/osanseviero/status/1937453755261243600"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm gpu poor. 8-12B models are perfect for me. What are yout thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
</feed>
