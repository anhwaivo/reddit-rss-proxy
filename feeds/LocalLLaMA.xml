<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-03T23:34:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j2ebbu</id>
    <title>GPT-4.5: “Not a frontier model”?</title>
    <updated>2025-03-03T08:44:05+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt; &lt;img alt="GPT-4.5: “Not a frontier model”?" src="https://external-preview.redd.it/8LyYdDmyWBG0ZHsbEltVZnSIMpxW1tK65WzlagG4_rk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50ad74bb245b188a58702733e963e35705c876d2" title="GPT-4.5: “Not a frontier model”?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/gpt-45-not-a-frontier-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ebbu/gpt45_not_a_frontier_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T08:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2f87b</id>
    <title>Approach to translate english to non english.</title>
    <updated>2025-03-03T09:52:50+00:00</updated>
    <author>
      <name>/u/Lamba_ghoda</name>
      <uri>https://old.reddit.com/user/Lamba_ghoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’m working on a solution to translate long English documents into non-English languages, starting with Japanese. The documents I’m dealing with have an average context length of around 9,000 words, so handling long-form translation effectively is a key challenge.&lt;/p&gt; &lt;p&gt;I haven’t started iterating yet, as I’m still researching the best approach for this. Given the length of the documents, I want to ensure that the translation captures context accurately while maintaining efficiency.&lt;/p&gt; &lt;p&gt;What would be the best approach to tackle this problem? Any recommendations on models, pipelines, or strategies for handling long-context translations effectively? Also can I evaluate the model performance without having any human in the loop? &lt;/p&gt; &lt;p&gt;As of resources I have access to all the models on Bedrock but sure I would like to reach a trade off between low cost and performance. Any help will be appreciated. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lamba_ghoda"&gt; /u/Lamba_ghoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2f87b/approach_to_translate_english_to_non_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T09:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2tmr5</id>
    <title>What's your go-to method for generating markdown from HTML?</title>
    <updated>2025-03-03T21:14:05+00:00</updated>
    <author>
      <name>/u/-Django</name>
      <uri>https://old.reddit.com/user/-Django</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to feed some news article data into an LLM. It seems like there's a hundred libraries to convert HTML to markdown. Some use LLMs, some use deterministic algorithms, I don't know what I should use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Django"&gt; /u/-Django &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2uoj5</id>
    <title>Deep Research Prompt Skeleton</title>
    <updated>2025-03-03T21:58:07+00:00</updated>
    <author>
      <name>/u/user0069420</name>
      <uri>https://old.reddit.com/user/user0069420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually use deep research by giving the following prompt, but first provide what I want to do to a reasoning model which structures the requirements in the given prompt style, optionally asking the model to ask clarifying questions for designing the prompt.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;**Task:** [Clearly state the overall goal, focusing on factual information retrieval. Be concise and use action verbs (e.g., &amp;quot;Find information about...&amp;quot;, &amp;quot;Identify...&amp;quot;, &amp;quot;Compile a list of...&amp;quot;).] **Specific Information Needs:** * **Question 1:** [Phrase as a direct question that can be answered with factual information. Avoid subjective terms.] * **Question 2:** [Phrase as a direct question. Be specific about the type of information needed.] * ... (Add more questions as needed. Each question should address a distinct aspect of the task.) **Keywords:** [Provide a comprehensive list of relevant keywords and phrases. Include: * Main topic keywords * Synonyms and related terms * Specific names or locations (if applicable) * Different phrasing variations (e.g., &amp;quot;cost of X,&amp;quot; &amp;quot;price of X,&amp;quot; &amp;quot;X pricing&amp;quot;)] **Constraints (Optional):** * **Time:** [Specify any relevant timeframes, dates, or periods (e.g., &amp;quot;published after 2023,&amp;quot; &amp;quot;during the summer months,&amp;quot; &amp;quot;historical data from the 19th century&amp;quot;).] * **Location:** [Specify geographic limitations or areas of focus (e.g., &amp;quot;within 50 miles of Chicago,&amp;quot; &amp;quot;in Southeast Asia,&amp;quot; &amp;quot;worldwide&amp;quot;).] * **Source Type:** [If you need information from specific types of sources, specify them here (e.g., &amp;quot;academic journals,&amp;quot; &amp;quot;news articles,&amp;quot; &amp;quot;government reports,&amp;quot; &amp;quot;company websites&amp;quot;).] * **Other:** [Any other specific limitations, requirements, or preferences (e.g., &amp;quot;excluding results that mention Y,&amp;quot; &amp;quot;only information available in English,&amp;quot; &amp;quot;focus on sustainable options&amp;quot;).] **Output Format:** * [Specify the desired format for the output. Be precise.] * For example: Use bullet points. * For example: Use a table format. * For example: first provide X and then Y. * For example: Each item must include A, B, and C. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;any suggested improvements? or should I just stick to giving the prompt I give the reasoning model for the details of the task&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/user0069420"&gt; /u/user0069420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2uoj5/deep_research_prompt_skeleton/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2uoj5/deep_research_prompt_skeleton/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2uoj5/deep_research_prompt_skeleton/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:58:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kido</id>
    <title>Tool-calling chatbot success stories</title>
    <updated>2025-03-03T14:58:51+00:00</updated>
    <author>
      <name>/u/edmcman</name>
      <uri>https://old.reddit.com/user/edmcman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had success with creating chatbots that are able to intelligently call tools &lt;em&gt;as needed&lt;/em&gt;? I have been using Langchain, which works great with closed source models. But have had bad luck with open source models. Some of these problems are due to Ollama having incorrect prompt templates. But I also recently tried using Groq, and even Llama 3.3 didn't work that well, for example. &lt;/p&gt; &lt;p&gt;If you have any success stories, I'd love to hear them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kind of tools was your LLM invoking?&lt;/li&gt; &lt;li&gt;What LLM were you using?&lt;/li&gt; &lt;li&gt;What framework/library are you using if any (langchain, smolagents, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edmcman"&gt; /u/edmcman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kido/toolcalling_chatbot_success_stories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2q5ym</id>
    <title>What Reinforcement Learning Method Should I Use for Poker AI with LLMs?</title>
    <updated>2025-03-03T18:51:32+00:00</updated>
    <author>
      <name>/u/musketsreddit</name>
      <uri>https://old.reddit.com/user/musketsreddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What Reinforcement Learning Method Should I Use for Poker AI with LLMs?&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m working on a poker AI project, where I’m training a large language model (LLM) to predict poker actions from given game states (check, call, bet, raise, etc.). My end goal is to create a model that can play poker at a high level, primarily by self-play and opponent modeling. However, I’m running into some challenges that I hope you can help me with!&lt;/p&gt; &lt;h1&gt;Here's the situation:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Training Method: I’m using supervised fine-tuning (SFT) on real poker hand history data to initially teach the LLM how to predict poker actions from game states. This means that the model learns from examples of past games, predicting the actions that players took in various situations.&lt;/li&gt; &lt;li&gt;Self-Play Setup: I plan to eventually move to self-play, where the LLM will play against itself (or other types of models that I create to simulate different play styles). I’ll use these self-play sessions to improve the model over time.&lt;/li&gt; &lt;li&gt;Opponent Pool: I’m creating 6 types of poker players (Loose Aggressive, Loose Passive, Tight Aggressive, Tight Passive, Maniac, and Nit), each trained at 5 different skill levels (Novice, Beg*nner, Intermediate, Advanced, Expert). This gives me a decent range of opponent behavior for training.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The problem:&lt;/h1&gt; &lt;p&gt;Here’s the catch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM I’m using only outputs discrete actions (e.g., bet 3BB, raise to 10BB, etc.) with no access to the probabilities of actions, so I can't directly use methods like policy gradients or Q-learning that rely on action probabilities or continuous action spaces. This makes applying traditional RL methods a bit tricky.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My question:&lt;/h1&gt; &lt;p&gt;Given that I don't have access to action probabilities, what RL method or strategy should I pursue to improve my model? Specifically, I’m looking for a way to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Incorporate self-play with reward-based learning.&lt;/li&gt; &lt;li&gt;Refine the model through reinforcement learning, without the need for continuous probabilities.&lt;/li&gt; &lt;li&gt;Ensure the model doesn’t just overfit to its own prior behavior but learns to adapt and exploit different strategies in poker.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve considered a few approaches like reward-weighted supervised fine-tuning or using simpler RL techniques like Monte Carlo updates, but I’m not sure which would work best with the LLM setup I have. I've also considered Q-learning or Deep Q-learning.&lt;/p&gt; &lt;p&gt;Any advice or suggestions on which RL approach I should take given my situation would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Yes I used AI to write this queston. But it captures everything I want to say, and I suck at writing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/musketsreddit"&gt; /u/musketsreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2q5ym/what_reinforcement_learning_method_should_i_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2q5ym/what_reinforcement_learning_method_should_i_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2q5ym/what_reinforcement_learning_method_should_i_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2r244</id>
    <title>What's the best 3d game engine/local AI, pair-up?</title>
    <updated>2025-03-03T19:27:23+00:00</updated>
    <author>
      <name>/u/Musenik</name>
      <uri>https://old.reddit.com/user/Musenik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By 'best', I mean the pairing of coding model to 3d engine, so that the least mistakes will be made by the AI.&lt;/p&gt; &lt;p&gt;Other factors (for my personal use): can run on a 96GB Macintosh. And the engine can build to lots of platforms. Engine efficiency doesn't need to be the best. For example: Panda3D looks like a great choice due to it's python scripting, but it's not so popular that an AI would very familiar with it.&lt;/p&gt; &lt;p&gt;I hope this thread gets plenty of answers that help other game devs to build their brilliant ideas more efficiently. I guess my question should actually be, what are 'good' pairings of game engine to local AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Musenik"&gt; /u/Musenik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T19:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j25luw</id>
    <title>Split brain "DeepSeek-R1-Distill-Qwen-1.5B" and "meta-llama/Llama-3.2-1B"</title>
    <updated>2025-03-03T00:10:33+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt; &lt;img alt="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" src="https://external-preview.redd.it/pO-T0WXJNrqFPBnG-HTGUxM6LLhRwBWSzO1Lk4Wc-C8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf26348223db7fd2c581d42fb599fb64ac7b8669" title="Split brain &amp;quot;DeepSeek-R1-Distill-Qwen-1.5B&amp;quot; and &amp;quot;meta-llama/Llama-3.2-1B&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I'd like to show you this silly project.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839"&gt;https://preview.redd.it/2cls4on27dme1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fba0ba6a56896044f529aa50c5264ad45706839&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my fun little side project to create a fusion layer system that will allow for you to utilize dual models to produce dual results. Does it work? Pfh, I dunno. I've been training it all day. Haven't finished it yet. But this seems like it would be pretty fun.&lt;/p&gt; &lt;p&gt;My original idea: We have MOE but why not force a MOE that operates simultaneously? You might say &amp;quot;We'll that's just a less efficient MOE.&amp;quot; Wrongggggggg. This system allows for cross contamination of the results. By utilizing the tokenization of both llms plus the cross contamination. You can possibly get split brain results where the models might argue and you could get two totally different results.&lt;/p&gt; &lt;p&gt;OR you can give instructions to one model to only follow these rules while you give the other model the request or &amp;quot;Command&amp;quot;&lt;/p&gt; &lt;p&gt;This can possibly lead to a &amp;quot;unattainable&amp;quot; system prompt that can't be fetched because model 1 is simply influencing the results of model two. &lt;/p&gt; &lt;p&gt;Or hell have two conversations at the same time. &lt;/p&gt; &lt;p&gt;Dunnoooooo I haven't finished it yet. &lt;/p&gt; &lt;p&gt;Code's here: &lt;a href="https://github.com/alientony/Split-brain"&gt;https://github.com/alientony/Split-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference code comes later when I have a model to test out.&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;Below this is ai assisted writing as I wanted to make this more enjoyable and professional rather than express my words poorly and only half the people understand.&lt;/p&gt; &lt;h1&gt;Multi-Model Fusion Architecture: Technical Explanation&lt;/h1&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;This dual-decoder architecture represents a novel approach to leveraging multiple pre-trained language models (PLMs) through enhanced cross-attention fusion. The architecture combines two distinct foundation models (in this case Qwen and Llama) into a unified system that enables both collaborative reasoning and specialized processing.&lt;/p&gt; &lt;h1&gt;Key Components&lt;/h1&gt; &lt;h1&gt;1. Base Model Encapsulation&lt;/h1&gt; &lt;p&gt;The architecture maintains two separate base models, each with their original parameter spaces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model 1 (Qwen)&lt;/strong&gt;: Processes input sequences in its native hidden dimension space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model 2 (Llama)&lt;/strong&gt;: Independently processes inputs in its own parameter space&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These models operate on separate GPUs to maximize memory efficiency and computational parallelism.&lt;/p&gt; &lt;h1&gt;2. Cross-Attention Fusion Layer&lt;/h1&gt; &lt;p&gt;The core innovation lies in the &lt;code&gt;EnhancedFusionLayer&lt;/code&gt; which implements bidirectional cross-attention:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Model1 → [Query1] → attends to → [Key2/Value2] ← Model2 Model2 → [Query2] → attends to → [Key1/Value1] ← Model1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This mechanism allows each model to selectively attend to the representations of the other model, essentially creating a communication channel between two otherwise independent neural architectures.&lt;/p&gt; &lt;p&gt;The cross-attention operations are defined as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context1_2&lt;/strong&gt;: Model1's representation after attending to Model2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context2_1&lt;/strong&gt;: Model2's representation after attending to Model1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are calculated using scaled dot-product attention with a numerically stable scaling factor.&lt;/p&gt; &lt;h1&gt;3. Dimensional Alignment&lt;/h1&gt; &lt;p&gt;Since the base models operate in different dimensionalities, the architecture includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Projection matrices (&lt;code&gt;proj1&lt;/code&gt;, &lt;code&gt;proj2&lt;/code&gt;) that align the hidden dimensions of both models to the common fusion dimension&lt;/li&gt; &lt;li&gt;Internal neural transformations that map between representation spaces via linear projections&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Gating Mechanism&lt;/h1&gt; &lt;p&gt;A sophisticated gating mechanism controls information flow between models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sigmoid gates (&lt;code&gt;gate1&lt;/code&gt;, &lt;code&gt;gate2&lt;/code&gt;) determine how much information from each model should be incorporated&lt;/li&gt; &lt;li&gt;This creates an adaptive weighting system that can prioritize one model's contribution depending on the task&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Multi-Head Output System&lt;/h1&gt; &lt;p&gt;Three different prediction heads provide specialized outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fused LM Head&lt;/strong&gt;: Generates predictions based on the combined representation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 1&lt;/strong&gt;: Generates predictions optimized for Model1's vocabulary&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Head 2&lt;/strong&gt;: Generates predictions optimized for Model2's vocabulary&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;6. Task Classification Logic&lt;/h1&gt; &lt;p&gt;An integrated task classifier determines whether the inputs represent:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Single-Task Mode&lt;/strong&gt;: Same prompt to both models (collaboration)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Task Mode&lt;/strong&gt;: Different prompts (specialized processing)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Training Methodology&lt;/h1&gt; &lt;p&gt;The system uses a multi-objective training approach that combines losses from different prediction heads:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In single-task mode, the fused representation receives greater weight (emphasizing collaboration)&lt;/li&gt; &lt;li&gt;In multi-task mode, the specialized heads receive greater weight (emphasizing specialization)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gradient accumulation handles memory constraints, while mixed-precision (FP16) training enables efficient computation.&lt;/p&gt; &lt;h1&gt;Inference Mode&lt;/h1&gt; &lt;p&gt;During inference, the &lt;code&gt;generate_dual&lt;/code&gt; method enables:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Simultaneous response generation from both models&lt;/li&gt; &lt;li&gt;Adaptive temperature-based sampling with configurable parameters&lt;/li&gt; &lt;li&gt;EOS (End-of-Sequence) handling for both decoders&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architectural Advantages&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Emergent Capabilities&lt;/strong&gt;: The cross-attention mechanism allows models to share information during processing, potentially enabling emergent capabilities beyond what either model can achieve independently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Computational Efficiency&lt;/strong&gt;: By distributing models across different GPUs, the architecture enables parallel computation with reduced memory pressure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Task Flexibility&lt;/strong&gt;: The system can operate in both collaborative mode (same prompt) and specialized mode (different prompts).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter Efficiency&lt;/strong&gt;: Only the fusion components require training while the base models remain frozen, significantly reducing the number of trainable parameters.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This architecture represents an advanced approach to model fusion that goes beyond simple ensemble methods, enabling deep integration between distinct foundation models while preserving their individual strengths.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j25luw/split_brain_deepseekr1distillqwen15b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2fgz6</id>
    <title>🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!</title>
    <updated>2025-03-03T10:10:03+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt; &lt;img alt="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" src="https://external-preview.redd.it/cnVmdWJ2ZHE3Z21lMUL3NkVhJ5GHYq-Z21ncNiL8qoaSdn9KQniKMkRjA96Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003596a3a1fa062c0f796fad4c45be6460d38cc" title="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bmyoyudq7gme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T10:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rx6q</id>
    <title>Benchmarks &amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB</title>
    <updated>2025-03-03T20:02:46+00:00</updated>
    <author>
      <name>/u/BobTheNeuron</name>
      <uri>https://old.reddit.com/user/BobTheNeuron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt; &lt;img alt="Benchmarks &amp;amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Benchmarks &amp;amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the first thing to do after building a new computer? Post benchmarks on Reddit, of course!&lt;/p&gt; &lt;p&gt;I hope this gives other local LLM noobs like me some pointers for building a machine for LLM.&lt;/p&gt; &lt;h1&gt;Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Asus GeForce DUAL-RTX3060-O12G-V2 (12 GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 5 8500G (6 cores / 12 threads)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: DDR5 6000 MHz CL36 64 GB (32 GB + 32 GB) in dual channel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: MSI B850 GAMING PLUS WIFI AM5 (can run multiple GPUs if I ever want a multi-GPU setup)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At first I was thinking of just getting a Mac Mini, but I decided to do a custom build for customizability, longevity, upgradability and performance.&lt;/p&gt; &lt;h1&gt;llama.cpp setup&lt;/h1&gt; &lt;p&gt;I built &lt;code&gt;llama.cpp&lt;/code&gt; with two backends: CPU (for CPU-only inference) and CUDA (for GPU inference).&lt;/p&gt; &lt;p&gt;The &amp;quot;CPU&amp;quot; backend benchmark was run with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build cmake --build build --config Release # Automatically run with 6 CPU cores ./build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &amp;quot;CUDA&amp;quot; backend benchmarks were run with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_CUDA=ON cmake --build build --config Release # Automatically run with GPU + 1 CPU core ./build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -ngl 99 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both used llama.cpp build 06c2b156 (4794).&lt;/p&gt; &lt;h1&gt;Benchmarks &amp;amp; power consumption&lt;/h1&gt; &lt;p&gt;Also see the charts at the end of this post.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Layers on GPU (ngl)&lt;/th&gt; &lt;th align="left"&gt;GPU VRAM usage, GB&lt;/th&gt; &lt;th align="left"&gt;Prompt processing (pp), t/s&lt;/th&gt; &lt;th align="left"&gt;Token generation (tg), t/s&lt;/th&gt; &lt;th align="left"&gt;Power (pp), W&lt;/th&gt; &lt;th align="left"&gt;Power (tg), W&lt;/th&gt; &lt;th align="left"&gt;GPU power limit, W&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU (3600 MHz single-channel)&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;23.67&lt;/td&gt; &lt;td align="left"&gt;4.73&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;87&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU (6000 MHz dual-channel)&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;24.50&lt;/td&gt; &lt;td align="left"&gt;11.24&lt;/td&gt; &lt;td align="left"&gt;125&lt;/td&gt; &lt;td align="left"&gt;126&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.748&lt;/td&gt; &lt;td align="left"&gt;471.61&lt;/td&gt; &lt;td align="left"&gt;11.25&lt;/td&gt; &lt;td align="left"&gt;159&lt;/td&gt; &lt;td align="left"&gt;126&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;2.474&lt;/td&gt; &lt;td align="left"&gt;606.00&lt;/td&gt; &lt;td align="left"&gt;14.55&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;161&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;3.198&lt;/td&gt; &lt;td align="left"&gt;870.32&lt;/td&gt; &lt;td align="left"&gt;20.44&lt;/td&gt; &lt;td align="left"&gt;191&lt;/td&gt; &lt;td align="left"&gt;175&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;4.434&lt;/td&gt; &lt;td align="left"&gt;1111.45&lt;/td&gt; &lt;td align="left"&gt;25.67&lt;/td&gt; &lt;td align="left"&gt;207&lt;/td&gt; &lt;td align="left"&gt;187&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;5.178&lt;/td&gt; &lt;td align="left"&gt;1550.70&lt;/td&gt; &lt;td align="left"&gt;34.84&lt;/td&gt; &lt;td align="left"&gt;232&lt;/td&gt; &lt;td align="left"&gt;221&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1872.08&lt;/td&gt; &lt;td align="left"&gt;54.54&lt;/td&gt; &lt;td align="left"&gt;248&lt;/td&gt; &lt;td align="left"&gt;248&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1522.43&lt;/td&gt; &lt;td align="left"&gt;44.37&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1741.38&lt;/td&gt; &lt;td align="left"&gt;53.39&lt;/td&gt; &lt;td align="left"&gt;203&lt;/td&gt; &lt;td align="left"&gt;203&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The power consumption numbers are from the wall socket for the whole system (without monitor). Those numbers are not super accurate since I was just eyeballing them from the power meter.&lt;/p&gt; &lt;p&gt;As seen on the last two rows, limiting the GPU's power with &lt;code&gt;nvidia-smi -pl 130&lt;/code&gt; helped drop the system power consumption significantly while the tokens/sec didn't drop almost at all, so it seems to make sense to limit the 3060's power to about 130 W instead of the default 170 W.&lt;/p&gt; &lt;h1&gt;Running both CPU and GPU inference at the same time&lt;/h1&gt; &lt;p&gt;I deliberately bought a lot of RAM so that I can run CPU-only inference alongside GPU-only inference. It allows me to do additional CPU-only inference in the background when I don't care about the tokens/sec as much, e.g. in agentic/batch workflows.&lt;/p&gt; &lt;p&gt;I tried running two llama-bench processes simultaneously (one on GPU, and one on CPU):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# CPU-only inference with 6 threads at 100% load ./llama.cpp-cuda/build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -ngl 99 -r 1000 # GPU inference (+ 1 CPU thread at 100% load) ./llama.cpp-cpu-only/build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -r 1000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running those two commands in parallel had 7 threads at 100% load. GPU power limit was at default (170 W).&lt;/p&gt; &lt;p&gt;The whole system consumes about &lt;strong&gt;286 W&lt;/strong&gt; when running &lt;em&gt;prompt processing&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The whole system consumes about &lt;strong&gt;290 W&lt;/strong&gt; when running &lt;em&gt;token generation&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;Optimizing idle power consumption&lt;/h1&gt; &lt;p&gt;As a sidenote, this machine seems to idle at around &lt;strong&gt;45 W&lt;/strong&gt; after doing the following optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shut down HDDs after 20 minutes with &lt;code&gt;hdparm -S 240&lt;/code&gt; (or immediately with &lt;code&gt;hdparm -Y&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Apply power optimizations with &lt;code&gt;powertop --auto-tune&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I could probably drop the idle power consumption even lower by undervolting and tweaking some BIOS settings.&lt;/p&gt; &lt;h1&gt;What models fit into 12 GB VRAM?&lt;/h1&gt; &lt;p&gt;With Ollama, these models seem to fit into 12 GB of VRAM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral-small:22b (Q4_0)&lt;/li&gt; &lt;li&gt;llama3.2-vision:11b (Q4_K_M)&lt;/li&gt; &lt;li&gt;deepseek-r1:14b (Q4_K_M)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These can be found on &lt;a href="https://ollama.com/search"&gt;https://ollama.com/search&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Charts&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zvo86sf16jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d80f828c63aa7e3b388d6de0a9ad7435400ae635"&gt;https://preview.redd.it/zvo86sf16jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d80f828c63aa7e3b388d6de0a9ad7435400ae635&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oz26ylj26jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94bba969defc2aca66afac1d0af4581dde0cb291"&gt;https://preview.redd.it/oz26ylj26jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94bba969defc2aca66afac1d0af4581dde0cb291&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zoe3q5336jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5175982e8232599304a8b36f652556e59d65e3d8"&gt;https://preview.redd.it/zoe3q5336jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5175982e8232599304a8b36f652556e59d65e3d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobTheNeuron"&gt; /u/BobTheNeuron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T20:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2pm6n</id>
    <title>Cache-Craft: Chunk-Level KV Cache Reuse for Faster and Efficient RAG (SIGMOD 2025)</title>
    <updated>2025-03-03T18:29:59+00:00</updated>
    <author>
      <name>/u/Lucky-Ad79</name>
      <uri>https://old.reddit.com/user/Lucky-Ad79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share &lt;strong&gt;&lt;em&gt;Cache-Craft&lt;/em&gt;&lt;/strong&gt; [&lt;a href="https://www.arxiv.org/pdf/2502.15734"&gt;PDF&lt;/a&gt;], our SIGMOD 2025 paper on efficient &lt;strong&gt;chunk-aware KV reuse&lt;/strong&gt; for RAG! 🚀&lt;/p&gt; &lt;p&gt;Large language models (LLMs) in retrieval-augmented generation (RAG) often recompute KV caches unnecessarily, leading to inefficiencies. &lt;strong&gt;&lt;em&gt;Cache-Craft&lt;/em&gt;&lt;/strong&gt; introduces a &lt;strong&gt;granular&lt;/strong&gt; &lt;strong&gt;chunk-level KV reuse strategy&lt;/strong&gt; that selectively recomputes only what’s necessary—reducing redundant computation while maintaining generation quality.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Key contributions:&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Chunked KV Reuse:&lt;/strong&gt; Efficiently caches and reuses KV states at a RAG chunk level, unlike traditional full-prefix-cache methods.&lt;br /&gt; ✅ &lt;strong&gt;Selective Recompute Planning:&lt;/strong&gt; Dynamically determines which KV states to reuse vs. recompute, optimizing for efficiency.&lt;br /&gt; ✅ &lt;strong&gt;Real-World Gains:&lt;/strong&gt; Evaluated on production-scale RAG traces, showing significant reductions in compute overhead.&lt;br /&gt; ✅ &lt;strong&gt;vLLM-based Open Source Coming Soon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts! How do you see caching evolving for efficient LLM inference? 🤔&lt;/p&gt; &lt;p&gt;&lt;em&gt;[1] Agarwal, S., Sundaresan, S., Mitra, S., Mahapatra, D., Gupta, A., Sharma, R., Kapu, N.J., Yu, T. and Saini, S., 2025. Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation. arXiv preprint arXiv:2502.15734.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucky-Ad79"&gt; /u/Lucky-Ad79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rtfm</id>
    <title>What is the best local model that can “replicate” gpt4o responses naturally?</title>
    <updated>2025-03-03T19:58:51+00:00</updated>
    <author>
      <name>/u/No_Expert1801</name>
      <uri>https://old.reddit.com/user/No_Expert1801</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"&gt; &lt;img alt="What is the best local model that can “replicate” gpt4o responses naturally?" src="https://preview.redd.it/r411lovs5jme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=276af561572f760eba7532962ba9b536ec699a7a" title="What is the best local model that can “replicate” gpt4o responses naturally?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like gpt4o is just so awesome. I love how it can show all these different types of writing styles &lt;/p&gt; &lt;p&gt;Like it’s not always “as your assistant, I’ll gladly help you” but it’s like really returning the energy. &lt;/p&gt; &lt;p&gt;Are there any cool local models that can do this well?&lt;/p&gt; &lt;p&gt;(16gb vram would be nice)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Expert1801"&gt; /u/No_Expert1801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r411lovs5jme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T19:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2tdtt</id>
    <title>Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp; eval. [KilnAI Guide]</title>
    <updated>2025-03-03T21:03:53+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt; &lt;img alt="Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp;amp; eval. [KilnAI Guide]" src="https://external-preview.redd.it/fkk_hfuiSuMOZjLy_dEtjSiqJMOwZz9w_oAKY_5Q2Nk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3dadc03291c7ac04f201561f33b9b740f85a835" title="Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp;amp; eval. [KilnAI Guide]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just released an update of Kiln on Github which provides a powerful toolkit for evaluating AI models and tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://docs.getkiln.ai/docs/evaluations#video-walkthrough"&gt;walkthrough vid&lt;/a&gt; shows the process from start to end&lt;/li&gt; &lt;li&gt;Our docs have &lt;a href="https://docs.getkiln.ai/docs/evaluations"&gt;evaluation guide&lt;/a&gt; if you want to try it out yourself&lt;/li&gt; &lt;li&gt;Here's the ~&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github repo&lt;/a&gt;~ with all of the source code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The eval feature includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple state of the art evaluation methods (G-Eval, LLM as Judge)&lt;/li&gt; &lt;li&gt;Synthetic data generation makes it easy to generaet hundreds or thousands of eval data samples in minutes.&lt;/li&gt; &lt;li&gt;Includes tooling to find the best evaluation method for your task. It finds the eval algo+model which best correlates to human preference (Kendall’s Tau, Spearman, MSE, etc).&lt;/li&gt; &lt;li&gt;Includes eval dashboard to find the highest quality method to run your task (prompt+model)&lt;/li&gt; &lt;li&gt;Fine-tunes: create then evaluate custom fine-tunes for your task&lt;/li&gt; &lt;li&gt;Intuitive UI for eval dataset management: create eval sets, manage golden sets, add human ratings, etc.&lt;/li&gt; &lt;li&gt;Automatic eval generation: it will examine your task definition, then automatically create an evaluator for you.&lt;/li&gt; &lt;li&gt;Supports custom evaluators: create evals for any score/goals/instructions you want.&lt;/li&gt; &lt;li&gt;Built in eval templates for common scenarios: toxicity, bias, jailbreaking, factual correctness, and maliciousness.&lt;/li&gt; &lt;li&gt;Synthetic data templates to generate adversarial datasets using uncensored and unaligned models like Dolphin/Grok. Weird use case where very inappropriate content has a very ethical use. The video has a demo of Dolphin trying to jailbreak the core model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: this release also includes the ability to distill Sonnet 3.7 Thinking into an open model you can run locally. I evaluate a few of these fine-tunes against foundation models, and they do quite well (at task-specific metrics).&lt;/p&gt; &lt;p&gt;Kiln runs locally and we never have access to your dataset. If you use Ollama, data never leaves your device.&lt;/p&gt; &lt;p&gt;If anyone wants to try Kiln, here's the &lt;a href="https://github.com/Kiln-AI/Kiln/releases/tag/v0.12.1"&gt;latest release on Github&lt;/a&gt; and the &lt;a href="https://docs.getkiln.ai/"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running. Let me know if you have any feedback or ideas! It really helps me improve Kiln. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j2tdtt/video/f4mqimpchjme1/player"&gt;Walkthrough of creating an AI Eval&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2vhhq</id>
    <title>Story writing benchmark/dataset</title>
    <updated>2025-03-03T22:32:38+00:00</updated>
    <author>
      <name>/u/CorrectLow9302</name>
      <uri>https://old.reddit.com/user/CorrectLow9302</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt; &lt;img alt="Story writing benchmark/dataset" src="https://external-preview.redd.it/C3b4DWbqwScAlqlFr8UQw42SuiBsrSRBBL5H3HRmHFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47dcafbfc7088a32a7fc1410e5b9704fd4e1a93" title="Story writing benchmark/dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c"&gt;https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;dataset: &lt;a href="https://huggingface.co/datasets/lars1234/story_writing_benchmark"&gt;https://huggingface.co/datasets/lars1234/story_writing_benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each model was instructed to write 568 short stories. Each story was then rated by 4 models: Llama 3.3 70B, Mistral Small 24B (2501), Gemma 2 9B (SPPO-Iter3), Aya Expanse 32B. The ranking correlation between the evaluators is approx. 90%. Evaluation criteria such as creativity, world-building and grammar were weighted equally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorrectLow9302"&gt; /u/CorrectLow9302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8x5</id>
    <title>Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?</title>
    <updated>2025-03-03T13:59:10+00:00</updated>
    <author>
      <name>/u/zR0B3ry2VAiH</name>
      <uri>https://old.reddit.com/user/zR0B3ry2VAiH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt; &lt;img alt="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" src="https://preview.redd.it/x73g8sumdhme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86a6a6523268d0e31eb6f4341c716de3ee799ab2" title="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;100gb nics are $330 with two ports each, so I’d run it direct connections between all three. Each server has two Xeon process with 512 gb of ram. Did some shuffling with the ram sticks to get R1 to run locally, but as you would expect, it’s pretty slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zR0B3ry2VAiH"&gt; /u/zR0B3ry2VAiH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x73g8sumdhme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2horr</id>
    <title>NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing</title>
    <updated>2025-03-03T12:36:04+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt; &lt;img alt="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" src="https://preview.redd.it/8gyz8kzsygme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88d058b7bc64d9941684f9dc53c45071cf7f231" title="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/"&gt;https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gyz8kzsygme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T12:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2m5a3</id>
    <title>I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!</title>
    <updated>2025-03-03T16:08:39+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt; &lt;img alt="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" src="https://external-preview.redd.it/MzV5cjJiYnAwaW1lMVP5w4KiH2jOdvVLO5M2qzHTvkueIwiHgKlPWJafTXUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85988097e8c3bdd3a2d386e68bcd0b55c2d2f2c0" title="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ys7qtcbp0ime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2leve</id>
    <title>new Hugging Face course on building reasoning models like deepseek r1</title>
    <updated>2025-03-03T15:37:25+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new FREE and CERTIFIED course is here, and It’s called &lt;strong&gt;The Reasoning Course.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To sign up for the course, follow the org: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the course will cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It will teach you to build your own reasoning model like Deepseek r1.&lt;/li&gt; &lt;li&gt;It’s suitable for code and non-coders with separate certification.&lt;/li&gt; &lt;li&gt;The course has material and exercises from Hugging Face, Maxime Labonne, Unsloth, and Marimo notebooks. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is how the course works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sign up now, the first release is already live.&lt;/li&gt; &lt;li&gt;Each week we’ll release new material and exercises. &lt;/li&gt; &lt;li&gt;We have interactive demos and quizzes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2usb0</id>
    <title>Is qwen 2.5 coder still the best?</title>
    <updated>2025-03-03T22:02:25+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anything better been released for coding? (&amp;lt;=32b parameters)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kdeb</id>
    <title>OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?</title>
    <updated>2025-03-03T14:52:19+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt; &lt;img alt="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" src="https://b.thumbs.redditmedia.com/WEagdfr42ScIzJVwvfP__c7O6w-pNG7grBkUGiA0rAk.jpg" title="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j2kdeb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2pw8i</id>
    <title>The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩</title>
    <updated>2025-03-03T18:40:59+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt; &lt;img alt="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩" src="https://external-preview.redd.it/ZmV0aG1ibndyaW1lMWFNzMK25hHzjNvt4Y-OO73o5sGhDV6XnH6G0Oq87xCn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e39fa10a7873118e9284eb276fb4ef9c51960837" title="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lakifgrwrime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
</feed>
