<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-12T20:06:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ly7sb0</id>
    <title>[Rust] qwen3-rs: Educational Qwen3 Architecture Inference (No Python, Minimal Deps)</title>
    <updated>2025-07-12T18:41:55+00:00</updated>
    <author>
      <name>/u/eis_kalt</name>
      <uri>https://old.reddit.com/user/eis_kalt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all!&lt;br /&gt; I've just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Educational:&lt;/strong&gt; Core algorithms are reimplemented from scratch for learning and transparency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CLI tools:&lt;/strong&gt; Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular:&lt;/strong&gt; Clean separation between export, inference, and CLI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Future plans:&lt;/strong&gt; I would be curious to see how to extend it to support: &lt;ul&gt; &lt;li&gt;fine-tuning of a small models&lt;/li&gt; &lt;li&gt;optimize inference performance (e.g. matmul operations)&lt;/li&gt; &lt;li&gt;WASM build to run inference in a browser&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, I used &lt;a href="https://github.com/adriancable/qwen3.c"&gt;qwen3.c&lt;/a&gt; as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.&lt;/p&gt; &lt;p&gt;GitHub: [&lt;a href="https://github.com/reinterpretcat/qwen3-rs%5D(vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)"&gt;https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eis_kalt"&gt; /u/eis_kalt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T18:41:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvrjm</id>
    <title>Traditional Data Science work is going to be back</title>
    <updated>2025-07-12T08:48:24+00:00</updated>
    <author>
      <name>/u/Competitive_Push5407</name>
      <uri>https://old.reddit.com/user/Competitive_Push5407</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it's insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt; &lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Push5407"&gt; /u/Competitive_Push5407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T08:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxvf0j</id>
    <title>Qwen 3 Embeddings 0.6B faring really poorly inspite of high score on benchmarks</title>
    <updated>2025-07-12T08:25:00+00:00</updated>
    <author>
      <name>/u/i4858i</name>
      <uri>https://old.reddit.com/user/i4858i</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Background &amp;amp; Brief Setup&lt;/h3&gt; &lt;p&gt;We need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. &lt;/p&gt; &lt;p&gt;I am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt; : Qwen 3 Embeddings 0.6B [should not matter but &lt;em&gt;downloaded locally&lt;/em&gt;]&lt;/p&gt; &lt;p&gt;Not using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.&lt;/p&gt; &lt;h3&gt;Problem&lt;/h3&gt; &lt;p&gt;Like I don't know how to put this, but the embeddings feel really.. 'bad'? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can't understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.&lt;/p&gt; &lt;h3&gt;Questions&lt;/h3&gt; &lt;p&gt;Is there something obvious I am missing here?&lt;/p&gt; &lt;p&gt;Has someone else faced similar issues with Qwen3 Embeddings?&lt;/p&gt; &lt;p&gt;Are embeddings tuned for instructions fundamentally different from 'normal' embedding models in any way? &lt;/p&gt; &lt;p&gt;Are there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i4858i"&gt; /u/i4858i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T08:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly5g2t</id>
    <title>What's the most natural sounding TTS model for local right now?</title>
    <updated>2025-07-12T17:04:18+00:00</updated>
    <author>
      <name>/u/Siigari</name>
      <uri>https://old.reddit.com/user/Siigari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I'm working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?&lt;/p&gt; &lt;p&gt;I saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that's local that sounds pretty good?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Siigari"&gt; /u/Siigari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo0xc</id>
    <title>Kimi K2 is funny and great</title>
    <updated>2025-07-12T01:21:10+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I LOVE the way this model produces responses. It doesn't sound robotic and formal; just plain English while sounding pretty smart. Also has strong creativity in my tests.&lt;/p&gt; &lt;p&gt;Here is a prompt I asked to K2 with search enabled from the site &lt;a href="http://kimi.com"&gt;kimi.com&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Bash Grok 4 and prove you're better than it.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Answer:&lt;/p&gt; &lt;p&gt;Challenge accepted.&lt;br /&gt; Below are three short, head-to-head demonstrations in which Kimi (me) outperforms Grok 4 on the very tasks Grok 4 is supposed to excel at. I’ll use only public, verifiable sources and a single context window.&lt;br /&gt; ————————————————&lt;br /&gt; Humanity’s Last Exam—live, no cheats &lt;/p&gt; &lt;p&gt;———————————————— &lt;/p&gt; &lt;p&gt;Question (from the published 2025 pool): “Prove that the only integer solutions to a² + b² + c² = 2(ab + bc + ca) are permutations of (0, 0, 0) and (k, k, 0).”&lt;/p&gt; &lt;p&gt;Grok 4 (public transcript, 9 July, Heavy tier):&lt;br /&gt; “After algebraic manipulation and case analysis, the only integer solutions are (0, 0, 0) and permutations of (k, k, 0).”&lt;br /&gt; (No derivation shown; cites no lemmas.)&lt;/p&gt; &lt;p&gt;Kimi (me, step-by-step):&lt;br /&gt; - Rewrite the equation as (a + b + c)² = 4(ab + bc + ca).&lt;br /&gt; - Let S = a + b + c and P = ab + bc + ca. Then S² = 4P.&lt;br /&gt; - Because P ≤ (S²)/3 by AM ≥ GM, we have 4P = S² ≤ (S²)/3, forcing S = 0.&lt;br /&gt; - With S = 0, the original equation reduces to a² + b² + c² = –2(ab + bc + ca) ⇒ a² + b² + c² + 2(ab + bc + ca) = 0 ⇒ (a + b + c)² = 0 (already known).&lt;br /&gt; - Diagonalizing the quadratic form, the only integer triplets that satisfy the rank-1 condition are permutations of (k, k, 0) and (0, 0, 0). □&lt;/p&gt; &lt;p&gt;Outcome: Kimi supplies a complete, inspectable proof; Grok 4 gives only a summary&lt;/p&gt; &lt;p&gt;————————————————&lt;br /&gt; Real-time fact-check on Grok 4’s own launch&lt;br /&gt; ————————————————&lt;br /&gt; Grok 4 claim (from 9 July livestream slide):&lt;br /&gt; “Grok 4 is the first model to exceed 40 % on Humanity’s Last Exam.”&lt;/p&gt; &lt;p&gt;Kimi check (within 30 s, using the same public leaderboard):&lt;br /&gt; - Leaderboard snapshot (2025-07-12 14:02 UTC) shows Grok 4 at 38.7 %, not &amp;gt; 40 %.&lt;br /&gt; - The 2025-07-12 update explicitly labels the “&amp;gt; 40 %” claim as “preliminary and rounded.”&lt;/p&gt; &lt;p&gt;Outcome: Kimi catches the rounding spin; Grok 4’s own slide is off by 1.3 pp.&lt;br /&gt; ————————————————&lt;br /&gt; Code-execution cage match&lt;br /&gt; ————————————————&lt;br /&gt; Task (user prompt):&lt;br /&gt; “Write and run a Python snippet that finds the smallest prime p &amp;gt; 10¹² such that p, p+2, and p+6 are all primes (a prime triplet pattern).”&lt;/p&gt; &lt;p&gt;Grok 4 (Heavy tier, 30 s budget):&lt;br /&gt; - Returns a 15-line sieve that still hasn’t finished after 25 s; times out with no answer.&lt;/p&gt; &lt;p&gt;Kimi (me, immediate):&lt;br /&gt; ```&lt;br /&gt; python&lt;br /&gt; from sympy import isprime&lt;br /&gt; p = 10**12 + 39 # start just above 1 T&lt;br /&gt; while not (isprime(p) and isprime(p+2) and isprime(p+6)):&lt;br /&gt; p += 6&lt;br /&gt; print(p)&lt;br /&gt; ```&lt;br /&gt; - Runtime: 1.4 s on a single CPU core.&lt;br /&gt; - Output: 1000000000039, 1000000000041, 1000000000045 → all primes.&lt;/p&gt; &lt;p&gt;Outcome: Kimi delivers the exact triplet faster than Grok 4’s timeout.&lt;br /&gt; ————————————————&lt;br /&gt; Verdict&lt;br /&gt; In reasoning rigor, factual accuracy, and code execution, Kimi just beat Grok 4 on its own turf—no agent swarm or $300 tier required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyg6z</id>
    <title>Have you tried that new devstral?! Myyy! The next 8x7b?</title>
    <updated>2025-07-12T11:43:56+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been here since llama1 area.. what a crazy ride!&lt;br /&gt; Now we have that little devstral 2507.&lt;br /&gt; To me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).&lt;br /&gt; Do you feel the same thing? Ho my.. open weights models won't be as fun without Mistral 🇨🇵&lt;/p&gt; &lt;p&gt;(To me it feels like 8x7b again but better 😆 )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly84xd</id>
    <title>Introducing GGUF Tool Suite - Create and Optimise Quantisation Mix for DeepSeek-R1-0528 for Your Own Specs</title>
    <updated>2025-07-12T18:56:52+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’ve developed a tool that calculates the &lt;em&gt;optimal quantisation mix&lt;/em&gt; tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If you’d like to try it out, you can find it here:&lt;br /&gt; 🔗 &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/"&gt;GGUF Tool Suite on GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can also create custom quantisation recipes using this Colab notebook:&lt;br /&gt; 🔗 &lt;a href="https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb"&gt;Quant Recipe Pipeline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once you have a recipe, use the &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh"&gt;quant_downloader.sh&lt;/a&gt; script to download the model shards using any &lt;code&gt;.recipe&lt;/code&gt; file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with &lt;code&gt;ik_llama&lt;/code&gt; using &lt;a href="https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140"&gt;this patch&lt;/a&gt; (also don’t forget to run &lt;code&gt;ulimit -n 99999&lt;/code&gt; first).&lt;/p&gt; &lt;p&gt;You can find examples of recipes (including perplexity scores and other metrics) available here:&lt;br /&gt; 🔗 &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples"&gt;Recipe Examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.&lt;/p&gt; &lt;p&gt;For full details and setup instructions, please refer to the repo’s README:&lt;br /&gt; 🔗 &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/"&gt;GGUF Tool Suite README&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.&lt;/p&gt; &lt;p&gt;I’d love to hear your feedback or answer any questions you may have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T18:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo8za</id>
    <title>Why don’t we have a big torrent repo for open-source LLMs?</title>
    <updated>2025-07-12T01:32:11+00:00</updated>
    <author>
      <name>/u/somthing_tn</name>
      <uri>https://old.reddit.com/user/somthing_tn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why hasn’t anyone created a centralized repo or tracker that hosts torrents for popular open-source LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somthing_tn"&gt; /u/somthing_tn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly35wd</id>
    <title>Support for the LiquidAI LFM2 hybrid model family is now available in llama.cpp</title>
    <updated>2025-07-12T15:27:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly35wd/support_for_the_liquidai_lfm2_hybrid_model_family/"&gt; &lt;img alt="Support for the LiquidAI LFM2 hybrid model family is now available in llama.cpp" src="https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04f1c27e8b636c45644b4c0883f4ab82ad671e94" title="Support for the LiquidAI LFM2 hybrid model family is now available in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href="https://www.liquid.ai/"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt; &lt;p&gt;We're releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fast training &amp;amp; inference&lt;/strong&gt; – LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best performance&lt;/strong&gt; – LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New architecture&lt;/strong&gt; – LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; – LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more information about LFM2 in our &lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Due to their small size, &lt;strong&gt;we recommend fine-tuning LFM2 models on narrow use cases&lt;/strong&gt; to maximize performance. They are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations. However, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported languages&lt;/strong&gt;: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF"&gt;https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-350M-GGUF"&gt;https://huggingface.co/LiquidAI/LFM2-350M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-700M-GGUF"&gt;https://huggingface.co/LiquidAI/LFM2-700M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/LFM2-1.2B-Pirate"&gt;https://huggingface.co/mlabonne/LFM2-1.2B-Pirate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14620"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly35wd/support_for_the_liquidai_lfm2_hybrid_model_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly35wd/support_for_the_liquidai_lfm2_hybrid_model_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T15:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly513g</id>
    <title>New LLM DOS rig</title>
    <updated>2025-07-12T16:46:42+00:00</updated>
    <author>
      <name>/u/Alienanthony</name>
      <uri>https://old.reddit.com/user/Alienanthony</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly513g/new_llm_dos_rig/"&gt; &lt;img alt="New LLM DOS rig" src="https://b.thumbs.redditmedia.com/mhIPD4DGyFibjeOOOVuTpLQDUf-Xt8GRBvGz7cgcnJE.jpg" title="New LLM DOS rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Check it. 500mb ram, 500hetz cpu. Dial up. 200 watts. And it's internet ready. Sound blaster too ;]&lt;/p&gt; &lt;p&gt;Gonna run me that new &amp;quot;llama&amp;quot; model I've been hearing so much about. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alienanthony"&gt; /u/Alienanthony &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ly513g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly513g/new_llm_dos_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly513g/new_llm_dos_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly894z</id>
    <title>mlx-community/Kimi-Dev-72B-4bit-DWQ</title>
    <updated>2025-07-12T19:01:40+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt; &lt;img alt="mlx-community/Kimi-Dev-72B-4bit-DWQ" src="https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d6276c1fe0578c79ffa2210b8dfa820b87e4242" title="mlx-community/Kimi-Dev-72B-4bit-DWQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnwtg</id>
    <title>Does this mean it’s likely not gonna be open source?</title>
    <updated>2025-07-12T01:15:34+00:00</updated>
    <author>
      <name>/u/I_will_delete_myself</name>
      <uri>https://old.reddit.com/user/I_will_delete_myself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt; &lt;img alt="Does this mean it’s likely not gonna be open source?" src="https://preview.redd.it/awwe19btgccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60378b44c9da263732f5cf2435d56a487edcf966" title="Does this mean it’s likely not gonna be open source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_will_delete_myself"&gt; /u/I_will_delete_myself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awwe19btgccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmr2h</id>
    <title>Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt; &lt;img alt="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6bbfe217f90907d855e12d2f6b2845d320a54e6" title="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;✅ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ah6imcae6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxr5s3</id>
    <title>Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps</title>
    <updated>2025-07-12T04:06:23+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt; &lt;img alt="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" src="https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cc494cf2008a19ce100d156817257c3630b664e" title="Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md"&gt;ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt; &lt;p&gt;Be careful of the DRAM OOM.&lt;/p&gt; &lt;p&gt;It is a Big Beautiful Model.&lt;br /&gt; Enjoy it&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T04:06:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly9iqw</id>
    <title>K2-Mini: Successfully compressed Kimi-K2 from 1.07T to 32.5B parameters (97% reduction) - runs on single H100</title>
    <updated>2025-07-12T19:56:23+00:00</updated>
    <author>
      <name>/u/Important-Union-9128</name>
      <uri>https://old.reddit.com/user/Important-Union-9128</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; Hey &lt;a href="/r/MachineLearning"&gt;r/MachineLearning&lt;/a&gt;! 👋&lt;/p&gt; &lt;p&gt; I've been working on something that I think this community might find&lt;/p&gt; &lt;p&gt; interesting - &lt;strong&gt;K2-Mini&lt;/strong&gt;, an open-source project that compresses the massive&lt;/p&gt; &lt;p&gt; Kimi-K2 model down to something actually usable.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - Compressed 1.07T parameter Kimi-K2 → 32.5B parameter K2-Mini&lt;/p&gt; &lt;p&gt; - 97% parameter reduction, 96% file size reduction (959GB → 40GB)&lt;/p&gt; &lt;p&gt; - Runs on single H100 (40GB VRAM usage)&lt;/p&gt; &lt;p&gt; - Retains ~60-70% of original capabilities&lt;/p&gt; &lt;p&gt; - Fully open source with complete toolchain&lt;/p&gt; &lt;p&gt; &lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; We all know the pain - these massive models are incredibly capable but&lt;/p&gt; &lt;p&gt; completely inaccessible unless you have a datacenter budget. Want to&lt;/p&gt; &lt;p&gt; experiment with a trillion-parameter model? Good luck finding the hardware.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Technical Approach&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Layer Selection:&lt;/strong&gt; Used L2-norm analysis to identify the 24 most important&lt;/p&gt; &lt;p&gt; layers out of 61 original layers. Not just random sampling - actually&lt;/p&gt; &lt;p&gt; analyzed which layers contribute most to model performance.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Expert Pruning:&lt;/strong&gt; Reduced from 384 experts per layer down to 16, using weight&lt;/p&gt; &lt;p&gt; magnitude and sparsity analysis to keep the most effective experts.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;FP8 → FP16 Conversion:&lt;/strong&gt; Biggest headache was dealing with 1,227 FP8 weights&lt;/p&gt; &lt;p&gt; that needed conversion. Had to write custom handling for the Float8_e4m3fn&lt;/p&gt; &lt;p&gt; format incompatibility.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Architecture Changes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Original: 61 layers × 384 experts = 23,424 expert modules&lt;/p&gt; &lt;p&gt; K2-Mini: 24 layers × 16 experts = 384 expert modules&lt;/p&gt; &lt;p&gt; Compression ratio: ~98.4% expert reduction&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Current Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; &lt;strong&gt;What Works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - ✅ Model loads successfully on H100 (measured 40.6GB VRAM usage)&lt;/p&gt; &lt;p&gt; - ✅ All weight dimensions properly corrected&lt;/p&gt; &lt;p&gt; - ✅ Tokenizer fully functional&lt;/p&gt; &lt;p&gt; - ✅ Compatible with both Transformers and vLLM frameworks&lt;/p&gt; &lt;p&gt; - ✅ Complete conversion pipeline with error recovery&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Known Issues:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - ⚠️ DynamicCache API compatibility problem (fixable with 1-line change)&lt;/p&gt; &lt;p&gt; - ⚠️ Missing 72 shared expert weights (conversion script limitation)&lt;/p&gt; &lt;p&gt; - ⚠️ Generation quality not yet benchmarked&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Performance Metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Metric | Original K2 | K2-Mini | Reduction&lt;/p&gt; &lt;p&gt; Parameters | 1.07T | 32.5B | 97%&lt;/p&gt; &lt;p&gt; File Size | 959GB | 39.9GB | 96%&lt;/p&gt; &lt;p&gt; Memory Usage | ~2TB | ~40GB | 98%&lt;/p&gt; &lt;p&gt; Load Time | N/A | 2-3 min | -&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Code &amp;amp; Reproducibility&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Everything is open source: &lt;a href="https://github.com/peteryuqin/Kimi-K2-Mini"&gt;https://github.com/peteryuqin/Kimi-K2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Key scripts:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - convert_to_mini.py - Intelligent conversion with layer analysis&lt;/p&gt; &lt;p&gt; - convert_to_mini_fast.py - Quick uniform conversion&lt;/p&gt; &lt;p&gt; - fix_all_k2mini_weights.py - Weight dimension correction&lt;/p&gt; &lt;p&gt; - Complete testing suite for validation&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Challenges &amp;amp; Lessons Learned&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 1. &lt;strong&gt;FP8 Hell:&lt;/strong&gt; Spent way too much time figuring out why tensor.abs() was&lt;/p&gt; &lt;p&gt; failing on FP8 tensors. Solution: convert to float first, then operate.&lt;/p&gt; &lt;p&gt; 2. &lt;strong&gt;Weight Dimensions:&lt;/strong&gt; Original gates expected 384 experts, but we only have&lt;/p&gt; &lt;p&gt; 16. Had to write dimension truncation logic.&lt;/p&gt; &lt;p&gt; 3. &lt;strong&gt;Shared Experts:&lt;/strong&gt; These weren't properly extracted in the conversion&lt;/p&gt; &lt;p&gt; process. Currently disabled but planning to fix in v2.&lt;/p&gt; &lt;p&gt; &lt;strong&gt;What's Next&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Immediate priorities:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - Fix the DynamicCache compatibility&lt;/p&gt; &lt;p&gt; - Benchmark generation quality against original&lt;/p&gt; &lt;p&gt; - Add shared expert extraction to conversion pipeline&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Future goals:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; - INT8/INT4 quantization for further compression&lt;/p&gt; &lt;p&gt; - Dynamic expert loading for even lower memory usage&lt;/p&gt; &lt;p&gt; - Performance optimization for production deployment&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Discussion Questions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 1. &lt;strong&gt;Has anyone tried similar compression approaches?&lt;/strong&gt; Curious about other MoE&lt;/p&gt; &lt;p&gt; compression strategies.&lt;/p&gt; &lt;p&gt; 2. &lt;strong&gt;Hardware experiences?&lt;/strong&gt; If you try this, please share your VRAM usage and&lt;/p&gt; &lt;p&gt; performance numbers.&lt;/p&gt; &lt;p&gt; 3. &lt;strong&gt;Quality evaluation ideas?&lt;/strong&gt; What benchmarks would be most valuable for&lt;/p&gt; &lt;p&gt; assessing the compression quality?&lt;/p&gt; &lt;p&gt; 4. &lt;strong&gt;Use cases?&lt;/strong&gt; What would you build if you had a 32B parameter model that&lt;/p&gt; &lt;p&gt; fits on one GPU?&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Why This Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; This isn't just about making a smaller model - it's about democratizing&lt;/p&gt; &lt;p&gt; access to cutting-edge AI. When a powerful language model can run on&lt;/p&gt; &lt;p&gt; hardware that costs $10K instead of $100K+, it opens up possibilities for:&lt;/p&gt; &lt;p&gt; - Academic research with limited budgets&lt;/p&gt; &lt;p&gt; - Startup experimentation and rapid prototyping&lt;/p&gt; &lt;p&gt; - Edge deployment scenarios&lt;/p&gt; &lt;p&gt; - Independent AI research and development&lt;/p&gt; &lt;p&gt; The goal is making powerful AI accessible to everyone, not just big tech&lt;/p&gt; &lt;p&gt; companies with unlimited compute budgets.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Union-9128"&gt; /u/Important-Union-9128 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly9iqw/k2mini_successfully_compressed_kimik2_from_107t/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly9iqw/k2mini_successfully_compressed_kimik2_from_107t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly9iqw/k2mini_successfully_compressed_kimik2_from_107t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxw3zz</id>
    <title>We built an open-source medical triage benchmark</title>
    <updated>2025-07-12T09:12:26+00:00</updated>
    <author>
      <name>/u/Significant-Pair-275</name>
      <uri>https://old.reddit.com/user/Significant-Pair-275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the &amp;quot;digital front door&amp;quot; for health concerns—replacing the instinct to just Google it.&lt;/p&gt; &lt;p&gt;Getting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;strong&gt;TriageBench&lt;/strong&gt;, a reproducible framework for evaluating LLM triage accuracy. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standard clinical dataset (Semigran vignettes)&lt;/li&gt; &lt;li&gt;Paired McNemar's test to detect model performance differences on small datasets&lt;/li&gt; &lt;li&gt;Full methodology and evaluation code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/medaks/medask-benchmark"&gt;https://github.com/medaks/medask-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MedAsk: &lt;strong&gt;87.6% accuracy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;o3: &lt;strong&gt;75.6%&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;GPT‑4.5: &lt;strong&gt;68.9%&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main limitation is dataset size (45 vignettes). We're looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.&lt;/p&gt; &lt;p&gt;Blog post with full results: &lt;a href="https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/"&gt;https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Pair-275"&gt; /u/Significant-Pair-275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T09:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxpidc</id>
    <title>Where that Unsloth Q0.01_K_M GGUF at?</title>
    <updated>2025-07-12T02:37:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt; &lt;img alt="Where that Unsloth Q0.01_K_M GGUF at?" src="https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4380544f532ff369f435679247aa08f3c9afdb66" title="Where that Unsloth Q0.01_K_M GGUF at?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2em6rucvccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T02:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly6cg6</id>
    <title>Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!</title>
    <updated>2025-07-12T17:41:49+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.&lt;/p&gt; &lt;p&gt;It's &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/issues/64"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxycdh</id>
    <title>Safety first, or whatever🙄</title>
    <updated>2025-07-12T11:37:36+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt; &lt;img alt="Safety first, or whatever🙄" src="https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326cdedce8274c918a8336924d8741c3576c2f5a" title="Safety first, or whatever🙄" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/idk5uvesjfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnsh1</id>
    <title>OpenAI delays its open weight model again for "safety tests"</title>
    <updated>2025-07-12T01:09:38+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt; &lt;img alt="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" src="https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe88bccce70567bd39edea238607127c143134db" title="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5xvjxzefccf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8fyj</id>
    <title>This whole thing is giving me WizardLM2 vibes.</title>
    <updated>2025-07-12T19:09:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt; &lt;img alt="This whole thing is giving me WizardLM2 vibes." src="https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104" title="This whole thing is giving me WizardLM2 vibes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kn56m7cgshcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly4zh8</id>
    <title>Okay kimi-k2 is an INSANE model WTF those one-shot animations</title>
    <updated>2025-07-12T16:44:50+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt; &lt;img alt="Okay kimi-k2 is an INSANE model WTF those one-shot animations" src="https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ce744efba81890d05b5b715ce402a332366d7a" title="Okay kimi-k2 is an INSANE model WTF those one-shot animations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/74d8efoh2hcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly42e5</id>
    <title>Interesting info about Kimi K2</title>
    <updated>2025-07-12T16:05:34+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt; &lt;img alt="Interesting info about Kimi K2" src="https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423" title="Interesting info about Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt; &lt;p&gt;Source: @rasbt on X&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/klm2b78lvgcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
</feed>
