<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-19T10:50:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m3kjsm</id>
    <title>Are P40s useful for 70B models</title>
    <updated>2025-07-19T02:06:11+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently discovered the wonders of LM Studio, which lets me run models without the CLI headache of OpenWebUI or ollama, and supposedly it supports multi-GPU splitting &lt;/p&gt; &lt;p&gt;The main model I want to use is LLaMA 3.3 70B, ideally Q8, and sometimes fallen Gemma3 27B Q8, but because of scalper scumbags, GPUs are insanely overpriced &lt;/p&gt; &lt;p&gt;P40s are actually a pretty good deal, and I want to get 4 of them &lt;/p&gt; &lt;p&gt;Because I use an 8GB GTX1070 for playing games, I'm stuck with CPU only inference, which gives me about 0.4 tok/sec with LLaMA 70B, and about 1 tok/sec on fallen Gemma3 27B (which rapidly drops as context is filled) if I try to do partial GPU offloading, it slows down even more &lt;/p&gt; &lt;p&gt;I don't need hundreds of tokens per second, or collosal models, pretty happy with LLaMA 70B (and I'm used to waiting literally 10-15 MINUTES for each reply) would 4 P40s be suitable for what I'm planning to do &lt;/p&gt; &lt;p&gt;Some posts here say they work fine for AI, others say they're junk &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T02:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m31z4z</id>
    <title>support for EXAONE 4.0 model architecture has been merged into llama.cpp</title>
    <updated>2025-07-18T13:12:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/"&gt; &lt;img alt="support for EXAONE 4.0 model architecture has been merged into llama.cpp" src="https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bcec270f5033ba2a559251096ffb9bdbd92f54c" title="support for EXAONE 4.0 model architecture has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We introduce &lt;strong&gt;EXAONE 4.0&lt;/strong&gt;, which integrates a &lt;strong&gt;Non-reasoning mode&lt;/strong&gt; and &lt;strong&gt;Reasoning mode&lt;/strong&gt; to achieve both the excellent usability of &lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-3.5"&gt;EXAONE 3.5&lt;/a&gt; and the advanced reasoning abilities of &lt;a href="https://github.com/LG-AI-EXAONE/EXAONE-Deep"&gt;EXAONE Deep&lt;/a&gt;. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean.&lt;/p&gt; &lt;p&gt;The EXAONE 4.0 model series consists of two sizes: a mid-size &lt;strong&gt;32B&lt;/strong&gt; model optimized for high performance, and a small-size &lt;strong&gt;1.2B&lt;/strong&gt; model designed for on-device applications.&lt;/p&gt; &lt;p&gt;In the EXAONE 4.0 architecture, we apply new architectural changes compared to previous EXAONE models as below:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hybrid Attention&lt;/strong&gt;: For the 32B model, we adopt hybrid attention scheme, which combines &lt;em&gt;Local attention (sliding window attention)&lt;/em&gt; with &lt;em&gt;Global attention (full attention)&lt;/em&gt; in a 3:1 ratio. We do not use RoPE (Rotary Positional Embedding) for global attention for better global context understanding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;QK-Reorder-Norm&lt;/strong&gt;: We reorder the LayerNorm position from the traditional Pre-LN scheme by applying LayerNorm directly to the attention and MLP outputs, and we add RMS normalization right after the Q and K projection. It helps yield better performance on downstream tasks despite consuming more computation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14630"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T13:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3nc51</id>
    <title>When Llama4 Nemotron 250B MoE?</title>
    <updated>2025-07-19T04:34:16+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying to summon new models by asking the question. Seeing all these new Nemo models coming out makes me wonder if we'll see a pared-down Llama 4 Maverick that's been given the Nemotron treatment. I feel like that may be much harder with MoE architecture, but maybe not.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3r8jb</id>
    <title>Offline STT in real time?</title>
    <updated>2025-07-19T08:33:43+00:00</updated>
    <author>
      <name>/u/Sea-Replacement7541</name>
      <uri>https://old.reddit.com/user/Sea-Replacement7541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whats the best solution if you want to transcribe your voice to text in real time, locally?&lt;/p&gt; &lt;p&gt;Not saving it in an audio file and have it transcribed after.&lt;/p&gt; &lt;p&gt;Any easy to use one click GUI solutions like LMstudio for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Replacement7541"&gt; /u/Sea-Replacement7541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3r8jb/offline_stt_in_real_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3r8jb/offline_stt_in_real_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3r8jb/offline_stt_in_real_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T08:33:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3643z</id>
    <title>DiffRhythm+ is coming soon</title>
    <updated>2025-07-18T15:57:08+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3643z/diffrhythm_is_coming_soon/"&gt; &lt;img alt="DiffRhythm+ is coming soon" src="https://external-preview.redd.it/MXlkdGF5cWhubmRmMSv0rwk6lmBNsgRFS1EmTOIwvPCsaBuvDmC8mQJm0Njq.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5875482acb08634809dd9ce1962083e7c060b948" title="DiffRhythm+ is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DiffRhythm+ is coming soon (text -&amp;gt; music)&lt;/p&gt; &lt;p&gt;Looks like the DiffRhythm team is preparing to release DiffRhythm+, an upgraded version of the existing open-source DiffRhythm model.&lt;/p&gt; &lt;p&gt;Hopefully will be open-sourced similar to the previous DiffRhythm model (Apache 2.0) 👀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/54s9fzqhnndf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3643z/diffrhythm_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3643z/diffrhythm_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T15:57:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qejc</id>
    <title>Viability of the Threadripper Platform for a General Purpose AI+Gaming Machine?</title>
    <updated>2025-07-19T07:40:18+00:00</updated>
    <author>
      <name>/u/FluffnPuff_Rebirth</name>
      <uri>https://old.reddit.com/user/FluffnPuff_Rebirth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to build a workstation PC that can &amp;quot;Do it all&amp;quot; with a budget of some ~$8000, and a build around the upcoming Threadrippers is beginning to seem quite appealing. I suspect my use case is far from niche (Being Generic it's the opposite), so a thread discussing this could serve some purpose for the people.&lt;/p&gt; &lt;p&gt;By &amp;quot;General Purpose&amp;quot; I mean the system will have to fulfill the following criteria:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Good for gaming:&lt;/strong&gt; Probably the real bottleneck here, so I am starting with this. It doesn't need to be &amp;quot;optimal for gaming&amp;quot;, but ideally it shouldn't be a significant compromise either. This crosses out the Macs, unfortunately. Very known issue with high end Threadrippers is that while they do have tons of cores, the clock speeds are quite bad and so is the gaming performance. However, the lower end variants (XX45, XX55 perhaps even XX65) seem to on the spec sheet have significantly higher clock speeds, close to what the regular desktop counterparts of the same AMD generation have. When eyeballing the spec sheets, I don't see any massive red flags that would completely nerf the gaming performance with the lower end variants. Advantage over an EPYC build here would be the gaming capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent LLM/ImgGen inference with partial CPU off-loading:&lt;/strong&gt; This is where most of the point of the build lies in. Now that even the lower end Threadrippers come with 8-Channels and chonky PCI-E Bandwidth support, a Threadripper with the GPUs seems quite attractive. Local training capabilities being deprioritized as the advantages of using the cloud within this price range seem too great. But at least this system would have a very respectable capability to train as well, if need be.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive Platform Support:&lt;/strong&gt; This is probably the largest question mark for me, as I come from quite &amp;quot;gamery&amp;quot; background, I have next to no experience with hardware beyond the common consumer models. As far as I know, there shouldn't be any issues where some driver etc would become an issue because of the Threadripper? But you don't know what you don't know, so I am just assuming that the overall universality of x86-64 CPUs applies here too.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DIU Components:&lt;/strong&gt; As a hobbyist I like the idea of being able to swap as many things if need be, and I'd like to be able to reuse my old PSU/Case and not pay for something I am not going to use, which means a prebuilt workstation would have to be an exceptionally good deal to be pragmatic for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With these criteria in mind, this is something I came up with as a starting point. &lt;strong&gt;Do bear in mind that the included prices are just ballpark figures I pulled out of my rear&lt;/strong&gt;. There will be significant regional variance in either direction and it could be that I just didn't find the cheapest one available. I am just taking my local listed prices with VAT included and converting them to dollars for universality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASROCK WRX90 WS EVO &lt;strong&gt;(~$1000)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; The upcoming Threadripper Pro 9955WX (16/32 Core, 4.5GHz(5.4GHz Boost). Assuming these won't be OEM only. &lt;strong&gt;(~$1700)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; Kingston 256GB (8 x 32GB) FURY Renegade Pro (6000MHz) &lt;strong&gt;(~$1700)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Used 4090 for ImgGen as the primary workhorse would be the thing I'd be getting, and then I'd slap in my old 3090 and 3060s in there too for extra LLM VRAM, maybe in the future replacing them with something better. System RAM being 8-channels @ 6000MHz should make the model not entirely fitting in VRAM much less of a compromise than it would normally be. &lt;strong&gt;(~$1200, Used 4090, Not counting the cards I had)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Seasonic 2200W PRIME PX-2200. With these multi-GPU builds running out of power cables can become a problem. Sure, slapping in more PSU:s is always an option, but won't be the cleanest build if you don't have a case that can house them all. PSU in question can support up to 2x 12V-2x6 and 9x 8-pin PCIe cables. &lt;strong&gt;($500)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 20TB HDD for model cold storage, 4TB SSD for frequently loaded models and everything else. &lt;strong&gt;(~$800)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cooling:&lt;/strong&gt; Some WRX90 compatible AIO with a warranty &lt;strong&gt;(~$500)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Totaling:&lt;/strong&gt; &lt;strong&gt;$7400&lt;/strong&gt; for 256GB 8-Channel 6000MHz RAM and 24GB of VRAM with a smooth upgrade path to add more VRAM by just beginning to build the 3090 Jenga tower for $500 each. Budget has enough lax to buy whatever case/accessories and for the 9955WX to be a few hundred bucks more expensive in the wild.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So now the question is whether this listing has some glaring issues to it. Or if there would be something that would achieve the same for cheaper or better for roughly the same price.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FluffnPuff_Rebirth"&gt; /u/FluffnPuff_Rebirth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qejc/viability_of_the_threadripper_platform_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qejc/viability_of_the_threadripper_platform_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qejc/viability_of_the_threadripper_platform_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T07:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qg3w</id>
    <title>Escaping quantization brain damage with BF16?</title>
    <updated>2025-07-19T07:43:08+00:00</updated>
    <author>
      <name>/u/bitrumpled</name>
      <uri>https://old.reddit.com/user/bitrumpled</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying various LLMs running locally (on a 64GB DDR4 Threadripper + 5090 box, on llama.cpp) to try to arrive at a co-maintainer for my established FOSS project. I would like it to see the code and propose patches in diff (or direct to git by MCP) form.&lt;/p&gt; &lt;p&gt;My current theory is that the pressure to run quantized models is a major cause of why I can't get any model to produce a diff / patch that will apply to my project, they are all broken or slide off into gibberish or forgetfulness. It's like a kind of pervasive brain damage. At least, that is my hope, it may get disproved at any time by slop diffs coming out of a BF16 model.&lt;/p&gt; &lt;p&gt;I am wondering if anyone has been able to run a large BF16 model successfully locally, or even remotely as a service, so I can assess whether my theory is just copium and it's all trash out there.&lt;/p&gt; &lt;p&gt;The next reachable step up for me seems to be an 8480ES + 512GB DDR5, but even this seems too small if the goal is to avoid quantization.&lt;/p&gt; &lt;p&gt;I am reluctant to rent a H100 machine because I can only spend part of my time on this and the costs rack up all the time.&lt;/p&gt; &lt;p&gt;A related difficulty is the context size, I guess most of the related sources can fit in 128K context, but this magnifies the compute needs accordingly.&lt;/p&gt; &lt;p&gt;Opinions and experience welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bitrumpled"&gt; /u/bitrumpled &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T07:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3nwlf</id>
    <title>Any local models with decent tooling capabilities worth running with 3090?</title>
    <updated>2025-07-19T05:06:41+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, noob here so forgive the noobitude.&lt;/p&gt; &lt;p&gt;Relatively new to the AI coding tool space, started with copilot in VScode, it was OK, then moved to cursor which is/was awesome for a couple months, now it's nerfed get capped even on $200 plan within a couple weeks of the month, auto mode is &amp;quot;ok&amp;quot;. Tried claude code but wasn't really for me, I prefer the IDE interface of cursor or VSCode.&lt;/p&gt; &lt;p&gt;I'm now finding that even claude code is constantly timing out, cursor auto just doesn't have the context window for a lot of what I need...&lt;/p&gt; &lt;p&gt;I have a 3090, I've been trying to find out if there are any models worth running locally which have tooling agentic capabilities to then run in either cursor or VSCode. From what I've read (not heaps) it sounds like a lot of the open source models that can be run on a 3090 aren't really set up to work with tooling, so won't give a similar experience to cursor or copilot yet. But the space moves so fast so maybe there is something workable now?&lt;/p&gt; &lt;p&gt;Obviously I'm not expecting Claude level performance, but I wanted to see what's available and give something a try. Even if it's only 70% as good, if it's at least reliable and cheap then it might be good enough for what I am doing.&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T05:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qc1g</id>
    <title>Would there be a reasoning version of Kimi K2?</title>
    <updated>2025-07-19T07:35:55+00:00</updated>
    <author>
      <name>/u/christian7670</name>
      <uri>https://old.reddit.com/user/christian7670</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is really fascinating. I find it absolutely amazing. I believe that if this model gets added reasoning abilities it will beat absolutely everything on the market right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/christian7670"&gt; /u/christian7670 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qc1g/would_there_be_a_reasoning_version_of_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qc1g/would_there_be_a_reasoning_version_of_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qc1g/would_there_be_a_reasoning_version_of_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T07:35:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3pez5</id>
    <title>Best Russian language conversational model?</title>
    <updated>2025-07-19T06:37:19+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best model for practicing my Russian, something that can understand Russian well, will consistently use proper grammar, and can translate between English and Russian. Ideally &amp;lt;32B parameters, but if something larger will give a significant uplift I'd be interested to hear other options. This model doesn't really have to have great world knowledge or reasoning abilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3pez5/best_russian_language_conversational_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3pez5/best_russian_language_conversational_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3pez5/best_russian_language_conversational_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T06:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3rhy2</id>
    <title>llama.cpp running too slow</title>
    <updated>2025-07-19T08:50:55+00:00</updated>
    <author>
      <name>/u/bridgebucket</name>
      <uri>https://old.reddit.com/user/bridgebucket</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running the same model on llama.cpp as I do with kobold.cpp. KCPP has very fast outputs while LCPP is considerably more sluggish. I run llama-server with -ngl 100, but the output time is seemingly unchanged. Is this just how it's meant to be, or can I fix it somehow?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bridgebucket"&gt; /u/bridgebucket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T08:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3osbo</id>
    <title>Local deep research that web searches only academic sources?</title>
    <updated>2025-07-19T05:59:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work in medicine, and I basically want something similar to &lt;a href="https://www.openevidence.com/"&gt;OpenEvidence&lt;/a&gt;, but local and totally private because I don’t like the idea of putting patient information in a website, even if they claim to be HIPAA compliant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3osbo/local_deep_research_that_web_searches_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3osbo/local_deep_research_that_web_searches_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3osbo/local_deep_research_that_web_searches_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T05:59:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m37o5r</id>
    <title>Drummer's Cydonia 24B v4 - A creative finetune of Mistral Small 3.2</title>
    <updated>2025-07-18T16:57:39+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m37o5r/drummers_cydonia_24b_v4_a_creative_finetune_of/"&gt; &lt;img alt="Drummer's Cydonia 24B v4 - A creative finetune of Mistral Small 3.2" src="https://external-preview.redd.it/6G70mocJ7FlYL72oh-mj0AFKdz49VfoKLHqJie2Cn9M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49875409c55a3398df36a163183dfddfc0a65efc" title="Drummer's Cydonia 24B v4 - A creative finetune of Mistral Small 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's next? Voxtral 3B, aka, Ministral 3B (that's actually 4B). Currently in the works!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m37o5r/drummers_cydonia_24b_v4_a_creative_finetune_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m37o5r/drummers_cydonia_24b_v4_a_creative_finetune_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T16:57:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3amtu</id>
    <title>Is there any promising alternative to Transformers?</title>
    <updated>2025-07-18T18:50:40+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe there is an interesting research project, which is not effective yet, but after further improvements, can open new doors in AI development?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3amtu/is_there_any_promising_alternative_to_transformers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3amtu/is_there_any_promising_alternative_to_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3amtu/is_there_any_promising_alternative_to_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T18:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m394zh</id>
    <title>new models from NVIDIA: OpenReasoning-Nemotron 32B/14B/7B/1.5B</title>
    <updated>2025-07-18T17:53:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenReasoning-Nemotron-32B is a large language model (LLM) which is a derivative of Qwen2.5-32B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning about math, code and science solution generation. The model supports a context length of 64K tokens. The OpenReasoning model is available in the following sizes: 1.5B, 7B and 14B and 32B. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial research use.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_openreasoningnemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T17:53:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m36d91</id>
    <title>Meta says it won't sign Europe AI agreement, calling it an overreach that will stunt growth</title>
    <updated>2025-07-18T16:06:43+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m36d91/meta_says_it_wont_sign_europe_ai_agreement/"&gt; &lt;img alt="Meta says it won't sign Europe AI agreement, calling it an overreach that will stunt growth" src="https://external-preview.redd.it/ZDQNAbHBrMvYa-03D_p7yDfcZDMcYM8c8izD9GxQq4o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b69ea52f58eed4c486e9ec11d064e7470250b2ff" title="Meta says it won't sign Europe AI agreement, calling it an overreach that will stunt growth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/07/18/meta-europe-ai-code.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m36d91/meta_says_it_wont_sign_europe_ai_agreement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m36d91/meta_says_it_wont_sign_europe_ai_agreement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T16:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3ssb2</id>
    <title>ARC AGI 3 is stupid</title>
    <updated>2025-07-19T10:17:53+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the first game, first level of 8, I completed the level after wasting a lot of time trying to figure out what functionality the spacebar and mouse clicks had. None, it turned out. On the second level, I got completely stuck, then read in another thread that you have to move on and off the first shape several times to loop through available shapes until hitting the target shape. I would never in a millioin years have figured this out because I would never consider anyone would make an intelligence test this stupid.&lt;/p&gt; &lt;p&gt;ARC AGI 1 and 2 were fine, well designed. But this 3 version is a test of stupid persistence, not intelligence.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3ssb2/arc_agi_3_is_stupid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T10:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3no1m</id>
    <title>Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy.</title>
    <updated>2025-07-19T04:53:01+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"&gt; &lt;img alt="Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy." src="https://preview.redd.it/wz3nkrm3hrdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52fcc39aab4ecdb879243bef5a4cc3e8c57f0e44" title="Built a forensic linguistics tool to verify disputed quotes using computational stylometry - tested it on the Trump/Epstein birthday letter controversy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;How the Forensic Linguistics Analysis Works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built this using established computational linguistics techniques for authorship attribution - the same methods used in legal cases and academic research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Corpus Building&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compiled 76 documents (14M characters) of verified Trump statements from debates, speeches, tweets, and press releases&lt;/li&gt; &lt;li&gt;Cleaned the data to remove metadata while preserving actual speech patterns&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Stylometric Feature Extraction&lt;/strong&gt; The system extracts 4 categories of linguistic &amp;quot;fingerprints&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lexical Features&lt;/strong&gt;: Average word length, vocabulary richness, hapax legomena ratio (words used only once), Yule's K diversity measure&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Syntactic Features&lt;/strong&gt;: Part-of-speech distributions, dependency parsing patterns, sentence complexity scores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Features&lt;/strong&gt;: 768-dimension embeddings from the STAR authorship attribution model (AIDA-UPM/star)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stylistic Features&lt;/strong&gt;: Modal verb usage, passive voice frequency, punctuation patterns, function word ratios&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Similarity Calculation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compares the disputed text against all corpus documents using cosine similarity and Jensen-Shannon divergence&lt;/li&gt; &lt;li&gt;Generates weighted scores across all four linguistic dimensions&lt;/li&gt; &lt;li&gt;The 89.6% syntactic similarity is particularly significant - sentence structure patterns are neurologically hardwired and hardest to fake&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Why This Matters&lt;/strong&gt; Syntactic patterns emerge from deep cognitive structures. You can consciously change topic or vocabulary, but your underlying grammatical architecture remains consistent. The high syntactic match (89.6%) combined with moderate lexical match (47.2%) suggests same author writing in a different context.&lt;/p&gt; &lt;p&gt;The system correctly identified this as &amp;quot;probably same author&amp;quot; with 66.1% overall confidence - which is forensically significant for disputed authorship cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wz3nkrm3hrdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3no1m/built_a_forensic_linguistics_tool_to_verify/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3jogm</id>
    <title>4k local image gen</title>
    <updated>2025-07-19T01:22:24+00:00</updated>
    <author>
      <name>/u/kor34l</name>
      <uri>https://old.reddit.com/user/kor34l</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"&gt; &lt;img alt="4k local image gen" src="https://preview.redd.it/dulis7vegqdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11887cbf80f7af36eb4f0e9abe4330534f8e6b5a" title="4k local image gen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built an AI Wallpaper Generator that creates ultra-high-quality 4K wallpapers automatically with weather integration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After months of development, I've created a comprehensive AI wallpaper system that generates stunning 4K desktop backgrounds using multiple AI models. &lt;strong&gt;&lt;em&gt;The system just hit v4.2.0&lt;/em&gt;&lt;/strong&gt; with a completely rewritten SDXL pipeline that produces much higher quality photorealistic images.&lt;/p&gt; &lt;p&gt;It is flexible and simple enough to be used for ALL your image gen needs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multiple AI Models&lt;/strong&gt;: Choose from FLUX.1-dev, DALL-E 3, GPT-Image-1, or SDXL with Juggernaut XL v9 + multi-LoRA stacking. Each model has its own optimized pipeline for maximum quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weather Integration&lt;/strong&gt;: Real-time weather data automatically influences artistic themes and moods. Rainy day? You get atmospheric, moody scenes. Sunny weather? Bright, vibrant landscapes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced Pipeline&lt;/strong&gt;: Generates at optimal resolution, upscales to 8K using Real-ESRGAN, then downsamples to perfect 4K for incredible detail and quality. No compromises - time and storage don't matter, only final quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smart Theme System&lt;/strong&gt;: 60+ curated themes across 10 categories including Nature, Urban, Space, Anime, and more. Features &amp;quot;chaos mode&amp;quot; for completely random combinations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Intelligent Prompting&lt;/strong&gt;: Uses DeepSeek-r1:14b locally to generate creative, contextual prompts tailored to each model's strengths and current weather conditions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automated Scheduling&lt;/strong&gt;: Set-and-forget cron integration for daily wallpaper changes. Wake up to a new masterpiece every morning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage Options:&lt;/strong&gt; - &lt;code&gt;./ai-wallpaper generate&lt;/code&gt; - Default FLUX generation - &lt;code&gt;./ai-wallpaper generate --model sdxl&lt;/code&gt; - Use specific model&lt;br /&gt; - &lt;code&gt;./ai-wallpaper generate --random-model&lt;/code&gt; - Weighted random model selection - &lt;code&gt;./ai-wallpaper generate --save-stages&lt;/code&gt; - Save intermediate processing stages - &lt;code&gt;./ai-wallpaper generate --theme cyberpunk&lt;/code&gt; - Force specific theme - &lt;code&gt;./ai-wallpaper generate --prompt &amp;quot;custom prompt&amp;quot;&lt;/code&gt; - Direct prompt override - &lt;code&gt;./ai-wallpaper generate --random-params&lt;/code&gt; - Randomize generation parameters - &lt;code&gt;./ai-wallpaper generate --seed 42&lt;/code&gt; - Reproducible generation - &lt;code&gt;./ai-wallpaper generate --no-wallpaper&lt;/code&gt; - Generate only, don't set wallpaper - &lt;code&gt;./ai-wallpaper test --model flux&lt;/code&gt; - Test specific model - &lt;code&gt;./ai-wallpaper config --show&lt;/code&gt; - Display current configuration - &lt;code&gt;./ai-wallpaper models --list&lt;/code&gt; - Show all available models with status - &lt;code&gt;./setup_cron.sh&lt;/code&gt; - Automated daily wallpaper scheduling&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recent v4.2.0 Updates:&lt;/strong&gt; - &lt;strong&gt;&lt;em&gt;Completely rewritten SDXL pipeline&lt;/em&gt;&lt;/strong&gt; with Juggernaut XL v9 base model - Multi-LoRA stacking system with automatic theme-based selection - Enhanced negative prompts - Photorealistic prompt enhancement with DSLR camera modifiers - Optimized settings: 80+ steps, CFG 8.0, ensemble base/refiner pipeline&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical Specs:&lt;/strong&gt; - &lt;strong&gt;Models&lt;/strong&gt;: FLUX.1-dev (24GB VRAM), DALL-E 3 (API), GPT-Image-1 (API), SDXL+LoRA (16GB VRAM) - &lt;strong&gt;Quality&lt;/strong&gt;: Maximum settings across all models - no speed optimizations - &lt;strong&gt;Output&lt;/strong&gt;: Native 4K (3840x2160) with professional color grading - &lt;strong&gt;Architecture&lt;/strong&gt;: Modular Python system with YAML configuration - &lt;strong&gt;Desktop&lt;/strong&gt;: XFCE4 multi-monitor/workspace support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; - NVIDIA GPU (RTX 3090 recommended for SDXL) - FLUX works off CPU entirely, if GPU is weak - Python 3.10+ with virtual environment - OpenAI API key (for DALL-E/GPT models)&lt;/p&gt; &lt;p&gt;The system is completely open source and designed to be &amp;quot;fail loud&amp;quot; - every error is verbose and clear, making it easy to troubleshoot. All configuration is in YAML files, and the modular architecture makes it simple to add new models or modify existing pipelines.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/expectbugs/ai-wallpaper"&gt;https://github.com/expectbugs/ai-wallpaper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The system handles everything from installation to daily automation. Check the README.md for complete setup instructions, model comparisons, and configuration options. &lt;/p&gt; &lt;p&gt;Would love feedback from the community! I'm excited to see what others create with it.&lt;/p&gt; &lt;p&gt;The documentation (and most of this post) were written by AI, the legacy monolithic fat scripts in the legacy directory where I started, were also written largly by AI. The complete system was made with a LOT of tools and a lot of manual effort and bugfixing and refactoring, plus, of course, AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kor34l"&gt; /u/kor34l &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dulis7vegqdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3jogm/4k_local_image_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T01:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3sgr1</id>
    <title>WordPecker: Open Source Personalized Duolingo</title>
    <updated>2025-07-19T09:57:11+00:00</updated>
    <author>
      <name>/u/arbayi</name>
      <uri>https://old.reddit.com/user/arbayi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt; &lt;img alt="WordPecker: Open Source Personalized Duolingo" src="https://external-preview.redd.it/NGdqZmJ0Y2F6c2RmMc80rXNWOGm_7GTyts0LIbg_WRs-xM2_snleUNsHfx6L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b297127eab67178a51943bc8535bfc9dfb9f671" title="WordPecker: Open Source Personalized Duolingo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/baturyilmaz/wordpecker-app"&gt;https://github.com/baturyilmaz/wordpecker-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbayi"&gt; /u/arbayi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5fximscazsdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3sgr1/wordpecker_open_source_personalized_duolingo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T09:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3qpxz</id>
    <title>What are the most intriguing AI papers of 2025</title>
    <updated>2025-07-19T08:00:18+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been keeping up with AI research in 2025, and DeepSeek R1 really stands out to me as game-changing. What other papers from this year do you consider to be truly revolutionary?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3qpxz/what_are_the_most_intriguing_ai_papers_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T08:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m390kj</id>
    <title>DGAF if it’s dumber. It’s mine.</title>
    <updated>2025-07-18T17:48:14+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt; &lt;img alt="DGAF if it’s dumber. It’s mine." src="https://preview.redd.it/8dnb7bl76odf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca95154378d607f250ca4e5e26488394250116bf" title="DGAF if it’s dumber. It’s mine." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8dnb7bl76odf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m390kj/dgaf_if_its_dumber_its_mine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T17:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3iv6s</id>
    <title>any idea how to open source that?</title>
    <updated>2025-07-19T00:42:15+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt; &lt;img alt="any idea how to open source that?" src="https://preview.redd.it/x9e7q7z59qdf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd978d7cb888d92fdfc0a24134a57d1d3821cd08" title="any idea how to open source that?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9e7q7z59qdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3iv6s/any_idea_how_to_open_source_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T00:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m3n89p</id>
    <title>(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models</title>
    <updated>2025-07-19T04:28:21+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt; &lt;img alt="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" src="https://preview.redd.it/edxmilbhdrdf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed39c860785fb34d8104df720311441abac8087" title="(Confirmed) Kimi K2’s “modified-MIT” license does NOT apply to synthetic data/distilled models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2’s “modified-MIT” license does NOT apply to synthetic data or models trained on synthetic data.&lt;/p&gt; &lt;p&gt;“Text data generated by the model is NOT considered as a derivative work.”&lt;/p&gt; &lt;p&gt;Hopefully this will lead to more open source agentic models! Who will be the first to distill Kimi?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edxmilbhdrdf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m3n89p/confirmed_kimi_k2s_modifiedmit_license_does_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-19T04:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m39uqi</id>
    <title>I made a 1000 hour NSFW TTS dataset</title>
    <updated>2025-07-18T18:20:34+00:00</updated>
    <author>
      <name>/u/hotroaches4liferz</name>
      <uri>https://old.reddit.com/user/hotroaches4liferz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can find and listen to the dataset on huggingface: &lt;a href="https://huggingface.co/datasets/setfunctionenvironment/testnew"&gt;https://huggingface.co/datasets/setfunctionenvironment/testnew&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The sample rate of all audio is 24,000 kHz&lt;/p&gt; &lt;p&gt;Stats:&lt;/p&gt; &lt;p&gt;Total audio files/samples: 556,667&lt;/p&gt; &lt;p&gt;Total duration: 1024.71 hours (3688949 seconds)&lt;/p&gt; &lt;p&gt;Average duration: 6.63 seconds&lt;/p&gt; &lt;p&gt;Shortest clip: 0.41 seconds&lt;/p&gt; &lt;p&gt;Longest clip: 44.97 seconds (all audio &amp;gt;45 seconds removed)&lt;/p&gt; &lt;p&gt;more and more TTS models are releasing and improving, the size of these models are decreasing some even being 0.5b 0.7b or 0.1b parameters but unfortunately they all dont have NSFW capability. It is a shame there are so many NSFW LLM finetunes out there but none exist for text to speech, so if anyone at all has the compute to finetune one of the existing TTS models (kokoro, zonos, F5, chatterbox, orpheus) on my dataset that would be very appreciated as I would like to try it 🙏🙏🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hotroaches4liferz"&gt; /u/hotroaches4liferz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T18:20:34+00:00</published>
  </entry>
</feed>
