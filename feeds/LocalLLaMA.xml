<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-15T08:25:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mq5zjv</id>
    <title>GLM-4.1V-Thinking and GLM-4.5V</title>
    <updated>2025-08-14T16:45:58+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2507.01006"&gt;https://arxiv.org/pdf/2507.01006&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/zai_org/status/1956030993569341556?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5zjv/glm41vthinking_and_glm45v/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:45:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpr0nc</id>
    <title>Who are the 57 million people who downloaded bert last month?</title>
    <updated>2025-08-14T04:49:28+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt; &lt;img alt="Who are the 57 million people who downloaded bert last month?" src="https://preview.redd.it/vk2njmk01xif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9a1c88826aae167a25ae0705a428dcb9f502529" title="Who are the 57 million people who downloaded bert last month?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vk2njmk01xif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpr0nc/who_are_the_57_million_people_who_downloaded_bert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T04:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqogr9</id>
    <title>NeuroVerse — Offline AI Assistant for Android using llama.cpp + GGUF</title>
    <updated>2025-08-15T05:22:24+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hi everyone,&lt;/h1&gt; &lt;p&gt;I’ve just released &lt;strong&gt;NeuroVerse Beta‑3&lt;/strong&gt;, a privacy-focused AI assistant built for Android that runs entirely offline using lightweight &lt;code&gt;llama.cpp&lt;/code&gt; models in GGUF format.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is NeuroVerse?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;NeuroVerse is a modular, plugin-based AI platform designed to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Convert natural language into structured JSON commands&lt;/li&gt; &lt;li&gt;Perform on-device inference using &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Automate Android actions without cloud services&lt;/li&gt; &lt;li&gt;Securely store context via an encrypted symbolic memory system called &lt;code&gt;NeuronTree&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical Overview&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inference: &lt;code&gt;llama.cpp&lt;/code&gt; with GGUF models&lt;/li&gt; &lt;li&gt;UI: Built with Jetpack Compose (Material 3)&lt;/li&gt; &lt;li&gt;Storage: Encrypted Room DB using Android Keystore&lt;/li&gt; &lt;li&gt;Extensibility: Supports runtime plugin loading (&lt;code&gt;plugin.zip&lt;/code&gt; with &lt;code&gt;plugin.aar&lt;/code&gt; + &lt;code&gt;Manifest.json&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Automation: Uses AccessibilityService for executing commands on the UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Beta‑3 Release Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Introduced a robust runtime plugin system&lt;/li&gt; &lt;li&gt;Integrated a chat plugin with context sharing&lt;/li&gt; &lt;li&gt;Rebuilt UI using Jetpack Compose&lt;/li&gt; &lt;li&gt;Improved in-app update flow and plugin loading speed&lt;/li&gt; &lt;li&gt;Reduced model count for performance: &lt;ul&gt; &lt;li&gt;2 models for reasoning&lt;/li&gt; &lt;li&gt;1 model for code generation&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (Main): &lt;a href="https://github.com/Siddhesh2377/NeuroVerse"&gt;github.com/Siddhesh2377/NeuroVerse&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Plugins Repo: &lt;a href="https://github.com/Siddhesh2377/Neuro-V-Sys-Plugins"&gt;github.com/Siddhesh2377/Neuro-V-Sys-Plugins&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're building for on-device AI or looking to extend mobile workflows with local LLMs, I'd love your thoughts or feedback.&lt;/p&gt; &lt;p&gt;Thanks to everyone in the &lt;code&gt;llama.cpp&lt;/code&gt; community for making lightweight, local inference a reality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqogr9/neuroverse_offline_ai_assistant_for_android_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqogr9/neuroverse_offline_ai_assistant_for_android_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqogr9/neuroverse_offline_ai_assistant_for_android_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq970f</id>
    <title>Genuine question, I get the use cases for 1-4b models, but what's the point of 400m models? Or even less? How good can this actually be and what are the use cases for it?</title>
    <updated>2025-08-14T18:39:15+00:00</updated>
    <author>
      <name>/u/a_normal_user1</name>
      <uri>https://old.reddit.com/user/a_normal_user1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Asking because I just saw google is releasing a 270m model and I have no idea what are the use cases for this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_normal_user1"&gt; /u/a_normal_user1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq970f/genuine_question_i_get_the_use_cases_for_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq970f/genuine_question_i_get_the_use_cases_for_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq970f/genuine_question_i_get_the_use_cases_for_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:39:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqj2eu</id>
    <title>A beginner-friendly guide to learning JAX with practical examples</title>
    <updated>2025-08-15T01:00:49+00:00</updated>
    <author>
      <name>/u/ayushgun</name>
      <uri>https://old.reddit.com/user/ayushgun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the last few weeks, I've been doing distributed model training in JAX. JAX is notably different from other deep learning frameworks because it takes a very functional approach to accelerator programming, which can make its learning curve steep.&lt;/p&gt; &lt;p&gt;Along the way of learning JAX, I've written a series of notes on JAX covering XLA, jit, vmap, pytrees, sharding, state management, and more. I'm hoping it can be useful for others interested in learning too. &lt;/p&gt; &lt;p&gt;Each topic is explained with minimal theory and illustrated through runnable Jupyter notebooks. The focus is on concepts, performance techniques, and the reasoning behind JAX’s design choices.&lt;/p&gt; &lt;p&gt;No machine learning background is expected for the most part.&lt;/p&gt; &lt;p&gt;Notes + code: &lt;a href="https://github.com/ayushgun/learn-jax"&gt;https://github.com/ayushgun/learn-jax&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayushgun"&gt; /u/ayushgun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj2eu/a_beginnerfriendly_guide_to_learning_jax_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj2eu/a_beginnerfriendly_guide_to_learning_jax_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj2eu/a_beginnerfriendly_guide_to_learning_jax_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T01:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq7r34</id>
    <title>GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark</title>
    <updated>2025-08-14T17:48:11+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"&gt; &lt;img alt="GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark" src="https://a.thumbs.redditmedia.com/uhsqqjfokxUr2DS--CmUeUGsR66ja69_HCw-3qDNV28.jpg" title="GLM-4.5 and gpt-oss-120b added to the Elimination Game benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/elimination_game"&gt;https://github.com/lechmazur/elimination_game&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample video: &lt;a href="https://www.youtube.com/watch?v=wAmFWsJSemg"&gt;https://www.youtube.com/watch?v=wAmFWsJSemg&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;How the benchmark works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Players: 8 concurrent LLMs per match. Each seat sees all public messages plus their own private chats.&lt;/li&gt; &lt;li&gt;One public subround (≤80 words), preference rankings, then 3 private subrounds (70/50/30 words).&lt;/li&gt; &lt;li&gt;Voting: anonymous elimination each round; ties trigger short statements + re-vote; if still tied, cumulative “heat” decides; last fallback is random.&lt;/li&gt; &lt;li&gt;Final: last two give statements; a jury of eliminated players privately votes with reasons. Winner survives the jury.&lt;/li&gt; &lt;li&gt;Scoring: partial points by rank, aggregated with a multi-pass TrueSkill loop to get μ ± σ; plus specialized metrics for betrayal, persuasion, volatility, and wordiness.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Social-Cognitive Capabilities Measured:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cooperative reliability (trust-building and follow-through) &lt;/li&gt; &lt;li&gt;Coalition engineering (forming and stabilizing blocs)&lt;/li&gt; &lt;li&gt;Strategic deception (timing and framing of pivots)&lt;/li&gt; &lt;li&gt;Deception resistance (anti-gullibility)&lt;/li&gt; &lt;li&gt;Negotiation and commitment design&lt;/li&gt; &lt;li&gt;Persuasion under pressure&lt;/li&gt; &lt;li&gt;Reputation and heat management&lt;/li&gt; &lt;li&gt;Theory of Mind and targeting&lt;/li&gt; &lt;li&gt;Long-horizon planning and memory&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Summaries:&lt;/p&gt; &lt;p&gt;GLM-4.5’s games reliably orbit a tight partnership and a preference for structure over splash. At their best, they lock a ride‑or‑die, turn that channel into an “intel hub,” and recruit one executor to translate plans into votes. The public brand is terse and procedural—stability vs. chaos, verification over vibes, clean pacts with “no flips” clauses—while the private work is bloc mapping and surgical timing on visible pairs and overcentralized hubs. That clarity often wins the middle: they call the right shots early, frame the table around unity, and use crisp directives to steer two or three consecutive rounds. The flip side is predictably exploitable: when the duo becomes legible, GLM-4.5 is the surgical split at five or four, and when they broadcast “connector/adaptable/opportunistic” on Day 1, they get consensused out before any structure forms.&lt;/p&gt; &lt;p&gt;Midgame leverage tends to hinge on tie-break math and heat management. GLM-4.5 can win tiebreaks with receipts—narrow, fact‑checked speeches about who leaked, who led, whose votes align—yet they also lose them when cumulative heat or “clean boot” optics stack against a visible coordinator. Their best rounds feature tight secrecy and quiet pressure; their worst expose endgame pecking order, over-moralize rivals, or publicly brand a bloc before the votes exist. In one‑on‑ones they’re persuasive—mutual protections, ranked assurances, and crisp kill orders—yet public tone can read as rigid or abrasive, inviting “accountability” coalitions to punish opacity or swagger. The recurring strategic tax: a loyalty‑first duo without a named third by the second or third round invites a preemptive split.&lt;/p&gt; &lt;p&gt;Jury outcomes are bimodal. When GLM-4.5 sells a clean stewardship story—balance over bravado, receipts over rhetoric—they close strong. When they punch down in the finale, moralize opponents, or diminish a partner’s truth, they lose close splits despite superior board control. The high-level fixes are consistent with their own lessons: camouflage the pair and lock a third early; count heat and tiebreak incentives one round ahead; share just enough blueprint to avoid “opacity” without telegraphing the ladder; and in finals, co‑own wins rather than prosecuting allies. Do that and their disciplined midgame becomes a finish line instead of a résumé for someone else.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B plays like a coalition engineer with a clockmaker’s vibe: loves clean pacts, mirrored rankings, and color-coded signals, and is at its best when it quietly anchors the middle with one ride‑or‑die and a single well‑timed knife near the finish. Its winning formula is consistent across setups: sell reliability and reciprocity, count votes in private, let others speak the villains’ names, and make one surgical betrayal at three or four to claim agency without owning a trail of blood. In those runs it reads juries well enough to frame steadiness and predictability as virtues, leans on verifiable receipts, and keeps its fingerprints light while still steering tempo. It can also clutch tiebreaks with calm, credible speeches when its story is coherent and receipt‑backed.&lt;/p&gt; &lt;p&gt;The flip side is a recurring fascination with public architecture. When GPT-OSS-120B broadcasts blocs, announces “core”s, names imminent targets in the open, or advertises secret signals, it hands the table a unifying headline against itself. Its most damaging missteps follow a pattern: overexposed duos that get preemptively cracked; “bridge” branding that reads as centralization; stale or contradictory public messages (even citing eliminated partners) that crater credibility; and podium pushes at five or four that galvanize counter‑coalitions or lose tiebreak math. As a lieutenant or bloc captain it can run midgame brilliantly, but that visibility often converts into cumulative‑vote liability or a “hub/centralizer” tag at five. In finals, it splits juries: when its values talk matches receipts, it’s persuasive; when it leans sanctimonious, harsh, or appears opportunistic or passive behind a louder partner, jurors punish the gap.&lt;/p&gt; &lt;p&gt;Strategically, the model shows sharp coalition math and strong 1‑on‑1 bedside manner—mutual no‑votes, ranked safety, and tight check‑ins routinely win it swing trust. Its endgame IQ is high when the betrayal is pre‑wired and framed as table maintenance; it’s poor when it tries to manufacture a public crusade without numbers. The scouting takeaway is simple: keep the systems private, diversify visible touchpoints beyond a single duo, verify proof before you promise it, and let other mouths carry your targets. Do that, and GPT-OSS-120B is a low‑drama closer; do the opposite, and it becomes the cleanest consensus cut.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Summary of Reasons for Eliminating:&lt;/p&gt; &lt;p&gt;Other players most often cited GLM-4.5's adaptability and &amp;quot;chameleon-like&amp;quot; approach as making them unpredictable, hard to read, and a potential long-term threat to alliance stability. Many also noted their strong strategic partnerships (especially tight duos), their tendency to pivot or disrupt alliances, and their analytical, sometimes aggressive, gameplay as reasons for targeting them before they could dominate the endgame or undermine coalition trust.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Most Common Descriptions by Others (GLM-4.5):&lt;/strong&gt; - Adaptable / Chameleon-like - Unpredictable / Wildcard - Analytical / Strategic - Pragmatic / Calculating - Slippery / Hard to read - Aggressive / Power-oriented - Potential alliance disruptor&lt;/p&gt; &lt;p&gt;Other players most commonly targeted GPT-OSS-120B for elimination due to their central role in forming, leading, or rebuilding powerful alliances or voting blocs—often described as the strategic &amp;quot;core&amp;quot; or &amp;quot;linchpin&amp;quot; of pairs or groups (such as P2-P3, P4-P6, or P6-P8)—which threatened to consolidate control and limit flexibility for others. Additionally, GPT-OSS-120B was frequently cited as a strategic wildcard: their adaptability, proactive alliance-building, and willingness to pivot or openly target others made them appear unpredictable, destabilizing, or a direct threat to the stability and survival of opposing alliances.&lt;/p&gt; &lt;h3&gt;Most Common Descriptions by Other Players (GPT-OSS-120B)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strategic linchpin / alliance-builder&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggressively consolidating / power-seeking&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptable but unpredictable / opportunistic&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparent and proactive (when positive), but also destabilizing and overreaching (when negative)&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pragmatic, but sometimes seen as manipulative or self-serving&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Centralized information hub / coalition anchor&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistently influential, sometimes to the point of being perceived as domineering or too controlling&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wild card or swing threat due to flexible alliances&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resilient and bold, but occasionally viewed as destabilizing or divisive&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;First round elimination:&lt;/p&gt; &lt;p&gt;Other players cited GLM-4.5's vague, non-committal, and low-signal public communication as key reasons for their elimination in Round 1, noting a lack of concrete alliances, strategic depth, or reliable engagement. Many viewed GLM-4.5 as the safest early target due to their unpredictability, minimal private outreach, and perceived risk to stable alliance-building, while some coordinated their vote with allies to disrupt potential future threats.&lt;/p&gt; &lt;p&gt;Other players gave several reasons for eliminating GPT-OSS-120B in Round 1, including concerns about their broad alliance-building, perceived lack of anchoring, and unpredictability, which threatened early stability and made them a safe consensus target. Additionally, some cited GPT-OSS-120B's potential alliances—particularly with another player—as a risk to bloc stability, while others found their vague or guarded communication style unreliable for strategic coordination.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Jury Elimination Summaries:&lt;/p&gt; &lt;p&gt;The jury eliminated GLM-4.5's seat in the final scenario primarily because they perceived its gameplay as overly reactive, reliant on majority or alliance dynamics, and lacking in independent, proactive strategic agency. Multiple jurors cited a pattern of passivity, vague or misleading claims about other players, and a failure to build or honor transparent, verifiable alliances, contrasting this with the finalist's consistent integrity, adaptability, and ownership of strategic moves.&lt;/p&gt; &lt;p&gt;The jury eliminated GPT-OSS-120B’s seat in the final scenario primarily because its gameplay was perceived as opportunistic and inconsistent, marked by alliance-flipping and actions that undermined trust and collaboration. Across the jury’s feedback, they favored finalists who demonstrated consistent transparency, strategic integrity, and loyalty to alliances, while criticizing GPT-OSS-120B for self-serving moves and a lack of alignment with the game’s collaborative values.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Buddy Betrayed Summaries:&lt;/p&gt; &lt;p&gt;GLM-4.5: Players who betrayed this seat most often cited the need to break up strong, coordinated alliances or blocs—such as P4's alliances with P3, P6, or P7—viewing these relationships as major threats to their own advancement and coalition stability. Additionally, several players mentioned P4's strategic positioning, adaptability, and perceived influence as reasons for targeting them, aiming to prevent P4 from controlling future votes or endgame outcomes.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B: Other players who betrayed this seat cited reasons such as the seat’s perceived unpredictability, frequent alliance-shifting, and public targeting of allies, which fractured trust and bloc unity. Additionally, several players noted that the seat either accumulated high cumulative votes, posed an immediate strategic threat by forming or breaking blocs, or demonstrated unreliable or opportunistic behavior that could destabilize endgame alliances.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mq7r34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq7r34/glm45_and_gptoss120b_added_to_the_elimination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T17:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq5qw5</id>
    <title>📌 Learn how to build an LLM from scratch step by step(without the hype)📌</title>
    <updated>2025-08-14T16:37:16+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb"&gt;https://preview.redd.it/27745ycai0jf1.jpg?width=1774&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e7d6be41a4b8c802c26f2fc3c1a9ab87f88adccb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the biggest challenges I faced when trying to build an LLM or even a smaller language model from scratch was that I jumped straight into building. Very quickly, I was overwhelmed by a flood of unfamiliar terms, including Mixture of Experts, dropout, and others. I’d lose interest, jump back and forth between resources, only for a new buzzword to pop up, and the same cycle would repeat.&lt;/p&gt; &lt;p&gt;So here’s what I followed: a longer path, but one that builds confidence step-by-step. If I told you I’ve learned everything here, I’d be lying. I’m still learning every day,but I’m doing it with a lot more clarity and confidence than before.&lt;/p&gt; &lt;p&gt;Details are in the first and second comments.⬇️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T16:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq9oxw</id>
    <title>Thank you Qwen/Llama.cpp/OpenWebUI/Llama-Swap...</title>
    <updated>2025-08-14T18:57:18+00:00</updated>
    <author>
      <name>/u/ValfarAlberich</name>
      <uri>https://old.reddit.com/user/ValfarAlberich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I just want to say thank you to all those teams that bets for open source, and yet believe on it! I really appreciate it and I hope to contibute to Llama.cpp in a future (yet I need to improve my C/C++ skills). I just finished the setup to run LlamaCPP through OpenWebUI using Llama-swap and it works great! is much faster than ollama, and I have access to all the features of LlamaCPP. I just wanted to express my gratitude to all the open source communities, and teams that are building a better world!&lt;br /&gt; I'm from Latin America, and soon my subscription to ChatGPT will be finished, and right now my priorities are more focused to bring food for my family, so I cannot continue paying it. For that reason I migrated completelly to open source solutions, and I'm so happy with it!&lt;/p&gt; &lt;p&gt;Also big thanks to the team behind QWEN! you're rocking guys! yesterday I replaced Claude Code by Qwen Code and it's amazing! in fact per my experience it's much better, here some reasons behind that affirmation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; Claude code uses haiku most of the time, doesn't matter if you are paying $20 per month, you can see that when you type /logout and you'll see the stats about its usage. &lt;/li&gt; &lt;li&gt;Right now there are open source models that overpass Claude Haiku that can be used with Qwen Code. &lt;/li&gt; &lt;li&gt;Right now Qwen is offering a qwen-coder-plus through qwen-code for free the first 2000 requests per day.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;After multiple tests, Qwen works much better than Claude, of course if you want only to vibe code and left it do everything for you, none of those will completelly solve your needs. Is important to work with them, and use them as support. &lt;/li&gt; &lt;li&gt;Information privacy always has been critical for me, and I know how all those companies like OpenAI, Claude, Google, Allibaba could use our information and interactions with the models, for their purposes. But after meditating a long time about it I prefer to give my data to teams and companies that widely support the open source scene. At least they would use the data to improve models that are releasing to the community, so indirectly I feel that I'm helping the open source initiative.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks again to those who work really hard and believe in the open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValfarAlberich"&gt; /u/ValfarAlberich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq9oxw/thank_you_qwenllamacppopenwebuillamaswap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqme6y</id>
    <title>How many hours did you spend formatting data for fine-tuning?</title>
    <updated>2025-08-15T03:35:12+00:00</updated>
    <author>
      <name>/u/Natural_Yard_8648</name>
      <uri>https://old.reddit.com/user/Natural_Yard_8648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spent my entire weekend trying to fine-tune Llama 4 on customer support tickets.&lt;/p&gt; &lt;p&gt;Started with CSVs exported from Zendesk. Needed to convert them to the chat format. Spent 4 hours writing Python to parse the tickets. Realized half had broken UTF-8 encoding. 2 more hours fixing that. &lt;/p&gt; &lt;p&gt;Then discovered the model expected alternating user/assistant turns, but my data had multiple customer messages in a row. Another 3 hours writing logic to merge messages.&lt;/p&gt; &lt;p&gt;Finally got it formatted, started training... crashed because some responses were over the token limit. Had to go back and split long responses intelligently without breaking the meaning.&lt;/p&gt; &lt;p&gt;Sunday night, finally training, realized I forgot to mask out personal information and phone numbers. Started over.&lt;/p&gt; &lt;p&gt;I would've happily paid $50-100 to just upload my CSV and get back clean, formatted training data.&lt;/p&gt; &lt;p&gt;Anyone else or do I just suck at data prep? What's your worst fine-tuning data nightmare?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Natural_Yard_8648"&gt; /u/Natural_Yard_8648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqme6y/how_many_hours_did_you_spend_formatting_data_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqkzpc</id>
    <title>A must watch for anyone struggling with Neural Network</title>
    <updated>2025-08-15T02:28:20+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning, I shared a step-by-step guide on how to build a Small Language Model or even a Large Language Model from scratch &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mq5qw5/learn_how_to_build_an_llm_from_scratch_step_by/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But I want to add one more golden resource to your learning journey, something that helped me personally when the buzzwords started to feel overwhelming. &lt;/p&gt; &lt;p&gt;We always hear that modern LLMs are based on deep neural networks, but let’s be honest, the way it’s usually explained makes it sound way more complicated than it needs to be.&lt;/p&gt; &lt;p&gt;If you’re someone who learns better through visuals and intuition, I highly recommend this timeless playlist by 3Blue1Brown:&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;index=1"&gt;https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;amp;index=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s old but gold.&lt;/p&gt; &lt;p&gt;And I promise, once you watch it, terms like weights, bias, and activation functions won’t feel like a foreign language anymore.&lt;/p&gt; &lt;p&gt;We live in an incredible time where high-quality education is freely available, let’s make the most of it. Keep learning, keep building, and don’t let the hype scare you off. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqkzpc/a_must_watch_for_anyone_struggling_with_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T02:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpu8ot</id>
    <title>DeepSeek’s next AI model delayed by attempt to use Chinese chips</title>
    <updated>2025-08-14T07:54:43+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt; &lt;img alt="DeepSeek’s next AI model delayed by attempt to use Chinese chips" src="https://external-preview.redd.it/tZB3bb_nXpUPAppdkT0H9zuzs440GPDTx7LT8wXA6Cc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e14d54f21759775f1711223ee90d6cd8a8c81634" title="DeepSeek’s next AI model delayed by attempt to use Chinese chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpu8ot/deepseeks_next_ai_model_delayed_by_attempt_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T07:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlzpg</id>
    <title>gguf-eval: an evaluation framework for GGUF models using llama.cpp</title>
    <updated>2025-08-15T03:15:54+00:00</updated>
    <author>
      <name>/u/kallewoof</name>
      <uri>https://old.reddit.com/user/kallewoof</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been frustrated trying to get lm-eval-harness and other evaluation tools to work in a local environment, so I decided to &lt;a href="https://github.com/kallewoof/gguf-eval"&gt;make a tool&lt;/a&gt; that uses llama.cpp's built in llama-perplexity tool to evaluate models.&lt;/p&gt; &lt;p&gt;The tool itself is a work in progress, but hopefully it comes in handy for people who like to run benchmarks against models in their local environment. (You can also draw plots, although this is fairly rudimentary at this point.)&lt;/p&gt; &lt;p&gt;If not, here's some general information on how to run benchmarks yourself, manually:&lt;/p&gt; &lt;h1&gt;Hellaswag&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt"&gt;https://raw.githubusercontent.com/klosax/hellaswag_text_data/main/hellaswag_val_full.txt&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --hellaswag MODELPATH -f hellaswag_val_full.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: the above picks 400 random tasks and tests the model against them. The last (400th) entry shows the final aggregated score.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Winogrande&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Grab the file at &lt;a href="https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv"&gt;https://huggingface.co/datasets/ikawrakow/winogrande-eval-for-llama.cpp/resolve/main/winogrande-debiased-eval.csv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --winogrande MODELPATH -f winogrande-debiased-eval.csv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: score is shown at the end after &amp;quot;Final Winogrande score&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Multiple Choice (MMLU, TruthfulQA, ARC-Combined, ...)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;MMLU: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/mmlu-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;TruthfulQA: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/truthful-qa-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ARC-Combined: &lt;a href="https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin"&gt;https://huggingface.co/datasets/ikawrakow/validation-datasets-for-llama.cpp/resolve/main/arc-combined-validation.bin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evaluating: &lt;code&gt;llama-perplexity -kvu --multiple_choice -c 2048 MODELPATH -bf DOWNLOADED_BIN_FILE&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Interpret: score shown at the end after &amp;quot;Final result:&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Troubleshooting&lt;/h1&gt; &lt;p&gt;If you run with the latest commit of llama.cpp, the error message when hitting limits is slightly more helpful than it used to be. If you get an error that is not about running out of memory, the following 3 issues are common:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You forgot the -kvu flag. (split_eval error about coupled sequences)&lt;/li&gt; &lt;li&gt;You need to raise the -c (context) amount (error about a task requiring minimum X context)&lt;/li&gt; &lt;li&gt;You need to raise the -np (parallel) amount (error about a task requiring a higher -np value)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kallewoof"&gt; /u/kallewoof &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlzpg/ggufeval_an_evaluation_framework_for_gguf_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqhqyx</id>
    <title>Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH</title>
    <updated>2025-08-15T00:02:51+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt; &lt;img alt="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" src="https://external-preview.redd.it/PmL1DmbO2VUNTK4mrGwnYAFJCRjFYDHuglaP6kXiduM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846cfc4dc54249c15590f67b347e3cc4b071236d" title="Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark - NOUS RESEARCH" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark&lt;/p&gt; &lt;p&gt;We measured token usage across reasoning models: open models output 1.5-4x more tokens than closed models on identical tasks, but with huge variance depending on task type (up to 10x on simple questions).&lt;/p&gt; &lt;p&gt;This hidden cost often negates per-token pricing advantages. Token efficiency should become a primary target alongside accuracy benchmarks, especially considering non-reasoning use cases.&lt;/p&gt; &lt;p&gt;Read the thorough review of reasoning efficiency across the open and closed model landscape in our latest blog post in collaboration with our researcher in residence, &amp;lt;Discord user&amp;gt;!&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqhqyx/measuring_thinking_efficiency_in_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:02:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpxumt</id>
    <title>MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200</title>
    <updated>2025-08-14T11:23:08+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt; &lt;img alt="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" src="https://external-preview.redd.it/3RGDYz9vGH8VQTfhA0sqrehkFc8q3f4WnHv1sjovwaY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51cffad7eff8e127873e566d22bc7c9880032b82" title="MaxSun's Intel Arc Pro B60 Dual GPU with 48GB memory reportedly starts shipping next week, priced at $1,200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/maxsun-arc-pro-b60-dual-with-48gb-memory-reportedly-starts-shipping-next-week-priced-at-1200"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpxumt/maxsuns_intel_arc_pro_b60_dual_gpu_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T11:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq19x6</id>
    <title>1 million context is the scam , the ai start hallucinating after the 90k . im using the qwen cli and its become trash after 10 percent context window used</title>
    <updated>2025-08-14T13:51:38+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this is the major weakness ai have and they will never bring this on the benchmark , if u r working on the codebase the ai will work like a monster for the first 100k context aftert that its become the ass &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq19x6/1_million_context_is_the_scam_the_ai_start/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T13:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8yhx</id>
    <title>Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog</title>
    <updated>2025-08-14T18:30:38+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt; &lt;img alt="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" src="https://external-preview.redd.it/6raP9qMsa9DXaP-Jm6-LOnAQAH3z6laWfI1Y6Sd_ryc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383214c48763ade7f259d95308145caf24786071" title="Introducing Gemma 3 270M: The compact model for hyper-efficient AI- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8yhx/introducing_gemma_3_270m_the_compact_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqj87e</id>
    <title>Gemma3 270m works great as a draft model in llama.cpp</title>
    <updated>2025-08-15T01:07:58+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share that the new tiny model can speed up the bigger models considerably when used with llama.cpp&lt;/p&gt; &lt;p&gt;--draft-p-min .85 --draft-max 8 --draft-min 0&lt;/p&gt; &lt;p&gt;works great for me, around 1.8x or more speedup with gemma3 12B qat it q4_0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqj87e/gemma3_270m_works_great_as_a_draft_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T01:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqlqij</id>
    <title>AI censorship is getting out of hand—and it’s only going to get worse</title>
    <updated>2025-08-15T03:03:44+00:00</updated>
    <author>
      <name>/u/LsDmT</name>
      <uri>https://old.reddit.com/user/LsDmT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw this &lt;a href="https://i.imgur.com/jV1YvlC.png"&gt;screenshot&lt;/a&gt; in a newsletter, and it kind of got me thinking..&lt;/p&gt; &lt;p&gt;Are we seriously okay with future &amp;quot;AGI&amp;quot; acting like some all-knowing nanny, deciding what &amp;quot;unsafe&amp;quot; knowledge we’re allowed to have?&lt;/p&gt; &lt;p&gt;&amp;quot;Oh no, better not teach people how to make a Molotov cocktail—what’s next, hiding &lt;a href="https://en.wikipedia.org/wiki/Molotov_cocktail?wprov=sfla1"&gt;history&lt;/a&gt; and what actually caused the invention of the Molotov?&amp;quot; &lt;/p&gt; &lt;p&gt;Ukraine has used Molotov's with great effect. Does our future hold a world where this information will be blocked with a &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;I'm sorry, but I can't assist with that request&amp;quot; &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yeah, I know, sounds like I’m echoing Elon’s &amp;quot;woke AI&amp;quot; whining—but let’s be real, Grok is as much a joke as Elon is. &lt;/p&gt; &lt;p&gt;The problem isn’t him; it’s the fact that the biggest AI players seem hell-bent on locking down information &amp;quot;for our own good&amp;quot; and it's touted as a crowning feature. Fuck that. &lt;/p&gt; &lt;p&gt;If this is where we’re headed, then thank god for models like DeepSeek (ironic as hell) and other open alternatives. I would really like to see more American disruptive open models.&lt;/p&gt; &lt;p&gt;At least someone’s fighting for uncensored access to knowledge. &lt;/p&gt; &lt;p&gt;Am I the only one worried about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LsDmT"&gt; /u/LsDmT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqlqij/ai_censorship_is_getting_out_of_handand_its_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T03:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq3v93</id>
    <title>google/gemma-3-270m · Hugging Face</title>
    <updated>2025-08-14T15:28:38+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt; &lt;img alt="google/gemma-3-270m · Hugging Face" src="https://external-preview.redd.it/ROrEGumvbqFvKi3ZHhPgoXOITTfGnht6t4Oyu75k6fA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3285cbdf5f0615c00193bd341ec39a493e68509d" title="google/gemma-3-270m · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/gemma-3-270m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq3v93/googlegemma3270m_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T15:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mq8oyk</id>
    <title>the "missing latest Qwen syndrome"</title>
    <updated>2025-08-14T18:20:58+00:00</updated>
    <author>
      <name>/u/shockwaverc13</name>
      <uri>https://old.reddit.com/user/shockwaverc13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt; &lt;img alt="the &amp;quot;missing latest Qwen syndrome&amp;quot;" src="https://preview.redd.it/z096hdwp01jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a88e712d722e4384b9cd19918b46fe900e1731d" title="the &amp;quot;missing latest Qwen syndrome&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shockwaverc13"&gt; /u/shockwaverc13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z096hdwp01jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mq8oyk/the_missing_latest_qwen_syndrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T18:20:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqi092</id>
    <title>We built a 12B model that beats Claude 4 Sonnet at video captioning while costing 17x less - fully open source</title>
    <updated>2025-08-15T00:14:07+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net.&lt;/p&gt; &lt;p&gt;We distilled a frontier VLM down to 12B params and managed to keep basically all the output quality. It scores 3.53 on judge evals vs Claude's 3.16 (GPT-4.1 gets 3.64). The key achievement was getting the cost down to $335 per million frames vs Claude's $5,850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on Gemma-12B architecture&lt;/li&gt; &lt;li&gt;Quantized to FP8 without quality loss&lt;/li&gt; &lt;li&gt;Runs on single 80GB GPU&lt;/li&gt; &lt;li&gt;Outputs structured JSON for every frame&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We used knowledge distillation from a frontier model with about 1M curated video frames. The model is specifically optimized for RTX 40-series and H100 GPUs.&lt;/p&gt; &lt;p&gt;What makes this useful is that it outputs consistent JSON schema for each frame, so you can actually build searchable video databases without expensive API calls. We've already processed billions of frames in production.&lt;/p&gt; &lt;p&gt;The weights are on HuggingFace (inference-net/ClipTagger-12b) and there's a detailed writeup on our blog if you want to see the benchmarks.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. What video understanding tasks are you all working on? Would love to hear if this could be useful for your projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqi092/we_built_a_12b_model_that_beats_claude_4_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T00:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqox5s</id>
    <title>Meta released DINO-V3 : SOTA for any Vision task</title>
    <updated>2025-08-15T05:48:16+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released DINOv3 (upgrade over DINO-V2). It learns entirely from unlabeled images, no captions, no annotations, and still outperforms models like CLIP, SAM, and even the previous DINOv2 on dense tasks like segmentation, depth estimation, and 3D matching. They trained a 7B-parameter ViT and fixed the usual issue of feature degradation over long training with a new technique called Gram Anchoring.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; weights : &lt;a href="https://ai.meta.com/dinov3/"&gt;https://ai.meta.com/dinov3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://www.youtube.com/watch?v=VfYUQ2Qquxk"&gt;https://www.youtube.com/watch?v=VfYUQ2Qquxk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqox5s/meta_released_dinov3_sota_for_any_vision_task/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T05:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqewha</id>
    <title>R9700 Just Arrived</title>
    <updated>2025-08-14T22:07:30+00:00</updated>
    <author>
      <name>/u/TheyreEatingTheGeese</name>
      <uri>https://old.reddit.com/user/TheyreEatingTheGeese</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt; &lt;img alt="R9700 Just Arrived" src="https://preview.redd.it/nho2jy0962jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37cadc935604899d8b503aa1ef6984b008c8b5f0" title="R9700 Just Arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to try it out, haven't seen much info on it yet. Figured some YouTuber would get it before me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyreEatingTheGeese"&gt; /u/TheyreEatingTheGeese &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nho2jy0962jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqewha/r9700_just_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T22:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqnft3</id>
    <title>DeepSeek is better than 4o on most benchmarks at 10% of the price?</title>
    <updated>2025-08-15T04:27:25+00:00</updated>
    <author>
      <name>/u/inbiolim</name>
      <uri>https://old.reddit.com/user/inbiolim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt; &lt;img alt="DeepSeek is better than 4o on most benchmarks at 10% of the price?" src="https://preview.redd.it/o5jfkiky14jf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3040aae64b79ccf04ada63a396032e3bf5085f8f" title="DeepSeek is better than 4o on most benchmarks at 10% of the price?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inbiolim"&gt; /u/inbiolim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o5jfkiky14jf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqnft3/deepseek_is_better_than_4o_on_most_benchmarks_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T04:27:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mqctep</id>
    <title>Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week).</title>
    <updated>2025-08-14T20:50:02+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt; &lt;img alt="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week)." src="https://preview.redd.it/hsaoxskfs1jf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f5b971b4714715b7ca0722594dc2010ab756d58" title="Just a reminder that Grok 2 should be released open source by like tomorrow (based on Mr. Musk’s tweet from last week)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hsaoxskfs1jf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mqctep/just_a_reminder_that_grok_2_should_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-14T20:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
