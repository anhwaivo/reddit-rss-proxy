<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-13T20:48:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jajoyo</id>
    <title>Insights of analyzing &gt;100 LLMs for the DevQualityEval v1.0 (generating quality code) in latest deep dive</title>
    <updated>2025-03-13T18:57:14+00:00</updated>
    <author>
      <name>/u/zimmski</name>
      <uri>https://old.reddit.com/user/zimmski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajoyo/insights_of_analyzing_100_llms_for_the/"&gt; &lt;img alt="Insights of analyzing &amp;gt;100 LLMs for the DevQualityEval v1.0 (generating quality code) in latest deep dive" src="https://external-preview.redd.it/phzynsyD3fKiD7kEK1KHbMosK8nwORgRRhrVnHsnHgg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89736cda0bd9b3f508a92ddc0a4e5c1f89591d54" title="Insights of analyzing &amp;gt;100 LLMs for the DevQualityEval v1.0 (generating quality code) in latest deep dive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4y3janqk7ioe1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6a5157dfd6d3e683c8ea7b0a29817c37868f5a7"&gt;https://preview.redd.it/4y3janqk7ioe1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b6a5157dfd6d3e683c8ea7b0a29817c37868f5a7&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üëë Google‚Äôs Gemini 2.0 Flash Lite is the king of cost-effectiveness (our previous king OpenAI‚Äôs o1-preview is 1124x more expensive, and worse in score)&lt;/li&gt; &lt;li&gt;ü•á Anthropic‚Äôs Claude 3.7 Sonnet is the functional best model (with help) ‚Ä¶ by far&lt;/li&gt; &lt;li&gt;üè° Qwen‚Äôs Qwen 2.5 Coder is the best model for local use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;_&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models are on average getting better at code generation, especially in Go&lt;/li&gt; &lt;li&gt;Only one model is on-par with static tooling for migrating JUnit 4 to 5 code&lt;/li&gt; &lt;li&gt;Surprise! providers are unreliable for days for new popular models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;_&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Let‚Äôs STOP the model naming MADNESS together: we proposed a convention for naming models&lt;/li&gt; &lt;li&gt;We counted all the votes, v1.1 will bring: JS, Python, Rust, ‚Ä¶&lt;/li&gt; &lt;li&gt;Our hunch with using static analytics to improve scoring continues to be true&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All the other models, details and how we continue to solve the &amp;quot;ceiling problem&amp;quot; in the deep dive: &lt;a href="https://symflower.com/en//company/blog/2025/dev-quality-eval-v1.0-anthropic-s-claude-3.7-sonnet-is-the-king-with-help-and-deepseek-r1-disappoints/"&gt;https://symflower.com/en//company/blog/2025/dev-quality-eval-v1.0-anthropic-s-claude-3.7-sonnet-is-the-king-with-help-and-deepseek-r1-disappoints/&lt;/a&gt;&lt;br /&gt; (now with interactive graphs üåà)&lt;/p&gt; &lt;p&gt;Looking forward to your feedback :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zimmski"&gt; /u/zimmski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajoyo/insights_of_analyzing_100_llms_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajoyo/insights_of_analyzing_100_llms_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jajoyo/insights_of_analyzing_100_llms_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jalesx</id>
    <title>Llama 3.2 vision 11B - enhancing my gaming experience</title>
    <updated>2025-03-13T20:09:12+00:00</updated>
    <author>
      <name>/u/Moose_bit_my_sister</name>
      <uri>https://old.reddit.com/user/Moose_bit_my_sister</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jalesx/llama_32_vision_11b_enhancing_my_gaming_experience/"&gt; &lt;img alt="Llama 3.2 vision 11B - enhancing my gaming experience" src="https://b.thumbs.redditmedia.com/YMiC5TWOaVKFcJYeISUhmisrhzlMGfg5vgUBI1uEPGY.jpg" title="Llama 3.2 vision 11B - enhancing my gaming experience" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is something cool that i want to share with people. I enjoy playing 4x games such as warhammer. Since I have a life my lore knowledge is lacking to say the least... BUT step in LLAMA vision! 10X my enjoyment by explaining/or inventing the lore!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kz7glnd9kioe1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8315184b076e7a95149b1647a52681d0af39bf84"&gt;it can just describe the lore from one image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ptm5isggkioe1.png?width=1075&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd0e05298a2e549b9cf4dfa20a655bd6412fd7d7"&gt;it actually looked at the image - did not hallucinate fully!!!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moose_bit_my_sister"&gt; /u/Moose_bit_my_sister &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jalesx/llama_32_vision_11b_enhancing_my_gaming_experience/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jalesx/llama_32_vision_11b_enhancing_my_gaming_experience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jalesx/llama_32_vision_11b_enhancing_my_gaming_experience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T20:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja4qz1</id>
    <title>Gemma 3 Deep Dive: Is Google Cranking Up the Compute Budget?</title>
    <updated>2025-03-13T05:17:17+00:00</updated>
    <author>
      <name>/u/mimirium_</name>
      <uri>https://old.reddit.com/user/mimirium_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been digging into the tech report details emerging on Gemma 3 and wanted to share some interesting observations and spark a discussion. Google seems to be making some deliberate design choices with this generation.&lt;/p&gt; &lt;p&gt;Key Takeaways (from my analysis of publicly available information):&lt;/p&gt; &lt;p&gt;FFN Size Explosion: The feedforward network (FFN) sizes for the 12B and 27B Gemma 3 models are significantly larger than their Qwen2.5 counterparts. We're talking a massive increase. This probably suggests a shift towards leveraging more compute within each layer.&lt;/p&gt; &lt;p&gt;Compensating with Hidden Size: To balance the FFN bloat, it looks like they're deliberately lowering the hidden size (d_model) for the Gemma 3 models compared to Qwen. This could be a clever way to maintain memory efficiency while maximizing the impact of the larger FFN.&lt;/p&gt; &lt;p&gt;Head Count Differences: Interesting trend here ‚Äì much fewer heads generally, but it seems the 4B model has more kv_heads than the rest. Makes you wonder if Google are playing with their version of MQA or GQA&lt;/p&gt; &lt;p&gt;Training Budgets: The jump in training tokens is substantial:&lt;/p&gt; &lt;p&gt;1B -&amp;gt; 2T (same as Gemma 2-2B) 2B -&amp;gt; 4T 12B -&amp;gt; 12T 27B -&amp;gt; 14T&lt;/p&gt; &lt;p&gt;Context Length Performance:&lt;/p&gt; &lt;p&gt;Pretrained on 32k which is not common, No 128k on the 1B + confirmation that larger model are easier to do context extension Only increase the rope (10k-&amp;gt;1M) on the global attention layer. 1 shot 32k -&amp;gt; 128k ?&lt;/p&gt; &lt;p&gt;Architectural changes:&lt;/p&gt; &lt;p&gt;No softcaping but QK-Norm Pre AND Post norm&lt;/p&gt; &lt;p&gt;Possible Implications &amp;amp; Discussion Points:&lt;/p&gt; &lt;p&gt;Compute-Bound? The FFN size suggests Google is throwing more raw compute at the problem, possibly indicating that they've optimized other aspects of the architecture and are now pushing the limits of their hardware.&lt;/p&gt; &lt;p&gt;KV Cache Optimizations: They seem to be prioritizing KV cache optimizations Scaling Laws Still Hold? Are the gains from a larger FFN linear, or are we seeing diminishing returns? How does this affect the scaling laws we've come to expect?&lt;/p&gt; &lt;p&gt;The &amp;quot;4B Anomaly&amp;quot;: What's with the relatively higher KV head count on the 4B model? Is this a specific optimization for that size, or an experimental deviation?&lt;/p&gt; &lt;p&gt;Distillation Strategies? Early analysis suggests they used small vs large teacher distillation methods&lt;/p&gt; &lt;p&gt;Local-Global Ratio: They tested Local:Global ratio on the perplexity and found the impact minimal What do you all think? Is Google betting on brute force with Gemma 3? Are these architectural changes going to lead to significant performance improvements, or are they more about squeezing out marginal gains? Let's discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mimirium_"&gt; /u/mimirium_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja4qz1/gemma_3_deep_dive_is_google_cranking_up_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9v3lf</id>
    <title>Gemma 3 - Insanely good</title>
    <updated>2025-03-12T21:18:47+00:00</updated>
    <author>
      <name>/u/kaizoku156</name>
      <uri>https://old.reddit.com/user/kaizoku156</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just shocked by how good gemma 3 is, even the 1b model is so good, a good chunk of world knowledge jammed into such a small parameter size, I'm finding that i'm liking the answers of gemma 3 27b on ai studio more than gemini 2.0 flash for some Q&amp;amp;A type questions something like &amp;quot;how does back propogation work in llm training ?&amp;quot;. It's kinda crazy that this level of knowledge is available and can be run on something like a gt 710&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaizoku156"&gt; /u/kaizoku156 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9v3lf/gemma_3_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T21:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j9vjf1</id>
    <title>üî• DeepSeek R1 671B Q4 - M3 Ultra 512GB with MLXüî•</title>
    <updated>2025-03-12T21:36:54+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes it works! First test, and I'm blown away!&lt;/p&gt; &lt;p&gt;Prompt: &amp;quot;Create an amazing animation using p5js&amp;quot;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;18.43 tokens/sec&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Generates a p5js zero-shot, tested at video's end&lt;/li&gt; &lt;li&gt;Video in real-time, no acceleration!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j9vjf1/video/nmcm91wpvboe1/player"&gt;https://reddit.com/link/1j9vjf1/video/nmcm91wpvboe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-12T21:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jagn5r</id>
    <title>DeepHermes - A Hybrid Reasoner model released</title>
    <updated>2025-03-13T16:52:37+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jagn5r/deephermes_a_hybrid_reasoner_model_released/"&gt; &lt;img alt="DeepHermes - A Hybrid Reasoner model released" src="https://b.thumbs.redditmedia.com/2L_Oz4vZxaWTTvWe-eHbY1KCr6mhH4iBl4RuzIQRZ6c.jpg" title="DeepHermes - A Hybrid Reasoner model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepHermes 24B Preview performs extremely well on reasoning tasks with reasoning mode ON, jumping over 4x in accuracy on hard math problems, and 43% on GPQA, a STEM based QA benchmark. &lt;/p&gt; &lt;p&gt;Built on MistralAI's excellent Mistral-Small-24B open model, its a perfect size for quantization on consumer GPUs.&lt;/p&gt; &lt;p&gt;With reasoning mode off, it performs comparably to Mistral's own instruct variant.&lt;/p&gt; &lt;p&gt;DeepHermes 24B is available on HuggingFace and the Nous Portal via our API now.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF Quantized Versions also available here: 24B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/nousresearch/status/1900218445763088766?s=46"&gt;https://x.com/nousresearch/status/1900218445763088766?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jagn5r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jagn5r/deephermes_a_hybrid_reasoner_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jagn5r/deephermes_a_hybrid_reasoner_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T16:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jah6d6</id>
    <title>DeepHermes - a Hybrid Reasoner LLM released</title>
    <updated>2025-03-13T17:14:14+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jah6d6/deephermes_a_hybrid_reasoner_llm_released/"&gt; &lt;img alt="DeepHermes - a Hybrid Reasoner LLM released" src="https://b.thumbs.redditmedia.com/vzMfwqrwJJWSDYk08QLD0wGKoyRFrW7DH7yMSESJfPY.jpg" title="DeepHermes - a Hybrid Reasoner LLM released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepHermes 24B Preview performs extremely well on reasoning tasks with reasoning mode ON, jumping over 4x in accuracy on hard math problems, and 43% on GPQA, a STEM based QA benchmark. &lt;/p&gt; &lt;p&gt;Built on MistralAI's excellent Mistral-Small-24B open model, its a perfect size for quantization on consumer GPUs.&lt;/p&gt; &lt;p&gt;With reasoning mode off, it performs comparably to Mistral's own instruct variant.&lt;/p&gt; &lt;p&gt;DeepHermes 24B is available on HuggingFace and the Nous Portal via API now.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF Quantized Versions also available here:&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/nousresearch/status/1900218445763088766?s=46"&gt;https://x.com/nousresearch/status/1900218445763088766?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jah6d6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jah6d6/deephermes_a_hybrid_reasoner_llm_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jah6d6/deephermes_a_hybrid_reasoner_llm_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaesjc</id>
    <title>Me: &lt;trying to formulate an intelligent question to ask the Google Gemma team during the AMA&gt;</title>
    <updated>2025-03-13T15:35:30+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaesjc/me_trying_to_formulate_an_intelligent_question_to/"&gt; &lt;img alt="Me: &amp;lt;trying to formulate an intelligent question to ask the Google Gemma team during the AMA&amp;gt;" src="https://preview.redd.it/1pekg6nx7hoe1.gif?width=216&amp;amp;crop=smart&amp;amp;s=5b30076d10aca712528cfb42952a47aee6eaf832" title="Me: &amp;lt;trying to formulate an intelligent question to ask the Google Gemma team during the AMA&amp;gt;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pekg6nx7hoe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaesjc/me_trying_to_formulate_an_intelligent_question_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaesjc/me_trying_to_formulate_an_intelligent_question_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T15:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja5m3w</id>
    <title>Open SORA 2.0 ! They are trolling openai again</title>
    <updated>2025-03-13T06:18:06+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://twitter.com/YangYou1991/status/1899973689460044010"&gt;https://twitter.com/YangYou1991/status/1899973689460044010&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo : &lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;https://github.com/hpcaitech/Open-Sora&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja5m3w/open_sora_20_they_are_trolling_openai_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T06:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabin6</id>
    <title>C4AI Command A 111B</title>
    <updated>2025-03-13T13:06:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cohere.com/blog/command-a"&gt;https://cohere.com/blog/command-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io"&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaftxa</id>
    <title>DeepHermes - a NousResearch Collection</title>
    <updated>2025-03-13T16:19:02+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"&gt; &lt;img alt="DeepHermes - a NousResearch Collection" src="https://external-preview.redd.it/1IlQBWESV8QV8X9vVxfshuMcnWyH3f4Z3MrqfDWjQNw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=994f341d0495a385f29282ceb0cf8ce537d0f6df" title="DeepHermes - a NousResearch Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/NousResearch/deephermes-67d2ff8c9246cc09a7bd8add"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaftxa/deephermes_a_nousresearch_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T16:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja0xnh</id>
    <title>Does Google not understand that DeepSeek R1 was trained in FP8?</title>
    <updated>2025-03-13T01:44:39+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"&gt; &lt;img alt="Does Google not understand that DeepSeek R1 was trained in FP8?" src="https://preview.redd.it/5kbayoq13doe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=477d6f469b86c99e53a84538ec86ec48a60f156a" title="Does Google not understand that DeepSeek R1 was trained in FP8?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5kbayoq13doe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja0xnh/does_google_not_understand_that_deepseek_r1_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T01:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaka3d</id>
    <title>TraceBack: A Novel Reverse Reasoning Model for Better and Cheaper Scaling of Synthetic Reasoning Generation</title>
    <updated>2025-03-13T19:21:31+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaka3d/traceback_a_novel_reverse_reasoning_model_for/"&gt; &lt;img alt="TraceBack: A Novel Reverse Reasoning Model for Better and Cheaper Scaling of Synthetic Reasoning Generation" src="https://external-preview.redd.it/Ekh7WXRn0YKPQcfKnUoYgREGxwCVj7D650nFO9ogFm8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa8f4838903531cd00f8b4344577124094ecc827" title="TraceBack: A Novel Reverse Reasoning Model for Better and Cheaper Scaling of Synthetic Reasoning Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/secemp9/TraceBack-12b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaka3d/traceback_a_novel_reverse_reasoning_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaka3d/traceback_a_novel_reverse_reasoning_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jal0yx</id>
    <title>There it is https://github.com/SesameAILabs/csm</title>
    <updated>2025-03-13T19:53:22+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...almost. Hugginface link is still 404ing. Let's wait some minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ja2ers</id>
    <title>The duality of man</title>
    <updated>2025-03-13T03:00:08+00:00</updated>
    <author>
      <name>/u/jhanjeek</name>
      <uri>https://old.reddit.com/user/jhanjeek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt; &lt;img alt="The duality of man" src="https://preview.redd.it/1ukvrj06hdoe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=392789bc2b1887610312054c20af823db4ab6078" title="The duality of man" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhanjeek"&gt; /u/jhanjeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ukvrj06hdoe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ja2ers/the_duality_of_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T03:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jajyz3</id>
    <title>Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval</title>
    <updated>2025-03-13T19:08:40+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"&gt; &lt;img alt="Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval" src="https://a.thumbs.redditmedia.com/VpvpROyRpiHrwFqib5SAlcggW0k_M2aiS5pJIm9FZv4.jpg" title="Gemma 3 27B scores on four independent benchmarks: wide variation depending on the eval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jajyz3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jajyz3/gemma_3_27b_scores_on_four_independent_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:08:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaje3r</id>
    <title>SoftWhisper update ‚Äì Transcribe 2 hours in 2 minutes!</title>
    <updated>2025-03-13T18:44:31+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt; &lt;img alt="SoftWhisper update ‚Äì Transcribe 2 hours in 2 minutes!" src="https://a.thumbs.redditmedia.com/KhBAAPQUSrS-HxirCaMUKrts96HY0vDLxz_SMOBUkm0.jpg" title="SoftWhisper update ‚Äì Transcribe 2 hours in 2 minutes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a long wait, a new release of SoftWhisper, your frontend to the Whisper API, is out! &lt;strong&gt;And what is best, NO MORE PYTORCH DEPENDENCIES! Now it's just install and run.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;The changes to the frontend are minimal, but in the backend they are quite drastic. The dependencies on Pytorch made this program much more complicated to install and run to the average user than they should ‚Äì which is why I decided to remove them!&lt;/p&gt; &lt;p&gt;Originally, I would use the original OpenAI AI + ZLUDA, but unfortunately Pytorch support is not quite there yet. So I decided to use Whisper.cpp as a backend. And this proved to be a good decision: now, we can transcribe 2 hours of video in around 2-3 minutes!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/75ueh1bk4ioe1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e39af5204135b1cf82c6bba2d37fe5c61762e7ce"&gt;https://preview.redd.it/75ueh1bk4ioe1.png?width=2008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e39af5204135b1cf82c6bba2d37fe5c61762e7ce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation steps:&lt;/p&gt; &lt;p&gt;If you use Windows, I have already provided a prebuilt release of Whisper.cpp as a backend with Vulkan support, so no extra steps are necessary: just download SoftWhisper and run it with:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;python&lt;/strong&gt; &lt;a href="http://SoftWhisper.py"&gt;&lt;strong&gt;SoftWhisper.py&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unfortunately, I haven't tested this software under Linux. I do plan to provide a prebuilt static version of Whisper.cpp for Linux as well, but in the meantime, Linux users can compile Whisper.cpp themselves and add the executable at the field &amp;quot;Whisper.cpp executable.&amp;quot;&lt;/p&gt; &lt;p&gt;Please also note that I couldn't get speaker diarization working in this release, so I had to remove it. I might add it back in the future. However, considering the performance increase, it is a small price to pay.&lt;/p&gt; &lt;p&gt;Enjoy, and let me know if you have any questions.&lt;/p&gt; &lt;p&gt;[Link to the original release: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fvncqc/comment/mh7t4z7/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1fvncqc/comment/mh7t4z7/?context=3&lt;/a&gt; ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaje3r/softwhisper_update_transcribe_2_hours_in_2_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaiux8</id>
    <title>The first Gemma3 finetune</title>
    <updated>2025-03-13T18:22:47+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote a really nice formatted post, but for some reason locallama auto bans it, and only approves low effort posts. So here's the short version: a new Gemma3 tune is up.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaiux8/the_first_gemma3_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jacyqt</id>
    <title>Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-13T14:15:51+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt; &lt;img alt="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/kzrghtnotgoe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=464ed1f234a1c459c1c1cc69de42594f8ce42dd7" title="Check out the new theme of my open sourced desktop app, you can run LLMs locally with built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kzrghtnotgoe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jacyqt/check_out_the_new_theme_of_my_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T14:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jag07t</id>
    <title>Nous Deephermes 24b and 3b are out !</title>
    <updated>2025-03-13T16:26:02+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official gguf:&lt;/p&gt; &lt;p&gt;24b: &lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3b:&lt;a href="https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF"&gt;https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jag07t/nous_deephermes_24b_and_3b_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T16:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabj70</id>
    <title>New model from Cohere: Command A!</title>
    <updated>2025-03-13T13:07:26+00:00</updated>
    <author>
      <name>/u/slimyXD</name>
      <uri>https://old.reddit.com/user/slimyXD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Command A is our new state-of-the-art addition to Command family optimized for demanding enterprises that require fast, secure, and high-quality models. &lt;/p&gt; &lt;p&gt;It offers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3.&lt;/p&gt; &lt;p&gt;It features 111b, a 256k context window, with: * inference at a rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o and 2.4x higher than DeepSeek-V3 * excelling performance on business-critical agentic and multilingual tasks * minimal hardware needs - its deployable on just two GPUs, compared to other models that typically require as many as 32&lt;/p&gt; &lt;p&gt;Check out our full report: &lt;a href="https://cohere.com/blog/command-a"&gt;https://cohere.com/blog/command-a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the model card: &lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via Cohere API as &lt;code&gt;command-a-03-2025&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slimyXD"&gt; /u/slimyXD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabj70/new_model_from_cohere_command_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabh4m</id>
    <title>CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face</title>
    <updated>2025-03-13T13:04:32+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt; &lt;img alt="CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face" src="https://external-preview.redd.it/8uC-fRvLBGmT6kbHk8wyBrnHknVe8WnRlviRost1iDM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=316e90209b2b29e953d13bdd363e048d518bc1d0" title="CohereForAI/c4ai-command-a-03-2025 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabh4m/cohereforaic4aicommanda032025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabmwz</id>
    <title>AMA with the Gemma Team</title>
    <updated>2025-03-13T13:12:40+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama! During the next day, the Gemma research and product team from DeepMind will be around to answer with your questions! Looking forward to them!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Technical Report: &lt;a href="https://goo.gle/Gemma3Report"&gt;https://goo.gle/Gemma3Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AI Studio: &lt;a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Technical blog post &lt;a href="https://developers.googleblog.com/en/introducing-gemma3/"&gt;https://developers.googleblog.com/en/introducing-gemma3/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3"&gt;https://www.kaggle.com/models/google/gemma-3&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahs0b</id>
    <title>OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch</title>
    <updated>2025-03-13T17:38:10+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt; &lt;img alt="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" src="https://external-preview.redd.it/YuFnFIavAP98hFeGzOxLZQ1jrf6fXSzPC6RHQ4YO4ew.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2312e28fa573cb9d493e784a1275b4624e4c905" title="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/03/13/openai-calls-deepseek-state-controlled-calls-for-bans-on-prc-produced-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
</feed>
