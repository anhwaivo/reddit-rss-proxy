<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-08T09:05:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j5lym7</id>
    <title>Lightweight Hallucination Detector for Local RAG Setups - No Extra LLM Calls Required</title>
    <updated>2025-03-07T12:05:37+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've been working on solving a common problem many of us face when running RAG systems with our local models - hallucinations. While our locally-hosted LLMs are impressive, they still tend to make things up when using RAG, especially when running smaller models with limited context windows.&lt;/p&gt; &lt;p&gt;I've released an &lt;strong&gt;open-source hallucination detector&lt;/strong&gt; that's specifically designed to be efficient enough to run on consumer hardware alongside your local LLMs. Unlike other solutions that require additional LLM API calls (which add latency and often external dependencies), this is a lightweight transformer-based classifier.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on modernBERT architecture&lt;/li&gt; &lt;li&gt;Inference speed: ~1 example/second on CPU, ~10-20 examples/second on modest GPU&lt;/li&gt; &lt;li&gt;Zero external API dependencies - runs completely local&lt;/li&gt; &lt;li&gt;Works with any LLM output, including Llama-2, Llama-3, Mistral, Phi-3, etc.&lt;/li&gt; &lt;li&gt;Integrates easily with LlamaIndex, LangChain, or your custom RAG pipeline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The detector evaluates your LLM's response against the retrieved context to identify when the model generates information not present in the source material. It achieves 80.7% recall on the RAGTruth benchmark, with particularly strong performance on data-to-text tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example integration with your local setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from adaptive_classifier import AdaptiveClassifier # Load the hallucination detector (downloads once, runs locally after) detector = AdaptiveClassifier.from_pretrained(&amp;quot;adaptive-classifier/llm-hallucination-detector&amp;quot;) # Your existing RAG pipeline context = retriever.get_relevant_documents(query) response = your_local_llm.generate(context, query) # Format for the detector input_text = f&amp;quot;Context: {context}\nQuestion: {query}\nAnswer: {response}&amp;quot; # Check for hallucinations prediction = detector.predict(input_text) if prediction[0][0] == 'HALLUCINATED' and prediction[0][1] &amp;gt; 0.6: print(&amp;quot;⚠️ Warning: Response appears to contain information not in the context&amp;quot;) # Maybe re-generate or add a disclaimer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The detector is part of the adaptive-classifier library which also has tools for routing between different local models based on query complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How have you been addressing hallucinations in your local RAG setups?&lt;/li&gt; &lt;li&gt;Would a token-level detector (highlighting exactly which parts are hallucinated) be useful?&lt;/li&gt; &lt;li&gt;What's your typical resource budget for this kind of auxiliary model in your stack?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://github.com/codelion/adaptive-classifier#hallucination-detector"&gt;https://github.com/codelion/adaptive-classifier#hallucination-detector&lt;/a&gt;&lt;br /&gt; Installation: &lt;code&gt;pip install adaptive-classifier&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T12:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6cz5o</id>
    <title>UPDATE THIS WEEK: Tool Calling for DeepSeek-R1 671B is now available on Microsoft Azure</title>
    <updated>2025-03-08T08:40:26+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Exciting news for DeepSeek-R1 enthusiasts! I've now successfully integrated DeepSeek-R1 671B support for tool calling on Microsoft Azure for BOTH Python AND JavaScript developers!&lt;/p&gt; &lt;p&gt;Python: &lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;JavaScript/TypeScript: &lt;a href="https://github.com/leockl/tool-ahead-of-time-ts"&gt;https://github.com/leockl/tool-ahead-of-time-ts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why is this important? Because now you can leverage tool calling with the incredible reasoning capabilities of DeepSeek-R1 671B with enterprise-grade infrastructure, security, and scalability! &lt;/p&gt; &lt;p&gt;Please give my GitHub repos a star if this was helpful. Hope this helps anyone who needs this. Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6cz5o/update_this_week_tool_calling_for_deepseekr1_671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6cz5o/update_this_week_tool_calling_for_deepseekr1_671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6cz5o/update_this_week_tool_calling_for_deepseekr1_671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T08:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j62lfa</id>
    <title>Question about AI memory databases using new breakthrough technologies.</title>
    <updated>2025-03-07T22:47:37+00:00</updated>
    <author>
      <name>/u/Furai69</name>
      <uri>https://old.reddit.com/user/Furai69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SpacetimeDB 1.0 was released recently, and the technology used in it made me think its use case for AI could be extreamly beneficial if built to utilize this tech? I'm more curious if people who are advanced in AI databases, do you think this database is beneficial for AI use cases?&lt;/p&gt; &lt;p&gt;Maybe not just a Supabase alternative, but something more like a combination of a front end/backend/database that scales all-in-one type system?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ClockworkLabs/SpacetimeDB"&gt;https://github.com/ClockworkLabs/SpacetimeDB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/kzDnA_EVhTU?si=heESzxcW7EeufXZ5"&gt;https://youtu.be/kzDnA_EVhTU?si=heESzxcW7EeufXZ5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Furai69"&gt; /u/Furai69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j62lfa/question_about_ai_memory_databases_using_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T22:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66udm</id>
    <title>How does OpenAI do their scraping for search?</title>
    <updated>2025-03-08T02:17:04+00:00</updated>
    <author>
      <name>/u/Ok_Coyote_8904</name>
      <uri>https://old.reddit.com/user/Ok_Coyote_8904</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing around with the search functionality in ChatGPT and it's honestly impressive. I'm particularly wondering how they scrape the internet in such a fast and accurate manner while retrieving high quality content from their sources. &lt;/p&gt; &lt;p&gt;Anyone have an idea? They're obviously caching and scraping at intervals, but anyone have a clue how or what their method is? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Coyote_8904"&gt; /u/Ok_Coyote_8904 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66udm/how_does_openai_do_their_scraping_for_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66udm/how_does_openai_do_their_scraping_for_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66udm/how_does_openai_do_their_scraping_for_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6bz0o</id>
    <title>QwQ</title>
    <updated>2025-03-08T07:26:49+00:00</updated>
    <author>
      <name>/u/SirTwitchALot</name>
      <uri>https://old.reddit.com/user/SirTwitchALot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6bz0o/qwq/"&gt; &lt;img alt="QwQ" src="https://b.thumbs.redditmedia.com/-aO7vL5frRHxo41rVo_iFlR1IerJP4NB5aBfhP0EAnk.jpg" title="QwQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0m0bnst34fne1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15cf90c5e5c26e70da71f593a59405ca11b9bedb"&gt;https://preview.redd.it/0m0bnst34fne1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15cf90c5e5c26e70da71f593a59405ca11b9bedb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirTwitchALot"&gt; /u/SirTwitchALot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6bz0o/qwq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6bz0o/qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6bz0o/qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T07:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66oe7</id>
    <title>RLAMA: A Simple RAG Interface to Chat with Your Documents via Ollama</title>
    <updated>2025-03-08T02:08:16+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.&lt;/p&gt; &lt;h1&gt;What it actually is&lt;/h1&gt; &lt;p&gt;RLAMA is a command-line tool that bridges your local documents and Ollama models. It implements RAG (Retrieval-Augmented Generation) in a minimalist way:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Index a folder of documents rlama rag llama3 project-docs ./documentation # Start an interactive session rlama run project-docs &amp;gt; How does the authentication module work? &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;You point the tool to a folder containing your files (.txt, .md, .pdf, source code, etc.)&lt;/li&gt; &lt;li&gt;RLAMA extracts text from the documents and generates embeddings via Ollama&lt;/li&gt; &lt;li&gt;When you ask a question, it retrieves relevant passages and sends them to the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The tool handles many formats automatically. For PDFs, it first tries pdftotext, then tesseract if necessary. For binary files, it has several fallback methods to extract what it can.&lt;/p&gt; &lt;h1&gt;Problems it solves&lt;/h1&gt; &lt;p&gt;I use it daily for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Finding information in old technical documents without having to reread everything&lt;/li&gt; &lt;li&gt;Exploring code I'm not familiar with (e.g., &amp;quot;explain how part X works&amp;quot;)&lt;/li&gt; &lt;li&gt;Creating summaries of long documents&lt;/li&gt; &lt;li&gt;Querying my research or meeting notes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real time-saver comes from being able to ask questions instead of searching for keywords. For example, I can ask &amp;quot;What are the possible errors in the authentication API?&amp;quot; and get consolidated answers from multiple files.&lt;/p&gt; &lt;h1&gt;Why use it?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It's simple&lt;/strong&gt;: four commands are enough (rag, run, list, delete)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's local&lt;/strong&gt;: no data is sent over the internet&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's lightweight&lt;/strong&gt;: no need for Docker or a complete stack&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It's flexible&lt;/strong&gt;: compatible with all Ollama models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I created it because other solutions were either too complex to configure or required sending my documents to external services.&lt;/p&gt; &lt;p&gt;If you already have Ollama installed and are looking for a simple way to query your documents, this might be useful for you.&lt;/p&gt; &lt;h1&gt;In conclusion&lt;/h1&gt; &lt;p&gt;I've found that in discussions on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; point to several pressing needs for local RAG without cloud dependencies: we need to simplify the ingestion of data (PDFs, web pages, videos...) via tools that can automatically transform them into usable text, reduce hardware requirements or better leverage common hardware (model quantization, multi-GPU support) to improve performance, and integrate advanced retrieval methods (hybrid search, rerankers, etc.) to increase answer reliability.&lt;/p&gt; &lt;p&gt;The emergence of integrated solutions (OpenWebUI, LangChain/Langroid, RAGStack, etc.) moves in this direction: the ultimate goal is a tool where users only need to provide their local files to benefit from an AI assistant trained on their own knowledge, while remaining 100% private and local so I wanted to develop something easy to use!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dontizi/rlama"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66oe7/rlama_a_simple_rag_interface_to_chat_with_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68ijn</id>
    <title>Local WebUI like Google AIStudio used to be like.</title>
    <updated>2025-03-08T03:48:47+00:00</updated>
    <author>
      <name>/u/NihilisticAssHat</name>
      <uri>https://old.reddit.com/user/NihilisticAssHat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone have a good suggestion for a WebUI or other GUI that resembles how aistudio.google.com used to be? They had a toggle to switch roles for each turn, and they still let you move/delete/edit any turn in history. This would be best if &lt;em&gt;continue responding&lt;/em&gt; were an option.&lt;/p&gt; &lt;p&gt;I tried Open WebUI and was rather let down. Half the time I'm inferencing through Jupyter Notebook anyway, but would like something that offers more user control over generation/history.&lt;/p&gt; &lt;p&gt;I like projects that get the fundamentals right over ones that boast too many features to maintain.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NihilisticAssHat"&gt; /u/NihilisticAssHat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68ijn/local_webui_like_google_aistudio_used_to_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j61cbx</id>
    <title>I just tried out a model and it blew me away: llama3.2 1b</title>
    <updated>2025-03-07T21:51:53+00:00</updated>
    <author>
      <name>/u/Firm_Newspaper3370</name>
      <uri>https://old.reddit.com/user/Firm_Newspaper3370</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thoroughly enjoying both llama3.3 70b and qwq 32b recently. Both are very impressive for their size, and these are the first models that a poor boy like me can run locally that I feel can give Chat got and Claude a run for their money.&lt;/p&gt; &lt;p&gt;But about an hour ago I had a thought.&lt;/p&gt; &lt;p&gt;If really good models are getting down to medium model sizes, how good are super tiny models getting?&lt;/p&gt; &lt;p&gt;So I downloaded llama3.3 1b.&lt;/p&gt; &lt;p&gt;Having run it through my normal phthon writing prompts, I am truly blown away. It's obviously not a 70b replacement, nor an 8b replacement. But I can hardly fathom that you can fit so much intelligence into 1.3gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firm_Newspaper3370"&gt; /u/Firm_Newspaper3370 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j61cbx/i_just_tried_out_a_model_and_it_blew_me_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6c556</id>
    <title>How to Summarize Long Documents on Mobile Devices with Hardware Constraints?</title>
    <updated>2025-03-08T07:38:53+00:00</updated>
    <author>
      <name>/u/Timely-Jackfruit8885</name>
      <uri>https://old.reddit.com/user/Timely-Jackfruit8885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm developing an AI-powered mobile app (&lt;a href="https://play.google.com/store/apps/details?id=com.DAI.DAIapp"&gt;https://play.google.com/store/apps/details?id=com.DAI.DAIapp&lt;/a&gt;)that needs to summarize long documents efficiently. The challenge is that I want to keep everything running locally, so I have to deal with hardware limitations (RAM, CPU, and storage constraints).&lt;/p&gt; &lt;p&gt;I’m currently using llama.cpp to run LLMs on-device and have integrated embeddings for semantic search. However, summarizing long documents is tricky due to context length limits and performance bottlenecks on mobile.&lt;/p&gt; &lt;p&gt;Has anyone tackled this problem before? Are there any optimized techniques, libraries, or models that work well on mobile hardware?&lt;/p&gt; &lt;p&gt;Any insights or recommendations would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely-Jackfruit8885"&gt; /u/Timely-Jackfruit8885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6c556/how_to_summarize_long_documents_on_mobile_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T07:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j69zl7</id>
    <title>Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp; news app.</title>
    <updated>2025-03-08T05:15:45+00:00</updated>
    <author>
      <name>/u/clockentyne</name>
      <uri>https://old.reddit.com/user/clockentyne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt; &lt;img alt="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." src="https://external-preview.redd.it/bzZmbWc3ZW9nZW5lMXX6-fvAK-sQelTG_mLXF8zDOxdsMWF5oRG9uoSpFfgL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3b18632d71a4317c761154b009ac0dbb3f2da7" title="Looking for beta testers for a local LLM, kokoro TTS supported, chat &amp;amp; news app." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clockentyne"&gt; /u/clockentyne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0wclz6eogene1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j69zl7/looking_for_beta_testers_for_a_local_llm_kokoro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5vjtr</id>
    <title>Cydonia 24B v2.1 - Bolder, better, brighter</title>
    <updated>2025-03-07T18:10:20+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt; &lt;img alt="Cydonia 24B v2.1 - Bolder, better, brighter" src="https://external-preview.redd.it/Y53sGNZqFT7O2LPxR0ifOBO74G3g4p3oD2d89H5ZiiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f99bb247d7af48f7df7184bd4ce33a8e1090f74f" title="Cydonia 24B v2.1 - Bolder, better, brighter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v2.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T18:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5s629</id>
    <title>The Genius of DeepSeek’s 57X Efficiency Boost [MLA]</title>
    <updated>2025-03-07T16:12:48+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt; &lt;img alt="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" src="https://external-preview.redd.it/cNA68CfuuchA-pKjC19aWS3fLxcXM9n1EmEaCjksBPI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6d61fc0d91e37ef5748cd5797b5f122d3ab207" title="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0VLAoVGf_74?si=OSwMMKsz9EpLOISJ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:12:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60bc4</id>
    <title>AMD &amp; tinygrad cooperation happening</title>
    <updated>2025-03-07T21:08:39+00:00</updated>
    <author>
      <name>/u/Danmoreng</name>
      <uri>https://old.reddit.com/user/Danmoreng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt; &lt;img alt="AMD &amp;amp; tinygrad cooperation happening" src="https://external-preview.redd.it/BbTfUmQNA0TFAIFilHM3Y4In9mUXZYXfhSxTgqgwTN8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b210445be3d20e6a3900593f94809ea783979b0" title="AMD &amp;amp; tinygrad cooperation happening" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danmoreng"&gt; /u/Danmoreng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AnushElangovan/status/1898101178728431637"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j681n3</id>
    <title>Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark</title>
    <updated>2025-03-08T03:22:26+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt; &lt;img alt="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" src="https://preview.redd.it/izw2ej1cwdne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73dec38ebaddd2720f9cf241a4ae7cf9de6d8481" title="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izw2ej1cwdne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j66mpo</id>
    <title>Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark</title>
    <updated>2025-03-08T02:05:50+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt; &lt;img alt="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" src="https://preview.redd.it/ig84dy8oidne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1ee2b26596ddc8e881d50f0eb5e76449003b478" title="Qwen QwQ slots between Claude 3.7 Sonnet Thinking and o1-mini on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ig84dy8oidne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j66mpo/qwen_qwq_slots_between_claude_37_sonnet_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wzea</id>
    <title>New AMD Driver Yields Up To 11% Performance Increase In koboldcpp</title>
    <updated>2025-03-07T19:02:32+00:00</updated>
    <author>
      <name>/u/WokeCapitalist</name>
      <uri>https://old.reddit.com/user/WokeCapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7900-xt.html"&gt;AMD's Adrenalin 25.3.1 driver&lt;/a&gt; release mentioned &lt;strong&gt;&lt;em&gt;&amp;quot;AI Performance Improvements on AMD Radeon™ RX 7000 Series&amp;quot;&lt;/em&gt;&lt;/strong&gt; in the release notes along with some large percentage increases for applications like Adobe Lightroom Denoise or DaVinci Resolve. As I had their previous WHQL recommended driver already installed, I decided to test it out in koboldcpp. It turns out there was a nice performance bump there, too. Worth a download if you haven't done so already!&lt;/p&gt; &lt;h1&gt;Hardware Test Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Microsoft Windows 11 Professional (x64) Build 26100.3194 (24H2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core Ultra 7 265K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 7900 XT (20GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Z890I Nova WiFi (BIOS 2.22)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk:&lt;/strong&gt; Lexar SSD NM800PRO 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 64GB (2×32GB DDR5 6400 CL32)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Boot, BitLocker &amp;amp; HVCI Enabled&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software Test Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; koboldcpp 1.83.1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; phi-4-q4,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 16384&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BLAS Batch Size&lt;/strong&gt;: 512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Layers&lt;/strong&gt;: 43/43&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Backend: Vulkan&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;30.312 s&lt;/td&gt; &lt;td align="left"&gt;27.607 s&lt;/td&gt; &lt;td align="left"&gt;8.95% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;537.21 T/s&lt;/td&gt; &lt;td align="left"&gt;589.85 T/s&lt;/td&gt; &lt;td align="left"&gt;9.84% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.203 s&lt;/td&gt; &lt;td align="left"&gt;5.301 s&lt;/td&gt; &lt;td align="left"&gt;1.87% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;19.22 T/s&lt;/td&gt; &lt;td align="left"&gt;18.86 T/s&lt;/td&gt; &lt;td align="left"&gt;1.88% lower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;35.515 s&lt;/td&gt; &lt;td align="left"&gt;32.908 s&lt;/td&gt; &lt;td align="left"&gt;7.35% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Backend: ROCm&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;24.861 s&lt;/td&gt; &lt;td align="left"&gt;22.370 s&lt;/td&gt; &lt;td align="left"&gt;10.06% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;655.00 T/s&lt;/td&gt; &lt;td align="left"&gt;727.94 T/s&lt;/td&gt; &lt;td align="left"&gt;11.15% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.831 s&lt;/td&gt; &lt;td align="left"&gt;5.586 s&lt;/td&gt; &lt;td align="left"&gt;4.20% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;17.15 T/s&lt;/td&gt; &lt;td align="left"&gt;17.90 T/s&lt;/td&gt; &lt;td align="left"&gt;4.32% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;30.692 s&lt;/td&gt; &lt;td align="left"&gt;27.956 s&lt;/td&gt; &lt;td align="left"&gt;8.97% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WokeCapitalist"&gt; /u/WokeCapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60wt1</id>
    <title>NVIDIA RTX "PRO" 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W</title>
    <updated>2025-03-07T21:34:40+00:00</updated>
    <author>
      <name>/u/newdoria88</name>
      <uri>https://old.reddit.com/user/newdoria88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt; &lt;img alt="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" src="https://external-preview.redd.it/8nLxMIJQrz_2tTIhvuryMxrtnbjPwWSOP7OO-C_HgM0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8c481144f93be83af79ca767c97e43a6750c4ac" title="NVIDIA RTX &amp;quot;PRO&amp;quot; 6000 X Blackwell GPU Spotted In Shipping Log: GB202 Die, 96 GB VRAM, TBP of 600W" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newdoria88"&gt; /u/newdoria88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-x-blackwell-leak-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60wt1/nvidia_rtx_pro_6000_x_blackwell_gpu_spotted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:34:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
</feed>
