<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-27T07:06:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ial3b0</id>
    <title>Baichuan-M1-14B</title>
    <updated>2025-01-26T17:51:15+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ial3b0/baichuanm114b/"&gt; &lt;img alt="Baichuan-M1-14B" src="https://external-preview.redd.it/9z_rw4aNOpBp5h5qe4HobNopHvgm6qQlFXOZD7DrGrU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1060b648aa37d19edc0f1cb3a5bd155fb0914204" title="Baichuan-M1-14B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ayy51uqhkdfe1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=49b4d4163de49b935afd6930ea85e5a5b992a8e8"&gt;https://preview.redd.it/ayy51uqhkdfe1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=49b4d4163de49b935afd6930ea85e5a5b992a8e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9mq0x7ejkdfe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=103d67f54466ce6c5687652703ce8de95797f16a"&gt;https://preview.redd.it/9mq0x7ejkdfe1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=103d67f54466ce6c5687652703ce8de95797f16a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/12utc7zoldfe1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36392fb390e0d2cb6b8c69bb3450643c5170e1ee"&gt;https://preview.redd.it/12utc7zoldfe1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36392fb390e0d2cb6b8c69bb3450643c5170e1ee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fk6r9rd6mdfe1.jpg?width=1330&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7e8f157bd4bb036e2133adc5a65ff08b2171a69e"&gt;https://preview.redd.it/fk6r9rd6mdfe1.jpg?width=1330&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7e8f157bd4bb036e2133adc5a65ff08b2171a69e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baichuan-14B-M1 is the industry's first open-source large language model developed from scratch by Baichuan Intelligence, specifically optimized for medical scenarios. While excelling in general capabilities, it demonstrates powerful performance in the medical field. It achieves results comparable to models of similar size in most general benchmark evaluations, while outperforming models five times larger in medical scenarios. Below are the core features of the model:&lt;/p&gt; &lt;p&gt;Trained from scratch on 20 trillion tokens of high-quality medical and general data. Specialized modeling for 20+ medical departments with fine-grained medical expertise. Introduces innovative model architecture, significantly improving context understanding and long-sequence task performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-Omni-1d5"&gt;Model Link (Base)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M1-14B-Instruct"&gt;Model link (Instruct)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ial3b0/baichuanm114b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ial3b0/baichuanm114b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ial3b0/baichuanm114b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaciu9</id>
    <title>Qwen 2.5 VL Release Imminent?</title>
    <updated>2025-01-26T11:45:21+00:00</updated>
    <author>
      <name>/u/iKy1e</name>
      <uri>https://old.reddit.com/user/iKy1e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've just created the collection for it on Hugging Face &amp;quot;updated about 2 hours ago&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;h1&gt;Qwen2.5-VL&lt;/h1&gt; &lt;p&gt;Vision-language model series based on Qwen2.5 &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKy1e"&gt; /u/iKy1e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T11:45:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia7v0x</id>
    <title>the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp.</title>
    <updated>2025-01-26T06:34:30+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt; &lt;img alt="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." src="https://external-preview.redd.it/GkVEj8SEXSUPkh7D3z-zTGfIOGq41yOTpDWE4Fj1WE4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01e2d3507d8b0dc551310e81cc376bab8fb4fc91" title="the MNN team at Alibaba has open-sourced multimodal Android app running without netowrk that supports: Audio , Image and Diffusion Models. with blazing-fast speeds on cpu with 2.3x faster decoding speeds compared to llama.cpp." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;app maim page: &lt;a href="https://github.com/alibaba/MNN"&gt;MNN-LLM-APP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xo6fjer8afe1.png?width=1780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2da05212d5e1af8855cedc2a23a8166e4a5340dc"&gt;the mulitimodal app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;inference speed vs llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/elrqgjh59afe1.gif"&gt;https://i.redd.it/elrqgjh59afe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia7v0x/the_mnn_team_at_alibaba_has_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T06:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iakpn7</id>
    <title>Baichuan-Omni-1.5</title>
    <updated>2025-01-26T17:37:02+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakpn7/baichuanomni15/"&gt; &lt;img alt="Baichuan-Omni-1.5" src="https://external-preview.redd.it/9z_rw4aNOpBp5h5qe4HobNopHvgm6qQlFXOZD7DrGrU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1060b648aa37d19edc0f1cb3a5bd155fb0914204" title="Baichuan-Omni-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2xpfatehjdfe1.png?width=2410&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f21159597fd67e87756a449ec36be14cf68e99fe"&gt;https://preview.redd.it/2xpfatehjdfe1.png?width=2410&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f21159597fd67e87756a449ec36be14cf68e99fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/442ygmbkjdfe1.png?width=2860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3bb1f177314037fd647f79c3568cb8d93cf22c1c"&gt;https://preview.redd.it/442ygmbkjdfe1.png?width=2860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3bb1f177314037fd647f79c3568cb8d93cf22c1c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Baichuan-Omni-1.5 is the latest, top-performing model in the Baichuan-omni series. This model is trained and inferred in an end-to-end manner. Compared with Baichuan-omni, this model has significant improvements in text/image/audio/video understanding and text/audio generation, and supports new features such as controllable real-time voice conversations and multi-modal real-time interactions. The main features of Baichuan-Omni-1.5 include:&lt;/p&gt; &lt;p&gt;🔥 Possess Multimodal Understanding and Interaction Capabilities. Baichuan-Omni-1.5 not only supports images, videos, text, and audio as input, and generates high-quality text and voice output, but also supports continuous video and audio streaming, and real-time voice interaction with users. In OminiBench, a comprehensive evaluation benchmark for omnimodal understanding, Baichuan-Omni-1.5 has achieved the first-class level of the open source community and surpassed GPT-4o-mini.&lt;/p&gt; &lt;p&gt;💪 Strong Visual Capability. Baichuan-Omni-1.5 has an average score of 73.3 on the OpenCompass list (comprehensive 10 mainstream multimodal evaluation benchmarks). With the size of 7B, it surpasses mainstream commercial closed-source multimodal large models such as GPT-4o-mini, Gemini 1.5 Pro and Claude 3.5 Sonnet in single-image understanding. In addition, its video understanding performance is also better than GPT-4V and Claude 3.5 Sonnet and open source omnimodal models.&lt;/p&gt; &lt;p&gt;🚀 Leading Medical Image Understanding Capabilities. Baichuan-Omni-1.5 achieved the best performance on GMAI-MMBench and Openmm-Medical. Using only 7B LLM, the average score exceeded Qwen2-VL-72b by 3%, i.e. 80.7% v.s 83.8%.&lt;/p&gt; &lt;p&gt;🎙 Excellent Voice Capabilities. Baichuan-Omni-1.5 supports high-quality, controllable voice bilingual real-time conversations in Chinese and English. It outperforms GPT-4o-realtime in speech understanding tasks (such as ASR and STT, etc.), and demonstrates the highest speech generation performance among open source models in semantic and acoustic evaluation of voice conversations.&lt;/p&gt; &lt;p&gt;🎬 Powerful Real-world Understanding and Other Features. Baichuan-Omni-1.5 further optimizes the many visual understanding capabilities of Baichuan-omni. It can process images of any aspect ratio and up to 1.8 million pixels (such as 1344x1344). It scored 68.8 points on RealWorldQA, surpassing commercial closed-source models such as GPT-4o-mini and recently open-sourced omnimodal models. It scored 85.6/83.6 on the English/Chinese evaluation subsets of MMBench, respectively, which is also in the first echelon of models with the same size.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-Omni-1d5"&gt;Model Link &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakpn7/baichuanomni15/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakpn7/baichuanomni15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iakpn7/baichuanomni15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaws33</id>
    <title>The Yellow Swan? Is it popping?</title>
    <updated>2025-01-27T01:54:52+00:00</updated>
    <author>
      <name>/u/Neat-Computer-6975</name>
      <uri>https://old.reddit.com/user/Neat-Computer-6975</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaws33/the_yellow_swan_is_it_popping/"&gt; &lt;img alt="The Yellow Swan? Is it popping?" src="https://b.thumbs.redditmedia.com/X5DQ9WTSf20TIGi0i-03bfkmL1muST1vwKSv7_F3ktA.jpg" title="The Yellow Swan? Is it popping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia opens down more than -5% in overnight trading in its first reaction to DeepSeek. The biggest market headwind just came out of nowhere.&lt;/p&gt; &lt;p&gt;Nasdaq 100 futures are now down -330 POINTS since the market opened just hours ago as DeepSeek takes #1 on the App Store.&lt;/p&gt; &lt;p&gt;🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat-Computer-6975"&gt; /u/Neat-Computer-6975 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iaws33"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaws33/the_yellow_swan_is_it_popping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaws33/the_yellow_swan_is_it_popping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T01:54:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaizyk</id>
    <title>Qwen 2.5 VL incoming</title>
    <updated>2025-01-26T16:32:42+00:00</updated>
    <author>
      <name>/u/Either-Job-341</name>
      <uri>https://old.reddit.com/user/Either-Job-341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 2 VL 7B and 72B are remarkable video models and this new series is expected to be even better.&lt;/p&gt; &lt;p&gt;Are you ready? ARE. YOU. READY?&lt;/p&gt; &lt;p&gt;Chinese labs are killing it and they sure know how to ride a wave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either-Job-341"&gt; /u/Either-Job-341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizyk/qwen_25_vl_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iasqcp</id>
    <title>Qwen2.5-VL - a Qwen Collection</title>
    <updated>2025-01-26T22:42:03+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasqcp/qwen25vl_a_qwen_collection/"&gt; &lt;img alt="Qwen2.5-VL - a Qwen Collection" src="https://external-preview.redd.it/cFJ02ezS0eEOVCS-1VgOowXrZBZl2WdNmkuRjBjf-7E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bde9154160167054944f0e88f1dfe291fd458aa0" title="Qwen2.5-VL - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 2.5 VL incoming.&lt;/p&gt; &lt;p&gt;They also released qwen 2.5 14b and 7b a million context !!: &lt;a href="https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba"&gt;https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasqcp/qwen25vl_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iasqcp/qwen25vl_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T22:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib19ll</id>
    <title>SotA TTS/STT, but for accuracy and not speed.</title>
    <updated>2025-01-27T05:57:28+00:00</updated>
    <author>
      <name>/u/vardonir</name>
      <uri>https://old.reddit.com/user/vardonir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of the models and packages I find are intended for speed, live-captioning and so on, but I don't really care about those. I need one that supports multilingual English/Hebrew + translate. I have a 3090Ti so I don't think I'll need optimization, either. &lt;/p&gt; &lt;p&gt;So far, I've been using OpenAI's whisper - it's fine, but I feel like there's something better out there. I found one Hebrew finetune but it doesn't seem to translate to English.&lt;/p&gt; &lt;p&gt;Further questions: Are there ways to run the inference multiple times to get better transcriptions? Or start off with a prompt saying &amp;quot;this is an audio file of a physics lecture&amp;quot; and then it'll transcribe/translate based on that context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vardonir"&gt; /u/vardonir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib19ll/sota_ttsstt_but_for_accuracy_and_not_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib19ll/sota_ttsstt_but_for_accuracy_and_not_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib19ll/sota_ttsstt_but_for_accuracy_and_not_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T05:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1iak7td</id>
    <title>Meet Qwen2.5-7B-Instruct-1M &amp; Qwen2.5-14B-Instruct-1M</title>
    <updated>2025-01-26T17:18:26+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1883557964759654608"&gt;https://x.com/Alibaba_Qwen/status/1883557964759654608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're leveling up the game with our latest open-source models, Qwen2.5-1M ! Now supporting a 1 MILLION TOKEN CONTEXT LENGTH &lt;/p&gt; &lt;p&gt;Here's what’s new: &lt;/p&gt; &lt;p&gt;Open Models: Meet Qwen2.5-7B-Instruct-1M &amp;amp; Qwen2.5-14B-Instruct-1M —our first-ever models handling 1M-token contexts! &lt;/p&gt; &lt;p&gt;Lightning-Fast Inference Framework: We’ve fully open-sourced our inference framework based on vLLM , integrated with sparse attention methods. Experience 3x to 7x faster processing for 1M-token inputs! &lt;/p&gt; &lt;p&gt;Tech Deep Dive: Check out our detailed Technical Report for all the juicy details behind the Qwen2.5-1M series! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iakhai</id>
    <title>Confucius-o1-14B</title>
    <updated>2025-01-26T17:28:14+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt; &lt;img alt="Confucius-o1-14B" src="https://external-preview.redd.it/CtcKiRELpTFksl1HeUnxFZoytd4EOkb5O7UUupfTGWI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf53452cf6e0846b5bffa7ef2c25002c5c625413" title="Confucius-o1-14B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yus83af2idfe1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfafdfe1b9f5e888515b26b26df2c905b12fabd5"&gt;https://preview.redd.it/yus83af2idfe1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfafdfe1b9f5e888515b26b26df2c905b12fabd5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Confucius-o1-14B is a o1-like reasoning model developed by the NetEase Youdao Team, it can be easily deployed on a single GPU without quantization. This model is based on the Qwen2.5-14B-Instruct model and adopts a two-stage learning strategy, enabling the lightweight 14B model to possess thinking abilities similar to those of o1. What sets it apart is that after generating the chain of thought, it can summarize a step-by-step problem-solving process from the chain of thought on its own. This can prevent users from getting bogged down in the complex chain of thought and allows them to easily obtain the correct problem-solving ideas and answers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/netease-youdao/Confucius-o1-14B"&gt;Model Link &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://confucius-o1-demo.youdao.com/"&gt;Demo &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaj0da</id>
    <title>AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)</title>
    <updated>2025-01-26T16:33:07+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt; &lt;img alt="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" src="https://preview.redd.it/x4xd7d7a8dfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d010b3576261b46f6f12443a4fd4ccbf4a63bf2d" title="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So TUS is a really hard medical specialization exam consisting of two parts (each part 100 questions, so 200 in total). Never has a person answered all the questions correctly in its history. Doctors in Turkey must pass this exam to begin their desired residency in a hospital.&lt;/p&gt; &lt;p&gt;Credit: Ahmet Ay, founder of TUSBuddy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x4xd7d7a8dfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaufzj</id>
    <title>Is Deepseek R1 on Groq will make it think faster?</title>
    <updated>2025-01-27T00:00:20+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"&gt; &lt;img alt="Is Deepseek R1 on Groq will make it think faster?" src="https://preview.redd.it/fwq4uvn2gffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d7b8df15c914a6161d3fa722a23906a8470e1b9" title="Is Deepseek R1 on Groq will make it think faster?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fwq4uvn2gffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T00:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9iy1</id>
    <title>DeepSeekR1 3D game 100% from scratch</title>
    <updated>2025-01-26T08:36:26+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt; &lt;img alt="DeepSeekR1 3D game 100% from scratch" src="https://preview.redd.it/qrdlt6i8vafe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8d13a97797fa31e558155d2f6738fd891080c24b" title="DeepSeekR1 3D game 100% from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked DeepSeek R1 to make me a game like kkrieger ( where most of the things are generated on run ) and it made me this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrdlt6i8vafe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iazi5b</id>
    <title>@emostaque : The future is local inference</title>
    <updated>2025-01-27T04:12:37+00:00</updated>
    <author>
      <name>/u/MrWidmoreHK</name>
      <uri>https://old.reddit.com/user/MrWidmoreHK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"&gt; &lt;img alt="@emostaque : The future is local inference" src="https://preview.redd.it/56cwsgs2pgfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68ee5a33606cf09deb184841e83242faf9fb6f01" title="@emostaque : The future is local inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWidmoreHK"&gt; /u/MrWidmoreHK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56cwsgs2pgfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T04:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iay2ik</id>
    <title>Wholesome interaction with deepseek v3</title>
    <updated>2025-01-27T03:01:55+00:00</updated>
    <author>
      <name>/u/ParadiseMaker69</name>
      <uri>https://old.reddit.com/user/ParadiseMaker69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"&gt; &lt;img alt="Wholesome interaction with deepseek v3" src="https://b.thumbs.redditmedia.com/0t-ogYx1QGnshPztysHW1iUP_uuqACStRsV6ljgQECo.jpg" title="Wholesome interaction with deepseek v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParadiseMaker69"&gt; /u/ParadiseMaker69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iay2ik"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaizfb</id>
    <title>Qwen2.5-1M Release on HuggingFace - The long-context version of Qwen2.5, supporting 1M-token context lengths!</title>
    <updated>2025-01-26T16:32:10+00:00</updated>
    <author>
      <name>/u/Silentoplayz</name>
      <uri>https://old.reddit.com/user/Silentoplayz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing to be the first to do it here.&lt;/p&gt; &lt;blockquote&gt; &lt;h1&gt;Qwen2.5-1M&lt;/h1&gt; &lt;p&gt;The long-context version of Qwen2.5, supporting 1M-token context lengths&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba"&gt;https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Related &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; post by another fellow regarding &amp;quot;Qwen 2.5 VL&amp;quot; models - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Edit:&lt;/h1&gt; &lt;p&gt;Blogpost: &lt;a href="https://qwenlm.github.io/blog/qwen2.5-1m/"&gt;https://qwenlm.github.io/blog/qwen2.5-1m/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf"&gt;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you &lt;a href="/u/Balance-"&gt;u/Balance-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silentoplayz"&gt; /u/Silentoplayz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:32:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ffq</id>
    <title>From this week's The Economist: "China’s AI industry has almost caught up with America’s"</title>
    <updated>2025-01-27T05:04:42+00:00</updated>
    <author>
      <name>/u/comfyui_user_999</name>
      <uri>https://old.reddit.com/user/comfyui_user_999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt; &lt;img alt="From this week's The Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" src="https://external-preview.redd.it/C2yiTG1ri5iDyKLc-Vn_3V_ISkOymhbpYsu1RacQ-tE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1461e932e29591186828398f4de9362c5ad8106" title="From this week's The Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/comfyui_user_999"&gt; /u/comfyui_user_999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.economist.com/briefing/2025/01/23/chinas-ai-industry-has-almost-caught-up-with-americas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iayi0m</id>
    <title>I miss the days when ClosedAI was OpenAI</title>
    <updated>2025-01-27T03:23:39+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since OpenAI became ClosedAI, they seem to have lost their innovativeness, under the delusion that they have created a moat that others cannot cross.&lt;/p&gt; &lt;p&gt;Maybe if they had continued to be OpenAI we would be seeing open source gpt5 and o5 by now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaubfm</id>
    <title>Someone needs to create a "Can You Run It?" tool for open-source LLMs</title>
    <updated>2025-01-26T23:54:25+00:00</updated>
    <author>
      <name>/u/oromissed</name>
      <uri>https://old.reddit.com/user/oromissed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-techie here! I’ve been itching to experiment with open-source LLMs (like Deepseek, LLaMA, Mistral, etc.), but every time I try, I hit the same wall: Will this model even run on my potato PC&lt;em&gt;?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Most guides assume you’re fluent in CUDA cores, VRAM, and quantization. Meanwhile, I’m just sitting here with my 8GB RAM laptop like 🥔.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We need a &amp;quot;Can You Run It?&amp;quot; equivalent for LLMs&lt;/strong&gt; — something like the &lt;a href="https://www.systemrequirementslab.com/cyri"&gt;System Requirements Lab&lt;/a&gt; tool for games. Imagine:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Select a model (e.g., &amp;quot;Llama3-8B&amp;quot; or &amp;quot;DeepSeek-R1&amp;quot;)&lt;/li&gt; &lt;li&gt;Upload your specs (CPU, RAM, GPU)&lt;/li&gt; &lt;li&gt;Get a simple ✅/❌ verdict: &lt;ul&gt; &lt;li&gt;&amp;quot;Yes, but expect 3 words per minute&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;No, your GPU will cry&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Try this quantized version instead&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Bonus points if it suggests optimizations (like Ollama flags or GGUF versions) for weaker hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oromissed"&gt; /u/oromissed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T23:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iarkra</id>
    <title>Major changes are coming this year. Buckle up.</title>
    <updated>2025-01-26T21:51:49+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If OpenAI can no longer demonstrate a significant lead over competitors in model development, securing necessary funding will become challenging. Investors are noting increased risk due to innovations from China, while OpenAI has lost several key researchers in recent months.&lt;/p&gt; &lt;p&gt;OpenAI faces mounting pressure. Sora's reception was underwhelming, DALL-E remains without updates, and their voice models lag behind ElevenLabs. Gemini offers competitive models at lower prices, while DeepSeek's pricing is highly competitive, and Open Source, including significant advances unique in the industry that optimize inference and improve results. Claude is better at coding, not to mention competition from LLama, and Elon gigantic compute farm. Further, Open Source Agentic models are coming that again push what people can do with an LLM.&lt;/p&gt; &lt;p&gt;o3 appears reactive to competitors' innovations, emerging after Anthropic demonstrated similar capabilities. OpenAI's position is precarious as competition intensifies rapidly. o3 is crucial for their future - if it shows only minimal improvements, investor funding will come at a premium, all while they attempt to transition to a for-profit model under scrutiny.&lt;/p&gt; &lt;p&gt;Major changes are coming this year. Buckle up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T21:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaebwp</id>
    <title>Financial Times: "DeepSeek shocked Silicon Valley"</title>
    <updated>2025-01-26T13:19:03+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;recent article&lt;/a&gt; in Financial Times says that US sanctions forced the AI companies in China to be more innovative &amp;quot;to maximise the computing power of a limited number of onshore chips&amp;quot;.&lt;/p&gt; &lt;p&gt;Most interesting to me was the claim that &amp;quot;DeepSeek’s singular focus on research makes it a dangerous competitor because it is willing to share its breakthroughs rather than protect them for commercial gains.&amp;quot;&lt;/p&gt; &lt;p&gt;What an Orwellian doublespeak! China, a supposedly closed country, leads the AI innovation and is willing to share its breakthroughs. And this makes them dangerous for ostensibly open countries where companies call themselves OpenAI but relentlessly hide information.&lt;/p&gt; &lt;p&gt;Here is the full link: &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;https://archive.md/b0M8i#selection-2491.0-2491.187&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T13:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawl12</id>
    <title>Byee</title>
    <updated>2025-01-27T01:44:42+00:00</updated>
    <author>
      <name>/u/amirulnaim2000</name>
      <uri>https://old.reddit.com/user/amirulnaim2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"&gt; &lt;img alt="Byee" src="https://preview.redd.it/jauvd8soyffe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f98d8a0c2447bbee55852101602ad1ba740a3960" title="Byee" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amirulnaim2000"&gt; /u/amirulnaim2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jauvd8soyffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T01:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqajh</id>
    <title>deepseek is a side project pt. 2</title>
    <updated>2025-01-26T21:02:46+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt; &lt;img alt="deepseek is a side project pt. 2" src="https://preview.redd.it/bawhrb3ekefe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e236c7b8478b2b0b98ff8cb74b60fac011ead97e" title="deepseek is a side project pt. 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bawhrb3ekefe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T21:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaz2or</id>
    <title>Deepseek like a boss</title>
    <updated>2025-01-27T03:52:40+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"&gt; &lt;img alt="Deepseek like a boss" src="https://preview.redd.it/d6slqdvilgfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b3de28e4d080e64aa2341a87efd989cd1ede176" title="Deepseek like a boss" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d6slqdvilgfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:52:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iasyc3</id>
    <title>Deepseek is #1 on the U.S. App Store</title>
    <updated>2025-01-26T22:52:07+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt; &lt;img alt="Deepseek is #1 on the U.S. App Store" src="https://preview.redd.it/sr4kvvnv3ffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a82ab88b43a6f7f3f1aa6d284ecb8edff2e4630" title="Deepseek is #1 on the U.S. App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr4kvvnv3ffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T22:52:07+00:00</published>
  </entry>
</feed>
