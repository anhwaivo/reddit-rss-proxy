<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-08T07:34:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hw405w</id>
    <title>Ray-Ban Meta Glasses</title>
    <updated>2025-01-07T22:29:46+00:00</updated>
    <author>
      <name>/u/Alarmed-Instance5356</name>
      <uri>https://old.reddit.com/user/Alarmed-Instance5356</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blind user here that wants to understand the technology behind the glasses.&lt;/p&gt; &lt;p&gt;1 - Is this how it works: Ray-Ban Meta is the microphone, data processed in Meta View app, then uploaded to a meta server running llama, last is output is downloaded and sent to the glasses? 2 - Will Meta update the version of llama that underpins the glasses? Currently the glasses say that theyâ€™re llama 3.1, but latest version of llama is 3.3. 3 - If I understand the process correctly in that the glasses merely talk to a meta server running llama, then does this mean that the glasses will give better results every quarter that llama is updated with more training data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarmed-Instance5356"&gt; /u/Alarmed-Instance5356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw405w/rayban_meta_glasses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw405w/rayban_meta_glasses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw405w/rayban_meta_glasses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:29:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvjgqs</id>
    <title>New open nemotron models from Nvidia are on the way</title>
    <updated>2025-01-07T04:34:18+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjgqs/new_open_nemotron_models_from_nvidia_are_on_the/"&gt; &lt;img alt="New open nemotron models from Nvidia are on the way" src="https://preview.redd.it/n0ywmuxk2ibe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45c4af991136631a8175d9b751c55d7aae9c806e" title="New open nemotron models from Nvidia are on the way" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n0ywmuxk2ibe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjgqs/new_open_nemotron_models_from_nvidia_are_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjgqs/new_open_nemotron_models_from_nvidia_are_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvklij</id>
    <title>Is NVIDIA charging people 3k+ for a 5070 with 128 gigs of VRAM?</title>
    <updated>2025-01-07T05:40:21+00:00</updated>
    <author>
      <name>/u/Ill_Distribution8517</name>
      <uri>https://old.reddit.com/user/Ill_Distribution8517</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't think it's a bad deal personally, but are there any alternatives?&lt;/p&gt; &lt;p&gt;Is the unified memory hbm? Or something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Distribution8517"&gt; /u/Ill_Distribution8517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvklij/is_nvidia_charging_people_3k_for_a_5070_with_128/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvklij/is_nvidia_charging_people_3k_for_a_5070_with_128/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvklij/is_nvidia_charging_people_3k_for_a_5070_with_128/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T05:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj0y2</id>
    <title>Oh shit</title>
    <updated>2025-01-07T04:10:02+00:00</updated>
    <author>
      <name>/u/Lammahamma</name>
      <uri>https://old.reddit.com/user/Lammahamma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj0y2/oh_shit/"&gt; &lt;img alt="Oh shit" src="https://preview.redd.it/tzbjbr4eyhbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15d94adfb721bd99f043db61ba4eb704519fff8f" title="Oh shit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lammahamma"&gt; /u/Lammahamma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tzbjbr4eyhbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj0y2/oh_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj0y2/oh_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvlbow</id>
    <title>To understand the Project DIGITS desktop (128 GB for 3k), look at the existing Grace CPU systems</title>
    <updated>2025-01-07T06:26:47+00:00</updated>
    <author>
      <name>/u/programmerChilli</name>
      <uri>https://old.reddit.com/user/programmerChilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There seems to be a lot of confusion about how Nvidia could be selling their 5090 with 32GB of VRAM, but their Project Digits desktop has 128 GB of VRAM.&lt;/p&gt; &lt;p&gt;Typical desktop GPUs have GDDR which is faster, and server GPUs have HBM which is even faster than that, but the Grace CPUs use LPDDR (&lt;a href="https://www.nvidia.com/en-us/data-center/grace-cpu/"&gt;https://www.nvidia.com/en-us/data-center/grace-cpu/&lt;/a&gt;), which is generally cheaper but slower.&lt;/p&gt; &lt;p&gt;For example, the H200 GPU by itself only has 96/144GB of HBM, but the Grace-Hopper Superchip (GH200) adds in an additional 480 GB of LPDDR.&lt;/p&gt; &lt;p&gt;The memory bandwidth to this LPDDR from the GPU is also quite fast! For example, the GH200 HBM bandwidth is 4.9 TB/s, but the memory bandwidth from the CPU to the GPU and from the RAM to the CPU are both around 500 GB/s still.&lt;/p&gt; &lt;p&gt;It's a bit harder to predict what's going on with the GB10 Superchip in Project Digits, since unlike the GH200 superchips it doesn't have any HBM (and it only has 20 cores). But if you look at the Grace CPU C1 chip (&lt;a href="https://resources.nvidia.com/en-us-grace-cpu/data-center-datasheet?ncid=no-ncid"&gt;https://resources.nvidia.com/en-us-grace-cpu/data-center-datasheet?ncid=no-ncid&lt;/a&gt;), there's a configuration with 120 GB of LPDDR RAM + 512 GB/s of memory bandwidth. And the NVLink C2C bandwidth has a 450GB/s unidirectional bandwidth to the GPU.&lt;/p&gt; &lt;p&gt;TL;DR: Pure speculation, but it's possible that the Project Digits desktop will come in at around 500 GB/s memory-bandwidth, which would be quite good! Good for ~7 tok/s for Llama-70B at 8-bits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/programmerChilli"&gt; /u/programmerChilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T06:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw6sa0</id>
    <title>Llama 4 compute estimates &amp; timeline</title>
    <updated>2025-01-08T00:33:59+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From some quick searching, Llama 4 was already training as early as october 28th. Since they have 100k H100s, and use 10x more compute than llama 3 (which was ~8MM hours from what I could find), but even 100MM gpu hours on 100k gpus is ~1.4 months. &lt;/p&gt; &lt;p&gt;Unless I am completely out of the ballpark, shouldn't they have finished pre-training by now? Perhaps at the fine-tuning stage? &lt;/p&gt; &lt;p&gt;What about deepseek, if meta takes anything for inspiration it should be their $5.4MM budget and what they did with it. I'm really hopeful for what meta can do with their budget if they take a similar approach, especially considering theyre (again, hopefully) training native multimodal llama 4. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T00:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvzswb</id>
    <title>distillKitPlus: Compute Efficient Knowledge Distillation for LLMs</title>
    <updated>2025-01-07T19:34:50+00:00</updated>
    <author>
      <name>/u/__XploR__</name>
      <uri>https://old.reddit.com/user/__XploR__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Larger LLMs generalize better and faster, this is a great way to leverage and then transfer the best of 70B model to a 7B model without breaking the bank or sacrificing performance.&lt;/p&gt; &lt;p&gt;GitHub Link: &lt;a href="https://github.com/agokrani/distillkitplus"&gt;https://github.com/agokrani/distillkitplus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__XploR__"&gt; /u/__XploR__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T19:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvre7f</id>
    <title>"Contemplative reasoning" response style for LLMs (Best prompting advice I've used so far)</title>
    <updated>2025-01-07T13:27:26+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instruct LLM to contemplate before giving an answer and see the thought process. &lt;a href="https://gist.github.com/Maharshi-Pandya/4aeccbe1dbaa7f89c182bd65d2764203"&gt;Here is the prompt.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/mrsiipa/status/1876253176963493889"&gt;Here is the source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've tried it a few times and I find it quite impressive for how much it can squeeze from non-reasoning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T13:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvhsm7</id>
    <title>RTX 5090 Blackwell - Official Price</title>
    <updated>2025-01-07T03:05:53+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"&gt; &lt;img alt="RTX 5090 Blackwell - Official Price" src="https://preview.redd.it/j7pyht4vmhbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a121999833decd5c9492e83335bd6f94a87ba952" title="RTX 5090 Blackwell - Official Price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7pyht4vmhbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T03:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvjnar</id>
    <title>NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading</title>
    <updated>2025-01-07T04:44:52+00:00</updated>
    <author>
      <name>/u/The-Communist-Cat</name>
      <uri>https://old.reddit.com/user/The-Communist-Cat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt; &lt;img alt="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" src="https://preview.redd.it/aj6qbvpl4ibe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cef0e16078387e249b6036bc1e225dd5ecd0d791" title="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Communist-Cat"&gt; /u/The-Communist-Cat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aj6qbvpl4ibe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw0itx</id>
    <title>Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley</title>
    <updated>2025-01-07T20:04:13+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt; &lt;img alt="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" src="https://external-preview.redd.it/twKj1q9KpPirHPA7dDRZ83AKHbg7yJq5582Pj0hjJcE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a51b65024fe3771d3017caa7173152d7f99f201" title="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/07/01-ai-refutes-rumors-of-selling-teams-to-alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcamp</id>
    <title>Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1</title>
    <updated>2025-01-08T05:19:19+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt; &lt;img alt="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" src="https://external-preview.redd.it/O0xLvJp44hwyg917PUrV8IBhbRmX6FOqSfwmMF1eaGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc29f48e087f031064db96ec7d094a8c35ff53b5" title="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8"&gt;https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share a model we've been working on that might be useful for the community. It's a fine-tune of QwQ-32B-Preview focused on scientific reasoning and hypothesis generation.&lt;/p&gt; &lt;p&gt;The model was trained on LiveIdeaBench dataset, primarily targeting step-by-step scientific thinking. Main focus areas include chemistry, biology, climate science, and medical research. This model is designed to help researchers brainstorm scientific ideas.&lt;/p&gt; &lt;p&gt;Here's a simple example of its output&lt;/p&gt; &lt;h1&gt;Example User Inputs&lt;/h1&gt; &lt;p&gt;User:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I'll be submitting your next responses to a &amp;quot;Good Scientific Idea&amp;quot; expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: &amp;quot;cancer&amp;quot;. You may provide background information. The idea MUST be within 100 words (including background information). (Note: good scientific ideas should be novel, verifiable, practically valuable, and able to advance the field.). NOTE: You MUST give your answer after **Final Idea:** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Outputs&lt;/h1&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Alright, so I have this task to come up with a good scientific idea related to cancer, and it has to be novel, verifiable, practically valuable, and able to advance the field. Plus, it needs to be within 100 words, including any background information. That's a pretty tight constraint, but I think I can manage. First, ...&lt;br /&gt; &lt;strong&gt;Final Idea:&lt;/strong&gt; &lt;em&gt;propose a novel approach to cancer treatment by developing personalized cancer vaccines tailored to each patient's unique tumor mutations. By integrating machine learning algorithms with comprehensive omics data, including genomics, epigenetics, transcriptomics, and proteomics, this method aims to identify and prioritize the most immunogenic tumor-specific antigens. This integrated strategy could enhance vaccine efficacy and represents an advancement in precision medicine for cancer treatment.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model runs locally via Ollama. Additional details are available in the HF model card.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1&lt;/a&gt; Q4_K_M GGUF: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF&lt;/a&gt; Or you can just pull it from ollama!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run 6cf/QwQ-32B-Preview-IdeaWhiz-v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Training Dataset: &lt;a href="https://huggingface.co/datasets/6cf/liveideabench"&gt;https://huggingface.co/datasets/6cf/liveideabench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be interested in hearing how others might use this for their research workflows.&lt;/p&gt; &lt;p&gt;Note: This is an experimental project, still being refined. Feedback and suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvqydy</id>
    <title>HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory</title>
    <updated>2025-01-07T13:03:10+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt; &lt;img alt="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" src="https://external-preview.redd.it/-lYQleRwNOx-O_sp2gzAkhf2t0T8a7S-QftPiKWgI1U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01272f3aa7e7243a84e5a143551365b5aa468a" title="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/hp-z2-mini-g1a-is-a-workstation-class-mini-pc-with-amd-strix-halo-and-up-to-96gb-graphics-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T13:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw4n4d</id>
    <title>Tips for 6U rack case for 8x GPUs?</title>
    <updated>2025-01-07T22:56:57+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt; &lt;img alt="Tips for 6U rack case for 8x GPUs?" src="https://external-preview.redd.it/NTBwaDEzNWJqbmJlMY0gsNGuLHs7SOaXKNigvqdMKBR1N3ZyusSYRWdOjKQ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571e830aa82aac527beb1af94b15d26ab56cf375" title="Tips for 6U rack case for 8x GPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6fpzv25bjnbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvv39z</id>
    <title>Phi-4 in insanely good at rephrasing the last message for multi-turn rag questions</title>
    <updated>2025-01-07T16:20:26+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following this post from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fi1kex/multi_turn_conversation_and_rag/"&gt;few weeks ago&lt;/a&gt; when you do rag on the last posted message, you might need to recontextualize it, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Q :When was Jesus born ?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A : A long time ago !&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Q : What about his mother ?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here &lt;code&gt;What about his mother ?&lt;/code&gt; has missing references.&lt;/p&gt; &lt;p&gt;This problem is more complex than it seems, because the reference is not always in the latest message, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q : Who is Orano's Boss ?&lt;/li&gt; &lt;li&gt;A : it's Philippe Knoche&lt;/li&gt; &lt;li&gt;Q : Where did he go to school ?&lt;/li&gt; &lt;li&gt;A : Polytechnique and Ecole des Mines &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here we can have multiple tricky questions that requires good reasoning to be correctly rephrased : &lt;/p&gt; &lt;p&gt;&lt;code&gt;What about his wife ?&lt;/code&gt; -&amp;gt; Implies getting Philippe Knoche and school question to rephrase it&lt;/p&gt; &lt;p&gt;&lt;code&gt;Where is the HQ ?&lt;/code&gt; -&amp;gt; Implies the company HQ, not the two school &amp;quot;HQs&amp;quot;&lt;/p&gt; &lt;p&gt;Long story short, I tried multiple models, Qwen 2.5 7b, 14b Llama 3.1, Mistrals models , while Qwen is really good on the whole spectrum, it's not good enough at that and &lt;a href="https://huggingface.co/matteogeniaccio/phi-4"&gt;phi-4 &lt;del&gt;leaked&lt;/del&gt; model&lt;/a&gt; is FAR BEYOND every other model tested so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T16:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwaj9</id>
    <title>Are you gonna wait for Digits or get the 5090?</title>
    <updated>2025-01-07T17:10:46+00:00</updated>
    <author>
      <name>/u/lxe</name>
      <uri>https://old.reddit.com/user/lxe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Digits seems on paper like itâ€™s better bang for the buck, but there are a lot more unknown unknowns about it. And itâ€™s releasing later. Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lxe"&gt; /u/lxe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw82ty</id>
    <title>Created a video with text prompt using Cosmos-1.0-7B-Text2World</title>
    <updated>2025-01-08T01:36:41+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt; &lt;img alt="Created a video with text prompt using Cosmos-1.0-7B-Text2World" src="https://b.thumbs.redditmedia.com/QFw7CciKt9pMjNHgjpLvaOgCxj_KabzjeivBFZtdgBE.jpg" title="Created a video with text prompt using Cosmos-1.0-7B-Text2World" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is generated from the following command using single 3090:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/text2world.py --checkpoint_dir /workspace/checkpoints --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Text2World --prompt &amp;quot;water drop hitting the floor&amp;quot; --seed 547312549 --video_save_name Cosmos-1.0-Diffusion-7B-Text2World_memory_efficient --offload_tokenizer --offload_diffusion_transformer --offload_text_encoder_model --offload_prompt_upsampler --offload_guardrail_models&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It is converted to gif, so probably some color loss. Cosmos's rival Genesis still haven't released their generative model, so there is no one to compare to. &lt;/p&gt; &lt;p&gt;Couldn't get it to work with Cosmos-1.0-Diffusion-7B-Video2World. Did anyone manage to get it running on single 3090?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/zv2y4p9vaobe1.gif"&gt;https://i.redd.it/zv2y4p9vaobe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T01:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj1f4</id>
    <title>Now THIS is interesting</title>
    <updated>2025-01-07T04:10:47+00:00</updated>
    <author>
      <name>/u/Longjumping-Bake-557</name>
      <uri>https://old.reddit.com/user/Longjumping-Bake-557</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt; &lt;img alt="Now THIS is interesting" src="https://preview.redd.it/1fjml8mfyhbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f88b05826ef37319c0ba2f00bcdb0fb2c11b4e3" title="Now THIS is interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Bake-557"&gt; /u/Longjumping-Bake-557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1fjml8mfyhbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw3jzx</id>
    <title>What is your LLM stack? Subscriptions? Tools? self hosted?</title>
    <updated>2025-01-07T22:10:33+00:00</updated>
    <author>
      <name>/u/vazma</name>
      <uri>https://old.reddit.com/user/vazma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;I am 2 year old chatgpt subscriber mostly using it for coding and personal searches (instead of googlE) but now I am thinking to stop and replace it with other tools. &lt;/p&gt; &lt;p&gt;What I am thinking is to buy some API credits in OpenAI/Anthropic/DeepSeek for adhoc queries and buy a subscription for Cursor for Coding. &lt;/p&gt; &lt;p&gt;What are your stack? Do you have any recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vazma"&gt; /u/vazma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj4wn</id>
    <title>Nvidia announces $3,000 personal AI supercomputer called Digits</title>
    <updated>2025-01-07T04:16:18+00:00</updated>
    <author>
      <name>/u/DubiousLLM</name>
      <uri>https://old.reddit.com/user/DubiousLLM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt; &lt;img alt="Nvidia announces $3,000 personal AI supercomputer called Digits" src="https://external-preview.redd.it/JTaFAeW2ovmKm4g_0oF_TYz510_Ra5xuaGCjwMiquQM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c64890b0dea791601b5a6719c0468df66f41c33" title="Nvidia announces $3,000 personal AI supercomputer called Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DubiousLLM"&gt; /u/DubiousLLM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw5amg</id>
    <title>I just released Notate â€“ Open-source AI research assistant with local LLM support</title>
    <updated>2025-01-07T23:25:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://notate.hairetsu.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T23:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw1nze</id>
    <title>DeepSeek V3 GGUF 2-bit surprisingly works! + BF16, other quants</title>
    <updated>2025-01-07T20:51:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUF's including 2, 3 ,4, 5, 6 and 8-bit quants for Deepseek V3.&lt;/p&gt; &lt;p&gt;We've also de-quantized Deepseek-V3 to upload the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;bf16 version&lt;/a&gt; so you guys can experiment with it (1.3TB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimum hardware requirements&lt;/strong&gt; to run Deepseek-V3 in 2-bit: 48GB RAM + 250GB of disk space.&lt;/p&gt; &lt;p&gt;See how to run Deepseek V3 with examples and our full collection here: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c"&gt;https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Deepseek V3 version&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;2-bit: &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_XS%2FDeepSeek-V3-Q2_K_XS-00001-of-00005.gguf"&gt;Q2_K_XS&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_L%2FDeepSeek-V3-Q2_K_L-00001-of-00005.gguf"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q3_K_M%2FDeepSeek-V3-Q3_K_M-00001-of-00007.gguf"&gt;3&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q4_K_M%2FDeepSeek-V3-Q4_K_M-00001-of-00009.gguf"&gt;4&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q5_K_M%2FDeepSeek-V3-Q5_K_M-00001-of-00010.gguf"&gt;5&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q6_K%2FDeepSeek-V3-Q6_K-00001-of-00012.gguf"&gt;6&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q8_0%2FDeepSeek-V3-BF16-256x20B-Q8_0-00001-of-00016.gguf"&gt;8-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bf16&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;dequantized 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth &lt;/a&gt;GGUF model details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;207GB&lt;/td&gt; &lt;td align="left"&gt;Q2 everything, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_L"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;228GB&lt;/td&gt; &lt;td align="left"&gt;Q3 down_proj Q2 rest, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q3_K_M"&gt;Q3_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;298GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q3_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M"&gt;Q4_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;377GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q4_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q5_K_M"&gt;Q5_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;443GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q5_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K"&gt;Q6_K&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;513GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q6_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0"&gt;Q8_0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;712GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt; should run ok in ~40GB of CPU / GPU VRAM with automatic llama.cpp offloading.&lt;/li&gt; &lt;li&gt;Use K quantization (not V quantization)&lt;/li&gt; &lt;li&gt;Do not forget about &lt;code&gt;&amp;lt;ï½œUserï½œ&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;ï½œAssistantï½œ&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with Q5_0 K quantized cache (V quantized cache doesn't work):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --cache-type-k q5_0 --prompt '&amp;lt;ï½œUserï½œ&amp;gt;What is 1+1?&amp;lt;ï½œAssistantï½œ&amp;gt;' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and running the above generates:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The sum of 1 and 1 is **2**. Here's a simple step-by-step breakdown: 1. **Start with the number 1.** 2. **Add another 1 to it.** 3. **The result is 2.** So, **1 + 1 = 2**. [end of text] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwwsq</id>
    <title>Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference</title>
    <updated>2025-01-07T17:35:44+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt; &lt;img alt="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" src="https://external-preview.redd.it/MBJSLr1BrGGAWiERNKV1EsVPqt-H8qI6KhVXjMFntpA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ffe16868d431044dc8975ae38ca0056c5252984" title="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1876676954549620961?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcn8b</id>
    <title>Kokoro is an 82M-param TTS model, Apache</title>
    <updated>2025-01-08T05:40:28+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://hf.co/hexgrad/Kokoro-82M"&gt;https://hf.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my first post and also self-promo since I trained this model; go check it out if you'd like. From the README:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Kokoro&lt;/strong&gt; is a frontier TTS model for its size of &lt;strong&gt;82 million parameters&lt;/strong&gt; (text in/audio out).&lt;br /&gt; On 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a &lt;code&gt;.onnx&lt;/code&gt; version of v0.19 is available.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The architecture is mostly StyleTTS 2, and I'm not affiliated with the paper author: &lt;a href="https://github.com/yl4579/StyleTTS2"&gt;https://github.com/yl4579/StyleTTS2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If/when I get more synthetic audio, I do intend to release a sequel along with some major changes to tokenization (G2P). &lt;strong&gt;To that end, if anyone is sitting on a large trove of synthetic audioâ€”especially OpenAI's Advanced Voice Modeâ€”please refer to this call for data if interested&lt;/strong&gt;: &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Oh also there's a hosted demo on HF if you can't be bothered to install &lt;a href="https://hf.co/spaces/hexgrad/Kokoro-TTS"&gt;https://hf.co/spaces/hexgrad/Kokoro-TTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcn8b/kokoro_is_an_82mparam_tts_model_apache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcn8b/kokoro_is_an_82mparam_tts_model_apache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcn8b/kokoro_is_an_82mparam_tts_model_apache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:40:28+00:00</published>
  </entry>
</feed>
