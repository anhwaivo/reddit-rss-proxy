<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-07T15:35:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j5ipc7</id>
    <title>An AI tool designed to generate explanations for every file in a project</title>
    <updated>2025-03-07T08:17:12+00:00</updated>
    <author>
      <name>/u/juzi5201314</name>
      <uri>https://old.reddit.com/user/juzi5201314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, I want to share a little Python tool that generates short text explanations for code projects. The inspiration comes from the fact that when I was learning programming, I often needed to study and read other famous projects, but the code style of each project and each person was different, this requires me to spend a lot of energy analyzing the effects of certain codes., which made me think whether I could let LLM explain these codes for me.&lt;/p&gt; &lt;p&gt;This tool supports any OpenAI compatible API, which means that any model can be used to explain the code (&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/juzi5201314/RepoExplainer"&gt;https://github.com/juzi5201314/RepoExplainer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juzi5201314"&gt; /u/juzi5201314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ipc7/an_ai_tool_designed_to_generate_explanations_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ipc7/an_ai_tool_designed_to_generate_explanations_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ipc7/an_ai_tool_designed_to_generate_explanations_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T08:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4wd9v</id>
    <title>Jamba 1.6 is out!</title>
    <updated>2025-03-06T14:14:28+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Who is ready for another model release?&lt;/p&gt; &lt;p&gt;Let's welcome AI21 Labs Jamba 1.6 Release. Here is some information&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Beats models from Mistral, Meta &amp;amp; Cohere on quality &amp;amp; speed:&lt;/strong&gt; Jamba Large 1.6 outperforms Mistral Large 2, Llama 3.3 70B, and Command R+ on quality (Arena Hard), and Jamba Mini 1.6 outperforms Ministral 8B, Llama 3.1 8B, and Command R7.&lt;/li&gt; &lt;li&gt;Built with novel hybrid &lt;strong&gt;SSM-Transformer architecture&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long context performance:&lt;/strong&gt; With a context window of 256K, Jamba 1.6 outperforms Mistral, Llama, and Cohere on RAG and long context grounded question answering tasks (CRAG, HELMET RAG + HELMET LongQA, FinanceBench FullDoc, LongBench)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private deployment: M&lt;/strong&gt;odel weights are available to download from &lt;a href="https://huggingface.co/ai21labs"&gt;Hugging Face&lt;/a&gt; under Jamba Open Model License to deploy privately on-prem or in-VPC&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; In addition to English, the models support Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog post: &lt;a href="https://www.ai21.com/blog/introducing-jamba-1-6/"&gt;https://www.ai21.com/blog/introducing-jamba-1-6/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5hn2v</id>
    <title>The mystery of Apple M3 Ultra GPU performance</title>
    <updated>2025-03-07T06:59:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The press release of M3 Ultra claimed that its GPU performance is 2.6x of M1 Ultra.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j4jpij/comment/mgg62l5/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j4jpij/comment/mgg62l5/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I thought the 2.6x gain was due to doubling of shader per core. But some people pointed out that the gain can be attributed to the existence of ray tracing cores in M3 that is not in M1/M2.&lt;/p&gt; &lt;p&gt;To investigate the impact of ray tracing, I looked up the previous press release for M3.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/hk/en/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/"&gt;https://www.apple.com/hk/en/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It says M3 Max is 1.5x M1 Max, M3 Pro is 1.4x M1 Pro and M3 is 1.65x M1. Then constructed this table:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;M3 vs M1&lt;/th&gt; &lt;th align="left"&gt;Ultra&lt;/th&gt; &lt;th align="left"&gt;Max&lt;/th&gt; &lt;th align="left"&gt;Pro&lt;/th&gt; &lt;th align="left"&gt;Vanilla&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Gain&lt;/td&gt; &lt;td align="left"&gt;2.6x&lt;/td&gt; &lt;td align="left"&gt;1.5x&lt;/td&gt; &lt;td align="left"&gt;1.4x&lt;/td&gt; &lt;td align="left"&gt;1.65x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M3 FP16&lt;/td&gt; &lt;td align="left"&gt;57.344&lt;/td&gt; &lt;td align="left"&gt;28.672&lt;/td&gt; &lt;td align="left"&gt;12.9024&lt;/td&gt; &lt;td align="left"&gt;7.168&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M1 FP16&lt;/td&gt; &lt;td align="left"&gt;42.5984&lt;/td&gt; &lt;td align="left"&gt;21.2992&lt;/td&gt; &lt;td align="left"&gt;10.6496&lt;/td&gt; &lt;td align="left"&gt;5.3248&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 Gain&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;td align="left"&gt;1.2132x&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RT Gain&lt;/td&gt; &lt;td align="left"&gt;1.9314x&lt;/td&gt; &lt;td align="left"&gt;1.1142x&lt;/td&gt; &lt;td align="left"&gt;1.154x&lt;/td&gt; &lt;td align="left"&gt;1.2257x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;If I assume no doubling of shader per core, then M3 Ultra FP16 is 57.344. I assume the overall GPU gain is the product of FP16 and RT. Then I calculated the RT Gain and noticed that M3 Ultra's RT Gain is very different from the others. If I assume M3 Ultra FP16 is 114.688, then RT Gain is 0.9657x. This is closer to the other RT Gain but still a bit off.&lt;/p&gt; &lt;p&gt;So the conclusion is that RT cores probably doesn't explain this 2.6x gain fully. Any ray tracing experts here that can share their opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53cgv</id>
    <title>Introducing LogiLlama: A 1B-Parameter Open Source Model with Logical Reasoning</title>
    <updated>2025-03-06T19:09:50+00:00</updated>
    <author>
      <name>/u/Secret_Ad_6448</name>
      <uri>https://old.reddit.com/user/Secret_Ad_6448</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We are a small team of engineers from the University of Toronto working to make smaller models smarter. LogiLlama is our first release, a 1B-parameter model fine-tuned from LLaMA with improved logical reasoning.&lt;/p&gt; &lt;p&gt;We are intending to open-source everything, including models, datasets, and training configs, and we would love your feedback on how it performs. Try it out, test its reasoning, and let us know what you think.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://huggingface.co/goppa-ai/Goppa-LogiLlama"&gt;Goppa-LogiLlama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Ad_6448"&gt; /u/Secret_Ad_6448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qofh</id>
    <title>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs</title>
    <updated>2025-03-07T15:20:15+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.02003"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qofh/hot_highlighted_chain_of_thought_for_referencing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qofh/hot_highlighted_chain_of_thought_for_referencing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4y0zy</id>
    <title>I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this.</title>
    <updated>2025-03-06T15:29:42+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt; &lt;img alt="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." src="https://preview.redd.it/66kml0c983ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67a387d19a905d3863a6b2b7d6fb3c89f4cdaee0" title="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66kml0c983ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T15:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4u57l</id>
    <title>Hunyuan Image to Video released!</title>
    <updated>2025-03-06T12:16:11+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt; &lt;img alt="Hunyuan Image to Video released!" src="https://external-preview.redd.it/bjcyc3R4bnc5Mm5lMSVRu4OBDxIXZycPsoc4EtwnK4B2nYL7URskxFmP5hp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2734e75b58f76380d802fe5f62995c44dffdc8c0" title="Hunyuan Image to Video released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yck5cznw92ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T12:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5d8x7</id>
    <title>[2503.00735] LADDER: Self-Improving LLMs Through Recursive Problem Decomposition</title>
    <updated>2025-03-07T02:38:25+00:00</updated>
    <author>
      <name>/u/Ok_Knowledge_8259</name>
      <uri>https://old.reddit.com/user/Ok_Knowledge_8259</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Knowledge_8259"&gt; /u/Ok_Knowledge_8259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.00735"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d8x7/250300735_ladder_selfimproving_llms_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d8x7/250300735_ladder_selfimproving_llms_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T02:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5cwn8</id>
    <title>Are ~14B models and lower useful for much? What cool stuff can I do with them?</title>
    <updated>2025-03-07T02:20:39+00:00</updated>
    <author>
      <name>/u/PineappleScanner</name>
      <uri>https://old.reddit.com/user/PineappleScanner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was excited to set up my own ollama instance, but I realized pretty quickly that being limited to 12-16B and below (thanks to my 5700 XT) made the models pretty meh for my use cases.&lt;/p&gt; &lt;p&gt;I mostly use them for homework, studying, brainstorming, and general reasoning when I need help with a logical problem. llama 3.2 14b and qwen2.5 14b both spit out nonsense incorrect garbage somewhat frequently which makes them a lot less trustworthy than most of the big 70B+ models I can use online.&lt;/p&gt; &lt;p&gt;Is there anything cool I can use them for besides writing AI slop? I do homelabbing and selfhosting so if there's anything cool there lmk&lt;/p&gt; &lt;p&gt;Also, if someone could recommend a good NSFW model it would be ... appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PineappleScanner"&gt; /u/PineappleScanner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T02:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5p7mw</id>
    <title>Help Test YourStory! A New Interactive RPG on Twitch</title>
    <updated>2025-03-07T14:27:15+00:00</updated>
    <author>
      <name>/u/Affectionate-Leg8133</name>
      <uri>https://old.reddit.com/user/Affectionate-Leg8133</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;I'm developing &lt;em&gt;YourStory&lt;/em&gt;, an interactive text-based RPG where viewers actively shape the adventure in real-time. This isn't just another text game—it's a fully narrated experience with visuals and music, and the story dynamically evolves based on your decisions.&lt;/p&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Viewers directly influence the story&lt;/li&gt; &lt;li&gt;AI-driven narration, characters, and world-building&lt;/li&gt; &lt;li&gt;Dynamic music and visuals that adapt to the story&lt;/li&gt; &lt;li&gt;A multi-agent system designed for scalability&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;The game runs on a local architecture, capable of handling multiple &lt;strong&gt;Ollama&lt;/strong&gt; servers. Unfortunately, I currently only have one rig available for testing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current system setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Main agent rig (Storyteller, Memory Manager, Character Manager, Background Agent, Music Agent)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX 3090 (24GB VRAM) + GTX 1080 Ti&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core i7-12700K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS and OBS rig&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; GeForce GTX 1050 Max-Q&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core i7-8750H @ 2.20GHz&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 32GB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; Kokoro (&lt;a href="https://huggingface.co/geneing/Kokoro"&gt;https://huggingface.co/geneing/Kokoro&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Planned Features&lt;/h1&gt; &lt;p&gt;Currently, &lt;em&gt;YourStory&lt;/em&gt; supports custom assets (images and music) that can be placed in designated folders. The agents autonomously select and use these assets to enhance the storytelling experience.&lt;/p&gt; &lt;p&gt;In the future, I plan to integrate AI-generated &lt;strong&gt;images&lt;/strong&gt; (or even short video sequences) and dynamically generated &lt;strong&gt;music&lt;/strong&gt; to create an even more immersive experience. This will allow the entire audiovisual presentation to be generated on the fly, adapting in real-time to the evolving narrative.&lt;/p&gt; &lt;h1&gt;Powered by:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; Cydonia-24B-v2, Mistral-Small-24B-Base-2501, Wayfarer-Large-70B-Llama-3.3&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Agents:&lt;/strong&gt; Storyteller, Memory Manager, Character Manager, Background Agent, and Music Agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm currently in the testing phase and need feedback to improve the system. If you're interested in interactive storytelling and want to see how AI-driven narration evolves in real-time, join the test session and help push the system to its limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Twitch Link:&lt;/strong&gt; &lt;a href="https://www.twitch.tv/thestarai"&gt;&lt;strong&gt;https://www.twitch.tv/thestarai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and participation. See you there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Leg8133"&gt; /u/Affectionate-Leg8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T14:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5fy49</id>
    <title>(Another) epub to audiobook converter (audiobook-generator)</title>
    <updated>2025-03-07T05:08:36+00:00</updated>
    <author>
      <name>/u/ibic</name>
      <uri>https://old.reddit.com/user/ibic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks, I'm just sharing this little python program I created for converting epub books to audiobooks here.&lt;/p&gt; &lt;h2&gt;tldr;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;With Python 3.10+, create a virtual environment and run &lt;code&gt;pip install audiobook-generator&lt;/code&gt; (Or just use &lt;code&gt;pipx&lt;/code&gt; instead which manages the python virtual environment for you automatically). Please be patient a bit as it downloads some large dependencies such as pytorch and the kokoro tts model. On Ubuntu, the dependencies take around 6GB.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run: &lt;code&gt;abg &amp;lt;epub path&amp;gt; &amp;lt;audio output directory&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;That's it.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Tested on Windows/Ubuntu/Mac.&lt;/li&gt; &lt;li&gt;Automatically selects cuda capable GPU if present.&lt;/li&gt; &lt;li&gt;Separate audio files (mp3) are generated per chapter according to the sequence and name of the chapter.&lt;/li&gt; &lt;li&gt;For the 2 books I tested, the total size is around 300 - 500MB, which is about 1/2 - 1/3 of the file size produced by other converters I tried.&lt;/li&gt; &lt;li&gt;The cover image, if present, is extracted to the same directory if present.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why another ebook to audiobook converter?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;I tried 2 converters, but both took me quite some time to figure out how to get them running on cuda GPU (CPU is fine but slow), so in the end, I decided to create one myself, which includes everything in its dependencies so you don't need to install anything extra if you want to use a cuda GPU.&lt;/li&gt; &lt;li&gt;It's purely personal taste, but I perfer to have one mp3 with the filename being the chapter name instead of one big file for audiobooks. I just copy the whole directory to audiobookshelf then the file names are displayed as the chapter names.&lt;/li&gt; &lt;li&gt;I want to keep the cover image as well.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;GitHub repository&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/houtianze/audiobook-generator"&gt;https://github.com/houtianze/audiobook-generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibic"&gt; /u/ibic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T05:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4zkiq</id>
    <title>QwQ-32B is now available on HuggingChat, unquantized and for free!</title>
    <updated>2025-03-06T16:35:11+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt; &lt;img alt="QwQ-32B is now available on HuggingChat, unquantized and for free!" src="https://external-preview.redd.it/2kRkvRdd0EYvAJfhYjUZDeV5rTNSqMYr7S8BF5canLM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef0d71dc8e5fbfb1db3c11812ff15ced70a08c8f" title="QwQ-32B is now available on HuggingChat, unquantized and for free!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T16:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j57b06</id>
    <title>Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)</title>
    <updated>2025-03-06T21:56:26+00:00</updated>
    <author>
      <name>/u/_underlines_</name>
      <uri>https://old.reddit.com/user/_underlines_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt; &lt;img alt="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" src="https://external-preview.redd.it/yqkxDs5A63axozkmSyct8MXlpRrbuMOdYI9OOnHcIk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5908068706fbbeeff49f0d24edaf236a083309b7" title="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_underlines_"&gt; /u/_underlines_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T21:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l0sv</id>
    <title>Mistral's New OCR Model (SaaS) - Best in Class</title>
    <updated>2025-03-07T11:04:24+00:00</updated>
    <author>
      <name>/u/Auslieferator</name>
      <uri>https://old.reddit.com/user/Auslieferator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral just announced a new model specialized on OCR, beating Azure OCR and Google Document AI. Sadly it is only offered as SaaS and not open weights as a lot of their models were. &lt;a href="https://mistral.ai/fr/news/mistral-ocr"&gt;https://mistral.ai/fr/news/mistral-ocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you know about any other OCR specialized LLMs or stacks of LLMs and other computer vision software, achieving similar results? I am currently playing with Qwen2.5 VL 7B Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Auslieferator"&gt; /u/Auslieferator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:04:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ao2j</id>
    <title>AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b</title>
    <updated>2025-03-07T00:28:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt; &lt;img alt="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" src="https://preview.redd.it/yfu1j9qhw5ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08e2bd1a7385f636f30fe8884c1fb096e06cdc40" title="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yfu1j9qhw5ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T00:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j59fue</id>
    <title>Meta drops AI bombshell: Latent tokens help to improve LLM reasoning</title>
    <updated>2025-03-06T23:29:50+00:00</updated>
    <author>
      <name>/u/Dense-Smf-6032</name>
      <uri>https://old.reddit.com/user/Dense-Smf-6032</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt; &lt;img alt="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" src="https://a.thumbs.redditmedia.com/5Ri36_q4vpEcMh6tQDQuJzQoWFUwmI_Eb-w-OMLVXh8.jpg" title="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper link: &lt;a href="https://arxiv.org/abs/2502.03275"&gt;https://arxiv.org/abs/2502.03275&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: The researcher from Meta AI found compressing text with a vqvae into latent-tokens and then adding them onto the training helps to improve LLM reasoning capability.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b"&gt;https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Smf-6032"&gt; /u/Dense-Smf-6032 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T23:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ievy</id>
    <title>FT: Llama 4 w/ voice expected in coming weeks</title>
    <updated>2025-03-07T07:55:53+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like good self-hosted voice chat that isn't tacked on will soon be available from both Sesame and Meta! Can't wait!&lt;/p&gt; &lt;p&gt;P.S. Is anyone working on an iOS app with CarPlay that lets me talk to my private AI server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/a1014427-c2ce-4204-b41a-001277309cea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5hb4p</id>
    <title>Ensure you use the appropriate temperature of 0.6 with QwQ-32B</title>
    <updated>2025-03-07T06:35:39+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt; &lt;img alt="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j5hb4p/video/jjveqqjjo7ne1/player"&gt;ball bouncing inside of a hexagon as it rotates&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;can you give me a pygame script that allows me to have a ball bouncing inside of a hexagon as it rotates, makes sure to handle collisions and gravity&amp;quot;&lt;/p&gt; &lt;p&gt;Yesterday, I tried this prompt it failed. I was very disappointed since it spend 15 mins on it. &lt;/p&gt; &lt;p&gt;Today, I notice the Ollama setting have been updated to with the correct temperature.&lt;br /&gt; You can find the recommend settings here.&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json"&gt;https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53w92</id>
    <title>Intro to DeepSeek's open-source week and why it's a big deal</title>
    <updated>2025-03-06T19:32:52+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt; &lt;img alt="Intro to DeepSeek's open-source week and why it's a big deal" src="https://preview.redd.it/3dvsybsvf4ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eddab159fdcfb7e8ea6d001d8fc521b45a52638" title="Intro to DeepSeek's open-source week and why it's a big deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dvsybsvf4ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5lym7</id>
    <title>Lightweight Hallucination Detector for Local RAG Setups - No Extra LLM Calls Required</title>
    <updated>2025-03-07T12:05:37+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've been working on solving a common problem many of us face when running RAG systems with our local models - hallucinations. While our locally-hosted LLMs are impressive, they still tend to make things up when using RAG, especially when running smaller models with limited context windows.&lt;/p&gt; &lt;p&gt;I've released an &lt;strong&gt;open-source hallucination detector&lt;/strong&gt; that's specifically designed to be efficient enough to run on consumer hardware alongside your local LLMs. Unlike other solutions that require additional LLM API calls (which add latency and often external dependencies), this is a lightweight transformer-based classifier.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on modernBERT architecture&lt;/li&gt; &lt;li&gt;Inference speed: ~1 example/second on CPU, ~10-20 examples/second on modest GPU&lt;/li&gt; &lt;li&gt;Zero external API dependencies - runs completely local&lt;/li&gt; &lt;li&gt;Works with any LLM output, including Llama-2, Llama-3, Mistral, Phi-3, etc.&lt;/li&gt; &lt;li&gt;Integrates easily with LlamaIndex, LangChain, or your custom RAG pipeline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The detector evaluates your LLM's response against the retrieved context to identify when the model generates information not present in the source material. It achieves 80.7% recall on the RAGTruth benchmark, with particularly strong performance on data-to-text tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example integration with your local setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from adaptive_classifier import AdaptiveClassifier # Load the hallucination detector (downloads once, runs locally after) detector = AdaptiveClassifier.from_pretrained(&amp;quot;adaptive-classifier/llm-hallucination-detector&amp;quot;) # Your existing RAG pipeline context = retriever.get_relevant_documents(query) response = your_local_llm.generate(context, query) # Format for the detector input_text = f&amp;quot;Context: {context}\nQuestion: {query}\nAnswer: {response}&amp;quot; # Check for hallucinations prediction = detector.predict(input_text) if prediction[0][0] == 'HALLUCINATED' and prediction[0][1] &amp;gt; 0.6: print(&amp;quot;⚠️ Warning: Response appears to contain information not in the context&amp;quot;) # Maybe re-generate or add a disclaimer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The detector is part of the adaptive-classifier library which also has tools for routing between different local models based on query complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How have you been addressing hallucinations in your local RAG setups?&lt;/li&gt; &lt;li&gt;Would a token-level detector (highlighting exactly which parts are hallucinated) be useful?&lt;/li&gt; &lt;li&gt;What's your typical resource budget for this kind of auxiliary model in your stack?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://github.com/codelion/adaptive-classifier#hallucination-detector"&gt;https://github.com/codelion/adaptive-classifier#hallucination-detector&lt;/a&gt;&lt;br /&gt; Installation: &lt;code&gt;pip install adaptive-classifier&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T12:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j55tnf</id>
    <title>Anthropic warns White House about R1 and suggests "equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention"</title>
    <updated>2025-03-06T20:53:47+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt; &lt;img alt="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" src="https://external-preview.redd.it/4jL_cfkf1eOugH8Cd6gazHmY0nNfDF0kn5G0AyNVMU4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ec84648eddc4b833c73bd77f2df99dd7c97d0bf" title="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
</feed>
