<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-31T07:22:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ie8z9y</id>
    <title>I'm confused. Here are some absolut noob questions.</title>
    <updated>2025-01-31T06:47:44+00:00</updated>
    <author>
      <name>/u/soyoucheckusernames</name>
      <uri>https://old.reddit.com/user/soyoucheckusernames</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone please help me out? I'm new in this Llama stuff and the deepseek hype made me get into it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Now I wanted to download deekseek and deepseek coding v2, and all I saw was some files which are 8 months old (on huggingface). Is this actually the correct version? Why are people just started talking it some days ago then?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Also what exactly does 1.5b, 7b, etc mean and are those below 10B models even useful? I've downaloded meta 1.5b (preset of lm studio) and for me it's not just slow, but also it just makes up fairy Tales whenni ask it something. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I've also got 7b deepseek (I hope it's the correct one) and it isnt really good either. Also takes way too long thinking and typing.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Also when I search for deepseek Coder v2 in lm Studio, it gives me out a file with a relatively small amount of downloads. But when I have googled Coder v2, there is also another version of it with a huge number of downloads. Why doesnt lm studio recommend me that?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Should I download Modules from hugging face instead of lm studio? (Which downloads also from huggingface, but see my question above)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;And last question: lm studio or ollama?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soyoucheckusernames"&gt; /u/soyoucheckusernames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8z9y/im_confused_here_are_some_absolut_noob_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8z9y/im_confused_here_are_some_absolut_noob_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8z9y/im_confused_here_are_some_absolut_noob_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T06:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1idqql6</id>
    <title>Mistral Small 3 24b's Context Window is Remarkably Efficient</title>
    <updated>2025-01-30T16:23:25+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt; &lt;img alt="Mistral Small 3 24b's Context Window is Remarkably Efficient" src="https://b.thumbs.redditmedia.com/tUYsJoEn9u94ym2whVhsPOc7Lcfh9qD4M48XkP1073Y.jpg" title="Mistral Small 3 24b's Context Window is Remarkably Efficient" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using the Mistral Small 3 24b-q6k model with a full 32K context (Q8 KV cache), and I still have 1.6GB of VRAM left.&lt;br /&gt; In comparison, Qwen2.5 32b Q4 KL is roughly the same size, but I could only manage to get 24K context before getting dangerously close to running out of VRAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730"&gt;https://preview.redd.it/adg5weajp5ge1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb3e935191ccc7000f402ce10e2c880ddb309730&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idqql6/mistral_small_3_24bs_context_window_is_remarkably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T16:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie7db5</id>
    <title>Chris Manning (top 3 NLP/Machine Learning researchers in the world) believes the Deepseek 6m dollar training costs due to the optimizations discussed in their paper</title>
    <updated>2025-01-31T05:03:02+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While a lot of the things discussed in the Deepseek paper have been verified, what has garnered the most skepticism is the training cost. &lt;/p&gt; &lt;p&gt;Chris manning, whose highly regarded as one of the top 3-5 NLP researchers in the world, gave a talk yesterday, which was live tweeted&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700131884490762"&gt;https://x.com/atroyn/status/1884700131884490762&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;deepseek have succeeded at producing models with large numbers of experts (256 in v3). combined with multi-head latent attention, plus training in fb8, dramatically reduces training costs. @chrmanning buys the $6M training compute cost.&amp;quot;&lt;/p&gt; &lt;p&gt;He buys the 6 million dollar training cost claimed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie7db5/chris_manning_top_3_nlpmachine_learning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T05:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie3mv7</id>
    <title>What is your favorite 12/13B model for NSFW RP?</title>
    <updated>2025-01-31T01:43:21+00:00</updated>
    <author>
      <name>/u/NullHypothesisCicada</name>
      <uri>https://old.reddit.com/user/NullHypothesisCicada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I guess it’s that time of the year. Last year, I’ve tested a lot of M-N models such as violet-lotus, mag-mell, etc. Though there are still some minor problems for each models, such as incoherent after 10k context, only suitable for 3rd person roleplay and so on.&lt;/p&gt; &lt;p&gt;Since they’re all released probably about half a year ago, I want to ask you what’s your favorite for some sweet sweaty RP?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullHypothesisCicada"&gt; /u/NullHypothesisCicada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie3mv7/what_is_your_favorite_1213b_model_for_nsfw_rp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T01:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1idwgay</id>
    <title>Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface</title>
    <updated>2025-01-30T20:20:28+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"&gt; &lt;img alt="Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface" src="https://external-preview.redd.it/KMwppOY-W87gB9d3tmURowTBAI22RUNa2m2fmKkqML0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92bbafd261aeee71b2a7db5b902101dab7c7ea22" title="Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/open-r1?utm_source=tldrai#what-is-deepseek-r1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idwgay/openr1_a_fully_open_reproduction_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T20:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1idokcx</id>
    <title>Mistral new open models</title>
    <updated>2025-01-30T14:47:21+00:00</updated>
    <author>
      <name>/u/konilse</name>
      <uri>https://old.reddit.com/user/konilse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt; &lt;img alt="Mistral new open models" src="https://preview.redd.it/5nnsoy4295ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d39024b2c7d0acbb55e2f3d01eee2b120c949e0" title="Mistral new open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral base and instruct 24B &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/konilse"&gt; /u/konilse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nnsoy4295ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idokcx/mistral_new_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:47:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1idvuch</id>
    <title>Re-Distilling DeepSeek R1</title>
    <updated>2025-01-30T19:55:23+00:00</updated>
    <author>
      <name>/u/sightio</name>
      <uri>https://old.reddit.com/user/sightio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve improved DeepSeek R1 distilled models using logits distillation—delivering +4-14% gains on GSM8K while only spending $3-18 per training run.&lt;/p&gt; &lt;p&gt;Details at &lt;a href="https://mobiusml.github.io/r1_redistill_blogpost/"&gt;https://mobiusml.github.io/r1_redistill_blogpost/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models are available on Hugging Face - run them efficiently with HQQ! &lt;a href="https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d"&gt;https://huggingface.co/collections/mobiuslabsgmbh/deepseek-r1-redistill-6793d3bea92c7fff0639ab4d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sightio"&gt; /u/sightio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idnyhh</id>
    <title>mistralai/Mistral-Small-24B-Base-2501 · Hugging Face</title>
    <updated>2025-01-30T14:18:23+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt; &lt;img alt="mistralai/Mistral-Small-24B-Base-2501 · Hugging Face" src="https://external-preview.redd.it/lDGKmq6pSZNpISh4piV15abwPTUoM5lDEjjJ9qZ_vd4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56053b8ce77cd587b1abeda9737783c65c0ebab8" title="mistralai/Mistral-Small-24B-Base-2501 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idnyhh/mistralaimistralsmall24bbase2501_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:18:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1idzxix</id>
    <title>Mistral-Small-24B-2501 vs Mistral-Small-2409</title>
    <updated>2025-01-30T22:48:32+00:00</updated>
    <author>
      <name>/u/citaman</name>
      <uri>https://old.reddit.com/user/citaman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"&gt; &lt;img alt="Mistral-Small-24B-2501 vs Mistral-Small-2409" src="https://preview.redd.it/705ahg8qm7ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b60f35e508f91598f803cbba687f8180633bbe1c" title="Mistral-Small-24B-2501 vs Mistral-Small-2409" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/citaman"&gt; /u/citaman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/705ahg8qm7ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idzxix/mistralsmall24b2501_vs_mistralsmall2409/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1idzimg</id>
    <title>Mistral Small 3 knows the truth</title>
    <updated>2025-01-30T22:30:25+00:00</updated>
    <author>
      <name>/u/magicduck</name>
      <uri>https://old.reddit.com/user/magicduck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"&gt; &lt;img alt="Mistral Small 3 knows the truth" src="https://preview.redd.it/8rp05jjjj7ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af0e6a9f5574e3c1cae3becd10fc86657b8b07d3" title="Mistral Small 3 knows the truth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magicduck"&gt; /u/magicduck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8rp05jjjj7ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idzimg/mistral_small_3_knows_the_truth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idp1z5</id>
    <title>No synthetic data?</title>
    <updated>2025-01-30T15:09:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt; &lt;img alt="No synthetic data?" src="https://preview.redd.it/98dq1wg2d5ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448fe61c33c8db28d89becf7c1d0ccbcf95ea88a" title="No synthetic data?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's reallllllly rare in 2025, did I understand this correctly? They didn't use any synthetic data to train this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/98dq1wg2d5ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idp1z5/no_synthetic_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T15:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie8jea</id>
    <title>What the fuck is abbas man🗿💔</title>
    <updated>2025-01-31T06:16:37+00:00</updated>
    <author>
      <name>/u/Fun-Property-5964</name>
      <uri>https://old.reddit.com/user/Fun-Property-5964</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"&gt; &lt;img alt="What the fuck is abbas man🗿💔" src="https://preview.redd.it/bi8mqxkuu9ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e82fac435f8a5f0ee1b5ec8398cb5ea3b6c1952d" title="What the fuck is abbas man🗿💔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Property-5964"&gt; /u/Fun-Property-5964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bi8mqxkuu9ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie8jea/what_the_fuck_is_abbas_man/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T06:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idt9xz</id>
    <title>Watch this SmolAgent save me over 100 hours of work.</title>
    <updated>2025-01-30T18:08:42+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt; &lt;img alt="Watch this SmolAgent save me over 100 hours of work." src="https://external-preview.redd.it/eXpvaDN2aXY4NmdlMaIWY-pKRTEFed4oaflr_50jeaU7y6AfPZ2q49QYyqUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ea3707bc7016b8ca7da0ee5c72fef8602edfdba" title="Watch this SmolAgent save me over 100 hours of work." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/je2gcviv86ge1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idt9xz/watch_this_smolagent_save_me_over_100_hours_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:08:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iduk3b</id>
    <title>Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)</title>
    <updated>2025-01-30T19:02:02+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt; &lt;img alt="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" src="https://preview.redd.it/gazbvr6gi6ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f6de32bcaa9f8ae8ff3f2ab317c2401bd2f5b73" title="Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gazbvr6gi6ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iduk3b/mistral_small_3_oneshotting_unsloths_flappy_bird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4brg</id>
    <title>DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked</title>
    <updated>2025-01-31T02:16:09+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt; &lt;img alt="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" src="https://external-preview.redd.it/FoBRfbFJiqbPvZWr-1-_kZti4liFY86vCxy63rbFeaE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8563af2625a34ede2f3818cc4a25bab8f7cf54c0" title="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thehackernews.com/2025/01/deepseek-ai-database-exposed-over-1.html?m=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T02:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ido3fn</id>
    <title>Are there ½ million people capable of running locally 685B params models?</title>
    <updated>2025-01-30T14:25:02+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"&gt; &lt;img alt="Are there ½ million people capable of running locally 685B params models?" src="https://b.thumbs.redditmedia.com/nUAmR_7owY5oJQcrzV0vL3H93-ccvgV-SDlaKg3CSyw.jpg" title="Are there ½ million people capable of running locally 685B params models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ido3fn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ido3fn/are_there_½_million_people_capable_of_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5tls</id>
    <title>If you can't afford to run R1 locally, then being patient is your best action.</title>
    <updated>2025-01-31T03:35:25+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pause for a minute and read &lt;a href="https://simonwillison.net/2024/Dec/9/llama-33-70b/"&gt;I can now run a GPT-4 class model on my laptop&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It only take &lt;em&gt;20 months&lt;/em&gt; for smaller model that can run on consumer hardware to surpass bigger older models.&lt;/p&gt; &lt;p&gt;Yes, it feels like an eternity for internet user. But 1.5 years is small for human lifespan. Don't believe me? Llama 1 is almost 2 years old! (Released on February 24, 2023)&lt;/p&gt; &lt;p&gt;In the next 20 months, there will be small model that are better than R1.&lt;/p&gt; &lt;p&gt;Just like patient gamer save money waiting for steam sale, we save money by waiting for better, more efficient smaller model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T03:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1idva1j</id>
    <title>Welcome back, Le Mistral!</title>
    <updated>2025-01-30T19:31:40+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt; &lt;img alt="Welcome back, Le Mistral!" src="https://preview.redd.it/4td7dsrjn6ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bcee379bd06ff66ce0c2532f18c365ea37c8d6d1" title="Welcome back, Le Mistral!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4td7dsrjn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idva1j/welcome_back_le_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:31:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1idny3w</id>
    <title>Mistral Small 3</title>
    <updated>2025-01-30T14:17:56+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt; &lt;img alt="Mistral Small 3" src="https://preview.redd.it/kj3s0jvr35ge1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0317aadc49155a8df1074618844c589ea3d2753d" title="Mistral Small 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kj3s0jvr35ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idny3w/mistral_small_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T14:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie0a8u</id>
    <title>QWEN just launched their chatbot website</title>
    <updated>2025-01-30T23:03:37+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt; &lt;img alt="QWEN just launched their chatbot website" src="https://preview.redd.it/vzgzfrhlp7ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa67cbeae08d4c800e7b5dc088c0330556268f" title="QWEN just launched their chatbot website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link: &lt;a href="https://chat.qwenlm.ai/"&gt;https://chat.qwenlm.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vzgzfrhlp7ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T23:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv7yb</id>
    <title>Marc Andreessen on Anthropic CEO's Call for Export Controls on China</title>
    <updated>2025-01-30T19:29:13+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt; &lt;img alt="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" src="https://preview.redd.it/wlsi25dcn6ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d695bb3258d357570ad11762d15df689f13fe2a8" title="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wlsi25dcn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtkll</id>
    <title>Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more.</title>
    <updated>2025-01-30T18:20:59+00:00</updated>
    <author>
      <name>/u/deoxykev</name>
      <uri>https://old.reddit.com/user/deoxykev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt; &lt;img alt="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." src="https://external-preview.redd.it/VCPkBGJsVaggWY7c9V20KQQGCJhrF411vyVYUsHeuns.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=495bbbb03e5ebeff92050c2a71f7e340cb4bbebc" title="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deoxykev"&gt; /u/deoxykev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6gv0</id>
    <title>It’s time to lead guys</title>
    <updated>2025-01-31T04:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt; &lt;img alt="It’s time to lead guys" src="https://preview.redd.it/4r69mh9f89ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f3c997deb132531af541fbe7a279f1544512cbb" title="It’s time to lead guys" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4r69mh9f89ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T04:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1idz487</id>
    <title>'we're in this bizarre world where the best way to learn about llms... is to read papers by chinese companies. i do not think this is a good state of the world' - us labs keeping their architectures and algorithms secret is ultimately hurting ai development in the us.' - Dr Chris Manning</title>
    <updated>2025-01-30T22:13:22+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700560500416881"&gt;https://x.com/atroyn/status/1884700560500416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:13:22+00:00</published>
  </entry>
</feed>
