<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-19T14:24:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kpwgjy</id>
    <title>Unlock Qwen3's Full Power: cot_proxy for Easy Mode Switching, Parameter Control &amp; Clean Outputs!</title>
    <updated>2025-05-18T22:35:12+00:00</updated>
    <author>
      <name>/u/ben1984th</name>
      <uri>https://old.reddit.com/user/ben1984th</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey AI Devs &amp;amp; Qwen3 Users! 👋&lt;/p&gt; &lt;p&gt;Struggling to effectively use Qwen3 models with their hybrid reasoning (&lt;code&gt;/think&lt;/code&gt;) and normal (&lt;code&gt;/no_think&lt;/code&gt;) modes? It can be a real challenge when each mode needs different sampling parameters, and tools like Cline or RooCode don't offer that fine-grained control.&lt;/p&gt; &lt;p&gt;That's where &lt;code&gt;cot_proxy&lt;/code&gt; comes in! 🚀&lt;/p&gt; &lt;p&gt;&lt;code&gt;cot_proxy&lt;/code&gt; is a lightweight, Dockerized reverse proxy that sits between your application and your LLM, giving you powerful control over the request lifecycle. It's particularly game-changing for models like Qwen3.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How&lt;/strong&gt; &lt;code&gt;cot_proxy&lt;/code&gt; &lt;strong&gt;makes your life easier:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Master Qwen3's Hybrid Nature:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automatic Mode Commands:&lt;/strong&gt; Configure &lt;code&gt;cot_proxy&lt;/code&gt; to automatically append &lt;code&gt;/think&lt;/code&gt; or &lt;code&gt;/no_think&lt;/code&gt; to your prompts based on the &amp;quot;pseudo-model&amp;quot; you call.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized Sampling Per Mode:&lt;/strong&gt; Define different sampling parameters (temperature, top_p, etc.) for your &amp;quot;thinking&amp;quot; and &amp;quot;non-thinking&amp;quot; Qwen3 configurations.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;🔧 &lt;strong&gt;Advanced Request Manipulation:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model-Specific Configurations:&lt;/strong&gt; Create &amp;quot;pseudo-models&amp;quot; in your &lt;code&gt;.env&lt;/code&gt; file (e.g., &lt;code&gt;Qwen3-32B-Creative-Thinking&lt;/code&gt; vs. &lt;code&gt;Qwen3-32B-Factual-Concise&lt;/code&gt;). &lt;code&gt;cot_proxy&lt;/code&gt; then applies the specific parameters, prompt additions, and upstream model mapping you've defined.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean Outputs:&lt;/strong&gt; Automatically strip out &lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt; tags from responses, delivering only the final, clean answer – even with streaming!&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;💡 &lt;strong&gt;Easy Integration:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Turnkey Qwen3 Examples:&lt;/strong&gt; Our &lt;a href="https://github.com/bold84/cot_proxy/blob/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides working configurations to get you started with Qwen3 immediately.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use with Any Client:&lt;/strong&gt; Seamlessly integrate Qwen3 (and other complex models) into applications that don't natively support advanced parameter or prompt adjustments.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially, &lt;code&gt;cot_proxy&lt;/code&gt; lets you abstract away the complexities of managing sophisticated models, allowing your client applications to remain simple while still leveraging the full power of models like Qwen3.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Check it out, star it, and simplify your LLM workflows!&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;GitHub Repository:&lt;/strong&gt; &lt;a href="https://github.com/bold84/cot_proxy"&gt;https://github.com/bold84/cot_proxy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback and see how you use it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ben1984th"&gt; /u/ben1984th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpwgjy/unlock_qwen3s_full_power_cot_proxy_for_easy_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T22:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq99yg</id>
    <title>3090 or 5060 Ti</title>
    <updated>2025-05-19T11:26:53+00:00</updated>
    <author>
      <name>/u/marius851000</name>
      <uri>https://old.reddit.com/user/marius851000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in building a new desktop computer, and would like to make sure to be able to run some local function-calling llm (for toying around, and maybe using it in some coding assistance tool) and also NLP.&lt;/p&gt; &lt;p&gt;I've seen those two devices. One is relativelly old but can be bought used at about 700€, while a 5060 ti 16GB can be bought cheaper at around 500€.&lt;/p&gt; &lt;p&gt;The 3090 appears to have (according to openbenchmarking) about 40% better performance in gaming and general performance, with a similar order for FP16 computation (according to Wikipedia), in addition to 8 extra GB of RAM.&lt;/p&gt; &lt;p&gt;However, it seems that the 3090 does not support lower resolution floats, unlike a 5090 which can go down to fp4. (althought I suspect I might have gotten something wrong. I see quantization with 5 or 6 bits. Which align to none of that) and so I am worried such a GPU would require me to use fp16, limited the amount of parameter I can use.&lt;/p&gt; &lt;p&gt;Is my worry correct? What would be your recommendation? Is there a performance benchmark for that use case somewhere?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;edit: I'll probably think twice if I'm willing to spend 200 extra euro for that, but I'll likely go with a 3090.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marius851000"&gt; /u/marius851000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq99yg/3090_or_5060_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq99yg/3090_or_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq99yg/3090_or_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9h8x</id>
    <title>What is the smoothest speech interface to run locally?</title>
    <updated>2025-05-19T11:38:49+00:00</updated>
    <author>
      <name>/u/winkler1</name>
      <uri>https://old.reddit.com/user/winkler1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M3 Mac, running Gemma 12B in LMStudio. Is low-latency natural speech possible? Or am I better off just using voice input transcription?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/winkler1"&gt; /u/winkler1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9h8x/what_is_the_smoothest_speech_interface_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9h8x/what_is_the_smoothest_speech_interface_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9h8x/what_is_the_smoothest_speech_interface_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpp5op</id>
    <title>is Qwen 30B-A3B the best model to run locally right now?</title>
    <updated>2025-05-18T17:18:58+00:00</updated>
    <author>
      <name>/u/S4lVin</name>
      <uri>https://old.reddit.com/user/S4lVin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got into running models locally, and just some days ago Qwen 3 got launched.&lt;/p&gt; &lt;p&gt;I saw a lot of posts about Mistral, Deepseek R1, end Llama, but since Qwen 3 got released recently, there isn't much information about it. But reading the benchmarks, it looks like Qwen 3 outperforms all the other models, and also the MoE version runs like a 20B+ model while using very little resources.&lt;/p&gt; &lt;p&gt;So i would like to ask, is it the only model i would need to get, or there are still other models that could be better than Qwen 3 in some areas? (My specs are: RTX 3080 Ti (12gb VRAM), 32gb of RAM, 12900K)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S4lVin"&gt; /u/S4lVin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpp5op/is_qwen_30ba3b_the_best_model_to_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kprsun</id>
    <title>Skeptical about the increased focus on STEM and CoT</title>
    <updated>2025-05-18T19:10:54+00:00</updated>
    <author>
      <name>/u/Quazar386</name>
      <uri>https://old.reddit.com/user/Quazar386</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Qwen3, I’ve been growing increasingly skeptical about the direction many labs are taking with CoT and STEM focused LLMs. With Qwen3, every model in the lineup follows a hybrid CoT approach and has a heavy emphasis on STEM tasks. This seems to be part of why the models feel “overcooked”. I have seen from other people that fine-tuning these models has been a challenge, especially with the reasoning baked in. This can be seen when applying instruction training data to the supposed base model that Qwen released. The training loss is surprisingly low which suggests that it’s already been instruction-primed to some extent, likely to better support CoT. This has not been a new thing as we have seen censorship and refusals from “base” models before.&lt;/p&gt; &lt;p&gt;Now, if the instruction-tuned checkpoints were always strong, maybe that would be acceptable. But I have seen a bunch of reports that these models tend to become overly repetitive in long multi-turn conversations. That’s actually what pushed some people to train their own base models for Qwen3. One possible explanation is that a large portion of the training seems focused on single-shot QA tasks for math and code.&lt;/p&gt; &lt;p&gt;This heavy emphasis on STEM capabilities has brought about an even bigger issue apart from fine-tuning. That is signs of knowledge degradation or what’s called catastrophic forgetting. Newer models, even some of the largest, are not making much headway on frontier knowledge benchmarks like Humanity’s Last Exam. This leads to hilarious results where Llama 2 7B beats out GPT 4.5 on that benchmark. While some might argue that raw knowledge isn’t a measure of intelligence, for LLMs, robust world knowledge is still critical for answering general questions or even coding for more niche applications. I don’t want LLMs to start relying on search tools for answering knowledge questions.&lt;/p&gt; &lt;p&gt;Going back to CoT, it’s also not a one-size-fits-all solution. It has an inherent latency since the model has to &amp;quot;think out loud&amp;quot; by generating thinking tokens before answering and often explores multiple unnecessary branches. While this could make models like R1 surprisingly charming in its human-like thoughts, the time it takes to answer can take too long, especially for more basic questions. While there have been some improvements in token efficiency, it’s still a bottleneck, especially in running local LLMs where hardware is a real limiting factor. It's what made me not that interested in running local CoT models as I have limited hardware.&lt;/p&gt; &lt;p&gt;More importantly, CoT doesn’t actually help with every task. In creative writing, for example, there’s no single correct answer to reason toward. Reasoning might help with coherence, but in my own testing, it usually results in less focused paragraphs. And at the end of the day, it’s still unclear whether these models are truly reasoning, or just remembering patterns from training. CoT models continue to struggle with genuinely novel problems, and we’ve seen that even without generating CoT tokens, some CoT models can still perform impressively compared to similarly sized non CoT trained models. I sometimes wonder if these models actually reason or just remember the steps to a memorized answer.&lt;/p&gt; &lt;p&gt;So yeah, I’m not fully sold on the CoT and STEM-heavy trajectory the field is on right now, especially when it comes at the cost of broad general capability and world knowledge. It feels like the field is optimizing for a narrow slice of tasks (math, code) while losing sight of what makes these models useful more broadly. This can already bee seen with the May release of Gemini 2.5 Pro where the only marketed improvement was in coding while everything else seems to be a downgrade from the March release of Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quazar386"&gt; /u/Quazar386 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kprsun/skeptical_about_the_increased_focus_on_stem_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T19:10:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kppdhb</id>
    <title>MSI PC with NVIDIA GB10 Superchip - 6144 CUDA Cores and 128GB LPDDR5X Confirmed</title>
    <updated>2025-05-18T17:28:06+00:00</updated>
    <author>
      <name>/u/shakhizat</name>
      <uri>https://old.reddit.com/user/shakhizat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ASUS, Dell, and Lenovo have released their version of Nvidia DGX Spark, and now MSI has as well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x"&gt;https://en.gamegpu.com/iron/msi-showed-edgeexpert-ms-c931-s-nvidia-gb10-superchip-confirmed-6144-cuda-yader-i-128-gb-lpddr5x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shakhizat"&gt; /u/shakhizat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kppdhb/msi_pc_with_nvidia_gb10_superchip_6144_cuda_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T17:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq2zgg</id>
    <title>SAGA - Semantic And Graph-enhanced Authoring</title>
    <updated>2025-05-19T04:23:40+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to share a little project I've been actively working on for the last couple weeks called SAGA. It is still very much under development, so I'd love to know your thoughts about it!.&lt;/p&gt; &lt;p&gt;SAGA (Semantic And Graph-enhanced Authoring) is a sophisticated AI-powered creative writing system designed to generate full-length novels with consistent characters, coherent world-building, and compelling narratives. Unlike simple prompt-based writing tools, SAGA employs a multi-stage pipeline that mirrors professional writing processes: planning, drafting, evaluation, and revision.&lt;/p&gt; &lt;p&gt;🌟 Key Features&lt;/p&gt; &lt;p&gt;- **Multi-Stage Writing Pipeline**: Separate planning, drafting, evaluation, and revision phases with specialized LLM prompts&lt;/p&gt; &lt;p&gt;- **Hybrid Knowledge Management**: Combines JSON-based character/world profiles with a knowledge graph for factual consistency&lt;/p&gt; &lt;p&gt;- **Intelligent Context Generation**: Uses semantic similarity and reliable knowledge facts to provide relevant context for each chapter&lt;/p&gt; &lt;p&gt;- **Comprehensive Quality Control**: Evaluates consistency, plot alignment, thematic coherence, and narrative depth&lt;/p&gt; &lt;p&gt;- **Agentic Planning**: Detailed scene-by-scene planning with focus elements for narrative depth&lt;/p&gt; &lt;p&gt;- **Provisional Data Tracking**: Marks data quality based on source reliability to maintain canon integrity&lt;/p&gt; &lt;p&gt;- **Adaptive Revision**: Targeted revision strategies based on specific evaluation feedback&lt;/p&gt; &lt;p&gt;The system will:&lt;/p&gt; &lt;p&gt;- Generate or load a plot outline&lt;/p&gt; &lt;p&gt;- Create initial world-building&lt;/p&gt; &lt;p&gt;- Pre-populate the knowledge graph&lt;/p&gt; &lt;p&gt;- Begin writing chapters iteratively&lt;/p&gt; &lt;p&gt;- Resume from the last chapter it left off on&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2zgg/saga_semantic_and_graphenhanced_authoring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T04:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq6d4u</id>
    <title>NVIDIA Launches GB10-Powered DGX Spark &amp; GB300-Powered DGX Station AI Systems, Blackwell Ultra With 20 PFLOPs Compute</title>
    <updated>2025-05-19T08:12:32+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6d4u/nvidia_launches_gb10powered_dgx_spark/"&gt; &lt;img alt="NVIDIA Launches GB10-Powered DGX Spark &amp;amp; GB300-Powered DGX Station AI Systems, Blackwell Ultra With 20 PFLOPs Compute" src="https://external-preview.redd.it/Kf9iwaDR1s4NlZiLQgFyqXBmfU0c5KzKmPwqHwOjBKE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc58fdb063571a703f73fdcc3db050aede808c75" title="NVIDIA Launches GB10-Powered DGX Spark &amp;amp; GB300-Powered DGX Station AI Systems, Blackwell Ultra With 20 PFLOPs Compute" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-gb10-powered-dgx-spark-gb300-powered-dgx-station-ai-systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6d4u/nvidia_launches_gb10powered_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6d4u/nvidia_launches_gb10powered_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqbh7g</id>
    <title>Been away for two months.. what's the new hotness?</title>
    <updated>2025-05-19T13:18:32+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the new hotness? Saw a Qwen model? I'm usually able to run things in the 20-23B range... but if there's low end stuff, I'm interested in that as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqbh7g/been_away_for_two_months_whats_the_new_hotness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T13:18:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq029v</id>
    <title>Is Qwen 2.5 Coder Instruct still the best option for local coding with 24GB VRAM?</title>
    <updated>2025-05-19T01:40:12+00:00</updated>
    <author>
      <name>/u/MrWeirdoFace</name>
      <uri>https://old.reddit.com/user/MrWeirdoFace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is Qwen 2.5 Coder Instruct still the best option for local coding with 24GB VRAM, or has that changed since Qwen 3 came out? I haven't noticed a coding model for it, but it's possible other models have come in gone that I've missed that handle python better than Qwen 2.5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWeirdoFace"&gt; /u/MrWeirdoFace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq029v/is_qwen_25_coder_instruct_still_the_best_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T01:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq4vrv</id>
    <title>OuteTTS v1.0 now supported by chatllm.cpp</title>
    <updated>2025-05-19T06:27:47+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"&gt; &lt;img alt="OuteTTS v1.0 now supported by chatllm.cpp" src="https://external-preview.redd.it/bGFzbDV5NGptbzFmMYcMr_Cq2gLg7E5zrnm6bi-e1D6-e2IdLt9Ao5b5ur7s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b016d5027138226027672f8409b61134b13cd138" title="OuteTTS v1.0 now supported by chatllm.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After Orpheus-TTS is implemented in &lt;a href="https://github.com/foldl/chatllm.cpp"&gt;ChatLLM.cpp&lt;/a&gt;, now here comes &lt;a href="https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B"&gt;OuteTTS v1.0&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cpcocy4jmo1f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4vrv/outetts_v10_now_supported_by_chatllmcpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T06:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq2wr0</id>
    <title>I made a tool to efficiently find optimal parameters</title>
    <updated>2025-05-19T04:19:00+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: &lt;a href="https://github.com/kooshi/TaguchiBench"&gt;https://github.com/kooshi/TaguchiBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Taguchi method lets you change multiple variables at once to test a bunch of stuff quickly, and I made a tool to do it for AI and other stuff&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I've been waking up inspired often recently, with the multiplying effect of Claude and Gemini, I can explore ideas as fast as I come up with them.&lt;/p&gt; &lt;p&gt;One seemed particularly compelling, partially because I've been looking for an excuse to use Orthogonal Arrays ever since I saw &lt;a href="https://youtu.be/5oULEuOoRd0"&gt;NightHawkInLight's video&lt;/a&gt; about them.&lt;/p&gt; &lt;p&gt;I wanted a way to test local llm sampler parameters to see what was really the best, and as it takes so long to run benchmarks, Orthogonal Arrays popped into my head as a way to efficiently test them.&lt;/p&gt; &lt;p&gt;I had no idea how much statistical math went into analyzing these things, but I just kept learning and coding. I'm sure it's nowhere near perfect, but it seems to be working pretty well, and I mostly cleaned things up enough to allow the scrutiny of the public eye.&lt;/p&gt; &lt;p&gt;At some point I realized it could be generalized to run &lt;em&gt;any&lt;/em&gt; command line tool and optimize those arguments as well, so I ended up completely refactoring it to break it into two components.&lt;/p&gt; &lt;p&gt;So here's what I have: &lt;a href="https://github.com/kooshi/TaguchiBench"&gt;https://github.com/kooshi/TaguchiBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; LiveBenchRunner - which just sets up and executes a LiveBench run with llama-server as the backend, which is useful by itself or with:&lt;/li&gt; &lt;li&gt;TaguchiBench.Engine &lt;ul&gt; &lt;li&gt;takes a set of parameters and values&lt;/li&gt; &lt;li&gt;attempts to fit them into a Taguchi (Orthogonal) array (harder than you'd think)&lt;/li&gt; &lt;li&gt;runs the tool an efficient number of times with the different values for the parameters&lt;/li&gt; &lt;li&gt;does a bunch of statistical analysis on the scores returned by the tool&lt;/li&gt; &lt;li&gt;makes some nice reports out of them&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It can also recover from an interrupted experiment, which is nice considering how long runs can take. (In the future I may take advantage of LiveBench's recovery ability as well)&lt;/p&gt; &lt;p&gt;I haven't actually found any useful optimization data yet, as I've just been focused on development, but now that it's pretty solid, I'm curious to validate &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kkuq7m/qwen_suggests_adding_presence_penalty_when_using/"&gt;Qwen3's recent recommendation to enable presence penalty&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What I'm really hoping though, is that someone else finds a use for this in their own work, since it can help optimize any process you can run from a command line. I looked around, and I didn't see any open source tool like it. I did find this &lt;a href="https://pypi.org/project/taguchi/"&gt;https://pypi.org/project/taguchi/&lt;/a&gt;, and shoutout to another NightHawkInLight fan, but it doesn't appear to do any analysis of returned values, and is generally pretty simple. Granted, mine's probably massively &lt;em&gt;over&lt;/em&gt;engineered, but so it goes.&lt;/p&gt; &lt;p&gt;Anyway, I hope you all like it, and have some uses for it, AI related or not!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq2wr0/i_made_a_tool_to_efficiently_find_optimal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T04:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpw9nw</id>
    <title>Unlimited text-to-speech using Kokoro-JS, 100% local, 100% open source</title>
    <updated>2025-05-18T22:26:10+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://streaming-kokoro.glitch.me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpw9nw/unlimited_texttospeech_using_kokorojs_100_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-18T22:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq1g7s</id>
    <title>The first author of the ParScale paper discusses how they turned ParScale from an idea into reality</title>
    <updated>2025-05-19T02:55:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt; &lt;img alt="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" src="https://b.thumbs.redditmedia.com/bAgKbu4x_vB4NNm42eNbQsAZxlwnjN3U6xEN99bLNKc.jpg" title="The first author of the ParScale paper discusses how they turned ParScale from an idea into reality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because many friends have given feedback that Zhihu cannot be accessed without registration, I am simply using a translation plugin to translate posts from Zhihu into English and taking screenshots. &lt;/p&gt; &lt;p&gt;The original author is keytoyze, who holds all rights to the article. The original address is:&lt;/p&gt; &lt;p&gt;&lt;a href="http://www.zhihu.com/question/1907422978985169131/answer/1907565157103694086"&gt;www.zhihu.com/question/1907422978985169131/answer/1907565157103694086&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb"&gt;https://preview.redd.it/coxrzxd6ln1f1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55637a7888ae9396e88a09ea0ed134bd153e7dcb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de"&gt;https://preview.redd.it/hudkuuf7ln1f1.png?width=862&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c9af9f77370961a07bdc6876c6be9e84c3ff2de&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a"&gt;https://preview.redd.it/xebnsy18ln1f1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b8c78a0d42bead0e4838d2f6f24da84d5a706b3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595"&gt;https://preview.redd.it/3yuzdfp8ln1f1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a03790528375bd05619f79e335c08cafa9659595&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3"&gt;https://preview.redd.it/z07wi6f9ln1f1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=230c6c9bba3ae8d72838c06d5ae6c0f7fdab16d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd"&gt;https://preview.redd.it/bs6cecy9ln1f1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b948927ff6a3edeea98ddc37377eac53e5a968fd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq1g7s/the_first_author_of_the_parscale_paper_discusses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T02:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq4ey4</id>
    <title>NVIDIA says DGX Spark releasing in July</title>
    <updated>2025-05-19T05:56:22+00:00</updated>
    <author>
      <name>/u/Aplakka</name>
      <uri>https://old.reddit.com/user/Aplakka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark should be available in July.&lt;/p&gt; &lt;p&gt;The 128 GB unified memory amount is nice, but there's been discussions about whether the bandwidth will be too slow to be practical. Will be interesting to see what independent benchmarks will show, I don't think it's had any outsider reviews yet. I couldn't find a price yet, that of course will be quite important too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers"&gt;https://nvidianews.nvidia.com/news/nvidia-launches-ai-first-dgx-personal-computing-systems-with-global-computer-makers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;|| || |System Memory|128 GB LPDDR5x, unified system memory|&lt;/p&gt; &lt;p&gt;|| || |Memory Bandwidth|273 GB/s|&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aplakka"&gt; /u/Aplakka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T05:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqab4m</id>
    <title>llama.cpp now supports Llama 4 vision</title>
    <updated>2025-05-19T12:22:27+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt; &lt;img alt="llama.cpp now supports Llama 4 vision" src="https://external-preview.redd.it/B8o6PBUKxWoyfTLHowtPtTQrUM4omNNyOv5t_-1MIqk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd011b3d7cd43123e6bb6b624eb22b92c82f10f5" title="llama.cpp now supports Llama 4 vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Vision support is picking up speed with the recent refactoring to better support it in general. Note that there's a minor(?) &lt;a href="https://github.com/ggml-org/llama.cpp/pull/13282"&gt;issue with Llama 4 vision&lt;/a&gt; in general, as you can see below. It's most likely with the model, not with the implementation in llama.cpp, as the issue also occurs on other inference engines than just llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c25p83fheq1f1.png?width=503&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6eeb50199641034f38969eb526581fe95ef46498"&gt;https://preview.redd.it/c25p83fheq1f1.png?width=503&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6eeb50199641034f38969eb526581fe95ef46498&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqab4m/llamacpp_now_supports_llama_4_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9mfl</id>
    <title>Intel Announces Arc Pro B-Series, "Project Battlematrix" Linux Software Improvements</title>
    <updated>2025-05-19T11:46:51+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"&gt; &lt;img alt="Intel Announces Arc Pro B-Series, &amp;quot;Project Battlematrix&amp;quot; Linux Software Improvements" src="https://external-preview.redd.it/GNAZ-cVMVgweIup1G1242zhFgkq9dtWIloVxhHHhjYo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a042c504c995ad73ede19093c65cdc62c3f82fe9" title="Intel Announces Arc Pro B-Series, &amp;quot;Project Battlematrix&amp;quot; Linux Software Improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/intel-arc-pro-b-series"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9mfl/intel_announces_arc_pro_bseries_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqa6l0</id>
    <title>KTransformers v0.3.1 now supports Intel Arc GPUs (A770 + new B-series): 7 tps DeepSeek R1 decode speed for a single CPU + a single A770</title>
    <updated>2025-05-19T12:16:17+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As shared in &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt;this post&lt;/a&gt;, Intel just dropped their new Arc Pro B-series GPUs today.&lt;/p&gt; &lt;p&gt;Thanks to early collaboration with Intel, KTransformers v0.3.1 is out now with Day 0 support for these new cards — including the previously supported A-series like the A770.&lt;/p&gt; &lt;p&gt;In our test setup with a single-socket Xeon 5 + DDR5 4800MT/s + Arc A770, we’re seeing around 7.5 tokens/sec decoding speed on &lt;em&gt;deepseek-r1 Q4&lt;/em&gt;. Enabling dual NUMA gives you even better throughput.&lt;/p&gt; &lt;p&gt;More details and setup instructions:&lt;br /&gt; &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/xpu.md"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/xpu.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support, and more updates soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa6l0/ktransformers_v031_now_supports_intel_arc_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqa7vx</id>
    <title>Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual</title>
    <updated>2025-05-19T12:18:09+00:00</updated>
    <author>
      <name>/u/Optifnolinalgebdirec</name>
      <uri>https://old.reddit.com/user/Optifnolinalgebdirec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"&gt; &lt;img alt="Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual" src="https://external-preview.redd.it/7FOJ3auvDt89Hg_vMibknvVEFUx6iCFTiqBX1eYmFSA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39cffe3cb1c59d44099155324ec96bb08d047073" title="Intel Arc B60 DUAL-GPU 48GB Video Card Tear-Down | MAXSUN Arc Pro B60 Dual" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/@GamersNexus"&gt;Gamers Nexus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optifnolinalgebdirec"&gt; /u/Optifnolinalgebdirec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Y8MWbPBP9i0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqa7vx/intel_arc_b60_dualgpu_48gb_video_card_teardown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq6ysz</id>
    <title>OuteTTS 1.0 (0.6B) — Apache 2.0, Batch Inference (~0.1–0.02 RTF)</title>
    <updated>2025-05-19T08:56:52+00:00</updated>
    <author>
      <name>/u/OuteAI</name>
      <uri>https://old.reddit.com/user/OuteAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"&gt; &lt;img alt="OuteTTS 1.0 (0.6B) — Apache 2.0, Batch Inference (~0.1–0.02 RTF)" src="https://external-preview.redd.it/mkj0c5KE7uG2t5lRcNFyEg2Rx_CYpgOSNHlXCK0pNG4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ca9a806b15f609c1dba2685c905de1ca8099ac2" title="OuteTTS 1.0 (0.6B) — Apache 2.0, Batch Inference (~0.1–0.02 RTF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just released OuteTTS-1.0-0.6B, a lighter variant built on Qwen-3 0.6B.&lt;/p&gt; &lt;p&gt;OuteTTS-1.0-0.6B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Architecture: Based on Qwen-3 0.6B.&lt;/li&gt; &lt;li&gt;License: Apache 2.0 (free for commercial and personal use)&lt;/li&gt; &lt;li&gt;Multilingual: 14 supported languages: English, Chinese, Dutch, French, Georgian, German, Hungarian, Italian, Japanese, Korean, Latvian, Polish, Russian, Spanish&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Python Package Update: outetts v0.4.2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EXL2 Async: batched inference&lt;/li&gt; &lt;li&gt;vLLM (Experimental): batched inference&lt;/li&gt; &lt;li&gt;Llama.cpp Async Server: continuous batching&lt;/li&gt; &lt;li&gt;Llama.cpp Server: external-URL model inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;⚡ Benchmarks (Single NVIDIA L40S GPU)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Batch→RTF&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;vLLM OuteTTS-1.0-0.6B FP8&lt;/td&gt; &lt;td&gt;16→0.11, 24→0.08, 32→0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;vLLM Llama-OuteTTS-1.0-1B FP8&lt;/td&gt; &lt;td&gt;32→0.04, 64→0.03, 128→0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 OuteTTS-1.0-0.6B 8bpw&lt;/td&gt; &lt;td&gt;32→0.108&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 OuteTTS-1.0-0.6B 6bpw&lt;/td&gt; &lt;td&gt;32→0.106&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;EXL2 Llama-OuteTTS-1.0-1B 8bpw&lt;/td&gt; &lt;td&gt;32→0.105&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server OuteTTS-1.0-0.6B Q8_0&lt;/td&gt; &lt;td&gt;16→0.22, 32→0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server OuteTTS-1.0-0.6B Q6_K&lt;/td&gt; &lt;td&gt;16→0.21, 32→0.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server Llama-OuteTTS-1.0-1B Q8_0&lt;/td&gt; &lt;td&gt;16→0.172, 32→0.166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama.cpp server Llama-OuteTTS-1.0-1B Q6_K&lt;/td&gt; &lt;td&gt;16→0.165, 32→0.164&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;📦 Model Weights (ST, GGUF, EXL2, FP8): &lt;a href="https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B"&gt;https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;📂 Python Inference Library: &lt;a href="https://github.com/edwko/OuteTTS"&gt;https://github.com/edwko/OuteTTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OuteAI"&gt; /u/OuteAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq6ysz/outetts_10_06b_apache_20_batch_inference_01002_rtf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T08:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1kpyn8g</id>
    <title>Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)</title>
    <updated>2025-05-19T00:24:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt; &lt;img alt="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" src="https://preview.redd.it/7q0xsc86um1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952df9feb0cce10d5227340e9e367e9fc6939abe" title="Qwen released new paper and model: ParScale, ParScale-1.8B-(P1-P8)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The original text says, 'We theoretically and empirically establish that scaling with P parallel streams is comparable to scaling the number of parameters by O(log P).' Does this mean that a 30B model can achieve the effect of a 45B model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7q0xsc86um1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kpyn8g/qwen_released_new_paper_and_model_parscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T00:24:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq8wo4</id>
    <title>Computex: Intel Unveils New GPUs for AI and Workstations</title>
    <updated>2025-05-19T11:05:10+00:00</updated>
    <author>
      <name>/u/MR_-_501</name>
      <uri>https://old.reddit.com/user/MR_-_501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"&gt; &lt;img alt="Computex: Intel Unveils New GPUs for AI and Workstations" src="https://external-preview.redd.it/a_EOuFMT3wImCaaTP_AxZtoh2M_kQm2Ho4iekIvJrVk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=296206ba445fb5eff3bb134ade0c97c527f347ae" title="Computex: Intel Unveils New GPUs for AI and Workstations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;24GB for $500&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MR_-_501"&gt; /u/MR_-_501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq8wo4/computex_intel_unveils_new_gpus_for_ai_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kqaqmr</id>
    <title>Is Intel Arc GPU with 48GB of memory going to take over for $1k?</title>
    <updated>2025-05-19T12:43:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At the 3:58 mark video says cost is expected to be less than $1K: &lt;a href="https://www.youtube.com/watch?v=Y8MWbPBP9i0"&gt;https://www.youtube.com/watch?v=Y8MWbPBP9i0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 24GB costs $500, which also seems like a no brainer.&lt;/p&gt; &lt;p&gt;Info on 24gb card:&lt;/p&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory"&gt;https://videocardz.com/newz/intel-announces-arc-pro-b60-24gb-and-b50-16gb-cards-dual-b60-features-48gb-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/"&gt;https://wccftech.com/intel-arc-pro-b60-24-gb-b50-16-gb-battlemage-gpus-pro-ai-3x-faster-dual-gpu-variant/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations"&gt;https://newsroom.intel.com/client-computing/computex-intel-unveils-new-gpus-ai-workstations&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kqaqmr/is_intel_arc_gpu_with_48gb_of_memory_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T12:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq590b</id>
    <title>Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)</title>
    <updated>2025-05-19T06:53:01+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt; &lt;img alt="Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" src="https://preview.redd.it/u6niruxjqo1f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92c4cac8e33b1fe68fdc0af3f66d45dcdcf1c55a" title="Clara — A fully offline, Modular AI workspace (LLMs + Agents + Automation + Image Gen)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I’ve been working on this for the past few months and finally feel good enough to share it.&lt;/p&gt; &lt;p&gt;It’s called &lt;strong&gt;Clara&lt;/strong&gt; — and the idea is simple:&lt;/p&gt; &lt;p&gt;🧩 &lt;strong&gt;Imagine building your own workspace for AI&lt;/strong&gt; — with local tools, agents, automations, and image generation.&lt;/p&gt; &lt;p&gt;Note: Created this becoz i hated the ChatUI for everything, I want everything in one place but i don't wanna jump between apps and its completely opensource with MIT Lisence &lt;/p&gt; &lt;p&gt;Clara lets you do exactly that — fully offline, fully modular.&lt;/p&gt; &lt;p&gt;You can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧱 Drop everything as widgets on a dashboard — rearrange, resize, and make it &lt;em&gt;yours with all the stuff mentioned below&lt;/em&gt;&lt;/li&gt; &lt;li&gt;💬 Chat with local LLMs with Rag, Image, Documents, Run Code like ChatGPT - Supports both Ollama and Any OpenAI Like API&lt;/li&gt; &lt;li&gt;⚙️ Create agents with built-in logic &amp;amp; memory &lt;/li&gt; &lt;li&gt;🔁 Run automations via native N8N integration (1000+ Free Templates in ClaraVerse Store)&lt;/li&gt; &lt;li&gt;🎨 Generate images locally using Stable Diffusion (ComfyUI) - (Native Build without ComfyUI Coming Soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Clara has app for everything - Mac, Windows, Linux&lt;/p&gt; &lt;p&gt;It’s like… instead of opening a bunch of apps, you build your own AI control room. And it all runs on your machine. No cloud. No API keys. No bs.&lt;/p&gt; &lt;p&gt;Would love to hear what y’all think — ideas, bugs, roast me if needed 😄&lt;br /&gt; If you're into local-first tooling, this might actually be useful.&lt;/p&gt; &lt;p&gt;Peace ✌️&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br /&gt; I built Clara because honestly... I was sick of bouncing between 10 different ChatUIs just to get basic stuff done.&lt;br /&gt; I wanted one place — where I could run LLMs, trigger workflows, write code, generate images — without switching tabs or tools.&lt;br /&gt; So I made it.&lt;/p&gt; &lt;p&gt;And yeah — it’s fully open-source, MIT licensed, no gatekeeping. Use it, break it, fork it, whatever you want.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6niruxjqo1f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq590b/clara_a_fully_offline_modular_ai_workspace_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T06:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kq9294</id>
    <title>Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs</title>
    <updated>2025-05-19T11:14:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt; &lt;img alt="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" src="https://external-preview.redd.it/lJpkUaWR7aRg9qhyrcIgwW2kvtG6PxI9-Hw_9dnqBZU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c87f9f3217c313d6276262cf0a6572a7d3d2af" title="Intel launches $299 Arc Pro B50 with 16GB of memory, 'Project Battlematrix' workstations with 24GB Arc Pro B60 GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While the B60 is designed for powerful 'Project Battlematrix' AI workstations... will carry a roughly $500 per-unit price tag&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/intel-launches-usd299-arc-pro-b50-with-16gb-of-memory-project-battlematrix-workstations-with-24gb-arc-pro-b60-gpus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kq9294/intel_launches_299_arc_pro_b50_with_16gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-19T11:14:29+00:00</published>
  </entry>
</feed>
