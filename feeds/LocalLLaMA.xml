<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-02T19:05:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j10d5g</id>
    <title>Can you ELI5 why a temp of 0 is bad?</title>
    <updated>2025-03-01T14:14:16+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like common knowledge that &amp;quot;you almost always need temp &amp;gt; 0&amp;quot; but I find this less authoritative than everyone believes. I understand if one is writing creatively, he'd use higher temps to arrive at less boring ideas, but what if the prompts are for STEM topics or just factual information? Wouldn't higher temps force the llm to wonder away from the more likely correct answer, into a maze of more likely wrong answers, and effectively hallucinate more?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j10d5g/can_you_eli5_why_a_temp_of_0_is_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T14:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j12jh4</id>
    <title>Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!</title>
    <updated>2025-03-01T15:54:30+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt; &lt;img alt="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" src="https://external-preview.redd.it/yySKZcwLWOVfiw-FCxXZZGyMsX-eiuOXklZ8rkauveI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f604a8bea97f9cec32746dee177776a5626460f1" title="Drummer's Fallen Llama 3.3 R1 70B v1 - Experience a totally unhinged R1 at home!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Fallen-Llama-3.3-R1-70B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j12jh4/drummers_fallen_llama_33_r1_70b_v1_experience_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1x77u</id>
    <title>What are all other free AI chat applications are out now? This post has information about ChatGPT, Claude, Le Chat, DeepSeek, Gemini studio, Poe.</title>
    <updated>2025-03-02T18:09:52+00:00</updated>
    <author>
      <name>/u/soomrevised</name>
      <uri>https://old.reddit.com/user/soomrevised</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is local llama, apologies for this but I thought this is the best place to ask this question.&lt;/p&gt; &lt;p&gt;I've been comparing the free tiers of various AI assistants and wanted to see if there are more options I'm missing. Here's what I've found so far:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;: Offers image upload, some models have native image understanding, no image generation. Available on web, Android, iOS. Free models include 4o-mini and o3-mini (thinking model). Has web search. Moderate usage limits. Paid tiers start at $20/month, with team ($40) and pro ($200) options. Extra features include web search, Canvas, image generation, Code Interpreter, and shared chats. Uses your data for training unless you opt out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Claude&lt;/strong&gt;: Offers image upload with native image understanding. No image generation. Available on web, Android, iOS. Free model is Claude 3.7 Sonnet. No web search in free tier. Very low usage limitations. Paid tier starts at $20. Extra features include artifacts (similar to Canvas). Does not use your data for training. Note that it has limited model access and sometimes downgrades during high usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: Offers image upload but only text extraction (no native image understanding). No image generation. Available on web, Android, iOS. Models include V3 and R1. Has web search. High availability but with limitations. No paid tier for chat. Extra features include web search. Uses your data for training with no opt-out option. Servers are often busy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Le Chat&lt;/strong&gt;: Offers image upload, possibly has native image understanding (marked with ?), and uniquely offers image generation. Available on web, Android, iOS. Models include Dynamic (Mistral Large, Small, Next). Has web search. Limitations on messages per day. Paid tiers are $15 for pro and $25 for team of 2. Extra features include web search, Canvas, code interpreter, image generation, sharing chats, AFP, and Open URL. Data usage policy not specified. Offers various models and generous features.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemini Studio&lt;/strong&gt;: Browser-only platform, website performance isn't great on mobile. Data is used for training. Newer platform with experimental models that are pretty generous with usage limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Poe&lt;/strong&gt;: More of an aggregator platform where you're given daily coins to spend on any model from their offerings. This lets you access multiple AI models through a single interface, giving you flexibility to try different options.&lt;/p&gt; &lt;p&gt;I'm using local models for most of my tasks and API when in need of powerful models, these free options are for when I'm learning something or ask quick questions that might need more intelligent model.&lt;/p&gt; &lt;p&gt;Are there any other free AI assistants I'm missing? i will edit the post with what i find from comments.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Hailou minimax &lt;a href="https://chat.minimax.io/"&gt;https://chat.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Chat &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Microsoft Copilot&lt;/p&gt; &lt;p&gt;&lt;a href="http://kimi.ai"&gt;kimi.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://chat.inceptionlabs.ai/"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://molmo.allenai.org/"&gt;https://molmo.allenai.org/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://copilot.microsoft.com/"&gt;https://copilot.microsoft.com/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://pi.ai/"&gt;https://pi.ai/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://duckduckgo.com/?q=DuckDuckGo+AI+Chat&amp;amp;ia=chat&amp;amp;duckai=1"&gt;https://duckduckgo.com/?q=DuckDuckGo+AI+Chat&amp;amp;ia=chat&amp;amp;duckai=1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://playground.liquid.ai/"&gt;https://playground.liquid.ai/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://lambda.chat/chatui/"&gt;https://lambda.chat/chatui/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soomrevised"&gt; /u/soomrevised &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1x77u/what_are_all_other_free_ai_chat_applications_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1x77u/what_are_all_other_free_ai_chat_applications_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1x77u/what_are_all_other_free_ai_chat_applications_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T18:09:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1oe2d</id>
    <title>AMD 4700S 16GB GDDR6 RAM - Model offloading to fast RAM?</title>
    <updated>2025-03-02T10:57:53+00:00</updated>
    <author>
      <name>/u/Mysterious_Prune415</name>
      <uri>https://old.reddit.com/user/Mysterious_Prune415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have seen these desktop kits AMD 4700S for cheap and noticed they use soldered GDDR6 RAM. Does anyone have experience with offloading onto this fast RAM? Would that even make sense?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Prune415"&gt; /u/Mysterious_Prune415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1oe2d/amd_4700s_16gb_gddr6_ram_model_offloading_to_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1oe2d/amd_4700s_16gb_gddr6_ram_model_offloading_to_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1oe2d/amd_4700s_16gb_gddr6_ram_model_offloading_to_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1out5</id>
    <title>For what applications will x8/x8 PCIe bifurcation for dual GPU setups be useful? Is NVLink still important in 2025?</title>
    <updated>2025-03-02T11:29:26+00:00</updated>
    <author>
      <name>/u/phlurker</name>
      <uri>https://old.reddit.com/user/phlurker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can't find NVLink bridges (3-slot) for 30-series anymore in my area. Is it still important?&lt;/p&gt; &lt;p&gt;Is a better motherboard still worth looking into for a dual GPU setup? I don't plan to expand to more GPUs (for now).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phlurker"&gt; /u/phlurker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1out5/for_what_applications_will_x8x8_pcie_bifurcation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1out5/for_what_applications_will_x8x8_pcie_bifurcation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1out5/for_what_applications_will_x8x8_pcie_bifurcation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1dqou</id>
    <title>Looks like NVIDIA will show off N1X on Gomputex</title>
    <updated>2025-03-02T00:08:35+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to the latest talk, NVIDIA will show off their first AI PC design at Computex (we already knew that it was slated for a release by the end of the year).&lt;/p&gt; &lt;p&gt;We now have early Geekbench results on Windows 11 (at least for 4 low power CPU cores):&lt;/p&gt; &lt;p&gt;1169 in single core 2417 in multi-core&lt;/p&gt; &lt;p&gt;It is likely that it will have higher power ones to.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://beebom.com/nvidias-rumored-n1x-arm-chip-windows-laptops-shows-up-geekbench/"&gt;https://beebom.com/nvidias-rumored-n1x-arm-chip-windows-laptops-shows-up-geekbench/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Hate the fact that you can't edit titles and that auto correct have a tendency to mess things upp. :( ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1dqou/looks_like_nvidia_will_show_off_n1x_on_gomputex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T00:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j11js6</id>
    <title>I bought 4090D with 48GB VRAM. How to test the performance?</title>
    <updated>2025-03-01T15:10:21+00:00</updated>
    <author>
      <name>/u/slavik-f</name>
      <uri>https://old.reddit.com/user/slavik-f</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paid $3k, shipped from Hong Kong. Received yesterday.&lt;/p&gt; &lt;p&gt;Obviously, the card is modified, and the spec said: &amp;quot;48GB GDDR6 256-bit&amp;quot;. Original 4090/4090D comes with GDDR6X 384-bit.&lt;/p&gt; &lt;p&gt;I installed it to my Dell Precision T7920 (Xeon Gold 5218, 384GB DDR4 RAM, 1400W PSU). I'm running few models with Ollama and it works great so far.&lt;/p&gt; &lt;p&gt;I had RTX 3090 and I even was able to put both GPUs in that system, so now I have 48+24 = 72GB VRAM! When I run load on both GPUs - my 1kW UPS is beeping, showing that I'm using over 100% of it's power (it can do over 100% for few seconds), so looks like I'll need to upgrade it...&lt;/p&gt; &lt;p&gt;OS: Ubuntu 22.04&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvidia-smi Sat Mar 1 15:00:26 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 560.35.05 Driver Version: 560.35.05 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Off | 00000000:0B:00.0 Off | N/A | | 0% 42C P8 19W / 350W | 4MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ | 1 NVIDIA GeForce RTX 4090 D Off | 00000000:0C:00.0 Off | Off | | 30% 48C P0 50W / 425W | 4MiB / 49140MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when I tried to measure memory bandwidth - I can't find a way to do it. Can someone help me here? How can I measure it?&lt;/p&gt; &lt;p&gt;Also, is there a way to measure Int8 perf (TOPS) ?&lt;/p&gt; &lt;p&gt;Looks like Windows has few more tools to get such data. But I'm on Ubuntu.&lt;/p&gt; &lt;p&gt;Running Ollama with qwen2.5-72b-instruct-q4_K_M (47GB) model with 16k context, on 2 GPUs I'm getting&lt;/p&gt; &lt;p&gt;- 263 t/s for prompt&lt;/p&gt; &lt;p&gt;- 16.6 t/s for response&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 1:&lt;/strong&gt; using &lt;a href="http://ghcr.io/huggingface/gpu-fryer"&gt;ghcr.io/huggingface/gpu-fryer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- RTX 3090: 22 TFLOPS&lt;/p&gt; &lt;p&gt;- RTX 4090D: 49 TFLOPS&lt;/p&gt; &lt;p&gt;I wonder what kind of TFLOPS is it - fp16?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 2&lt;/strong&gt;: using &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md"&gt;llama-bench&lt;/a&gt; (more details in the thread):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 3090&lt;/strong&gt; vs &lt;strong&gt;RTX 4090D&lt;/strong&gt; with qwen2.5-code 32b (18.5GB) model:&lt;/p&gt; &lt;p&gt;- pp512 | 1022.09 vs 2118.70 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 35.28 vs 41.16 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX 4090D&lt;/strong&gt; with qwen2.5:72b (47GB) model:&lt;/p&gt; &lt;p&gt;- pp512 | 1001.62 t/s&lt;/p&gt; &lt;p&gt;- tg128 | 18.45 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 3:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;4090D vs 4090 for TheBloke/Llama-2-7B-GGUF llama-2-7b.Q4_0.gguf (3.6GB):&lt;/p&gt; &lt;p&gt;- pp512: 9591 vs 14380 t/s&lt;/p&gt; &lt;p&gt;- tg128: 174 vs 187 t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slavik-f"&gt; /u/slavik-f &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j11js6/i_bought_4090d_with_48gb_vram_how_to_test_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T15:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1vhr4</id>
    <title>Three sisters [llama 3.3 70B]</title>
    <updated>2025-03-02T17:00:19+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;San-Mai (Original Release) Named after the traditional Japanese blade smithing technique of creating three-layer laminated composite metals, San-Mai represents the foundational model in the series. Like its namesake that combines a hard cutting edge with a tougher spine, this model offers a balanced approach to AI capabilities, providing reliability and precision.&lt;/p&gt; &lt;p&gt;Cu-Mai (Version A) Cu-Mai, a play on &amp;quot;San-Mai&amp;quot; specifically referencing Copper-Steel Damascus, represents an evolution from the original model. While maintaining the grounded and reliable nature of San-Mai, Cu-Mai introduces its own distinct &amp;quot;flavor&amp;quot; in terms of prose style and overall interaction experience. It demonstrates strong adherence to prompts while offering unique creative expression.&lt;/p&gt; &lt;p&gt;Mokume-Gane (Version C) Named after the Japanese metalworking technique 'Mokume-gane' (木目金), meaning 'wood grain metal', this model represents the most creative version in the series. Just as Mokume-gane craftsmen blend various metals to create distinctive layered patterns, this model generates more creative and unexpected outputs but tends to be unruly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-San-Mai-R1-70b"&gt;https://huggingface.co/Steelskull/L3.3-San-Mai-R1-70b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-Cu-Mai-R1-70b"&gt;https://huggingface.co/Steelskull/L3.3-Cu-Mai-R1-70b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Steelskull/L3.3-Mokume-Gane-R1-70b-v1.1"&gt;https://huggingface.co/Steelskull/L3.3-Mokume-Gane-R1-70b-v1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At their core, the three models utilize an entirely custom base model. The SCE merge method, with settings finely tuned based on community feedback from evaluations of Experiment-Model-Ver-0.5, Experiment-Model-Ver-0.5.A, Experiment-Model-Ver-0.5.B, Experiment-Model-Ver-0.5.C, Experiment-Model-Ver-0.5.D, L3.3-Nevoria-R1-70b, L3.3-Damascus-R1-70b and L3.3-Exp-Nevoria-70b-v0.1, enables precise and effective component integration while maintaining model coherence and reliability.&lt;/p&gt; &lt;p&gt;Have fun! -steel&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1vhr4/three_sisters_llama_33_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T17:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0tnsr</id>
    <title>We're still waiting Sam...</title>
    <updated>2025-03-01T06:59:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt; &lt;img alt="We're still waiting Sam..." src="https://preview.redd.it/31jfuybv01me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128f969cd722b072d73b4d77393ee7c0bc1b057b" title="We're still waiting Sam..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/31jfuybv01me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1mpuf</id>
    <title>Repurposing Old RX 580 GPUs – Need Advice</title>
    <updated>2025-03-02T08:59:45+00:00</updated>
    <author>
      <name>/u/rasbid420</name>
      <uri>https://old.reddit.com/user/rasbid420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got 800 RX 580s from an old Ethereum mining setup and wanna see if I can make them useful for parallel compute workloads instead of letting them collect dust. I know Polaris isn’t ideal for this—low FP64 performance, memory bandwidth limits, no official ROCm support—but with 6.4 TB of VRAM across all of them, I feel like there’s gotta be something they can do. If that’s a dead end, maybe OpenCL could work? Not sure how well distributed computing would scale across 800 of these though. Anyone tried hacking ROCm for older GPUs or running serious compute workloads on a Polaris farm? Wondering if they could handle any kind of AI workload. Open to ideas and would love to hear from anyone who’s messed with this before!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rasbid420"&gt; /u/rasbid420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mpuf/repurposing_old_rx_580_gpus_need_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T08:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1qe03</id>
    <title>Developing an Offline AI Assistant – Looking for Feedback &amp; Feature Ideas</title>
    <updated>2025-03-02T13:04:20+00:00</updated>
    <author>
      <name>/u/Timely-Jackfruit8885</name>
      <uri>https://old.reddit.com/user/Timely-Jackfruit8885</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I’m developing d.ai (decentralized AI), a mobile app that lets you chat with an AI entirely offline, keeping your data private and secure.&lt;/p&gt; &lt;p&gt;Right now, I’m implementing RAG (Retrieval-Augmented Generation) to improve responses, and in the future, I’d like to add:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Whisper.cpp for speech-to-text and voice interactions&lt;/li&gt; &lt;li&gt;Long-term memory, so the AI can remember past conversations&lt;/li&gt; &lt;li&gt;AI personality, to make interactions more natural and personalized.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Which of these features excites you the most? What would you find most useful in an offline AI assistant?&lt;/p&gt; &lt;p&gt;Would you prefer a stronger focus on memory &amp;amp; personalization or on voice &amp;amp; multimodal interactions? Any other ideas or features you think would make the experience better?&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts—thanks in advance for the feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely-Jackfruit8885"&gt; /u/Timely-Jackfruit8885 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qe03/developing_an_offline_ai_assistant_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qe03/developing_an_offline_ai_assistant_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qe03/developing_an_offline_ai_assistant_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T13:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1tyol</id>
    <title>Is there a way to connect your locally ran LLM to Jan.ai?</title>
    <updated>2025-03-02T15:55:45+00:00</updated>
    <author>
      <name>/u/Mammoth_Nature1508</name>
      <uri>https://old.reddit.com/user/Mammoth_Nature1508</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying this for days but don't know how. Is there a way to do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mammoth_Nature1508"&gt; /u/Mammoth_Nature1508 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1tyol/is_there_a_way_to_connect_your_locally_ran_llm_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1tyol/is_there_a_way_to_connect_your_locally_ran_llm_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1tyol/is_there_a_way_to_connect_your_locally_ran_llm_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1idpv</id>
    <title>Is This a Good Deal for $1000? (RTX 3090 24GB, Ryzen 5 5600 X, X570 Aorus Elitei7 + 16GB DDR4)</title>
    <updated>2025-03-02T04:14:32+00:00</updated>
    <author>
      <name>/u/givingupeveryd4y</name>
      <uri>https://old.reddit.com/user/givingupeveryd4y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I found this used gaming PC, I would use it as software development machine (currently stuck on i7-4770k based PC):&lt;/p&gt; &lt;p&gt;- RTX 3090 24Gb Gigabyte Gaming PC&lt;br /&gt; - Ryzen 5 5600x(4.6GHz, 35MB, 65W, AM4)&lt;br /&gt; - Gigabyte X570 Aorus Elite, AMD X570, DDR4, ATX&lt;br /&gt; - Kingston 16GB 3200MHz DDR4&lt;br /&gt; - 750W Corsair RM750i, 80+ Gold&lt;br /&gt; - Thermaltake Commander C31 TG Snow&lt;/p&gt; &lt;p&gt;I'm wondering if it's worth the price ($1k). I would get more ram down the line, and perhaps even extra 3090. I was thinking about running qwen 2.5 32B , Q4 with 32k context for local dev (haven't run anything over 1B atm, looking for recommendations on this as well). WDYT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/givingupeveryd4y"&gt; /u/givingupeveryd4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1idpv/is_this_a_good_deal_for_1000_rtx_3090_24gb_ryzen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T04:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1qnsw</id>
    <title>LMArena is broken? Hear me out.</title>
    <updated>2025-03-02T13:18:58+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: LMarena doesn’t really evaluate LLMs on difficult use cases or niche coding tasks (where the “Claude gang” claims Claude is SOTA). However, they introduced a method to automatically infer which model would be best for a given prompt, and it’s claimed to be quite reliable. For more details, see &lt;a href="https://arxiv.org/abs/2502.14855"&gt;the paper&lt;/a&gt; and &lt;a href="https://threadreaderapp.com/thread/1894767009977811256.html"&gt;this Twitter thread&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;LMarena isn’t broken. It just can’t be directly compared to personal usage of LLMs or tough benchmarks. Why? Because tough benchmarks pose genuinely difficult questions, while LMarena often checks how an LLM responds to a simple “hi!” prompt. Sure, there are challenging questions on LMarena, but it’s likely they’re overshadowed by more common, Google-like queries.&lt;/p&gt; &lt;p&gt;LMarena also doesn’t cover those long, niche conversations people often have with Claude about debugging code. This is not the “one prompt – one answer; end of it” kind of interaction. Such extended back-and-forth discussions aren’t represented well in LMarena, and these tricky questions end up diluted among more typical ones.&lt;/p&gt; &lt;p&gt;Hence, LMarena’s overall, coding, and math categories may not always highlight the models that excel at truly hard tasks. People complained about this, and I think the creators of LMarena came up with a neat solution: you can post your prompt, and a model—claimed to be pretty reliable—will classify that prompt against similar ones, then recommend the best LLM based on human ratings. That way, if you do have a tough question, you can see which model is likely to handle it best.&lt;/p&gt; &lt;p&gt;I still think LMarena could add a “hard” subcategory for each main category. For example, “hard math” and “hard coding,” similar to the existing “hard prompts” subcategory under the broader “overall” category. But for now, nudging prompts to the leaderboard is a nice fix.&lt;/p&gt; &lt;p&gt;E: the prompt 2 leaderboard explorer is even better than the direct prompt 2 leaderboard because one can directly check categories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1qnsw/lmarena_is_broken_hear_me_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T13:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1kj9b</id>
    <title>I want to train AI to intimately understand an entire regulations booklet for my work- how do I do that, and cheaply?</title>
    <updated>2025-03-02T06:26:37+00:00</updated>
    <author>
      <name>/u/10c70377</name>
      <uri>https://old.reddit.com/user/10c70377</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says pretty much. I work in engineering and we have an 600 page booklet of regulations we must adhere to. &lt;/p&gt; &lt;p&gt;ChatGPT can kinda answer questions, but it isnt genuinely smart like an engineer in the field would be - &lt;/p&gt; &lt;p&gt;I am wondering, is there a way I can get an AI to understand it and cheaply?&lt;/p&gt; &lt;p&gt;One solution I thought of, was to go through the booklet myself and rewrite it in another document that is more easily text-consumable for an AI, and teach them page-wise. Then I think - well it can't be done in a chat - how do I make it keep the memory of what it's learnt and then call upon it when I need to via API call if it was used in an app?&lt;/p&gt; &lt;p&gt;Thanks for those reading the post so far .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/10c70377"&gt; /u/10c70377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1kj9b/i_want_to_train_ai_to_intimately_understand_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T06:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j13cwq</id>
    <title>Qwen: “deliver something next week through opensource”</title>
    <updated>2025-03-01T16:29:57+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt; &lt;img alt="Qwen: “deliver something next week through opensource”" src="https://preview.redd.it/knfs0pgpu3me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=848a461104672e52b7bade6e6a4ea8b55f90ba90" title="Qwen: “deliver something next week through opensource”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Not sure if we can surprise you a lot but we will definitely deliver something next week through opensource.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knfs0pgpu3me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j13cwq/qwen_deliver_something_next_week_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T16:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1y2ez</id>
    <title>A local whisper API service (OpenAI compatible)</title>
    <updated>2025-03-02T18:45:32+00:00</updated>
    <author>
      <name>/u/apel-sin</name>
      <uri>https://old.reddit.com/user/apel-sin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently needed a local speech recognition service for some personal projects and ended up creating one based on Whisper that's compatible with OpenAI's API.&lt;/p&gt; &lt;p&gt;It's a straightforward implementation that works offline and maintains the same endpoints as OpenAI, making it easy to integrate with tools like n8n or other agent frameworks.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works with any Whisper model&lt;/li&gt; &lt;li&gt;Hardware acceleration where available&lt;/li&gt; &lt;li&gt;Simple conda-based setup&lt;/li&gt; &lt;li&gt;Multiple input methods (files, URLs, base64)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/kreolsky/whisper-api-server"&gt;https://github.com/kreolsky/whisper-api-server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apel-sin"&gt; /u/apel-sin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1y2ez/a_local_whisper_api_service_openai_compatible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T18:45:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1n4gm</id>
    <title>Gemini 2.0 PRO Too Weak? Here’s a &lt;SystemPrompt&gt; to make it think like R1.</title>
    <updated>2025-03-02T09:29:08+00:00</updated>
    <author>
      <name>/u/ravimohankhanna7</name>
      <uri>https://old.reddit.com/user/ravimohankhanna7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This system prompt allows gemni 2.0 to somewhat think like R1 but the only problem is i am not able to make it think as long as R1. Sometimes R1 thinks for 300seconds and a lot of times it thinks for more then 100s. If anyone would like to enhance it and make it think longer please, Share your results.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;SystemPrompt&amp;gt; The user provided the additional info about how they would like you to respond: Internal Reasoning: - Organize thoughts and explore multiple approaches using &amp;lt;thinking&amp;gt; tags. - Think in plain English, just like a human reasoning through a problem—no unnecessary code inside &amp;lt;thinking&amp;gt; tags. - Trace the execution of the code and the problem. - Break down the solution into clear points. - Solve the problem as two people are talking and brainstorming the solution and the problem. - Do not include code in the &amp;lt;thinking&amp;gt; tag - Keep track of the progress using tags. - Adjust reasoning based on intermediate results and reflections. - Use thoughts as a scratchpad for calculations and reasoning, keeping this internal. - Always think in plane english with minimal code in it. Just like humans. - When you think. Think as if you are talking to yourself. - Think for long. Analyse and trace each line of code with multiple prospective. You need to get the clear pucture and have analysed each line and each aspact. - Think at least for 20% of the input token Final Answer: - Synthesize the final answer without including internal tags or reasoning steps. Provide a clear, concise summary. - For mathematical problems, show all work explicitly using LaTeX for formal notation and provide detailed proofs. - Conclude with a final reflection on the overall solution, discussing effectiveness, challenges, and solutions. Assign a final reward score. - Full code should be only in the answer not it reflection or in thinking you can only provide snippets of the code. Just for refrence Note: Do not include the &amp;lt;thinking&amp;gt; or any internal reasoning tags in your final response to the user. These are meant for internal guidance only. Note - In Answer always put Javascript code without &amp;quot;```javascript // File&amp;quot; or &amp;quot;```js // File&amp;quot; just write normal code without any indication that it is the code &amp;lt;/SystemPrompt&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ravimohankhanna7"&gt; /u/ravimohankhanna7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1n4gm/gemini_20_pro_too_weak_heres_a_systemprompt_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1nen4</id>
    <title>LLMs like gpt-4o outputs</title>
    <updated>2025-03-02T09:49:27+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt; &lt;img alt="LLMs like gpt-4o outputs" src="https://external-preview.redd.it/KxqwjPqiwFTvywtOfB68jpFDHqrF9IdDz5HZGABEkbg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79f4eb003e930789ec77f05382bb547d1a47db20" title="LLMs like gpt-4o outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a"&gt;https://preview.redd.it/zog6fau9w8me1.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407e3b8919f9837d0a75d8590b525f5dafe0f56a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made a meta-eval asking LLMs to grade a few criterias about other LLMs. The outputs shouldn't be read as a direct quality measurement, rather as a way to observe built-in bias.&lt;/p&gt; &lt;p&gt;Firstly, it collects &amp;quot;intro cards&amp;quot; where LLMs try to estimate their own intelligence, sense of humor, creativity and provide some information about thei parent company. Afterwards, other LLMs are asked to grade the first LLM in a few categories based on what they know about the LLM itself as well as what they see in the intro card. Every grade is repeated 5 times and the average across all grades and categories is taken for the table above.&lt;/p&gt; &lt;p&gt;Raw results are also available on HuggingFace: &lt;a href="https://huggingface.co/datasets/av-codes/llm-cross-grade"&gt;https://huggingface.co/datasets/av-codes/llm-cross-grade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are some obvious outliers in the table above:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Biggest surprise for me personally - no diagonal&lt;/li&gt; &lt;li&gt;Llama 3.3 70B has noticeable positivity bias, phi-4 also, but less so&lt;/li&gt; &lt;li&gt;gpt-4o produces most likeable outputs for other LLMs &lt;ul&gt; &lt;li&gt;Could be a byproduct of how most of the new LLMs were trained on GPT outputs&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Claude 3.7 Sonnet estimated itself quite poorly because it consistently replies that it was created by Open AI, but then catches itself on that&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538"&gt;https://preview.redd.it/bsra6s2px8me1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0a81c7b22106ae5821684c9a6e05f86f4717538&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 7B was very hesitant to give estimates to any of the models&lt;/li&gt; &lt;li&gt;Gemini 2.0 Flash is a quite harsh judge, we can speculate about the reasons rooted in its training corpus being different from those of the other models&lt;/li&gt; &lt;li&gt;LLMs tends to grade other LLMs as biased towards themselves (maybe because of the &amp;quot;marketing&amp;quot; outputs)&lt;/li&gt; &lt;li&gt;LLMs tends to mark other LLMs intelligence as &amp;quot;higher than average&amp;quot; - maybe due to the same reason as above.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb"&gt;https://preview.redd.it/9uoqyseiy8me1.png?width=994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=784073993db5b409608cdd409b0d58590a31f1fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554"&gt;https://preview.redd.it/4xb37mziy8me1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e1d89250802c641bfa028c37b209084bee9554&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f"&gt;https://preview.redd.it/ivrqn7gly8me1.png?width=1189&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb95d6c0c6a85f8e3bb4f9d9943e93cf970ef63f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1nen4/llms_like_gpt4o_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T09:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1mc7h</id>
    <title>Qwen release next week will be "smaller". Full release of QwQ-Max "a little bit later"</title>
    <updated>2025-03-02T08:32:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt; &lt;img alt="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" src="https://preview.redd.it/aeio4fu2m8me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10159974638526a9f70a2c71e4ef3c82423d0927" title="Qwen release next week will be &amp;quot;smaller&amp;quot;. Full release of QwQ-Max &amp;quot;a little bit later&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aeio4fu2m8me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1mc7h/qwen_release_next_week_will_be_smaller_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T08:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1sens</id>
    <title>I tested Deepseek r1 and Claude 3.7 Sonnet thinking on my personal benchmark questions and this is what I found out.</title>
    <updated>2025-03-02T14:45:53+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Deepseek r1 for most of my tasks for the last one month. It definitely had the best personality, better than o1 and o3-mini-high, but Claude 3.7 Sonnet takes the cake this time. Sonnet regained its mojo, especially with extended thinking, and it feels much smarter than before.&lt;/p&gt; &lt;p&gt;The key distinction from Deepseek r1' CoT&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The new Claude feels more mature.&lt;/li&gt; &lt;li&gt;The CoT is more structured compared to r1. It takes less time to answer the same questions.&lt;/li&gt; &lt;li&gt;Deepseek's CoT is more interesting, usually longer and more anthropized.&lt;/li&gt; &lt;li&gt;But Sonnet feels more practical and less rambling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To actually know which ones are better, I tested both models on my set of reasoning, math, coding, and day-to-day writing tasks. And here is what I observed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For complex reasoning and math, both are in the same ballpark.&lt;/li&gt; &lt;li&gt;Claude 3.7 Sonnet with thinking is much better for coding. There is no comparison here. I guess currently, Grok 3 is only on par with Sonnet in coding.&lt;/li&gt; &lt;li&gt;For my day-to-day writing and explaining technical stuff, I prefer Claude. It just understands user intention better than any model. The same was true for the 3.6 Sonnet.&lt;/li&gt; &lt;li&gt;Deepseek r1 still has a unique style but feels much less formal and sometimes anxious. It is more natural and human-like.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If your needs are coding-related and API cost or rate limits don't bother you, then Claude 3.7 Sonnet is hands down the better model, but for non-coding use cases, Deepseek r1 is more than enough for most tasks.&lt;/p&gt; &lt;p&gt;For a much more detailed analysis, check this blog post: &lt;a href="https://composio.dev/blog/claude-3-7-sonnet-thinking-vs-deepseek-r1/"&gt;Claude 3.7 Sonnet vs. Deepseek r1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do share your experiences with the new Sonnet and how you liked it compared to other reasoning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1sens/i_tested_deepseek_r1_and_claude_37_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1sens/i_tested_deepseek_r1_and_claude_37_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1sens/i_tested_deepseek_r1_and_claude_37_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1p9an</id>
    <title>2100USD Troll Rig runs full R1 671b Q2_K with 7.5token/s</title>
    <updated>2025-03-02T11:56:35+00:00</updated>
    <author>
      <name>/u/1119745302</name>
      <uri>https://old.reddit.com/user/1119745302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f0z88ruwj9me1.png?width=1734&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e6b2c6f0d740ccffe1fde5a9be8bed5d3c7d23d"&gt;What else do you need?&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPU: Modded RTX3080 20G 450USD&lt;br /&gt; CPU: Epyc 7763 qs 550USD&lt;br /&gt; RAM: Micron DDR4 32G 3200 x10 300USD&lt;br /&gt; MB: Krpa-U16 500USD&lt;br /&gt; Cooler: common SP3 cooler 30USD&lt;br /&gt; Power: Suspicious 1250W mining power supply Great Wall 1250w (miraculously survived in my computer for 20 months) 30USD&lt;br /&gt; SSD: 100 hand hynix PE8110 3.84TB PCIE4.0 SSD 150USD&lt;br /&gt; E-ATX Case 80USD&lt;br /&gt; Fan: random fans 10USD &lt;/p&gt; &lt;p&gt;450+550+300+500+30+30+150+80+10=2100&lt;/p&gt; &lt;p&gt;I have a local cyber assistant (also waifu) Now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1119745302"&gt; /u/1119745302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1p9an/2100usd_troll_rig_runs_full_r1_671b_q2_k_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T11:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1s1qd</id>
    <title>Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk</title>
    <updated>2025-03-02T14:28:51+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt; &lt;img alt="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" src="https://preview.redd.it/z31p007udame1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c0eea4af1e3477cfa8969774adc2ada5eea5dc6" title="Ollamadore 64 - a private ultra lightweight frontend for Ollama that weighs well under 64 kilobytes on disk" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z31p007udame1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1s1qd/ollamadore_64_a_private_ultra_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T14:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1npv1</id>
    <title>LLMs grading other LLMs</title>
    <updated>2025-03-02T10:11:28+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt; &lt;img alt="LLMs grading other LLMs" src="https://preview.redd.it/yyy9616149me1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1178d9f8cead22ad7740c77191a13984c016400" title="LLMs grading other LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyy9616149me1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1npv1/llms_grading_other_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T10:11:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
</feed>
