<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-13T04:48:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hzfjmp</id>
    <title>Parking Systems analysis and Report Generation with Computer vision and Ollama</title>
    <updated>2025-01-12T05:15:40+00:00</updated>
    <author>
      <name>/u/oridnary_artist</name>
      <uri>https://old.reddit.com/user/oridnary_artist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt; &lt;img alt="Parking Systems analysis and Report Generation with Computer vision and Ollama " src="https://external-preview.redd.it/ZDUxcHMwOGt5aGNlMZNdfj6QUni_z9Bf_NJiTzUymfkgPwnfSrss06zjR7A1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f892ce79dd89dd0ae0171dc7ac8e70e942f8504" title="Parking Systems analysis and Report Generation with Computer vision and Ollama " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oridnary_artist"&gt; /u/oridnary_artist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2tf8yz7kyhce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzfjmp/parking_systems_analysis_and_report_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T05:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzsvr5</id>
    <title>What are the current best low spec LLMs</title>
    <updated>2025-01-12T18:13:22+00:00</updated>
    <author>
      <name>/u/tuxPT</name>
      <uri>https://old.reddit.com/user/tuxPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I'm looking either for advice or a benchmark with the best low spec LLMs. I define low spec as any llm that can run locally in a mobile device or in low spec laptop(integrated GPU+8/12gb ram).&lt;/p&gt; &lt;p&gt;As for tasks, mainly text transformation or questions about the text. No translation needed, the input and output would be in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuxPT"&gt; /u/tuxPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvr5/what_are_the_current_best_low_spec_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T18:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmljb</id>
    <title>In the Terminator's vision overlay, the "ANALYSIS" is probably the image embedding ðŸ¤”</title>
    <updated>2025-01-12T13:25:15+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"&gt; &lt;img alt="In the Terminator's vision overlay, the &amp;quot;ANALYSIS&amp;quot; is probably the image embedding ðŸ¤”" src="https://preview.redd.it/b3if9y5tdkce1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cdf7470becd28e71636379b90a99d0da4fd0cc3" title="In the Terminator's vision overlay, the &amp;quot;ANALYSIS&amp;quot; is probably the image embedding ðŸ¤”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b3if9y5tdkce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmljb/in_the_terminators_vision_overlay_the_analysis_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0182p</id>
    <title>Anyone worked with distributed inference on Llama.cpp?</title>
    <updated>2025-01-13T00:17:19+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have it sort of working with:&lt;br /&gt; build-rpc-cuda/bin/rpc-server -p 7000 (on the first gpu rig)&lt;br /&gt; build-rpc-cuda/bin/rpc-server -p 7001 (on the second gpu rig)&lt;br /&gt; build-rpc/bin/llama-cli -m ../model.gguf -p &amp;quot;Hello, my name is&amp;quot; --repeat-penalty 1.0 -n 64 --rpc 127.0.0.1:7000,127.0.0.1:7001 -ngl 99 &lt;/p&gt; &lt;p&gt;This does distributed inference across the 2 machines, but I'm having to reload the entire model for each query. &lt;/p&gt; &lt;p&gt;I skimmed through the llama-cli -h and didn't see a way to make it keep the model loaded, or listen for connections instead of directly doing inference inside the command line.&lt;/p&gt; &lt;p&gt;Also skimmed though llama-server, which would allow keeping the model loaded and hosting an api, but doesn't appear to support RPC servers.&lt;/p&gt; &lt;p&gt;I assume I am missing something right?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md"&gt;https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/rpc"&gt;https://github.com/ggerganov/llama.cpp/tree/master/examples/rpc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0182p/anyone_worked_with_distributed_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0182p/anyone_worked_with_distributed_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0182p/anyone_worked_with_distributed_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T00:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz97my</id>
    <title>they donâ€™t know how good gaze detection is on moondream</title>
    <updated>2025-01-11T23:38:28+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt; &lt;img alt="they donâ€™t know how good gaze detection is on moondream" src="https://external-preview.redd.it/anBia3RnaGhhZ2NlMSTi0DO1FtxEm4mYFQVOtZR8uuj4lv59wjB_E-Pc4Mjr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3206ad0d968f5e06525f4113c574566e35551fb1" title="they donâ€™t know how good gaze detection is on moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xgysp5nhagce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz97my/they_dont_know_how_good_gaze_detection_is_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T23:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzz44r</id>
    <title>SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution</title>
    <updated>2025-01-12T22:38:39+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzz44r/swefixer_training_opensource_llms_for_effective/"&gt; &lt;img alt="SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution " src="https://external-preview.redd.it/7IU0NPKIyIxtqFcMeMTaEMBvLbn4HjBzM2rLWrfl5cg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7ff734462a81c0a8142d7d1baafa4f4fe72ac1c" title="SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/InternLM/SWE-Fixer"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzz44r/swefixer_training_opensource_llms_for_effective/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzz44r/swefixer_training_opensource_llms_for_effective/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T22:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i05sl8</id>
    <title>Janus goes off the rails if you say hello after asking it to generate an image</title>
    <updated>2025-01-13T04:17:19+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i05sl8/janus_goes_off_the_rails_if_you_say_hello_after/"&gt; &lt;img alt="Janus goes off the rails if you say hello after asking it to generate an image" src="https://preview.redd.it/u2znn5m2toce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=650f2adc5982d20ab8b754318ef2a625a0c38f10" title="Janus goes off the rails if you say hello after asking it to generate an image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u2znn5m2toce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i05sl8/janus_goes_off_the_rails_if_you_say_hello_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i05sl8/janus_goes_off_the_rails_if_you_say_hello_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T04:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i04pm9</id>
    <title>What is the cheapest way to run Deepseek on a US Hosted company?</title>
    <updated>2025-01-13T03:17:22+00:00</updated>
    <author>
      <name>/u/MarsupialNo7544</name>
      <uri>https://old.reddit.com/user/MarsupialNo7544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a bit concerned about the privacy policies- especially considering PII data. I love how DeepSeek pricing is on their website- but has anyone tried to load their model in a service provider and see what costing structure works? if so, would like to hear more. thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarsupialNo7544"&gt; /u/MarsupialNo7544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T03:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzsvkz</id>
    <title>Volo: An easy and local way to RAG with Wikipedia!</title>
    <updated>2025-01-12T18:13:08+00:00</updated>
    <author>
      <name>/u/procraftermc</name>
      <uri>https://old.reddit.com/user/procraftermc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt; &lt;img alt="Volo: An easy and local way to RAG with Wikipedia!" src="https://external-preview.redd.it/fpG2F-y2fJG3IOgvl1H5IK_WS4ZurjXL_2_4lQMcpvY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b4fece65aa394b3776194c084c3d99fcc5693c8" title="Volo: An easy and local way to RAG with Wikipedia!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the biggest problems with AI models is their tendency to hallucinate. This project aims to fix that by giving them access to an offline copy of Wikipedia (about 57 GB)&lt;/p&gt; &lt;p&gt;It uses a copy of Wikipedia created by Kiwix as the offline database and Qwen2.5:3B as the LLM.&lt;/p&gt; &lt;p&gt;Install instructions are on the Github: &lt;a href="https://github.com/AdyTech99/volo/"&gt;https://github.com/AdyTech99/volo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ye31knzzslce1.png?width=3015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f15216c70371d13352667b4ddce95e3b57e1ffc5"&gt;Example of Volo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/procraftermc"&gt; /u/procraftermc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T18:13:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1i01j4k</id>
    <title>PSA: You can use Ollama to generate your git commit messages locally</title>
    <updated>2025-01-13T00:32:18+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using git commit hooks you can ask any model from Ollama to generate a git commit message for you:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/usr/bin/env sh # .git/hooks/prepare-commit-msg # Make this file executable: chmod +x .git/hooks/prepare-commit-msg echo &amp;quot;Running prepare-commit-msg hook&amp;quot; COMMIT_MSG_FILE=&amp;quot;$1&amp;quot; # Get the staged diff DIFF=$(git diff --cached) # Generate a summary with ollama CLI and phi4 model SUMMARY=$( ollama run phi4 &amp;lt;&amp;lt;EOF Generate a raw text commit message for the following diff. Keep commit message concise and to the point. Make the first line the title (100 characters max) and the rest the body: $DIFF EOF ) if [ -f &amp;quot;$COMMIT_MSG_FILE&amp;quot; ]; then # Save the AI generated summary to the commit message file echo &amp;quot;$SUMMARY&amp;quot; &amp;gt;&amp;quot;$COMMIT_MSG_FILE&amp;quot; # Append existing message if it exists if [ -n &amp;quot;$EXISTING_MSG&amp;quot; ]; then echo &amp;quot;&amp;quot; &amp;gt;&amp;gt;&amp;quot;$COMMIT_MSG_FILE&amp;quot; echo &amp;quot;$EXISTING_MSG&amp;quot; &amp;gt;&amp;gt;&amp;quot;$COMMIT_MSG_FILE&amp;quot; fi fi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use tools like &lt;a href="https://github.com/mohsen1/yek"&gt;yek&lt;/a&gt; to put the entire repo plus the changes in the prompt to give the model more context for better messages&lt;/p&gt; &lt;p&gt;You can also cap the maximum time this should take with &lt;code&gt;--keep-alive&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01j4k/psa_you_can_use_ollama_to_generate_your_git/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01j4k/psa_you_can_use_ollama_to_generate_your_git/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i01j4k/psa_you_can_use_ollama_to_generate_your_git/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T00:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzany5</id>
    <title>We are an AI company now!</title>
    <updated>2025-01-12T00:47:37+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt; &lt;img alt="We are an AI company now!" src="https://preview.redd.it/0yl0970umgce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53963d0db45722eea8467f27c91ca48e5a7cf6fc" title="We are an AI company now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0yl0970umgce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzany5/we_are_an_ai_company_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T00:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i004dd</id>
    <title>Whatâ€™s likely for Llama4?</title>
    <updated>2025-01-12T23:24:25+00:00</updated>
    <author>
      <name>/u/SocialDinamo</name>
      <uri>https://old.reddit.com/user/SocialDinamo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So with all the breakthroughs and changing opinions since Llama 3 dropped back in July, Iâ€™ve been wonderingâ€”whatâ€™s Meta got cooking next?&lt;/p&gt; &lt;p&gt;Not trying to make this a low-effort post, Iâ€™m honestly curious. Anyone heard any rumors or have any thoughts on where they might take the Llama series from here?&lt;/p&gt; &lt;p&gt;Would love to hear what yâ€™all think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SocialDinamo"&gt; /u/SocialDinamo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i004dd/whats_likely_for_llama4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i004dd/whats_likely_for_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i004dd/whats_likely_for_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T23:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzn5b6</id>
    <title>Forget AI waifus. Are there local AI assistants to increase my productivity?</title>
    <updated>2025-01-12T13:54:45+00:00</updated>
    <author>
      <name>/u/-oshino_shinobu-</name>
      <uri>https://old.reddit.com/user/-oshino_shinobu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title suggests, lots of lonely men out there looking to fine tune their own AI gf. But I really just want an AI secretary who can help me make plans, trivial tasks like respond to messages/emails, and generally increase my productivity.&lt;/p&gt; &lt;p&gt;What model do you guys suggest? I assume itâ€™ll need huge context length to fit enough data about me? Also hoping thereâ€™s a way to make AI periodically text me and give me updates. I have 48GB of vram to spare for this LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-oshino_shinobu-"&gt; /u/-oshino_shinobu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzn5b6/forget_ai_waifus_are_there_local_ai_assistants_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzwun4</id>
    <title>Current best local models for companionship? for random small talk for lonely people</title>
    <updated>2025-01-12T21:01:10+00:00</updated>
    <author>
      <name>/u/MasterScrat</name>
      <uri>https://old.reddit.com/user/MasterScrat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Asking for a friend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterScrat"&gt; /u/MasterScrat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzwun4/current_best_local_models_for_companionship_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzwun4/current_best_local_models_for_companionship_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzwun4/current_best_local_models_for_companionship_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1i04oze</id>
    <title>Updated Vector Companion to include multi-agent chatting, increasing the number of agents to 4 in Chat Mode and 1 in Analysis Mode.</title>
    <updated>2025-01-13T03:16:26+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04oze/updated_vector_companion_to_include_multiagent/"&gt; &lt;img alt="Updated Vector Companion to include multi-agent chatting, increasing the number of agents to 4 in Chat Mode and 1 in Analysis Mode." src="https://external-preview.redd.it/eXZzenFnOThpb2NlMdotc4n4PuHiprd-_0UOPMgBjCQcLOw7jb0_FYlyDNdR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30af9a7e5c418cf45db10becc6d162fe946c0621" title="Updated Vector Companion to include multi-agent chatting, increasing the number of agents to 4 in Chat Mode and 1 in Analysis Mode." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4824oh98ioce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04oze/updated_vector_companion_to_include_multiagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i04oze/updated_vector_companion_to_include_multiagent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T03:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzyjsj</id>
    <title>Search-o1: Agentic Search-Enhanced Large Reasoning Models - Renmin University of China</title>
    <updated>2025-01-12T22:13:56+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://search-o1.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyjsj/searcho1_agentic_searchenhanced_large_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyjsj/searcho1_agentic_searchenhanced_large_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T22:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz28ld</id>
    <title>Bro whaaaat?</title>
    <updated>2025-01-11T18:24:57+00:00</updated>
    <author>
      <name>/u/Specter_Origin</name>
      <uri>https://old.reddit.com/user/Specter_Origin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt; &lt;img alt="Bro whaaaat?" src="https://preview.redd.it/cwi5l2ziqece1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6895d12163dd294798940a5c5b6368da7f91b2f" title="Bro whaaaat?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specter_Origin"&gt; /u/Specter_Origin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwi5l2ziqece1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz28ld/bro_whaaaat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T18:24:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzyy6h</id>
    <title>I forbade a model from using its own token predictions to choose the next word â€“ QwQ 32b is adorably freaking out sometimes</title>
    <updated>2025-01-12T22:31:24+00:00</updated>
    <author>
      <name>/u/Shir_man</name>
      <uri>https://old.reddit.com/user/Shir_man</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyy6h/i_forbade_a_model_from_using_its_own_token/"&gt; &lt;img alt="I forbade a model from using its own token predictions to choose the next word â€“ QwQ 32b is adorably freaking out sometimes" src="https://external-preview.redd.it/NWtucDc0aGUzbmNlMQJW0EyliUT-_L3Ce_WpeK6JjB__h_GT9gcIW1SOMMne.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b5787ef88074f1be6511276d768068b8a99742a" title="I forbade a model from using its own token predictions to choose the next word â€“ QwQ 32b is adorably freaking out sometimes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I set up a small experiment with QwQ-32B-Preview, a model known for its ability to reason and follow instructions. The idea was simple: it had to predict its next word without being allowed to rely on its own predictions as an LLM&lt;/p&gt; &lt;p&gt;The model started in confusion but soon shifted into self-analysis, hypothesis testing, and even philosophical contemplation. It was like watching it wrestle with its own constraints, occasionally freaking out in the most adorable ways.&lt;/p&gt; &lt;p&gt;Here is a link with an experiment: &lt;a href="https://shir-man.com/amibroken/"&gt;https://shir-man.com/amibroken/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shir_man"&gt; /u/Shir_man &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0sijc7ke3nce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyy6h/i_forbade_a_model_from_using_its_own_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyy6h/i_forbade_a_model_from_using_its_own_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T22:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i04iqo</id>
    <title>PS5 for inference</title>
    <updated>2025-01-13T03:07:15+00:00</updated>
    <author>
      <name>/u/Chemical_Mode2736</name>
      <uri>https://old.reddit.com/user/Chemical_Mode2736</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For ~$350 for the whole system is there anything better? This thing packs 3060-tier tflops, 16gb unified gddr6 with ~450gbps bandwidth with 350W PSU. not to mention that this sits in so many people's living rooms, I'm not using any llms while gaming anyways, so PS5 could actually be dual purpose.&lt;/p&gt; &lt;p&gt;Currently looking into how I could run llms on PS5, if anyone has any leads let me know.&lt;/p&gt; &lt;p&gt;I wasn't aware that systems with unified ram using gddr actually existed, let alone that amd did it 5 years ago and so they could release their own DIGITS based on strix halo but with vram instead of ddr...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical_Mode2736"&gt; /u/Chemical_Mode2736 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T03:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1i02hpf</id>
    <title>Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints</title>
    <updated>2025-01-13T01:20:17+00:00</updated>
    <author>
      <name>/u/fedirz</name>
      <uri>https://old.reddit.com/user/fedirz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt; &lt;img alt="Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints" src="https://b.thumbs.redditmedia.com/mOOa_qpBkwYizQvxqZvc7QQ7OW8ESL1W7rDx6Ym6TPM.jpg" title="Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just released Speaches v0.6.0 (previously named &lt;code&gt;faster-whisper-server&lt;/code&gt;). The main feature added in this release is support for Piper and Kokoro Text-to-Speech models. Below is a full feature list:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU and CPU support.&lt;/li&gt; &lt;li&gt;&lt;a href="https://speaches-ai.github.io/speaches/installation/"&gt;Deployable via Docker Compose / Docker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://speaches-ai.github.io/speaches/configuration/"&gt;Highly configurable&lt;/a&gt;&lt;/li&gt; &lt;li&gt;OpenAI API compatible. All tools and SDKs that work with OpenAI's API should work with &lt;code&gt;speaches&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Streaming support (transcription is sent via SSE as the audio is transcribed. You don't need to wait for the audio to fully be transcribed before receiving it). &lt;ul&gt; &lt;li&gt;LocalAgreement2 (&lt;a href="https://aclanthology.org/2023.ijcnlp-demo.3.pdf"&gt;paper&lt;/a&gt; | &lt;a href="https://github.com/ufal/whisper_streaming"&gt;original implementation&lt;/a&gt;) algorithm is used for live transcription.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Live transcription support (audio is sent via WebSocketbe fully as it's generated).&lt;/li&gt; &lt;li&gt;Dynamic model loading/offloading. In the request, specify which model you want to use. It will be loaded automatically and unloaded after a period of inactivity.&lt;/li&gt; &lt;li&gt;Text-to-Speech via &lt;code&gt;kokoro&lt;/code&gt;(Ranked #1 in the &lt;a href="https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena"&gt;TTS Arena&lt;/a&gt;) and &lt;code&gt;piper&lt;/code&gt; models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches/issues/231"&gt;Coming soon&lt;/a&gt;: Audio generation (chat completions endpoint) &lt;ul&gt; &lt;li&gt;Generate a spoken audio summary of a body of text (text in, audio out)&lt;/li&gt; &lt;li&gt;Perform sentiment analysis on a recording (audio in, text out)&lt;/li&gt; &lt;li&gt;Async speech to speech interactions with a model (audio in, audio out)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches/issues/115"&gt;Coming soon&lt;/a&gt;: Realtime API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Project: &lt;a href="https://github.com/speaches-ai/speaches"&gt;https://github.com/speaches-ai/speaches&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the documentation to get started: &lt;a href="https://speaches-ai.github.io/speaches/"&gt;https://speaches-ai.github.io/speaches/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS functionality demo&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i02hpf/video/xfqgsah1xnce1/player"&gt;https://reddit.com/link/1i02hpf/video/xfqgsah1xnce1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Generating an audio a second or third time is much faster because the model is kept in memory)&lt;/p&gt; &lt;p&gt;NOTE: The published hugging face space is currently broken, but the GradioUI should work when you spin it up locally using Docker&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fedirz"&gt; /u/fedirz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T01:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzp789</id>
    <title>Mark Zuckerberg believes in 2025, Meta will probably have a mid-level engineer AI that can write code, and over time it will replace people engineers.</title>
    <updated>2025-01-12T15:34:50+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/slow_developer/status/1877798620692422835?mx=2"&gt;https://x.com/slow_developer/status/1877798620692422835?mx=2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=USBW0ESLEK0"&gt;https://www.youtube.com/watch?v=USBW0ESLEK0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year"&gt;https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is he too optimistic, or can we expect &lt;strong&gt;vastly improved&lt;/strong&gt; (coding) LLMs very soon? Will this be Llama 4? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T15:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzkw3f</id>
    <title>DeepSeek V3 is the gift that keeps on giving!</title>
    <updated>2025-01-12T11:37:25+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt; &lt;img alt="DeepSeek V3 is the gift that keeps on giving!" src="https://preview.redd.it/fj10nizoujce1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b294748a76dfcaf7f0f25300479cd3ea3b25308" title="DeepSeek V3 is the gift that keeps on giving!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fj10nizoujce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T11:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzuw4z</id>
    <title>Kokoro #1 on TTS leaderboard</title>
    <updated>2025-01-12T19:38:14+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a short time and a few sabotage attempts, Kokoro is now #1 on the TTS Arena Leaderboard:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena"&gt;https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hadn't done any comparative tests to see whether it was better than XTTSv2 (which I was using previously) but the smaller model size and licensing was enough for me to switch after using it just for a few minutes.&lt;/p&gt; &lt;p&gt;I'd like to see work do produce a F16 and Int8 version (currently, I'm running the full F32 version). But this is a very nice model in terms of size performance when you just need simple TTS rendering of text.&lt;/p&gt; &lt;p&gt;I guess the author is busy developing, but I'd love to see a paper on this to understand how the model size was chosen and whether even smaller model sizes were explored.&lt;/p&gt; &lt;p&gt;It would be nice eventually if the full training pipeline and training data would also be open sourced to allow for reproduction, but even having the current voices and model is already very nice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T19:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmpuq</id>
    <title>VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?</title>
    <updated>2025-01-12T13:31:43+00:00</updated>
    <author>
      <name>/u/SpudMonkApe</name>
      <uri>https://old.reddit.com/user/SpudMonkApe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt; &lt;img alt="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" src="https://external-preview.redd.it/aphKSMbfvfDHStraL4JSGgDfke__oze-3mdG_k4jOVQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37b50e3ca1b1a72567f853cc77c80c80b325c53a" title="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpudMonkApe"&gt; /u/SpudMonkApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcmag.com/news/vlc-media-player-to-use-ai-to-generate-subtitles-for-videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i01k4s</id>
    <title>Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them</title>
    <updated>2025-01-13T00:33:44+00:00</updated>
    <author>
      <name>/u/Applemoi</name>
      <uri>https://old.reddit.com/user/Applemoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt; &lt;img alt="Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them " src="https://preview.redd.it/w5j543q9pnce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61adf904110b30f4cba98ecbd9c36a7462cf005f" title="Llama goes off the rails if you ask it for 5 odd numbers that donâ€™t have the letter E in them " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Applemoi"&gt; /u/Applemoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w5j543q9pnce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T00:33:44+00:00</published>
  </entry>
</feed>
