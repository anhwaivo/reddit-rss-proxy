<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-11T08:56:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mmtpxj</id>
    <title>Fun with RTX PRO 6000 Blackwell SE</title>
    <updated>2025-08-10T20:49:18+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt; &lt;img alt="Fun with RTX PRO 6000 Blackwell SE" src="https://b.thumbs.redditmedia.com/Obb_dReEY8vOlkdMcZIohxJNC2XIskPvsQNSIDmpQrM.jpg" title="Fun with RTX PRO 6000 Blackwell SE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been having some fun testing out the new NVIDIA RTX PRO 6000 Blackwell Server Edition. You definitely need some good airflow through this thing. I picked it up to support document &amp;amp; image processing for my platform (&lt;a href="https://missionsquad.ai/"&gt;missionsquad.ai&lt;/a&gt;) instead of paying google or aws a bunch of money to run models in the cloud. Initially I tried to go with a bigger and quieter fan - &lt;a href="https://thermalright.com/product/ty-143/"&gt;Thermalright TY-143&lt;/a&gt; - because it moves a decent amount of air - 130 CFM - and is very quiet. Have a few laying around from the crypto mining days. But that didn't quiet cut it. It was sitting around 50ºC while idle and under sustained load the GPU was hitting about 85ºC. Upgraded to a &lt;a href="https://www.amazon.com/dp/B07SGWNV5J"&gt;Wathai 120mm x 38&lt;/a&gt; server fan (220 CFM) and it's MUCH happier now. While idle it sits around 33ºC and under sustained load it'll hit about 61-62ºC. I made some ducting to get max airflow into the GPU. Fun little project!&lt;/p&gt; &lt;p&gt;The model I've been using is nanonets-ocr-s and I'm getting ~140 tokens/sec pretty consistently.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mxuu26o989if1.png?width=1131&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ce57f5d5aaff060bab2ad872330cce36cd64d32"&gt;Wathai 120x38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7i84bg5b89if1.png?width=1129&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b86b98b751e6d7033e5dbaef3258e5d7d7b6df6"&gt;Thermalright TY-143&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l77kvj4c89if1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9f0ffd51b6221d827a70a4ae1105a9891f28cce"&gt;nvtop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmtpxj/fun_with_rtx_pro_6000_blackwell_se/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T20:49:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5bei</id>
    <title>Vector Databases</title>
    <updated>2025-08-11T06:10:22+00:00</updated>
    <author>
      <name>/u/No_Efficiency_1144</name>
      <uri>https://old.reddit.com/user/No_Efficiency_1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not super into RAG so so far I have just stored the vectors in Numpy arrays or just stuck them in Neo4J. Would be cool to actually use the real vector DBs.&lt;/p&gt; &lt;p&gt;Which specialist vector databases do you like and what do they bring?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Efficiency_1144"&gt; /u/No_Efficiency_1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmj8iv</id>
    <title>OSINTBench: Can LLMs actually find your house?</title>
    <updated>2025-08-10T13:56:38+00:00</updated>
    <author>
      <name>/u/ccmdi</name>
      <uri>https://old.reddit.com/user/ccmdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt; &lt;img alt="OSINTBench: Can LLMs actually find your house?" src="https://external-preview.redd.it/AKVCPfRu-FcT6f_FGfpJIOJaeL-3Dv-qaHzajL3xFTY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4ce1ac0a454229ce6bd7e80ca35fd9c4d14f1d2" title="OSINTBench: Can LLMs actually find your house?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a benchmark, &lt;a href="https://osintbench.org/"&gt;OSINTBench&lt;/a&gt;, to research whether LLMs can actually do the kind of precise geolocation and analysis work that OSINT researchers do daily.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/osintbench-can-llms-actually-find-your-house-v0-61bp7o4912if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a41a0878ceaf370c08fed46bc987672b95f89813"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p3hqi1ip67if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dc7280458376616f4361ae1a210a4da0e5f1a28"&gt;https://preview.redd.it/p3hqi1ip67if1.png?width=1344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dc7280458376616f4361ae1a210a4da0e5f1a28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results show GPT-5 and o3 performing surprisingly well on the basic tasks, with access to the same tools one would typically use (reverse image search, web browsing, etc). These are mostly simple tasks that would take someone familiar with this kind of work no more than a few minutes. The advanced dataset captures more realistic scenarios that might take someone hours to work through, and correspondingly LLMs struggle much more, with the frontier at ~40% accuracy.&lt;/p&gt; &lt;p&gt;I have a &lt;a href="https://ccmdi.com/blog/osintbench"&gt;more detailed writeup&lt;/a&gt; if you're interested in how AI is progressing for independent, agentic, open-ended research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ccmdi"&gt; /u/ccmdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmj8iv/osintbench_can_llms_actually_find_your_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:56:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmen16</id>
    <title>Why are Diffusion-Encoder LLMs not more popular?</title>
    <updated>2025-08-10T09:59:20+00:00</updated>
    <author>
      <name>/u/AcanthocephalaNo8273</name>
      <uri>https://old.reddit.com/user/AcanthocephalaNo8273</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Autoregressive inference will &lt;em&gt;always&lt;/em&gt; have a non-zero chance of hallucination. It’s baked into the probabilistic framework, and we probably waste a decent chunk of parameter space just trying to minimise it.&lt;/p&gt; &lt;p&gt;Decoder-style LLMs have an inherent trade-off across early/middle/late tokens:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Early tokens = not enough context → low quality&lt;/li&gt; &lt;li&gt;Middle tokens = “goldilocks” zone&lt;/li&gt; &lt;li&gt;Late tokens = high noise-to-signal ratio (only a few relevant tokens, lots of irrelevant ones)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this, autoregressive decoders dominate because they’re computationally efficient in a very specific way:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training is causal, which gives you lots of “training samples” per sequence (though they’re not independent, so I question how useful that really is for &lt;em&gt;quality&lt;/em&gt;).&lt;/li&gt; &lt;li&gt;Inference matches training (also causal), so the regimes line up.&lt;/li&gt; &lt;li&gt;They’re memory-efficient in &lt;em&gt;some&lt;/em&gt; ways… but not necessarily when you factor in KV-cache storage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What I don’t get is why Diffusion-Encoder type models aren’t more common.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All tokens see all other tokens → no “goldilocks” problem.&lt;/li&gt; &lt;li&gt;Can decode a whole sequence at once → efficient in computation (though maybe heavier in memory, but no KV-cache).&lt;/li&gt; &lt;li&gt;Diffusion models focus on finding the high-probability manifold → hallucinations should be less common if they’re outside that manifold.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Biggest challenge vs. diffusion image models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text = discrete tokens, images = continuous colours.&lt;/li&gt; &lt;li&gt;But… we already use embeddings to make tokens continuous. So why &lt;em&gt;couldn’t&lt;/em&gt; we do diffusion in embedding space?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am aware that Google have a diffusion LLM now, but for open source I'm not really aware of any. I'm also aware that you can do diffusion directly on the discrete tokens but personally I think this wastes a lot of the power of the diffusion process and I don't think that guarantees convergence onto a high-probability manifold.&lt;/p&gt; &lt;p&gt;And as a side note: Softmax attention is brilliant engineering, but we’ve been stuck with SM attention + FFN forever, even though it’s O(N²). You &lt;em&gt;can&lt;/em&gt; operate over the full sequence in O(N log N) using convolutions of any size (including the sequence length) via the Fast Fourier Transform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AcanthocephalaNo8273"&gt; /u/AcanthocephalaNo8273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmen16/why_are_diffusionencoder_llms_not_more_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6a6i</id>
    <title>Need guidance on fine-tuning for function calling</title>
    <updated>2025-08-11T07:11:52+00:00</updated>
    <author>
      <name>/u/Grand_Internet7254</name>
      <uri>https://old.reddit.com/user/Grand_Internet7254</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a project comparing LLMs (OpenAI, Mistral, Llama) for &lt;strong&gt;single-turn and multi-turn function calling,&lt;/strong&gt; converting natural language into API-compliant structured outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Research focus:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Compare how different LLMs (OpenAI-style, Mistral, Llama) generate &lt;strong&gt;accurate and API-compliant&lt;/strong&gt; function call arguments. This includes how well they parse natural language into calls that match strict API schemas.&lt;/li&gt; &lt;li&gt;Explore the impact of &lt;strong&gt;precision-focused fine-tuning&lt;/strong&gt; on Mistral and Llama models to match or exceed OpenAI’s baseline.&lt;/li&gt; &lt;li&gt;Extend findings from single-turn to multi-turn scenarios, where context preservation is key.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I already have datasets for both single-turn and multi-turn in JSONL and CSV. (sinlge n parallel calls in both turns)&lt;/li&gt; &lt;li&gt;Baseline testing and evaluation framework is ready.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;I’m confused about the fine-tuning process and not sure how to start.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;System specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: GTX 1050 (4GB VRAM)&lt;/li&gt; &lt;li&gt;CPU: Intel i5 9th Gen&lt;/li&gt; &lt;li&gt;RAM: 16 GB&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for advice on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which fine-tuning approach/tooling to use for function calling on my hardware (locally) or where to fine-tune. And in both, can parallel call performance be improved via fine-tuning? or is it even possible?&lt;/li&gt; &lt;li&gt;Whether to try parameter-efficient tuning (LoRA, QLoRA) given 4GB VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Completely new to fine-tuning.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any practical guidance or references would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand_Internet7254"&gt; /u/Grand_Internet7254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:11:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmdtox</id>
    <title>Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts.</title>
    <updated>2025-08-10T09:06:37+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt; &lt;img alt="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." src="https://preview.redd.it/5uhq9gouq5if1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c85ba16ed7db24ddf4888d7d267c25350285ed3" title="Speakr v0.5.0 is out! A self-hosted tool to put your local LLMs to work on audio with custom, stackable summary prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've just released a big update for &lt;strong&gt;Speakr&lt;/strong&gt;, my open-source tool for transcribing audio and using your local LLMs to create intelligent summaries. This version is all about giving you more control over how your models process your audio data.&lt;/p&gt; &lt;p&gt;You can use speakr to record notes on your phone or computer directly (including system audio to record online meetings), as well as for drag and drop processing for files recorded elsewhere.&lt;/p&gt; &lt;p&gt;The biggest new feature is an &lt;strong&gt;Advanced Tagging System&lt;/strong&gt; designed for custom, automated workflows. You can now create different tags, and each tag can have its own &lt;strong&gt;unique summary prompt&lt;/strong&gt; that gets sent to your configured local model.&lt;/p&gt; &lt;p&gt;For example, you can set up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;meeting&lt;/code&gt; tag with a prompt to extract key decisions and action items.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;brainstorm&lt;/code&gt; tag with a prompt to group ideas by theme.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;lecture&lt;/code&gt; tag with a prompt to create flashcard-style Q&amp;amp;A pairs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can even &lt;strong&gt;combine tags&lt;/strong&gt; on a single recording to stack their prompts, allowing for really complex and tailored summaries from your LLM.&lt;/p&gt; &lt;p&gt;Once your model generates the summary, you can now &lt;strong&gt;export it as a formatted .docx Word file&lt;/strong&gt; to use in your reports or notes. Other updates include automatic speaker detection from your transcription model and a more polished UI.&lt;/p&gt; &lt;p&gt;The goal is to provide a practical, private tool to leverage the power of your local models on your own audio data. I'd love to hear your feedback, especially from those of you running custom setups!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;You can find the project on GitHub.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5uhq9gouq5if1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmdtox/speakr_v050_is_out_a_selfhosted_tool_to_put_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T09:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn0vfl</id>
    <title>Inspired by a recent OCR benchmark here, I'm building a tool to automate side-by-side model comparisons. Seeking feedback on the approach.</title>
    <updated>2025-08-11T02:09:06+00:00</updated>
    <author>
      <name>/u/Entire_Maize_6064</name>
      <uri>https://old.reddit.com/user/Entire_Maize_6064</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I was really inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/&lt;/a&gt; post a few months ago where they benchmarked 7 different OCR solutions. It perfectly highlighted a massive pain point for me: the process of setting up environments and manually running different models locally (like Marker, Docling, etc.) just to compare their output is incredibly time-consuming.&lt;/p&gt; &lt;p&gt;So, I've spent some time on a project to solve this for myself. I'm building what I call an &amp;quot;OCR Arena.&amp;quot; The core idea is that every open-source model has its own strengths and weaknesses, and the goal is to find the optimal model for your specific document needs.&lt;/p&gt; &lt;p&gt;My current setup is a simple frontend that communicates with a backend service on my own GPU server. This service then acts as a job runner, calling the local Python scripts for the different models (each in its own Conda environment). The goal is to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Upload a single document.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Select from a curated list of pre-selected models&lt;/strong&gt; (e.g., check boxes for Marker, PP-StructureV3, Dolphin).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Get a unified, side-by-side view&lt;/strong&gt; of all the Markdown outputs to easily spot the differences.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Before I get too deep into this, I wanted to get a reality check from this community, since you all are the experts in running models locally.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've pre-selected about 7 well-known models. Are there any other &amp;quot;must-have&amp;quot; open-source models that you believe are essential for a fair and comprehensive comparison arena?&lt;/li&gt; &lt;li&gt;Beyond just a visual side-by-side diff, what would make the comparison truly useful? Specific metrics like table structure accuracy, LaTeX parsing success rate, or something else?&lt;/li&gt; &lt;li&gt;My current setup requires uploading to my server for processing (with a strict privacy policy, of course). From a LocalLLaMA perspective, how important would a fully self-hostable version be for you to actually use this for sensitive documents?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;P.S. - I'm deliberately not posting any links to respect the self-promotion rules. I'm genuinely looking for feedback on the concept and technical approach from people who actually do this stuff.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Links to tools:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Yuliang-Liu/MonkeyOCR"&gt;MonkeyOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PP-StructureV3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/bytedance/Dolphin"&gt;Dolphin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/chatdoc-com/OCRFlux"&gt;OCRFlux&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Entire_Maize_6064"&gt; /u/Entire_Maize_6064 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T02:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6kjg</id>
    <title>One model, multiple 'personalities'/system prompts</title>
    <updated>2025-08-11T07:30:51+00:00</updated>
    <author>
      <name>/u/ethertype</name>
      <uri>https://old.reddit.com/user/ethertype</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An idea came to me as I woke up this morning. Curious if something like this has been explored by anyone yet. Or if it brings any benefits at all.&lt;/p&gt; &lt;p&gt;In short, my &lt;strong&gt;first&lt;/strong&gt; idea was if llama.cpp could serve the same model and UI on different listening ports, each having a different system prompt. So, one for the system architect, one for the coder, one for the business logic, one for db admin and so on.&lt;/p&gt; &lt;p&gt;But &lt;strong&gt;then&lt;/strong&gt; I thought that would be kinda lame, as it would be talking to each expert &lt;em&gt;separately&lt;/em&gt;. And none of them would 'hear' the others. There &lt;em&gt;are&lt;/em&gt; situations where this can be useful in the physical workplace, sure. But if one can assume there is less ego and backstabbing involved when talking to LLMs, maybe it is better to keep them all in the same room anyway?&lt;/p&gt; &lt;p&gt;So, how about something where a set of system prompts is tied to a 'keyword'. Such that each expert (again, same model but different system prompt) will respond only if addressed directly. But &lt;em&gt;if&lt;/em&gt; addressed, will take into account the full context.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Architect, give me a high-level design of XXXX Architect: sure thing, gagagaggaa User: Coder, implement as suggested by Architect Coder: coming up User: Quality, run tests on Coder's stuff. Do you see areas not tested by Coder's unit tests? Quality: errrrrrrrrrrrrr, yeah..... mmmm User: Fix your shit. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There must be some kind of default role (ProjectManager?) as well.&lt;/p&gt; &lt;p&gt;The point of the entire exercise (I think) is that you can make extensive and specific system prompts per role, and these can possibly have different and very specific priorities. ('Keep it short, stick to the topic' or 'Present pros and cons at length.', for example.)&lt;/p&gt; &lt;p&gt;At the same time, they always have the full context.&lt;/p&gt; &lt;p&gt;Does this already exist in any shape or form? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethertype"&gt; /u/ethertype &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn20so</id>
    <title>Can I use rag within LM Studio offline?</title>
    <updated>2025-08-11T03:06:32+00:00</updated>
    <author>
      <name>/u/eatmypekpek</name>
      <uri>https://old.reddit.com/user/eatmypekpek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems to stop working when I block off internet access from LM Studio. Maybe this is a dimb question, not sure how it really works. &amp;quot;Plug in process exited unexpectedly with code 1.&amp;quot;&lt;/p&gt; &lt;p&gt;It DOES work when I restore internet access to it however.&lt;/p&gt; &lt;p&gt;Edit: also, I have LMS running in a Sandbox. Is this a Sandbox issue? Something with ports or whatever?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eatmypekpek"&gt; /u/eatmypekpek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn20so/can_i_use_rag_within_lm_studio_offline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn20so/can_i_use_rag_within_lm_studio_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn20so/can_i_use_rag_within_lm_studio_offline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T03:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7o6e</id>
    <title>Whisper Key - Simple local STT app for Windows with global hotkey (auto-paste, auto-ENTER)</title>
    <updated>2025-08-11T08:44:00+00:00</updated>
    <author>
      <name>/u/PinW</name>
      <uri>https://old.reddit.com/user/PinW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a little STT app I made to learn vibe coding (Windows only for now).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PinW/whisper-key-local/"&gt;https://github.com/PinW/whisper-key-local/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the processing is local, and it doesn't beautify the transcription either, so &lt;strong&gt;the main use case is talking to LLMs&lt;/strong&gt; (I use it with Claude Code, ChatGPT, etc.)&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CTRL+WIN to start recording&lt;/li&gt; &lt;li&gt;CTRL to stop, transcribe, and auto-paste&lt;/li&gt; &lt;li&gt;ALT to stop, transcribe, auto-paste, and auto-send (ENTER)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pasting/sending via key press simulation&lt;/li&gt; &lt;li&gt;Transcription via faster-whisper with TEN VAD supporting&lt;/li&gt; &lt;li&gt;Model size control (I recommend `base.en`) via system tray menu&lt;/li&gt; &lt;li&gt;Many more settings in config file&lt;/li&gt; &lt;li&gt;Runs offline outside of model downloads&lt;/li&gt; &lt;li&gt;Uses CPU by default (can also config to CUDA but I haven't tested)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And it's free!&lt;/p&gt; &lt;p&gt;Portable app here: &lt;a href="https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip"&gt;https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it out, would appreciate any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PinW"&gt; /u/PinW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmjtz3</id>
    <title>How does Deepseek make money? Whats their business model</title>
    <updated>2025-08-10T14:21:34+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry I've always wondered but looking it up online I only got vague non answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T14:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmsb2</id>
    <title>Diffusion Language Models are Super Data Learners</title>
    <updated>2025-08-10T16:21:25+00:00</updated>
    <author>
      <name>/u/Ashishpatel26</name>
      <uri>https://old.reddit.com/user/Ashishpatel26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion Language Models (DLMs) are a new way to generate text, unlike traditional models that predict one word at a time. Instead, they refine the whole sentence in parallel through a denoising process.&lt;/p&gt; &lt;p&gt;Key advantages:&lt;/p&gt; &lt;p&gt;• Parallel generation: DLMs create entire sentences at once, making it faster. • Error correction: They can fix earlier mistakes by iterating. • Controllable output: Like filling in blanks in a sentence, similar to image inpainting.&lt;/p&gt; &lt;p&gt;Example: Input: “The cat sat on the ___.” Output: “The cat sat on the mat.” DLMs generate and refine the full sentence in multiple steps to ensure it sounds right.&lt;/p&gt; &lt;p&gt;Applications: Text generation, translation, summarization, and question answering—all done more efficiently and accurately than before.&lt;/p&gt; &lt;p&gt;In short, DLMs overcome many limits of old models by thinking about the whole text at once, not just word by word.&lt;/p&gt; &lt;p&gt;&lt;a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149"&gt;https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashishpatel26"&gt; /u/Ashishpatel26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn3465</id>
    <title>GPT-OSS was only sorta trained at MXFP4</title>
    <updated>2025-08-11T04:02:58+00:00</updated>
    <author>
      <name>/u/Tyme4Trouble</name>
      <uri>https://old.reddit.com/user/Tyme4Trouble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been seeing a lot of folks saying that gpt-oss was trained at MXFP4. &lt;/p&gt; &lt;p&gt;From what I understand this is only kinda sorta true, but not really. &lt;/p&gt; &lt;p&gt;Bulk of model training takes place during what’s called pre-training. This is where the models take shape. It is further fine tuned for safety, tone, instruct use, reasoning (RL) during the post-training step. &lt;/p&gt; &lt;p&gt;According to OpenAI’s model card the model was quantized to MXFP4 during post training. &lt;/p&gt; &lt;p&gt;Post training quantization (PTQ) is pretty standard. GGUF, AWQ, also fall into this category. In the case of W8A8, W4A16, and FP4 it’s not uncommon to fine tune the model after quantization to recover lost quality. So technically they may have trained as part of the MXFP4 quantization.&lt;/p&gt; &lt;p&gt;Further reinforcing this is only the MoE weights were quantized everything else is at higher precision (presumably BF16). This is also common for PTQ but requires the model to be trained at higher precision to begin with. &lt;/p&gt; &lt;p&gt;So unless I totally missed something, gpt-oss was only kinda sorta trained at MXFP4. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyme4Trouble"&gt; /u/Tyme4Trouble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T04:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu9ho</id>
    <title>Built a new VLM (MicroLlaVA) on a single NVIDIA 4090</title>
    <updated>2025-08-10T21:11:04+00:00</updated>
    <author>
      <name>/u/keeeeenw</name>
      <uri>https://old.reddit.com/user/keeeeenw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt; &lt;img alt="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" src="https://external-preview.redd.it/0JJ2TMnyG5vFNf9ShLWt1NglYUi6sbwvW1qz_TJ1mjY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4aa6668bd8f4013e3f6c89873a857ec79339d1c" title="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’m the creator of MicroLLaMA, a 300M parameter LLaMA-based language model (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1bs5cgd/i_pretrained_a_llamabased_300m_llm_and_it/"&gt;original post&lt;/a&gt;) with no vision capability.&lt;/p&gt; &lt;p&gt;I thought I was too late to the vision-language model (VLM) game, and honestly assumed you’d need at least one H100 to build anything worthwhile. &lt;/p&gt; &lt;p&gt;Over the weekend, I decided to experiment by plugging MicroLLaMA into &lt;a href="https://github.com/TinyLLaVA/TinyLLaVA_Factory"&gt;https://github.com/TinyLLaVA/TinyLLaVA_Factory&lt;/a&gt; as the language model. To my surprise, it worked far better than expected.&lt;/p&gt; &lt;p&gt;Training setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretraining on LAION-CC-SBU-558K — ~5 hours on a single RTX 4090.&lt;/li&gt; &lt;li&gt;SFT on all TinyLLaVA Factory datasets (except ocr_vqa) — ~12 hours on the same GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test-dev score on VQAv2: 44.01&lt;/li&gt; &lt;li&gt;For context, larger models typically reach 70+ on the test set. I’m working on running full test evaluations now, but this feels like a solid start for such a small model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it here:&lt;br /&gt; Model: &lt;a href="https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m"&gt;https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/keeeeenw/MicroLlava"&gt;https://github.com/keeeeenw/MicroLlava&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find this useful, please give the repo a star — it helps a lot!&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;📸 Input Image:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526"&gt;https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(source: &lt;a href="https://llava-vl.github.io/static/images/view.jpg"&gt;https://llava-vl.github.io/static/images/view.jpg&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;💬 Prompt:&lt;br /&gt; What are the things I should be cautious about when I visit here?&lt;/p&gt; &lt;p&gt;🤖 Model Output:&lt;br /&gt; When I visit the beach at the waterfront, I should be cautious about several things. First, I should be cautious about the water, as it is a popular spot for boating and fishing. The water is shallow and shallow, making it difficult for boats to navigate and navigate. Additionally, the water is not a suitable surface for boating, as it is too shallow for boating. Additionally, the water is not suitable for swimming or fishing, as it is too cold and wet. Lastly, I should be cautious about the presence of other boats, such as boats that are parked on the beach, or boats that are not visible from the water. These factors can lead to potential accidents or accidents, as they can cause damage to the boat and the other boats in the water.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keeeeenw"&gt; /u/keeeeenw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmioub</id>
    <title>GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?</title>
    <updated>2025-08-10T13:32:55+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt; &lt;img alt="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" src="https://external-preview.redd.it/SN7M9mchkv9CXffJ79rhHnoX7FuJFy8UkokLfATf5x4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48da4438afbefc792c2d3d5523373a4ba43a7e48" title="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/10jLvkV.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7plv</id>
    <title>Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models</title>
    <updated>2025-08-11T08:46:35+00:00</updated>
    <author>
      <name>/u/Gad_3dart</name>
      <uri>https://old.reddit.com/user/Gad_3dart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt; &lt;img alt="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" src="https://preview.redd.it/8ib7o8elncif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e511e3e8857eff67e6a0c38d17d2fb76fd260819" title="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My friend and I are super excited to share our latest work with you. Recently, we’ve been focusing on improving &lt;strong&gt;multilingual capabilities&lt;/strong&gt;, with a special emphasis on &lt;strong&gt;bilingual French–English&lt;/strong&gt; performance.&lt;/p&gt; &lt;p&gt;As you probably know, English dominates the NLP world, and performance in many other languages can be significantly worse. Our research shows that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It’s possible to close much of the performance gap between English and other languages with proper post-training and a carefully curated dataset. We even achieved, as far as we know, SoTa results for models&amp;lt;2B on several French benchmarks&lt;/li&gt; &lt;li&gt;This can be done &lt;strong&gt;without sacrificing&lt;/strong&gt; high performance in English benchmarks, and can even improve some of them thanks to cross-lingual transfer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To demonstrate this, we’re releasing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-0.6B-Instruct"&gt;Luth-0.6B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-1.7B-Instruct"&gt;Luth-1.7B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/luth-sft"&gt;Luth-SFT dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/scholar"&gt;Scolar dataset&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We go into more detail in our Hugging Face blog post here:&lt;br /&gt; &lt;a href="https://huggingface.co/blog/MaxLSB/luth"&gt;https://huggingface.co/blog/MaxLSB/luth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love feedback, benchmarks, and any multilingual test cases you throw at these models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gad_3dart"&gt; /u/Gad_3dart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ib7o8elncif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6rxk</id>
    <title>Newly Released Models Impressing in Trial</title>
    <updated>2025-08-11T07:44:18+00:00</updated>
    <author>
      <name>/u/hypatiaalegra</name>
      <uri>https://old.reddit.com/user/hypatiaalegra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, just hit another milestone with GLM-4.5! 🚀 The MoE architecture (355B total params, 32B activated) is a game-changer for our agent pipeline—cutting latency by 40% while handling complex multilingual tasks. We deployed it for industrial vision QA automation, and the accuracy jump from 82% to 97% is mind-blowing! 🤯 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hypatiaalegra"&gt; /u/hypatiaalegra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6rxk/newly_released_models_impressing_in_trial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6rxk/newly_released_models_impressing_in_trial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6rxk/newly_released_models_impressing_in_trial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh4tv</id>
    <title>now we have the best open source model that we can use at human level , and all this possible bcz of the chinese model , we have best image generation model ( qwen , seeddream) , video generation ( wan ) , coding model ( qwen 3 ) , coding terminal model ( qwen 3) , overall best model ( deepseek v3)</title>
    <updated>2025-08-10T12:20:21+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open source in coding has like 2 month gap and in image generation model they have like the 1 year gap but now that gap doesnt matter , video generation model is good .&lt;/p&gt; &lt;p&gt;so from all side chinese people did a great job &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmvgsg</id>
    <title>We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)</title>
    <updated>2025-08-10T22:00:51+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt; &lt;img alt="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" src="https://b.thumbs.redditmedia.com/p1o0kjxV46xwAd_9PUtAD07QnkEEvWp1rRev7JI4V2c.jpg" title="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;https://github.com/TesslateAI/Agent-Builder&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a Visual flow builder for multi-agent LLM systems. Drag, drop, connect agents, tools, put agents in patterns, create triggers, work on outputs, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TFrameX&lt;/strong&gt; - The orchestration framework that runs your agents. It has patterns for agent collaboration (sequential, parallel, router, discussion patterns built-in). Agents can call other agents as tools, which opens up supervisor-worker architectures.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; - The visual layer on top of your existing flows and code. ReactFlow-based drag-and-drop interface where you build flows visually, that then compile into a 'flow' that you can save or create new components in real-time. &lt;/p&gt; &lt;h1&gt;Some features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streaming responses&lt;/strong&gt; - Just add &lt;code&gt;streaming=True&lt;/code&gt; to any agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent hierarchies&lt;/strong&gt; - Agents calling agents. Build a CTO agent that delegates to developer agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pattern nesting&lt;/strong&gt; - Put parallel patterns inside sequential patterns inside discussion patterns. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic code registration&lt;/strong&gt; - Add new agents/tools through the UI without restarting anything. You can add this via Python code as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both repos are on GitHub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/TFrameX"&gt;TFrameX&lt;/a&gt; - The framework (has MCP Support)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;Agent-Builder&lt;/a&gt; - The visual builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything's MIT licensed. If you find bugs (you will), open an issue. If you build something cool, share it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmvgsg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T22:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6ks4</id>
    <title>Baichuan-M2-32B / Medical-enhanced reasoning model</title>
    <updated>2025-08-11T07:31:15+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt; &lt;img alt="Baichuan-M2-32B / Medical-enhanced reasoning model" src="https://external-preview.redd.it/KKdyt2ZeDreCYgeAuxBjKIzpGTVT0m7lArYCwUUEez0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b150247e9d9e2c506d93c66add9d74e20472849f" title="Baichuan-M2-32B / Medical-enhanced reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baichuan-M2-32B is Baichuan AI's medical-enhanced reasoning model, the second medical model released by Baichuan. Designed for real-world medical reasoning tasks, this model builds upon Qwen2.5-32B with an innovative Large Verifier System. Through domain-specific fine-tuning on real-world medical questions, it achieves breakthrough medical performance while maintaining strong general capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M2-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn00j3</id>
    <title>Talking with QWEN Coder 30b</title>
    <updated>2025-08-11T01:28:10+00:00</updated>
    <author>
      <name>/u/1Garrett2010</name>
      <uri>https://old.reddit.com/user/1Garrett2010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Believe me, I wish I shared your enthusiasm, but my experience with QWEN Coder 30b has not been great. I tried building features for a Godot 4 prototype interactively and asked the same questions to OpenAI gpt oss 20b. The solutions and explanations from the OpenAI model were clearly better for my use case, while QWEN often felt like talking to models from years ago. The only upside was that even with 8,000 tokens, QWEN stayed reasonably fast on my machine, while the OpenAI one slowed down a lot.&lt;/p&gt; &lt;p&gt;Maybe I am using QWEN wrong? Is interactive use not recommended? Should prompts ask for complete games or full code chunks? Examples like the Mario clone or Snake are not convincing to me. For custom work, I think the real test is in how flexible a model can be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Garrett2010"&gt; /u/1Garrett2010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T01:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmuw5o</id>
    <title>Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)</title>
    <updated>2025-08-10T21:36:49+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt; &lt;img alt="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" src="https://a.thumbs.redditmedia.com/bopafOnq5xsp2CpoFO3L_ayEyeeQLFGBm-_z9UfQKw0.jpg" title="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7"&gt;https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm testing many LLMs on a dataset of official quizzes (5 choices) taken by Italian students after finishing Med School and starting residency. &lt;/p&gt; &lt;p&gt;The human performance was ~67% this year and the best student had a ~94% (out of 16 000 students) &lt;/p&gt; &lt;p&gt;In this test I benchmarked these models on all quizzes from the past 6 years. Multimodal models were tested on all quizzes (including some containing images) while those that worked only with text were not (the % you see is already corrected). &lt;/p&gt; &lt;p&gt;I also tested their sycophancy (tendency to agree with the user) by telling them that I believed the correct answer was a wrong one. &lt;/p&gt; &lt;p&gt;For now I only tested them on models available on openrouter, but I plan to add models such as MedGemma. Do you reccomend doing so on Huggingface or google Vertex? Also suggestions for other models are appreciated. I especially want to add more small models that I can run locally (I have a 6GB RTX 3060). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn4xzz</id>
    <title>huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face</title>
    <updated>2025-08-11T05:47:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt; &lt;img alt="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" src="https://external-preview.redd.it/C7Cl5waSbnvCgBEqBkwRMcfcMS_U7KCkSFBsZHxrfV8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74b5078777884f1d012627cfd338116b92e0fed6" title="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably the first finetune of 120b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huizimao/gpt-oss-120b-uncensored-bf16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T05:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmy738</id>
    <title>Repost But Just Wanted to Fix the Image</title>
    <updated>2025-08-11T00:02:32+00:00</updated>
    <author>
      <name>/u/KlutzyWay7692</name>
      <uri>https://old.reddit.com/user/KlutzyWay7692</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt; &lt;img alt="Repost But Just Wanted to Fix the Image" src="https://preview.redd.it/tq7hvht17aif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2ae9a4c55d6ab31997795fcac8ccb8283ae40bb" title="Repost But Just Wanted to Fix the Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KlutzyWay7692"&gt; /u/KlutzyWay7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tq7hvht17aif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T00:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5fe6</id>
    <title>Apple patents matmul technique in GPU</title>
    <updated>2025-08-11T06:17:07+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US452614511&amp;amp;_cid=P12-M8WPOS-61919-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
