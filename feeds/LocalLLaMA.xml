<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-14T16:07:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jy813d</id>
    <title>From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models</title>
    <updated>2025-04-13T13:47:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2504.06214"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy813d/from_128k_to_4m_efficient_training_of_ultralong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T13:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyquyo</id>
    <title>AlexBefest's CardProjector-v4 series</title>
    <updated>2025-04-14T04:52:05+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Name: AlexBefest/CardProjector-27B-v4&lt;/p&gt; &lt;p&gt;Model URL: &lt;a href="https://huggingface.co/AlexBefest/CardProjector-27B-v4"&gt;https://huggingface.co/AlexBefest/CardProjector-27B-v4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Author: AlexBefest, &lt;a href="https://www.reddit.com/user/AlexBefest/"&gt;u/AlexBefest&lt;/a&gt;, &lt;a href="https://huggingface.co/AlexBefest"&gt;AlexBefest&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What's new in v4?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Absolute focus on personality development! This version places an absolute emphasis on designing character personalities, focusing on depth and realism. Eight (!) large datasets were collected, oriented towards all aspects of in-depth personality development. Extensive training was also conducted on a dataset of MBTI profiles with Enneagrams from psychology. The model was carefully trained to select the correct personality type according to both the MBTI and Enneagram systems. I highly recommend using these systems (see Usage recommendations); they provide an incredible boost to character realism. I conducted numerous tests with many RP models ranging from 24-70B parameters, and the MBTI profile system significantly impacts the understanding of the character's personality (especially on 70B models), making the role-playing performance much more realistic. You can see an example of a character's MBTI profile &lt;a href="https://www.personality-database.com/profile/7610/muffins-derpy-hooves-ditzy-doo-my-little-pony-friendship-is-magic-2010-mbti-personality-type"&gt;here&lt;/a&gt;. Currently, version V4 yields the deepest and most realistic characters.&lt;/li&gt; &lt;li&gt;Reduced likelihood of positive bias! I collected a large toxic dataset focused on creating and editing aggressive, extremely cruel, and hypersexualized characters, as well as transforming already &amp;quot;good harmless&amp;quot; characters into extremely cruel anti-versions of the original. Thanks to this, it was possible to significantly reduce the overall positive bias (especially in Gemma 3, where it is quite pronounced in its vanilla state), and make the model more balanced and realistic in terms of creating negative characters. It will no longer strive at all costs to create a cute, kind, ideal character, unless specifically asked to do so. All you need to do is just ask the model to &amp;quot;not make a positive character, but create a realistic one,&amp;quot; and with that one phrase, the entire positive bias goes away.&lt;/li&gt; &lt;li&gt;Moving to Gemma 3! After a series of experiments, it turned out that this model is ideally suited for the task of character design, as it possesses much more developed creative writing skills and higher general knowledge compared to Mistral 2501 in its vanilla state. Gemma 3 also seemed much more logical than its French competitor.&lt;/li&gt; &lt;li&gt;Vision ability! Due to the reason mentioned in the point above, you can freely use vision in this version. If you are using GGUF, you can download the mmproj model for the 27B version from bartowski (a vanilla mmproj will suffice, as I didn't perform vision tuning).&lt;/li&gt; &lt;li&gt;The overall quality of character generation has been significantly increased by expanding the dataset approximately 5 times compared to version V3.&lt;/li&gt; &lt;li&gt;This model is EXTREMELY sensitive to the user's prompt. So you should give instructions with caution, carefully considering.&lt;/li&gt; &lt;li&gt;In version V4, I concentrated only on one model size, 27B. Unfortunately, training multiple models at once is extremely expensive and consumes too much effort and time, so I decided it would be better to direct all my resources into just one model to avoid scattering focus. I hope you understand üôè&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Overview:&lt;/h1&gt; &lt;p&gt;CardProjector is a specialized series of language models, fine-tuned to generate character cards for &lt;strong&gt;SillyTavern&lt;/strong&gt; and &lt;strong&gt;now for creating characters in general&lt;/strong&gt;. These models are designed to assist creators and roleplayers by automating the process of crafting detailed and well-structured character cards, ensuring compatibility with SillyTavern's format.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyquyo/alexbefests_cardprojectorv4_series/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T04:52:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz25lm</id>
    <title>Suggest me best Speech Language Models</title>
    <updated>2025-04-14T15:48:01+00:00</updated>
    <author>
      <name>/u/Ai_Peep</name>
      <uri>https://old.reddit.com/user/Ai_Peep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently exploring speech language models available on the market for my project. I'd appreciate any recommendations or insights you might have. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ai_Peep"&gt; /u/Ai_Peep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz25lm/suggest_me_best_speech_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz25lm/suggest_me_best_speech_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz25lm/suggest_me_best_speech_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T15:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk399</id>
    <title>Dual 5090 va single 5090</title>
    <updated>2025-04-13T22:40:38+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt; &lt;img alt="Dual 5090 va single 5090" src="https://preview.redd.it/z1xl2ob1koue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a09792ff8b0785b5b36ea4cb15fed716f6a7feaf" title="Dual 5090 va single 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man these dual 5090s are awesome. Went from 4t/s on 29b Gemma 3 to 28t/s when going from 1 to 2. I love these things! Easily runs 70b fast! I only wish they were a little cheaper but can‚Äôt wait till the RTX 6000 pro comes out with 96gb because I am totally eyeballing the crap out of it‚Ä¶. Who needs money when u got vram!!!&lt;/p&gt; &lt;p&gt;Btw I got 2 fans right under earn, 5 fans in front, 3 on top and one mac daddy on the back, and bout to put the one that came with the gigabyte 5090 on it too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xl2ob1koue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk399/dual_5090_va_single_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyo2ds</id>
    <title>Word Synth - Llama 3.2 tiny LLM with sampling parameters exposed</title>
    <updated>2025-04-14T02:09:01+00:00</updated>
    <author>
      <name>/u/Brave_Variety6275</name>
      <uri>https://old.reddit.com/user/Brave_Variety6275</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built this as an intuition builder around LLM sampling--it's a bit rough around the edges but sharing in case its useful to anyone else trying to get it straight which sampling parameters do what. &lt;/p&gt; &lt;p&gt;&lt;a href="http://wordsynth.latenthomer.com/"&gt;http://wordsynth.latenthomer.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your browser will yell at you because I didn't use https. Sorry. &lt;/p&gt; &lt;p&gt;Also apologies if it breaks or is really slow, this was also an experiment to deploy. &lt;/p&gt; &lt;p&gt;Thanks for reading :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave_Variety6275"&gt; /u/Brave_Variety6275 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyo2ds/word_synth_llama_32_tiny_llm_with_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T02:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz2iuc</id>
    <title>glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination</title>
    <updated>2025-04-14T16:02:56+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt; &lt;img alt="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" src="https://a.thumbs.redditmedia.com/0ZVmyCDJmIJZEOxoroufeOti6K9sNcLZDn_FLYF-vJ4.jpg" title="glm-4 0414 is out. 9b, 32b, with and without reasoning and rumination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6 new models and interesting benchmarks&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-32B-0414&lt;/strong&gt; is a reasoning model with deep thinking capabilities. This was developed based on GLM-4-32B-0414 through cold start, extended reinforcement learning, and further training on tasks including mathematics, code, and logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to solve complex tasks. During training, we also introduced general reinforcement learning based on pairwise ranking feedback, which enhances the model's general capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-Z1-Rumination-32B-0414&lt;/strong&gt; is a deep reasoning model with rumination capabilities (against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model is capable of deeper and longer thinking to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks. The model shows significant improvements in research-style writing and complex tasks.&lt;/p&gt; &lt;p&gt;Finally, &lt;strong&gt;GLM-Z1-9B-0414&lt;/strong&gt; is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jz2iuc/video/t1b3wsidqtue1/player"&gt;write a Python program that shows a ball bouncing inside a spinning hexagon. The ball should be affected by gravity and friction, and it must bounce off the rotating walls realistically &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b"&gt;https://preview.redd.it/sk32ghamqtue1.png?width=836&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=080510c78ea7272379a4cfe5a23581e740301f9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1"&gt;https://preview.redd.it/1sjbnbboqtue1.png?width=682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60b42679e45242651bf1d91276c1d581f67839d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz2iuc/glm4_0414_is_out_9b_32b_with_and_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T16:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzuxv</id>
    <title>What do I need to deploy my own LLM</title>
    <updated>2025-04-14T14:12:45+00:00</updated>
    <author>
      <name>/u/Vinser_98</name>
      <uri>https://old.reddit.com/user/Vinser_98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I was wondering the hardware requirements to deploy a local LLM. Is there a table or a websites that compare different LLMs in terms of RAM and GPU requirements, inference time and electrical power required to run it? This is considering a pre-trained model only used for inference. Thank you for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vinser_98"&gt; /u/Vinser_98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzuxv/what_do_i_need_to_deploy_my_own_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzuxv/what_do_i_need_to_deploy_my_own_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzuxv/what_do_i_need_to_deploy_my_own_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jys33y</id>
    <title>Finally got Local LLM running on rx 9070 xt using onnx and directml</title>
    <updated>2025-04-14T06:14:33+00:00</updated>
    <author>
      <name>/u/dharayM</name>
      <uri>https://old.reddit.com/user/dharayM</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No i am not talking about brainwashed llama that comes with adrenaline app.&lt;/p&gt; &lt;p&gt;With vulkan broken for windows and Linux, rocm not being supported for windows and seemingly broken for linux, directml was my only hope&lt;/p&gt; &lt;p&gt;only directml-onnx models works with my solution which essentially consists of phi models but something is better than nothing&lt;/p&gt; &lt;p&gt;Here is the repo:&lt;br /&gt; &lt;a href="https://github.com/dharay/directml-onnx-local-llm"&gt;https://github.com/dharay/directml-onnx-local-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this is a work in progress, will probably abandon once we gets rocm support for rx 9000 series on windows&lt;/p&gt; &lt;p&gt;helpful resources:&lt;br /&gt; &lt;a href="https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html"&gt;https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dharayM"&gt; /u/dharayM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jys33y/finally_got_local_llm_running_on_rx_9070_xt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jy6ns6</id>
    <title>Coming soon‚Ä¶..</title>
    <updated>2025-04-13T12:36:19+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt; &lt;img alt="Coming soon‚Ä¶.." src="https://preview.redd.it/1cwv3wz7klue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abbae222e535c2c110583987226650f6391ac918" title="Coming soon‚Ä¶.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1cwv3wz7klue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jy6ns6/coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T12:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyx6yb</id>
    <title>Hybrid Mamba Transformer VS Transformer architecture explanation</title>
    <updated>2025-04-14T12:05:24+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx6yb/hybrid_mamba_transformer_vs_transformer/"&gt; &lt;img alt="Hybrid Mamba Transformer VS Transformer architecture explanation" src="https://external-preview.redd.it/vUqwSGK4ls7qPLhFNgUyUViik9ORJBr2OXMEammJfpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2041d21b3afca22c1de980b3f5fc8348ac954c" title="Hybrid Mamba Transformer VS Transformer architecture explanation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1jyx6yb/video/5py7irqhjsue1/player"&gt;https://reddit.com/link/1jyx6yb/video/5py7irqhjsue1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A short video explaining the differences between Transformer architecture and RNN (Recurrent Neural Networks) and the decisions that lead companies like Hunyuan to use Hybrid Mamba Transformer architecture that combines both.&lt;/p&gt; &lt;p&gt;X Post: &lt;a href="https://x.com/tencenthunyuan/status/1911746333662404932"&gt;https://x.com/tencenthunyuan/status/1911746333662404932&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx6yb/hybrid_mamba_transformer_vs_transformer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx6yb/hybrid_mamba_transformer_vs_transformer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx6yb/hybrid_mamba_transformer_vs_transformer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T12:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1jygxmu</id>
    <title>Open-Weights Model next week?</title>
    <updated>2025-04-13T20:16:32+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt; &lt;img alt="Open-Weights Model next week?" src="https://preview.redd.it/iph04cputnue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f58d7addbdbe94c34055c810ba04a1042cb757a3" title="Open-Weights Model next week?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iph04cputnue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jygxmu/openweights_model_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T20:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyxwob</id>
    <title>GMKtec EVO-X2 Presale Opens 15 April 12am PDT!</title>
    <updated>2025-04-14T12:42:39+00:00</updated>
    <author>
      <name>/u/NeonRitual</name>
      <uri>https://old.reddit.com/user/NeonRitual</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really excited as framework doesn't deliver to my place&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeonRitual"&gt; /u/NeonRitual &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.gmktec.com/pages/evo-x2?spm=..index.image_slideshow_1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyxwob/gmktec_evox2_presale_opens_15_april_12am_pdt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyxwob/gmktec_evox2_presale_opens_15_april_12am_pdt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T12:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyygcj</id>
    <title>What can I do with RTX 5090 that I couldn't do with RTX 4090</title>
    <updated>2025-04-14T13:08:45+00:00</updated>
    <author>
      <name>/u/polawiaczperel</name>
      <uri>https://old.reddit.com/user/polawiaczperel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, the question like in the topic, i am not limiting myself only to llm. It could be video generation/sound/text/3d models etc.&lt;/p&gt; &lt;p&gt;Best regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/polawiaczperel"&gt; /u/polawiaczperel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyygcj/what_can_i_do_with_rtx_5090_that_i_couldnt_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyygcj/what_can_i_do_with_rtx_5090_that_i_couldnt_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyygcj/what_can_i_do_with_rtx_5090_that_i_couldnt_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T13:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyz4a1</id>
    <title>Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F</title>
    <updated>2025-04-14T13:39:42+00:00</updated>
    <author>
      <name>/u/frunkp</name>
      <uri>https://old.reddit.com/user/frunkp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt; &lt;img alt="Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F" src="https://external-preview.redd.it/LUWPRYVOciUJ48L73KckvO0xvxcEDfER5j_R7LwvqHE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d8c19b4f8528a22fe82f93610c49e12edac5a4" title="Kimina-Prover Preview - New SOTA on theorem proving 80.7% miniF2F" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New SOTA of 80.7% for theorem proving on `miniF2F`! &lt;/p&gt; &lt;p&gt;Idea is to combine reasoning models (o1/r1-style) with formal maths (Lean 4) and apply RL to get human-readable proofs.&lt;/p&gt; &lt;p&gt;Distilled Kimina-Prover 1.5B &amp;amp; 7B models on &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-preview-67fb536b883d60e7ca25d7f9"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hxdploeysue1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81f9b08c6e6eb2382c7eecb53bd589e0f2c3e3cd"&gt;https://preview.redd.it/5hxdploeysue1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=81f9b08c6e6eb2382c7eecb53bd589e0f2c3e3cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IMO 1968 P5 (1st part) solution found by Kimina-Prover:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96slg6sszsue1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52904f263895c9f13318e3c9fb1933855aa4c4f8"&gt;https://preview.redd.it/96slg6sszsue1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52904f263895c9f13318e3c9fb1933855aa4c4f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ns8p29lwzsue1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=039dfa8aab4bc272b8578642502e1a9eb33e6aeb"&gt;https://preview.redd.it/ns8p29lwzsue1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=039dfa8aab4bc272b8578642502e1a9eb33e6aeb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìë Technical report: &lt;a href="https://github.com/MoonshotAI/Kimina-Prover-Preview/blob/master/Kimina_Prover_Preview.pdf"&gt;Kimina_Prover_Preview.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Models: &lt;a href="https://huggingface.co/collections/AI-MO/kimina-prover-preview-67fb536b883d60e7ca25d7f9"&gt;AI-MO/kimina-prover-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frunkp"&gt; /u/frunkp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyz4a1/kiminaprover_preview_new_sota_on_theorem_proving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T13:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyk213</id>
    <title>Still true 3 months later</title>
    <updated>2025-04-13T22:38:59+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt; &lt;img alt="Still true 3 months later" src="https://preview.redd.it/7644n1vqjoue1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0b79a5e35c4e594b33dc646534a2248d3db9159" title="Still true 3 months later" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They rushed the release so hard it's been full of implementation bugs. And let's not get started on the custom model to hill climb lmarena alop&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7644n1vqjoue1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyk213/still_true_3_months_later/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-13T22:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzvcr</id>
    <title>New Tutorial on GitHub - Build an AI Agent with MCP</title>
    <updated>2025-04-14T14:13:16+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This tutorial walks you through: Building your own MCP server with real tools (like crypto price lookup) Connecting it to Claude Desktop and also creating your own custom agent Making the agent reason when to use which tool, execute it, and explain the result what's inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Practical Implementation of MCP from Scratch&lt;/li&gt; &lt;li&gt;End-to-End Custom Agent with Full MCP Stack&lt;/li&gt; &lt;li&gt;Dynamic Tool Discovery and Execution Pipeline&lt;/li&gt; &lt;li&gt;Seamless Claude 3.5 Integration&lt;/li&gt; &lt;li&gt;Interactive Chat Loop with Stateful Context&lt;/li&gt; &lt;li&gt;Educational and Reusable Code Architecture&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the tutorial:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/mcp-tutorial.ipynb"&gt;https://github.com/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/mcp-tutorial.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;enjoy :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzvcr/new_tutorial_on_github_build_an_ai_agent_with_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:13:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyx01y</id>
    <title>Latest frontier models are drunk professors</title>
    <updated>2025-04-14T11:55:20+00:00</updated>
    <author>
      <name>/u/hyperknot</name>
      <uri>https://old.reddit.com/user/hyperknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx01y/latest_frontier_models_are_drunk_professors/"&gt; &lt;img alt="Latest frontier models are drunk professors" src="https://external-preview.redd.it/XGaUVK_EVRH_MCYAfieRk4YNXuJYYPRovHM-615L90g.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3b6984d7c6f953e489fb7c256b7dba8dcdee621" title="Latest frontier models are drunk professors" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyperknot"&gt; /u/hyperknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/hyperknot/status/1911747818890432860"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx01y/latest_frontier_models_are_drunk_professors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyx01y/latest_frontier_models_are_drunk_professors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyvzqg</id>
    <title>GLM-4-0414 (9B/32B) (w. &amp; wo. reasoning) Ready to Release</title>
    <updated>2025-04-14T10:55:40+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt; &lt;img alt="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" src="https://external-preview.redd.it/rUMQwGzzv049_AQ65R_I2zx8r9Fk1GPQDozFx082Elc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71971286a2d2292f2a0a2b67094dc5e3c3a4b46e" title="GLM-4-0414 (9B/32B) (w. &amp;amp; wo. reasoning) Ready to Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems the developer is making final preparations : &lt;a href="https://github.com/zRzRzRzRzRzRzR/GLM-4"&gt;https://github.com/zRzRzRzRzRzRzR/GLM-4&lt;/a&gt; (note this is developer's fork, only for reference. Also note: some benchmarks in the page are from old versions of GLM model)&lt;/p&gt; &lt;p&gt;Huggingface collection is created (but empty for now): &lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The release contains following models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395"&gt;https://preview.redd.it/6j2pwsl17sue1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55349ae54f8626f4a068dde1f33b750d87236395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyvzqg/glm40414_9b32b_w_wo_reasoning_ready_to_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T10:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyp2no</id>
    <title>If we had models like QwQ-32B and Gemma-3-27B two years ago, people would have gone crazy.</title>
    <updated>2025-04-14T03:04:47+00:00</updated>
    <author>
      <name>/u/Proud_Fox_684</name>
      <uri>https://old.reddit.com/user/Proud_Fox_684</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Imagine if we had QwQ-32B or Gemma-3-27B or some of the smaller models, 18-24 months ago. It would have been the craziest thing.&lt;/p&gt; &lt;p&gt;24 months ago, GPT-4 was released. GPT-4o was released 11 months ago. Sometimes we not only forgot how quick things have been moving, but we also forget how good these small models actually are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Fox_684"&gt; /u/Proud_Fox_684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyp2no/if_we_had_models_like_qwq32b_and_gemma327b_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T03:04:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jywg95</id>
    <title>Why is Qwen 2.5 Omni not being talked about enough?</title>
    <updated>2025-04-14T11:23:03+00:00</updated>
    <author>
      <name>/u/BeetranD</name>
      <uri>https://old.reddit.com/user/BeetranD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think the Qwen models are pretty good, I've been using a lot of them locally.&lt;br /&gt; They recently (a week or some ago) released 2.5 Omni, which is a 7B real-time multimodal model, that simultaneously generates text and natural speech. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;Qwen/Qwen2.5-Omni-7B ¬∑ Hugging Face&lt;/a&gt;&lt;br /&gt; I think It would be great to use for something like a local AI alexa clone. But on youtube there's almost no one testing it, and even here, not a lot of people talking about it.&lt;/p&gt; &lt;p&gt;What is it?? Am I over-expecting from this model? or I'm just not well informed about alternatives, please enlighten me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BeetranD"&gt; /u/BeetranD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jywg95/why_is_qwen_25_omni_not_being_talked_about_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T11:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jz1oxv</id>
    <title>NVIDIA has published new Nemotrons!</title>
    <updated>2025-04-14T15:29:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;what a week....!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-47B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K"&gt;https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jz1oxv/nvidia_has_published_new_nemotrons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T15:29:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jysiwc</id>
    <title>DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack</title>
    <updated>2025-04-14T06:45:54+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt; &lt;img alt="DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack" src="https://external-preview.redd.it/-j5EXG21mJ1IrGfaacZfdTPmLfMidR-DBjShQEW0nM4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8674fe0d9158595daad240e374a62be90da4c4d6" title="DeepSeek will open-source parts of its inference engine ‚Äî sharing standalone features and optimizations instead of the full stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jysiwc/deepseek_will_opensource_parts_of_its_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T06:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyzl0g</id>
    <title>DGX B200 Startup ASMR</title>
    <updated>2025-04-14T14:00:52+00:00</updated>
    <author>
      <name>/u/Chemical-Mixture3481</name>
      <uri>https://old.reddit.com/user/Chemical-Mixture3481</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt; &lt;img alt="DGX B200 Startup ASMR" src="https://external-preview.redd.it/YTF4eTdsdnozdHVlMTsTLvzMSe_uV5dg8VNzSYJEyMCa9wyDSSGv4dzqg19H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=635d486d1d0cd25569bc0a249087d4f5360ec61e" title="DGX B200 Startup ASMR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just installed one of these beasts in our datacenter. Since I could not find a video that shows one of these machines running with original sound here you go!&lt;/p&gt; &lt;p&gt;Thats probably ~110dB of fan noise given that the previous generation was at around 106dB according to Nvidia. Cooling 1kW GPUs seems to be no joke given that this machine sounds like a fighter jet starting its engines next to you :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical-Mixture3481"&gt; /u/Chemical-Mixture3481 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yy6c2lvz3tue1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyzl0g/dgx_b200_startup_asmr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T14:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jyu06v</id>
    <title>llama was so deep that now ex employee saying that we r not involved in that project</title>
    <updated>2025-04-14T08:36:06+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt; &lt;img alt="llama was so deep that now ex employee saying that we r not involved in that project" src="https://preview.redd.it/49mfsia3irue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3266a093713e9cb503b3634a7a8b1f7fb0852f0" title="llama was so deep that now ex employee saying that we r not involved in that project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/49mfsia3irue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jyu06v/llama_was_so_deep_that_now_ex_employee_saying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jytw62</id>
    <title>DeepSeek is about to open-source their inference engine</title>
    <updated>2025-04-14T08:27:29+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt; &lt;img alt="DeepSeek is about to open-source their inference engine" src="https://preview.redd.it/1am95yongrue1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967ad74640babe443b3c9a2867547f568219bda6" title="DeepSeek is about to open-source their inference engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek is about to open-source their inference engine, which is a modified version based on vLLM. Now, DeepSeek is preparing to contribute these modifications back to the community.&lt;/p&gt; &lt;p&gt;I really like the last sentence: 'with the goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0.'&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine"&gt;https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1am95yongrue1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-14T08:27:29+00:00</published>
  </entry>
</feed>
