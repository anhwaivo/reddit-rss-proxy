<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-22T00:53:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k4gl0k</id>
    <title>Trying to add emotion conditioning to Gemma-3</title>
    <updated>2025-04-21T15:37:25+00:00</updated>
    <author>
      <name>/u/FOerlikon</name>
      <uri>https://old.reddit.com/user/FOerlikon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"&gt; &lt;img alt="Trying to add emotion conditioning to Gemma-3" src="https://b.thumbs.redditmedia.com/rHRvAjz9hBfUOUAXuneChyyaVvWZbDMq2GT3_bBVjlk.jpg" title="Trying to add emotion conditioning to Gemma-3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was curious to make LLM influenced by something more than just the text, so I made a small attempt to add emotional input to smallest Gemma-3-1B, which is honestly pretty inconsistent, and it was only trained on short sequences of synthetic dataset with emotion markers.&lt;/p&gt; &lt;p&gt;The idea: alongside text there is an emotion vector, and it trainable projection then added to the token embeddings before they go into the transformer layers, and trainable LoRA is added on top. &lt;/p&gt; &lt;p&gt;Here are some (cherry picked) results, generated per same input/seed/temp but with different joy/sadness. I found them kind of intriguing to share (even though the dataset looks similar)&lt;/p&gt; &lt;p&gt;My question is has anyone else has played around with similar conditioning? Does this kind approach even make much sense to explore further? I mostly see RP-finetunes when searching for existing emotion models.&lt;/p&gt; &lt;p&gt;Curious to hear any thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FOerlikon"&gt; /u/FOerlikon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4gl0k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gl0k/trying_to_add_emotion_conditioning_to_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1k47wlc</id>
    <title>üöÄ Dive v0.8.0 is Here ‚Äî Major Architecture Overhaul and Feature Upgrades!</title>
    <updated>2025-04-21T07:44:24+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt; &lt;img alt="üöÄ Dive v0.8.0 is Here ‚Äî Major Architecture Overhaul and Feature Upgrades!" src="https://external-preview.redd.it/NXEwbDIyYmU3NXdlMdvd-QuaL2Iymjf8AR2toHyHT4xxu-3H8nMusFAc2zhu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baa192645e7ad1e9a4b256373e06dac1ec7f2235" title="üöÄ Dive v0.8.0 is Here ‚Äî Major Architecture Overhaul and Feature Upgrades!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hgg9u2be75we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k47wlc/dive_v080_is_here_major_architecture_overhaul_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T07:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4na8t</id>
    <title>GMK Evo-X2 versus Framework Desktop versus Mac Studio M3 Ultra</title>
    <updated>2025-04-21T20:09:58+00:00</updated>
    <author>
      <name>/u/dylan_dev</name>
      <uri>https://old.reddit.com/user/dylan_dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which would you buy for LocalLLaMA? I'm partial to the GMK Evo-X2 and the Mac Studio M3 Ultra. GMK has a significant discount for preorders, but I've never used GMK products. Apple's Mac Studio is a fine machine that gives you the Mac ecosystem, but is double the price.&lt;/p&gt; &lt;p&gt;I'm thinking of selling my 4090 and buying one of these machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dylan_dev"&gt; /u/dylan_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4na8t/gmk_evox2_versus_framework_desktop_versus_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4na8t/gmk_evox2_versus_framework_desktop_versus_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4na8t/gmk_evox2_versus_framework_desktop_versus_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T20:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1k3wuud</id>
    <title>nsfw orpheus early v1</title>
    <updated>2025-04-20T21:21:27+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview"&gt;https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;update: &amp;quot;v2-later checkpoint still early&amp;quot; -&amp;gt; &lt;a href="https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview-v1-8600"&gt;https://huggingface.co/MrDragonFox/mOrpheus_3B-1Base_early_preview-v1-8600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;can do the common sounds / generalises pretty well - preview has only 1 voice but good enough to get an idea of where we are heading&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k3wuud/nsfw_orpheus_early_v1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-20T21:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43htm</id>
    <title>Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image</title>
    <updated>2025-04-21T02:58:47+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt; &lt;img alt="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" src="https://b.thumbs.redditmedia.com/CBmqcJTwYbnQt5krSlEFj-sxPkmBsS_zYzMxkxvM3-Y.jpg" title="Hunyuan open-sourced InstantCharacter - image generator with character-preserving capabilities from input image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InstantCharacter is an innovative, tuning-free method designed to achieve character-preserving generation from a single image&lt;/p&gt; &lt;p&gt;One image + text ‚Üí custom poses, styles &amp;amp; scenes 1Ô∏è‚É£ First framework to balance character consistency, image quality, &amp;amp; open-domain flexibility/generalization 2Ô∏è‚É£ Compatible with Flux, delivering high-fidelity, text-controllable results 3Ô∏è‚É£ Comparable to industry leaders like GPT-4o in precision &amp;amp; adaptability&lt;/p&gt; &lt;p&gt;Try it yourself onÔºö üîóHugging Face Demo: &lt;a href="https://huggingface.co/spaces/InstantX/InstantCharacter"&gt;https://huggingface.co/spaces/InstantX/InstantCharacter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dive Deep into InstantCharacter: üîóProject Page: &lt;a href="https://instantcharacter.github.io/"&gt;https://instantcharacter.github.io/&lt;/a&gt; üîóCode: &lt;a href="https://github.com/Tencent/InstantCharacter"&gt;https://github.com/Tencent/InstantCharacter&lt;/a&gt; üîóPaperÔºö&lt;a href="https://arxiv.org/abs/2504.12395"&gt;https://arxiv.org/abs/2504.12395&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k43htm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43htm/hunyuan_opensourced_instantcharacter_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4b5xl</id>
    <title>I built a Local AI Voice Assistant with Ollama + gTTS with interruption</title>
    <updated>2025-04-21T11:26:21+00:00</updated>
    <author>
      <name>/u/typhoon90</name>
      <uri>https://old.reddit.com/user/typhoon90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I just built OllamaGTTS, a lightweight voice assistant that brings AI-powered voice interactions to your local Ollama setup using Google TTS for natural speech synthesis. It‚Äôs fast, interruptible, and optimized for real-time conversations. I am aware that some people prefer to keep everything local so I am working on an update that will likely use Kokoro for local speech synthesis. I would love to hear your thoughts on it and how it can be improved.&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time voice interaction (Silero VAD + Whisper transcription)&lt;/li&gt; &lt;li&gt;Interruptible speech playback (no more waiting for the AI to finish talking)&lt;/li&gt; &lt;li&gt;FFmpeg-accelerated audio processing (optional speed-up for faster * replies)&lt;/li&gt; &lt;li&gt;Persistent conversation history with configurable memory&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/ExoFi-Labs/OllamaGTTS"&gt;GitHub Repo: https://github.com/ExoFi-Labs/OllamaGTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone Repo&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install requirements&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run ollama_gtts.py&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am working on integrating Kokoro STT at the moment, and perhaps Sesame in the coming days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/typhoon90"&gt; /u/typhoon90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4b5xl/i_built_a_local_ai_voice_assistant_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4quhi</id>
    <title>Copilot Workspace being underestimated...</title>
    <updated>2025-04-21T22:39:28+00:00</updated>
    <author>
      <name>/u/itzco1993</name>
      <uri>https://old.reddit.com/user/itzco1993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've recently been using Copilot Workspace (link in comments), which is in technical preview. I'm not sure why it is not being mentioned more in the dev community. It think this product is the natural evolution of localdev tools such as Cursor, Claude Code, etc.&lt;/p&gt; &lt;p&gt;As we gain more trust in coding agents, it makes sense for them to gain more autonomy and leave your local dev. They should handle e2e tasks like a co-dev would do. Well, Copilot Workspace is heading that direction and it works super well.&lt;/p&gt; &lt;p&gt;My experience so far is exactly what I expect for an AI co-worker. It runs cloud, it has access to your repo and it open PRs automatically. You have this thing called &amp;quot;sessions&amp;quot; where you do follow up on a specific task.&lt;/p&gt; &lt;p&gt;I wonder why this has been in preview since Nov 2024. Has anyone tried it? Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itzco1993"&gt; /u/itzco1993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4quhi/copilot_workspace_being_underestimated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4quhi/copilot_workspace_being_underestimated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4quhi/copilot_workspace_being_underestimated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T22:39:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4f3a2</id>
    <title>Local LLM performance results on Raspberry Pi devices</title>
    <updated>2025-04-21T14:36:47+00:00</updated>
    <author>
      <name>/u/fatihustun</name>
      <uri>https://old.reddit.com/user/fatihustun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt; &lt;img alt="Local LLM performance results on Raspberry Pi devices" src="https://b.thumbs.redditmedia.com/chzBzCsnADn6F4qhCPwQHx6IBkrGvkQhmVAcHbvNnDM.jpg" title="Local LLM performance results on Raspberry Pi devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Method (very basic):&lt;/strong&gt;&lt;br /&gt; I simply installed Ollama and downloaded some small models (listed in the table) to my Raspberry Pi devices, which have a clean Raspbian OS (lite) 64-bit OS, nothing else installed/used. I run models with the &amp;quot;--verbose&amp;quot; parameter to get the performance value after each question. I asked 5 same questions to each model and took the average.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are the results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/igp229o077we1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa65f28686c212c76f04c344ea767b20cdbe2196"&gt;https://preview.redd.it/igp229o077we1.png?width=1348&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa65f28686c212c76f04c344ea767b20cdbe2196&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If you have run a local model on a Raspberry Pi device, please share the model and the device variant with its performance result.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fatihustun"&gt; /u/fatihustun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4f3a2/local_llm_performance_results_on_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T14:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43g7a</id>
    <title>Why are so many companies putting so much investment into free open source AI?</title>
    <updated>2025-04-21T02:56:18+00:00</updated>
    <author>
      <name>/u/Business_Respect_910</name>
      <uri>https://old.reddit.com/user/Business_Respect_910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont understand alot of the big pictures for these companies, but considering how many open source options we have and how they will continue to get better. How will these companies like OpenAI or Google ever make back their investment?&lt;/p&gt; &lt;p&gt;Personally i have never had to stay subscribed to a company because there's so many free alternatives. Not to mention all these companies have really good free options of the best models.&lt;/p&gt; &lt;p&gt;Unless one starts screaming ahead of the rest in terms of performance what is their end goal?&lt;/p&gt; &lt;p&gt;Not that I'm complaining, just want to know.&lt;/p&gt; &lt;p&gt;EDIT: I should probably say i know OpenAI isn't open source yet from what i know but they also offer a very high quality free plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Business_Respect_910"&gt; /u/Business_Respect_910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43g7a/why_are_so_many_companies_putting_so_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T02:56:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k43x1h</id>
    <title>Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)</title>
    <updated>2025-04-21T03:21:32+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt; &lt;img alt="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" src="https://external-preview.redd.it/ZHd1MzQzZGp3M3dlMYSg_wSm3961EYqonF0X5c18rpErhfTomdHPrQd5DrBK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c49081fab5d934bbcd3802ccff3f83e4f2270f76" title="Using KoboldCpp like its 1999 (noscript mode, Internet Explorer 6)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8hsjp4q1w3we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k43x1h/using_koboldcpp_like_its_1999_noscript_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T03:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4s70i</id>
    <title>Gemini 2.5 - The BEST writing assistant. PERIOD.</title>
    <updated>2025-04-21T23:40:53+00:00</updated>
    <author>
      <name>/u/GrungeWerX</name>
      <uri>https://old.reddit.com/user/GrungeWerX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's get to the point: &lt;strong&gt;Google Gemini 2.5 is THE BEST writing assistant. Period.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've tested everything people have recommended (mostly). I've tried Claude. DeepSeek R1. GPT-4o. Grok 3. Qwen 2.5. Qwen 2.5 VL. QWQ. Mistral variants. Cydonia variants. Gemma variants. Darkest Muse. Ifable. And more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My use case:&lt;/strong&gt; I'm not interested in an LLM writing a script for me. I can do that myself just fine. I want it to work based on a specified template that I give it, and create a detailed treatment based on a set of notes. The template sets the exact format of how it should be done, and provides instructions on my own writing method and goals. I feed it the story notes. Based on my prompt template, I expect it to be able to write a fully functioning treatment.&lt;/p&gt; &lt;p&gt;I want &lt;strong&gt;specifics&lt;/strong&gt;. Not abstract ideas - which most LLMs struggle with - but literal scenes. Show, don't tell.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My expectations&lt;/strong&gt;: Intelligence. Creativity. Context. Relevance. Inventiveness. Nothing contrived. No slop. The notes should drive the drama. The treatment needs to maintain its own consistency. It needs to know what it's doing and why it's doing it. Like a writer.&lt;/p&gt; &lt;p&gt;Every single llm either flat-out failed the assignment, or turned out poor results. The caveat: The template is a bit wordy, and the output will naturally be wordy. I typically expect - at the minimum - 8K ouput, based on the requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemini 2.5 is the only LLM that completed the assignment 100% correctly, and did a really good job.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It isn't perfect. There was one output that started spitting out races and cultures that were obviously from Star Wars. Clearly part of its training data. It was garbage. But that was a one-off.&lt;/p&gt; &lt;p&gt;Subsequent outputs were of varying quality, but generally decent. But the most important part: &lt;strong&gt;all of them correctly completed the assignment.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gemini kept every scene building upon the previous ones. It directed it towards a natural conclusion. It built upon the elements within the story that &lt;strong&gt;IT&lt;/strong&gt; created, and used those to fashion a unique outcome. It succeeded in maintaining the character arc and the character's growth. It was able to complete certain requirements within the story despite not having a lot of specific context provided from my notes. It raised the tension. And above all, it maintained the rigid structure without going off the rails into a random rabbit hole.&lt;/p&gt; &lt;p&gt;At one point, I got so into it that I just reclined, reading from my laptop. The narrative really pulled me in, and I was anticipating every subsequent scene. I'll admit, it was pretty good.&lt;/p&gt; &lt;p&gt;I would grade it a solid 85%. And that's the best any of these LLMs have produced, IMO.&lt;/p&gt; &lt;p&gt;Also, at this point I would say that Gemini holds a significant lead above the other closed source models. OpenAI wasn't even close and tried its best to just rush through the assignment, providing 99% useless drivel. Claude was extremely generic, and most of its ideas were like someone that only glanced at the assignment before turning in their work. There were tons of mistakes it made simply because it just &amp;quot;ignored&amp;quot; the notes.&lt;/p&gt; &lt;p&gt;Keep in mind, this is for writing, and that based on a specific, complex assignment. Not a general &amp;quot;write me a story about x&amp;quot; prompt, which I suspect is what most people are testing these models on. That's useless for most real writers. We need an LLM that can work based on very detailed and complex parameters, and I believe this is how these LLMs should be truly tested. Under those circumstances, I believe many of you guys will find the real world usage doesn't match the benchmarks.&lt;/p&gt; &lt;p&gt;As a side note, I've tested it out on coding, and it failed repeatedly on all of my tasks. People swear it's the god of coding, but that hasn't been my experience. Perhaps my use cases are too simple, perhaps I'm not prompting right, perhaps it works better for more advanced coders. I really don't know. But I digress.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Source Results:&lt;/strong&gt; Sorry guys, but none of the open source apps turned in anything really useful. Some completed the assignment to a degree, but the outputs were often useless, and therefore not worth mentioning. It sucks, because I believe in open source and I'm a big Qwen fan. Maybe Qwen 3 will change things in this department. I hope so. I'll be testing it out when it drops.&lt;/p&gt; &lt;p&gt;If you have any additional suggestions for open source models that you believe can handle the task, let me know.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notable Mentions:&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Gemma-2 Ifable&lt;/em&gt;&lt;/strong&gt; &amp;quot;gets it&amp;quot;, but it couldn't handle the long context and just completely fell apart very early. But Ifable is consistently my go-to for lower context assignments, sometimes partnered with darkest muse. But Ifable is my personal favorite for these sorts of assignments because it just understands what you're trying to do, pays attention to what you're saying, and - unlike other models - pulls out aspects of the story that are just below the surface and expands upon those ideas, enriching the concepts. Other open source models write well, but ifable is the only model I've used that has the presence of really working with a writer, someone who doesn't just spit out sentences/words, but gets the concepts and tries to build upon them and make them better.&lt;/p&gt; &lt;p&gt;My personal desire is for someone to develop an IFable 2, with a significantly larger context window and increased intelligence, because I think - with a little work - it has the potential to be the best open source writing assistant available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrungeWerX"&gt; /u/GrungeWerX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4s70i/gemini_25_the_best_writing_assistant_period/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4s70i/gemini_25_the_best_writing_assistant_period/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4s70i/gemini_25_the_best_writing_assistant_period/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T23:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4oufe</id>
    <title>Orpheus-TTS local speech synthesizer in C#</title>
    <updated>2025-04-21T21:13:48+00:00</updated>
    <author>
      <name>/u/ajpy</name>
      <uri>https://old.reddit.com/user/ajpy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TheAjaykrishnanR/TaraSharp"&gt;Repo&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No python dependencies&lt;/li&gt; &lt;li&gt;No LM Studio&lt;/li&gt; &lt;li&gt;Should work out of the box &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Uses LlamaSharp (llama.cpp) backend for inference and TorchSharp for decoding. Requires .NET 9 and Cuda 12.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajpy"&gt; /u/ajpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oufe/orpheustts_local_speech_synthesizer_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4fwck</id>
    <title>The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension</title>
    <updated>2025-04-21T15:09:57+00:00</updated>
    <author>
      <name>/u/zanatas</name>
      <uri>https://old.reddit.com/user/zanatas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"&gt; &lt;img alt="The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension" src="https://preview.redd.it/v6dt6jrre7we1.gif?width=640&amp;amp;crop=smart&amp;amp;s=083ba989dbd3b12db4098ac42b096e6b6d88fee0" title="The age of AI is upon us and obviously what everyone wants is an LLM-powered unhelpful assistant on every webpage, so I made a Chrome extension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: someone at work made a joke about creating a really unhelpful Clippy-like assistant that exclusively gives you weird suggestions, one thing led to another and I ended up making a whole Chrome extension.&lt;/p&gt; &lt;p&gt;It was part me having the habit of transforming throwaway jokes into very convoluted projects, part a ‚ú®ViBeCoDiNg‚ú® exercise, part growing up in the early days of the internet, where stuff was just dumb/fun for no reason (I blame Johnny Castaway and those damn Macaronis dancing Macarena).&lt;/p&gt; &lt;p&gt;You'll need either Ollama (lets you pick any model, send in page context) or a Gemini API key (likely better/more creative performance, but only reads the URL of the tab). &lt;/p&gt; &lt;p&gt;Full source here: &lt;a href="https://github.com/yankooliveira/toads"&gt;https://github.com/yankooliveira/toads&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zanatas"&gt; /u/zanatas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6dt6jrre7we1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4fwck/the_age_of_ai_is_upon_us_and_obviously_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4blth</id>
    <title>üöÄ Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)</title>
    <updated>2025-04-21T11:51:54+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"&gt; &lt;img alt="üöÄ Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)" src="https://b.thumbs.redditmedia.com/2-0WhWHvw-I2OXhO_fkhxKNabIJ7N9A1mmJoTYLUdfM.jpg" title="üöÄ Run LightRAG on a Bare Metal Server in Minutes (Fully Automated)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuing my journey documenting self-hosted AI tools - today I‚Äôm dropping a new tutorial on how to run the amazing LightRAG project on your own bare metal server with a GPU‚Ä¶ in just minutes ü§Ø&lt;/p&gt; &lt;p&gt;Thanks to full automation (Ansible + Docker Compose + Sbnb Linux), you can go from an empty machine with no OS to a fully running RAG pipeline.&lt;/p&gt; &lt;p&gt;TL;DR: Start with a blank PC with a GPU. End with an advanced RAG system, ready to answer your questions.&lt;/p&gt; &lt;p&gt;Tutorial link: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-LightRAG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy experimenting! Let me know if you try it or run into anything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4blth"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4blth/run_lightrag_on_a_bare_metal_server_in_minutes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4sxh7</id>
    <title>So, is it reasonable to expect the next generation of local oriented models to be QAT out of the oven?</title>
    <updated>2025-04-22T00:16:02+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Gemma3 news and posts all around‚Ä¶ would next Gen of model‚Äôs, Either Dense or MoE, go from 32b to 128b, ‚ÄúQAT‚Äôed‚Äù since training, aiming to be deployed in common VRAM sizes of 8-16-24/32 in the end anyway?&lt;/p&gt; &lt;p&gt;Is QAT less resource intense during training, or is the same?&lt;/p&gt; &lt;p&gt;Just elaborating here‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4sxh7/so_is_it_reasonable_to_expect_the_next_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-22T00:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4p8a1</id>
    <title>HyperAgent: open-source Browser Automation with LLMs</title>
    <updated>2025-04-21T21:29:40+00:00</updated>
    <author>
      <name>/u/LawfulnessFlat9560</name>
      <uri>https://old.reddit.com/user/LawfulnessFlat9560</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"&gt; &lt;img alt="HyperAgent: open-source Browser Automation with LLMs" src="https://external-preview.redd.it/pG2fXTePiHCTWQNdBrjC6ZrKGhk2WsgN52b6QusON9Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3492e14aad472fd9e73368bb9ecf897800ec09b0" title="HyperAgent: open-source Browser Automation with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to show you &lt;strong&gt;HyperAgent&lt;/strong&gt;, a wrapper around Playwright that lets you control pages with LLMs.&lt;/p&gt; &lt;p&gt;With HyperAgent, you can run functions like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;await page.ai(&amp;quot;search for noise-cancelling headphones under $100 and click the best option&amp;quot;); or const data = await page.ai( &amp;quot;Give me the director, release year, and rating for 'The Matrix'&amp;quot;, { outputSchema: z.object({ director: z.string().describe(&amp;quot;The name of the movie director&amp;quot;), releaseYear: z.number().describe(&amp;quot;The year the movie was released&amp;quot;), rating: z.string().describe(&amp;quot;The IMDb rating of the movie&amp;quot;), }), } ); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We built this because automation is still too brittle and manual. HTML keeps changing and selectors break constantly, Writing full automation scripts is overkill for quick one-offs. Also, and possibly most importantly, AI Agents need some way to interact with the web with natural language.&lt;/p&gt; &lt;p&gt;Excited to see what you all think! We are rapidly adding new features so would love any ideas for how we can make this better :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LawfulnessFlat9560"&gt; /u/LawfulnessFlat9560 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hyperbrowserai/HyperAgent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4p8a1/hyperagent_opensource_browser_automation_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k49h0n</id>
    <title>24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?</title>
    <updated>2025-04-21T09:35:34+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt; &lt;img alt="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" src="https://external-preview.redd.it/WdUGpP3unGKFZihZrELR3GH6ZUOa768rHIdn2YSXrsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=461a9ba85d2877f5c00bb8c11f93f1ceac11d893" title="24GB Arc GPU might still be on the way - less expensive alternative for a 3090/4090/7900XTX to run LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/sparkle-confirms-arc-battlemage-gpu-with-24gb-memory-slated-for-may-june"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k49h0n/24gb_arc_gpu_might_still_be_on_the_way_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T09:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4gqje</id>
    <title>[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli</title>
    <updated>2025-04-21T15:46:41+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt; &lt;img alt="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" src="https://external-preview.redd.it/chyB3Fwy2UcKLBJMyzabSe7PfMaM2G1ZJw5k660LQOY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97a934134489f4b1d7e573c0218731d1a8a5d5b" title="[llama.cpp git] mtmd: merge llava, gemma3 and minicpmv CLI into single llama-mtmd-cli" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/84a9bf2fc2875205f0806fbbfbb66dc67204094c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4gqje/llamacpp_git_mtmd_merge_llava_gemma3_and_minicpmv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4m3az</id>
    <title>Here is the HUGE Ollama main dev contribution to llamacpp :)</title>
    <updated>2025-04-21T19:21:36+00:00</updated>
    <author>
      <name>/u/Nexter92</name>
      <uri>https://old.reddit.com/user/Nexter92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt; &lt;img alt="Here is the HUGE Ollama main dev contribution to llamacpp :)" src="https://b.thumbs.redditmedia.com/70s_LNOtMI87Du6TdCS17PIa9bztkX6iXhhbwCa8IeY.jpg" title="Here is the HUGE Ollama main dev contribution to llamacpp :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Less than 100 lines of code ü§°&lt;/p&gt; &lt;p&gt;If you truly want to support open source LLM space, use anything else than ollama specily if you have an AMD GPU, you loose way to much performance in text generation using ROCm with ollama.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6979nmxwm8we1.png?width=2020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91e49f15bee12d308716de607ce6763b8e1870b3"&gt;https://preview.redd.it/6979nmxwm8we1.png?width=2020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=91e49f15bee12d308716de607ce6763b8e1870b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nexter92"&gt; /u/Nexter92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4m3az/here_is_the_huge_ollama_main_dev_contribution_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T19:21:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4avlq</id>
    <title>What's the best models available today to run on systems with 8 GB / 16 GB / 24 GB / 48 GB / 72 GB / 96 GB of VRAM today?</title>
    <updated>2025-04-21T11:08:51+00:00</updated>
    <author>
      <name>/u/Severin_Suveren</name>
      <uri>https://old.reddit.com/user/Severin_Suveren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, since many aren't that experienced with running local LLMs and the choice of models, what are the best models available today for the different ranges of VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severin_Suveren"&gt; /u/Severin_Suveren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T11:08:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4oqpi</id>
    <title>Skywork releases SkyReels-V2 - unlimited duration video generation model</title>
    <updated>2025-04-21T21:09:27+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"&gt; &lt;img alt="Skywork releases SkyReels-V2 - unlimited duration video generation model" src="https://b.thumbs.redditmedia.com/b-ouJGfwVRVGxRTcTn9sJ47FxSTE7u69a29L1PRi4yg.jpg" title="Skywork releases SkyReels-V2 - unlimited duration video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Available in 1.3B and 14B, these models allow us to generate Infinite-Length videos. &lt;/p&gt; &lt;p&gt;They support both text-to-video (T2V) and image-to-video (I2V)tasks.&lt;/p&gt; &lt;p&gt;According to the benchmarks shared in model‚Äôs card, SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2504.13074"&gt;https://huggingface.co/papers/2504.13074&lt;/a&gt; Models: &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All-in-one creator toolkit and guide: &lt;a href="https://x.com/ai_for_success/status/1914159352812036463?s=46"&gt;https://x.com/ai_for_success/status/1914159352812036463?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k4oqpi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4oqpi/skywork_releases_skyreelsv2_unlimited_duration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:09:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4ov9e</id>
    <title>Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks</title>
    <updated>2025-04-21T21:14:47+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"&gt; &lt;img alt="Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks" src="https://external-preview.redd.it/dXlleHRwcW03OXdlMTJfwlZ1QfIuL9mmXOeUB99y5PuEqD7QQlGvCc8SfvTb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ed0a94e182a642f2f9da5dfdbf4855a0e1e9902" title="Meta Perception Language Model: Enhancing Understanding of Visual Perception Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuing their work on perception, Meta is releasing the Perception Language Model (PLM), an open and reproducible vision-language model designed to tackle challenging visual recognition tasks.&lt;/p&gt; &lt;p&gt;Meta trained PLM using synthetic data generated at scale and open vision-language understanding datasets, without any distillation from external models. They then identified key gaps in existing data for video understanding and collected 2.5 million new, human-labeled fine-grained video QA and spatio-temporal caption samples to fill these gaps, forming the largest dataset of its kind to date.&lt;/p&gt; &lt;p&gt;PLM is trained on this massive dataset, using a combination of human-labeled and synthetic data to create a robust, accurate, and fully reproducible model. PLM offers variants with 1, 3, and 8 billion parameters, making it well suited for fully transparent academic research.&lt;/p&gt; &lt;p&gt;Meta is also sharing a new benchmark, PLM-VideoBench, which focuses on tasks that existing benchmarks miss: fine-grained activity understanding and spatiotemporally grounded reasoning. It is hoped that their open and large-scale dataset, challenging benchmark, and strong models together enable the open source community to build more capable computer vision systems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498"&gt;Download the model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/perception_models"&gt;Download the code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/datasets/plm-data/"&gt;Download the dataset&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/"&gt;Read the paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5n4izmqm79we1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4ov9e/meta_perception_language_model_enhancing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T21:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4juhd</id>
    <title>Don‚Äôt Trust This Woman ‚Äî She Keeps Lying</title>
    <updated>2025-04-21T17:53:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt; &lt;img alt="Don‚Äôt Trust This Woman ‚Äî She Keeps Lying" src="https://b.thumbs.redditmedia.com/Ynyt95TWgEz727VUzv3tN5Ii66-Y9GWTf5C3vgNu0QA.jpg" title="Don‚Äôt Trust This Woman ‚Äî She Keeps Lying" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j64rtjys78we1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dad28af7dddad7a111f8585f646fa0fb66940fc"&gt;Qwen Official Denial&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e6t4n4x78we1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31e0c2ca15b925b8d06dfede2548d4c375505196"&gt;New Deepseek Rumor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4juhd/dont_trust_this_woman_she_keeps_lying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T17:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4god7</id>
    <title>GLM-4 32B is mind blowing</title>
    <updated>2025-04-21T15:41:35+00:00</updated>
    <author>
      <name>/u/Timely_Second_6414</name>
      <uri>https://old.reddit.com/user/Timely_Second_6414</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt; &lt;img alt="GLM-4 32B is mind blowing" src="https://external-preview.redd.it/KGA1Keg1D7oCkdV6UW_ifq_mQe-5jNP1DvhwwJ2Stbs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128cb2de26af5c23c04d8bec8b39d61ce2d36274" title="GLM-4 32B is mind blowing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/815w430kg7we1/player"&gt;GLM-4 32B pygame earth simulation, I tried this with gemini 2.5 flash which gave an error as output.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Title says it all. I tested out GLM-4 32B Q8 locally using PiDack's llama.cpp pr (&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12957/"&gt;https://github.com/ggml-org/llama.cpp/pull/12957/&lt;/a&gt;) as ggufs are currently broken.&lt;/p&gt; &lt;p&gt;I am absolutely amazed by this model. It outperforms every single other ~32B local model and even outperforms 72B models. It's literally Gemini 2.5 flash (non reasoning) at home, but better. It's also fantastic with tool calling and works well with cline/aider.&lt;/p&gt; &lt;p&gt;But the thing I like the most is that this model is not afraid to output a lot of code. It does not truncate anything or leave out implementation details. Below I will provide an example where it 0-shot produced 630 lines of code (I had to ask it to continue because the response got cut off at line 550). I have no idea how they trained this, but I am really hoping qwen 3 does something similar. &lt;/p&gt; &lt;p&gt;Below are some examples of 0 shot requests comparing GLM 4 versus gemini 2.5 flash (non-reasoning). GLM is run locally with temp 0.6 and top_p 0.95 at Q8. Output speed is 22t/s for me on 3x 3090.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solar system&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: Create a realistic rendition of our solar system using html, css and js. Make it stunning! reply with one file.&lt;/p&gt; &lt;p&gt;Gemini response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/vhn6r9kmi7we1/player"&gt;Gemini 2.5 flash: nothing is interactible, planets dont move at all&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM response:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/ylcl9s4ri7we1/player"&gt;GLM-4-32B response. Sun label and orbit rings are off, but it looks way better and theres way more detail.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Neural network visualization&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;prompt: code me a beautiful animation/visualization in html, css, js of how neural networks learn. Make it stunningly beautiful, yet intuitive to understand. Respond with all the code in 1 file. You can use threejs&lt;/p&gt; &lt;p&gt;Gemini:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/nkgj1wc1j7we1/player"&gt;Gemini response: network looks good, but again nothing moves, no interactions.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4:&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1k4god7/video/equidag5j7we1/player"&gt;GLM 4 response (one shot 630 lines of code): It tried to plot data that will be fit on the axes. Although you dont see the fitting process you can see the neurons firing and changing in size based on their weight. Theres also sliders to adjust lr and hidden size. Not perfect, but still better.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also did a few other prompts and GLM generally outperformed gemini on most tests. Note that this is only Q8, I imaging full precision might be even a little better. &lt;/p&gt; &lt;p&gt;Please share your experiences or examples if you have tried the model. I havent tested the reasoning variant yet, but I imagine its also very good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Second_6414"&gt; /u/Timely_Second_6414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4god7/glm4_32b_is_mind_blowing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T15:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k4lmil</id>
    <title>A new TTS model capable of generating ultra-realistic dialogue</title>
    <updated>2025-04-21T19:02:56+00:00</updated>
    <author>
      <name>/u/aadoop6</name>
      <uri>https://old.reddit.com/user/aadoop6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt; &lt;img alt="A new TTS model capable of generating ultra-realistic dialogue" src="https://external-preview.redd.it/wNcjaJIfjy5w8Wuatdn7tqqANxkzwO5-UB9WQmMCT3w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f1d15a76610dd0dbe8a436684ca2985b2cc492b" title="A new TTS model capable of generating ultra-realistic dialogue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aadoop6"&gt; /u/aadoop6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nari-labs/dia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k4lmil/a_new_tts_model_capable_of_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-21T19:02:56+00:00</published>
  </entry>
</feed>
