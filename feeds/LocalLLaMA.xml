<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-31T10:23:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n451ka</id>
    <title>GPT OSS Fine-tuning QAT</title>
    <updated>2025-08-30T15:51:53+00:00</updated>
    <author>
      <name>/u/Short_Struggle7803</name>
      <uri>https://old.reddit.com/user/Short_Struggle7803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read more about our (Nvidia) end to end example on GPT OSS fine tuning QAT + SGlang deployment 👉 &lt;a href="https://lmsys.org/blog/2025-08-28-gpt-oss-qat/"&gt;https://lmsys.org/blog/2025-08-28-gpt-oss-qat/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine-tuning QAT helps keep the original MXFP4 quantization of GPT OSS while adapting to downstream task.&lt;/p&gt; &lt;p&gt;We have some example results (and comparisons to Nvidia’s NVFP4 format) here :&lt;/p&gt; &lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/"&gt;https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do checkout 🙃!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short_Struggle7803"&gt; /u/Short_Struggle7803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4qyr0</id>
    <title>Need help with lm studio context issue</title>
    <updated>2025-08-31T10:07:26+00:00</updated>
    <author>
      <name>/u/No_Disk_6915</name>
      <uri>https://old.reddit.com/user/No_Disk_6915</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I am using a 10k context length and have set the context management to stop at limit, i have a system prompt of 1500 tokens when I send the exact same back to back 6 queries to the model each being about 200 tokens. If this is done in chat the model acts way smarter and uses and understands the system prompt as I expect it to be understood and the total context length that is shown at the bottom after each query only increase by the said query amount so after 6 queries it's about 3000 , but if same is done through requests made with API the model just stops after 6th query which i think is due to the stop at limit, but why is this happening i sending the same amount but in chat it only fills context to 3k but with API it exceeds the 10k limit even though the docs say that each request is treated as a new chat IE stateless no history mode yet this happens any fix also the model is way dumb when talked through api using same queries and just to be clear I am not sending any system prompt through api as I have set the system prompt server side in lm studio through the context bar on the server management page &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Disk_6915"&gt; /u/No_Disk_6915 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4qyr0/need_help_with_lm_studio_context_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4qyr0/need_help_with_lm_studio_context_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4qyr0/need_help_with_lm_studio_context_issue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T10:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3xxm5</id>
    <title>🌟Introducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinking🌟</title>
    <updated>2025-08-30T10:08:44+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3- &lt;strong&gt;Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to &amp;quot;think in rap lyrics&amp;quot; or &amp;quot;use bullet points to organize thoughts&amp;quot; and it will literally reason that way before giving you an answer.&lt;/p&gt; &lt;p&gt;You can check out the model on HuggingFace: &lt;a href="https://huggingface.co/AGI-0/Art-0-8B"&gt;https://huggingface.co/AGI-0/Art-0-8B&lt;/a&gt; (please leave a like in the repo if you like this model)&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card &lt;a href="https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab"&gt;https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T10:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3ldon</id>
    <title>Qwen3-coder is mind blowing on local hardware (tutorial linked)</title>
    <updated>2025-08-29T22:35:27+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt; &lt;img alt="Qwen3-coder is mind blowing on local hardware (tutorial linked)" src="https://external-preview.redd.it/MHAyYm12N3NjMW1mMWyTIaaq8py0BbLEXek7RrX8ohVlR1FrRoAdOlxuqQ67.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d298923517a79cfa7fc3e04c1533fbc4c70a8f3b" title="Qwen3-coder is mind blowing on local hardware (tutorial linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello!&lt;/p&gt; &lt;p&gt;I'm honestly blown away by how far local models have gotten in the past 1-2 months. Six months ago, local models were completely useless in Cline, which tbf is pretty heavyweight in terms of context and tool-calling demands. And then a few months ago I found one of the qwen models to actually be somewhat usable, but not for any real coding.&lt;/p&gt; &lt;p&gt;However, qwen3-coder-30B is really impressive. 256k context and is actually able to complete tool calls and diff edits reliably in Cline. I'm using the 4-bit quantized version on my 36GB RAM Mac.&lt;/p&gt; &lt;p&gt;My machine does turn into a bit of a jet engine after a while, but the performance is genuinely useful. My setup is LM Studio + Qwen3 Coder 30B + Cline (VS Code extension). There are some critical config details that can break it (like disabling KV cache quantization in LM Studio), but once dialed in, it just works.&lt;/p&gt; &lt;p&gt;This feels like the first time local models have crossed the threshold from &amp;quot;interesting experiment&amp;quot; to &amp;quot;actually useful coding tool.&amp;quot; I wrote a full technical walkthrough and setup guide: &lt;a href="https://cline.bot/blog/local-models"&gt;https://cline.bot/blog/local-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/75bfhw7sc1mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T22:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4p27l</id>
    <title>Dual PCIe CPU Slots vs Dual PCIe (CPU and Chipset)</title>
    <updated>2025-08-31T08:03:32+00:00</updated>
    <author>
      <name>/u/Daniokenon</name>
      <uri>https://old.reddit.com/user/Daniokenon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, wonderful community! I have a question about performance. Is there a real difference in performance (for the same graphics cards) when they are connected to slots that are connected to the CPU vs. mixed CPU and chipset slots?&lt;/p&gt; &lt;p&gt;The question concerns consumer motherboards (currently I use two cards - one in the CPU PCIe slot, the other in the chipset PCIe slot) I use it with LLM and text generation - mainly Vulkan, sometimes ROCm.&lt;/p&gt; &lt;p&gt;I'm planning to upgrade my motherboard soon, and I'm wondering if it's worth getting one that has both slots connected to the CPU—there aren't many like that.&lt;/p&gt; &lt;p&gt;Do both PCIe slots connected to the CPU make any real difference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniokenon"&gt; /u/Daniokenon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4p27l/dual_pcie_cpu_slots_vs_dual_pcie_cpu_and_chipset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4p27l/dual_pcie_cpu_slots_vs_dual_pcie_cpu_and_chipset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4p27l/dual_pcie_cpu_slots_vs_dual_pcie_cpu_and_chipset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T08:03:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4329n</id>
    <title>How do you people run GLM 4.5 locally ?</title>
    <updated>2025-08-30T14:30:45+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context i have a dual rtx 3090 rig with 128gb of ddr5 ram and no matter what i try i get around 6 tokens per second...&lt;br /&gt; On CPU only inference i get between 5 and 6 tokens while on partial GPU offload i get between 5.5 and 6.8 tokens.&lt;br /&gt; I tried 2 different versions the one from unsloth Q4_K_S (&lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt;) and the one from LovedHeart MXFP4 (&lt;a href="https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1%5C_M"&gt;https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1\_M&lt;/a&gt;)&lt;br /&gt; The one from unsloth is 1 token per second slower but still no story change.&lt;br /&gt; I changed literally all settings from lmstudio, even managed to get it to load with the full 131k context but still nowhere near the speed other users get on a single 3090 with offloading.&lt;br /&gt; I tried installing vllm but i get too much errors and i gave up.&lt;br /&gt; Is there another program i should try ? Have i chose the wrong models ?&lt;br /&gt; It's really frustrating and it's taking me too much hours to solve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4nf4b</id>
    <title>Advice on AI workstation for research use-cases</title>
    <updated>2025-08-31T06:19:12+00:00</updated>
    <author>
      <name>/u/fIoatingworld</name>
      <uri>https://old.reddit.com/user/fIoatingworld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;I am investing in a workstation to locally host LLMs for myself, and to test research designs that use LLMs that I need absolute control over w.r.t system prompts and guardrails. For privacy and safety reasons, as well as reproducability of research, my team doesn't want to rely on the leading APIs. My end goals are to be able to run 70b models for myself and to host 13b-30b models for 2-3 other testers, probably not concurrently in most cases.&lt;/p&gt; &lt;p&gt;Outside of this, I'll use this PC to handle my R tasks, including MCMC and spatial data processes. I currently use an M2 Pro 16gb Macbook for these tasks but I'm getting to the upper bound of what it can do within reasonable time. I plan on using Ubuntu if it matters. &lt;/p&gt; &lt;p&gt;The budget is around 2.5k. I like the Framework desktop but have heard it's not the best bang for your buck, and not sufficiently modular. My main worry is going obsolete in a few years, with a product that doesn't have as much resell value as say a Mac. With the release of some local models far above 70B, I also fear the machine not being too capable in a year or two.&lt;/p&gt; &lt;p&gt;Any thoughts on these would be greatly appreciated! Thanks for your time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fIoatingworld"&gt; /u/fIoatingworld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4nf4b/advice_on_ai_workstation_for_research_usecases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4nf4b/advice_on_ai_workstation_for_research_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4nf4b/advice_on_ai_workstation_for_research_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4m30g</id>
    <title>ISO: VS Code + LocalLlama help</title>
    <updated>2025-08-31T04:59:22+00:00</updated>
    <author>
      <name>/u/1EvilSexyGenius</name>
      <uri>https://old.reddit.com/user/1EvilSexyGenius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt; &lt;img alt="ISO: VS Code + LocalLlama help" src="https://preview.redd.it/de87bjkbeamf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fccda791b6be5274998a0a9f3d601c60b70c374" title="ISO: VS Code + LocalLlama help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do I connect vs code's latest default AI chat system to use my locally running llamacpp server that's currently running in OpenAI API mode? &lt;/p&gt; &lt;p&gt;Is this even possible? 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1EvilSexyGenius"&gt; /u/1EvilSexyGenius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/de87bjkbeamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4m30g/iso_vs_code_localllama_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4matr</id>
    <title>how to use intel npu for lm studio</title>
    <updated>2025-08-31T05:12:05+00:00</updated>
    <author>
      <name>/u/TopSecre-t</name>
      <uri>https://old.reddit.com/user/TopSecre-t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt; &lt;img alt="how to use intel npu for lm studio" src="https://a.thumbs.redditmedia.com/oF_VLz5o0T7sHHvIj_JdH0QlI_y3btKgoFY02MsEXn0.jpg" title="how to use intel npu for lm studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have intel core ultra 7 255hx and rtx 5060 with 8gb vram&lt;/p&gt; &lt;p&gt;the intel cpu has an integrated npu and it have 18gb shared memory also the gpu tab shows total 26gb memory with its 8gb vram&lt;/p&gt; &lt;p&gt;but how do i use the 18gb shared memory? when i ask something on my local ai on lm studio it only uses the gpu vram of 8gb.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TopSecre-t"&gt; /u/TopSecre-t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n4matr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4matr/how_to_use_intel_npu_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n40ngf</id>
    <title>What is the slowest Token/sec you can live with?</title>
    <updated>2025-08-30T12:41:26+00:00</updated>
    <author>
      <name>/u/OrganicApricot77</name>
      <uri>https://old.reddit.com/user/OrganicApricot77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me:&lt;/p&gt; &lt;p&gt;5tok/s is the slowest I’ll accept&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicApricot77"&gt; /u/OrganicApricot77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n45lx2</id>
    <title>GLM-4.5V model for Computer Use</title>
    <updated>2025-08-30T16:15:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt; &lt;img alt="GLM-4.5V model for Computer Use" src="https://external-preview.redd.it/MjZwOTdqc3psNm1mMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f81993da62e75eef832974ce71abbca064f96f" title="GLM-4.5V model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either locally via Hugging Face or Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eicije20m6mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4pspb</id>
    <title>GPT-OSS VLLM RTX 6000 PRO blackwell sm120</title>
    <updated>2025-08-31T08:51:43+00:00</updated>
    <author>
      <name>/u/festr2</name>
      <uri>https://old.reddit.com/user/festr2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Install the latest dev VLLM (no compilation needed)&lt;/p&gt; &lt;p&gt;I have gathered this post from vllm github PR, can anyone explain if this is using the native fp4 feature on rtx pro or if this transforms it to the FP8 internally? &lt;/p&gt; &lt;p&gt;It is fully working though&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#I'm using ubuntu 24.04 with installed basic develop packages including cuda 12.9 #vllm recently switched to the pytorch 2.8.0 the installation is dead simple now: conda create -n vllm6 python=3.12 -y conda activate vllm6 wget https://vllm-wheels.s3.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl pip uninstall torch pip install vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl pip install flashinfer-python export VLLM_SKIP_P2P_CHECK=1 export VLLM_USE_FLASHINFER_MOE_FP8=1 export VLLM_USE_FLASHINFER_MOE_FP4=1 export VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8=1 export VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB='{&amp;quot;2&amp;quot;:32,&amp;quot;4&amp;quot;:32,&amp;quot;8&amp;quot;:8}' # ASYNC_SCHEDULING_FLAG=&amp;quot;--async-scheduling&amp;quot; ASYNC_SCHEDULING_FLAG=&amp;quot;&amp;quot; FUSION_FLAG='{&amp;quot;pass_config&amp;quot;:{&amp;quot;enable_fi_allreduce_fusion&amp;quot;:true,&amp;quot;enable_attn_fusion&amp;quot;:true,&amp;quot;enable_noop&amp;quot;:true},&amp;quot;custom_ops&amp;quot;:[&amp;quot;+quant_fp8&amp;quot;,&amp;quot;+rms_norm&amp;quot;],&amp;quot;cudagraph_mode&amp;quot;:&amp;quot;FULL_DECODE_ONLY&amp;quot;,&amp;quot;splitting_ops&amp;quot;:[]}' vllm serve ${MODEL_NAME} \ --host 0.0.0.0 \ --port 8000 \ --kv-cache-dtype auto \ --trust-remote-code \ --gpu-memory-utilization 0.9 \ --compilation-config ${FUSION_FLAG} \ ${ASYNC_SCHEDULING_FLAG} \ --enable-chunked-prefill \ --no-enable-prefix-caching \ --pipeline-parallel-size 1 \ --tensor-parallel-size 1 --enable-expert-parallel \ --max-num-seqs 128 \ --max-num-batched-tokens 8192 \ --max-model-len 2048 &amp;amp; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/festr2"&gt; /u/festr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pspb/gptoss_vllm_rtx_6000_pro_blackwell_sm120/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pspb/gptoss_vllm_rtx_6000_pro_blackwell_sm120/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pspb/gptoss_vllm_rtx_6000_pro_blackwell_sm120/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T08:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n48lcx</id>
    <title>10,000 $ Budget for a rig that will run ai (24/7)</title>
    <updated>2025-08-30T18:18:07+00:00</updated>
    <author>
      <name>/u/Holiday_Leg8427</name>
      <uri>https://old.reddit.com/user/Holiday_Leg8427</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I want to make myself at home a set-up that can run AI 24/7, i need it mainly to replace the general use of llms ( chatgpt, gemini etc...), was thinking and saw lots of posts and info aboout getting an macbook studio with maximum ram capacity, is that the best way?&lt;br /&gt; Thank you for your responses (in advance)!&lt;/p&gt; &lt;p&gt;Edit: Guys, i dont think i need a llm anymore, Imma just ask on reddit everything I need, and get result from you guys, thank for all the help and tips, and btw I have some sort of &amp;quot;credit&amp;quot; (and i can write it off basically entirely on my company) for a high end pc/pc parts, thats why I wanted to invest into something that can be as usefull as possible, and i work with many legal/medical teams in europe where gdpr is king&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Leg8427"&gt; /u/Holiday_Leg8427 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mo1r</id>
    <title>Best Way to Use Qwen3-Coder for Local AI Coding?</title>
    <updated>2025-08-31T05:33:56+00:00</updated>
    <author>
      <name>/u/Creative-Scene-6743</name>
      <uri>https://old.reddit.com/user/Creative-Scene-6743</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for some guidance on setting up Qwen Coder models locally for AI-assisted coding work. Normally I’d dive in and figure it out myself, but between work and vacation, I’ve fallen behind and want to avoid a time-consuming rabbit hole.&lt;/p&gt; &lt;p&gt;I have a couple of key questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How close have you gotten Qwen Code to rival Claude's code capabilities?&lt;/strong&gt; I’m particularly interested in performance for actual dev work, not just benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What’s the best setup you’ve found so far?&lt;/strong&gt; Are you integrating Qwen into an existing Claude Code by swapping the model? Are you using a like cline integration or something else entirely?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any lessons learned or tips would be hugely appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Scene-6743"&gt; /u/Creative-Scene-6743 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4hkar</id>
    <title>I tried almost every tts model on my ryzen 7 5000 series 16gb ram rtx 3060 laptop 6-8GB Vram</title>
    <updated>2025-08-31T00:58:19+00:00</updated>
    <author>
      <name>/u/This_is_difficult_0</name>
      <uri>https://old.reddit.com/user/This_is_difficult_0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I needed a decent voiceover for my projects and I am using minimax audio currently. It's really good. But I wanted to try some local tts and i tested around 5 of them and here are my results.&lt;/p&gt; &lt;h1&gt;1. Kokoro - 0.4-0.5 x realtime .Fast and decent quality&lt;/h1&gt; &lt;p&gt;Fast koko api one from github has lower quality than the hexgrad. &lt;/p&gt; &lt;h1&gt;2. Xtts v2 - 0.8-1x realtime tried it from pinokio ai : speed is little slower than kokoro but decent quality with cloning capabilities.&lt;/h1&gt; &lt;h1&gt;3. Chatterbox - if you have lower end gpu and cpu like me don't try ! It's slow (1.5-2x realtime but high quality , expressive. I found out when i wanted it to be neutral or soothing, it still exaggerated or sounded too robotic but not in the middle.&lt;/h1&gt; &lt;p&gt;Installation was painful for me. Took me whole day figuring out to improve the speed. I tried different forks from github and messed up file paths😂 Extremely slow , so not useful for me.&lt;/p&gt; &lt;h1&gt;4. Zonos : 2-3x realtime Slow... Uninstalled it pretty soon&lt;/h1&gt; &lt;h1&gt;5. Orpheus : yet to try. Looks expressive but mismatch the rythm frequently in the huggin space. Maybe it will be slow on my laptop too&lt;/h1&gt; &lt;h1&gt;6. F5tts : around 1x realtime tried it from pinokio ai. Looks decent but somewhere in the middle, it hallucinates , so not useful&lt;/h1&gt; &lt;p&gt;I am thinking of trying Piper next. I want a lightweight better than kokoro. For now , i would rank kokoro as the best option for my pc. Do you have any good suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_is_difficult_0"&gt; /u/This_is_difficult_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T00:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46mk9</id>
    <title>LongCat-Flash-Chat is here, yet another Chinese open weight model</title>
    <updated>2025-08-30T16:56:47+00:00</updated>
    <author>
      <name>/u/MindlessScrambler</name>
      <uri>https://old.reddit.com/user/MindlessScrambler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt; &lt;img alt="LongCat-Flash-Chat is here, yet another Chinese open weight model" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="LongCat-Flash-Chat is here, yet another Chinese open weight model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Chat"&gt;https://github.com/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web: &lt;a href="https://longcat.ai"&gt;https://longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f"&gt;https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindlessScrambler"&gt; /u/MindlessScrambler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4o0sw</id>
    <title>Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp; Mistral support</title>
    <updated>2025-08-31T06:57:26+00:00</updated>
    <author>
      <name>/u/Strange-Dimension675</name>
      <uri>https://old.reddit.com/user/Strange-Dimension675</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"&gt; &lt;img alt="Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp;amp; Mistral support" src="https://preview.redd.it/64ik5rfbzamf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c15775d90e863fae68ed2abc166444fd9ad0d057" title="Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp;amp; Mistral support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m a 20yo student and this is my first project. I’m building a BMO robot from scratch using a Raspberry Pi 5. This repo is the voice assistant part, but it’s also useful as a general-purpose voice assistant.&lt;/p&gt; &lt;p&gt;Key features: • Automatic switching between OpenAI and local Mistral server (script included) • Easily extensible to other APIs or custom agents • Classes interacting with APIs follow a consistent structure for simplicity&lt;/p&gt; &lt;p&gt;It’s still a hobby project and far from perfect, but I thought it could be interesting for anyone experimenting with local LLMs or voice assistants.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ivegotanheadache/BMO"&gt;https://github.com/ivegotanheadache/BMO&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strange-Dimension675"&gt; /u/Strange-Dimension675 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64ik5rfbzamf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4pt0x</id>
    <title>Top-k 0 vs 100 on GPT-OSS-120b</title>
    <updated>2025-08-31T08:52:17+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"&gt; &lt;img alt="Top-k 0 vs 100 on GPT-OSS-120b" src="https://preview.redd.it/1p4r0n9gibmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a722fa2967b2e81a9bf33eb4c23859a3dd096ec7" title="Top-k 0 vs 100 on GPT-OSS-120b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using a M4 Max Macbook Pro 128 GB I am comparing the speed boost of setting top-k to 100. OpenAI says to set top-k to 0 while Unsloth proposes that one could try 100 instead. &lt;/p&gt; &lt;p&gt;Top-k 0 means use the full vocabulary of the model. Any other value specifies that we should only consider the top k most likely tokens of the vocabulary. If the value is too small, we might get a worse response from the model. Typical values for top-k seems to be 20-40 and 100 would be considered a relatively large value. By using a large value we aim to get the same result as top-k 0 but faster.&lt;/p&gt; &lt;p&gt;My test shows a very substantial gain by using top-k 100.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1p4r0n9gibmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T08:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mxrj</id>
    <title>MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max.</title>
    <updated>2025-08-31T05:50:09+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt; &lt;img alt="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." src="https://preview.redd.it/co3taaq1mamf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a7f592ad4e744736d9c8af891ea21008331265e" title="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co3taaq1mamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4n9yx</id>
    <title>56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)</title>
    <updated>2025-08-31T06:10:19+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt; &lt;img alt="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" src="https://preview.redd.it/k0im592roamf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf3307b2439da45726bdc7e079057842ef0c0904" title="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally planned to put the 3090 in a lower x4 slot, but it wouldn't fit to PSU case clearance. Builder put the 3090 in the upper x16 slot instead, and the 5090 just barely fit in the second x16.&lt;br /&gt; Both cards running x8/x8 rather than the original planned x16/x4 configuration - but I'm cool with it. The 3090 fans are literally 1mm from the backplate of the 5090 yet the thermals are fine with 7x 140mm case fans. After the anxiety of my dream build I'm not doing heavy testing yet, but now looking to get into serious fine-tuning pretty soon. &lt;/p&gt; &lt;p&gt;I've the developer of a local AI app designed for dual GPU systems (&lt;a href="https://github.com/boneylizard/Eloquent"&gt;https://github.com/boneylizard/Eloquent&lt;/a&gt;) and I've found that with expanded capabilities comes expanded imagination. Haven't done a git push in a while and there's an issue I really need to get around to addressing, but that explains the build. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0im592roamf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4lh7s</id>
    <title>GPT-OSS-120B on Single RTX 6000 PRO</title>
    <updated>2025-08-31T04:24:33+00:00</updated>
    <author>
      <name>/u/Ill_Recipe7620</name>
      <uri>https://old.reddit.com/user/Ill_Recipe7620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 120 token/second with a single RTX 6000 PRO. It took 33 seconds to crank out the following answer after thinking for just 3 seconds.&lt;/p&gt; &lt;p&gt;If people aren't impressed with this model then they cannot be impressed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &amp;quot;compare the heat transfer coefficient of liquid water at a high pressure to steam at these conditions and IGNORE the boiling part only convection, search the web if you ned any information&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heat‑transfer‑coefficient ( h ) comparison – liquid water vs. steam at the same high pressure&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(boiling/phase‑change is deliberately excluded – only convective transport is considered)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1. What “the same conditions” really means&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantity&lt;/th&gt; &lt;th align="left"&gt;Typical high‑pressure case&lt;/th&gt; &lt;th align="left"&gt;Why it matters for h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Pressure&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5–20 MPa (≈50–200 bar) – common in super‑critical boilers, geothermal wells, high‑pressure reactors&lt;/td&gt; &lt;td align="left"&gt;Raises density of both phases, compresses the gas, and moves steam toward the liquid side of the phase diagram.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Bulk temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;350–550 °C (well above the normal boiling point of water)&lt;/td&gt; &lt;td align="left"&gt;Determines fluid properties (µ, k, ρ, Cp, Pr) that appear in the dimensionless groups governing convection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Flow regime&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Forced convection in a pipe or channel, fully developed turbulent flow (Re &amp;gt; 10 000) – the regime that gives the largest, most useful h values.&lt;/td&gt; &lt;td align="left"&gt;The Nusselt number correlations (Dittus‑Boelter, Gnielinski, Sieder‑Tate…) are based on turbulent forced flow.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geometry&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Typically a circular tube, D = 10–30 mm; length long enough for fully‑developed flow.&lt;/td&gt; &lt;td align="left"&gt;Geometry enters the Reynolds number and the hydraulic diameter used in correlations.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mass‑flow rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Chosen so that the &lt;strong&gt;mean velocity&lt;/strong&gt; &lt;em&gt;U&lt;/em&gt; is the same for both fluids (e.g., 2 m s⁻¹). This isolates the effect of the fluid properties.&lt;/td&gt; &lt;td align="left"&gt;With the same U, the only differences in h arise from ρ, µ, k, Cp, Pr.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Governing dimensionless groups&lt;/h1&gt; &lt;p&gt;For turbulent forced convection in a smooth pipe the Nusselt number is usually expressed as&lt;/p&gt; &lt;p&gt;where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Re&lt;/em&gt;=&lt;em&gt;μρUD&lt;/em&gt;​ – Reynolds number (inertia/viscous)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Pr&lt;/em&gt;=&lt;em&gt;kcp&lt;/em&gt;​&lt;em&gt;μ&lt;/em&gt;​ – Prandtl number (momentum/thermal diffusivity)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Nu&lt;/em&gt;=&lt;em&gt;khD&lt;/em&gt;​ – Nusselt number (convective/ conductive)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The constants &lt;em&gt;C&lt;/em&gt;,&lt;em&gt;m&lt;/em&gt;,&lt;em&gt;n&lt;/em&gt; depend on the correlation (Dittus‑Boelter, Gnielinski, etc.).&lt;br /&gt; The heat‑transfer coefficient follows directly:&lt;/p&gt; &lt;p&gt;Thus &lt;strong&gt;differences in h&lt;/strong&gt; arise from the three property groups (&lt;em&gt;ρ&lt;/em&gt;,&lt;em&gt;μ&lt;/em&gt;,&lt;em&gt;k&lt;/em&gt;,&lt;em&gt;cp&lt;/em&gt;​) that appear in &lt;em&gt;Re&lt;/em&gt;, &lt;em&gt;Pr&lt;/em&gt; and the explicit factor &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;3. Thermophysical properties at high pressure&lt;/h1&gt; &lt;p&gt;Below is a representative data set for &lt;strong&gt;10 MPa (≈100 bar)&lt;/strong&gt; at &lt;strong&gt;500 °C&lt;/strong&gt; (the pressure is the same for both fluids; the temperature is above the critical temperature of water, 374 °C, so both fluids are in the &lt;em&gt;supercritical&lt;/em&gt; regime, but we still distinguish “liquid‑like” (dense) and “steam‑like” (less dense) states).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Property&lt;/th&gt; &lt;th align="left"&gt;Liquid‑like water (ρ ≈ 650 kg m⁻³)&lt;/th&gt; &lt;th align="left"&gt;Steam‑like water (ρ ≈ 120 kg m⁻³)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density, ρ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;650 kg m⁻³&lt;/td&gt; &lt;td align="left"&gt;120 kg m⁻³&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Dynamic viscosity, µ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.4 × 10⁻⁴ Pa s&lt;/td&gt; &lt;td align="left"&gt;1.6 × 10⁻⁴ Pa s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity, k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.70 W m⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;0.45 W m⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Specific heat, cₚ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.1 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;2.4 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number, Pr = cₚµ/k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 7.3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 0.85&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Sources&lt;/em&gt;: NIST REFPROP 10.0, IAPWS‑95 formulation, extrapolated to 10 MPa and 500 °C.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the numbers tell us&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Density&lt;/strong&gt; – liquid‑like water is ~5 × denser, giving a Reynolds number ~5 × larger for the same velocity and pipe diameter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Viscosity&lt;/strong&gt; – steam is slightly less viscous, which &lt;em&gt;increases&lt;/em&gt; its Reynolds number a little, but the dominant factor is density.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal conductivity&lt;/strong&gt; – liquid water conducts heat ≈ 55 % better.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prandtl number&lt;/strong&gt; – liquid water has a &lt;strong&gt;high Pr&lt;/strong&gt; (≈ 7) while steam has a &lt;strong&gt;low Pr&lt;/strong&gt; (≈ 0.85). A high Pr means the thermal boundary layer is thinner than the velocity boundary layer, boosting h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Quantitative h‑comparison (forced turbulent flow)&lt;/h1&gt; &lt;h1&gt;4.1. Chosen flow parameters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pipe diameter &lt;em&gt;D&lt;/em&gt;=0.020m (20 mm)&lt;/li&gt; &lt;li&gt;Mean velocity &lt;em&gt;U&lt;/em&gt;=2.0m s−1 – typical for high‑pressure steam turbines or heat‑exchanger tubes.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4.2. Reynolds numbers&lt;/h1&gt; &lt;p&gt;Both are &lt;strong&gt;turbulent&lt;/strong&gt; (Re &amp;gt; 4 000).&lt;/p&gt; &lt;h1&gt;4.3. Nusselt numbers (Gnielinski correlation – valid for 0.1&amp;lt;Pr&amp;lt;2000 and 3000&amp;lt;Re&amp;lt;5×106)&lt;/h1&gt; &lt;p&gt;Carrying out the calculation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Fluid&lt;/th&gt; &lt;th align="left"&gt;Friction factor &lt;em&gt;f&lt;/em&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;em&gt;Nu&lt;/em&gt; (Gnielinski)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Liquid‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.014&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;liq​≈1.0×103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Steam‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.028&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;steam​≈3.1×102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;4.4. Heat‑transfer coefficient&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;under the same pressure, temperature, geometry and bulk velocity, liquid water transfers heat by convection roughly five times more efficiently than steam&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;5. Why the liquid is so much better (physics insight)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;Liquid water (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Steam (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Effect on h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density (ρ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5× larger → larger momentum → higher Re → thinner velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Low ρ → smaller Re → thicker velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Higher Re → higher Nu&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity (k)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;55 % larger → more conductive path across the thermal BL&lt;/td&gt; &lt;td align="left"&gt;Smaller k → poorer conduction&lt;/td&gt; &lt;td align="left"&gt;Directly multiplies Nu in h = Nu·k/D&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number (Pr)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;≈ 7 (thermal diffusion slower than momentum) → thermal BL &lt;strong&gt;much thinner&lt;/strong&gt; than velocity BL&lt;/td&gt; &lt;td align="left"&gt;≈ 0.85 (thermal diffusion faster) → thermal BL comparable to or thicker than velocity BL&lt;/td&gt; &lt;td align="left"&gt;Thin thermal BL → larger temperature gradient at wall → larger h&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Viscosity (µ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Slightly higher, but the density effect dominates the Reynolds number&lt;/td&gt; &lt;td align="left"&gt;Lower µ but low ρ keeps Re low&lt;/td&gt; &lt;td align="left"&gt;Net: liquid still gets larger Re&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Compressibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Near‑incompressible; flow properties change little with pressure&lt;/td&gt; &lt;td align="left"&gt;Gas‑like compressibility reduces effective velocity and thickens BL&lt;/td&gt; &lt;td align="left"&gt;Keeps liquid’s convective performance high&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;6. Influence of pressure itself&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pressure change&lt;/th&gt; &lt;th align="left"&gt;Liquid water&lt;/th&gt; &lt;th align="left"&gt;Steam&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Increasing pressure&lt;/strong&gt; (while staying sub‑critical for the liquid)&lt;/td&gt; &lt;td align="left"&gt;ρ ↑, µ ↓ (slightly) → &lt;strong&gt;Re ↑&lt;/strong&gt;; k ↑ modestly; Pr ↑ → &lt;strong&gt;h rises&lt;/strong&gt; (often 10‑30 % per 100 bar).&lt;/td&gt; &lt;td align="left"&gt;ρ ↑ dramatically (≈ 5–10× as you approach the critical point), µ ↓, k ↑, Pr ↑ from ~0.7 toward 1‑2. When steam becomes &lt;em&gt;dense&lt;/em&gt; (near‑critical), its Re and Pr increase enough that &lt;strong&gt;h can climb 2–4×&lt;/strong&gt;. However, even at 20 MPa the steam’s Pr never reaches the ~7 of liquid water, so &lt;em&gt;liquid still beats steam&lt;/em&gt; unless you cross into the &lt;em&gt;supercritical fluid&lt;/em&gt; regime where the distinction blurs.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Very near the critical point&lt;/strong&gt; (≈ 22 MPa, 374 °C)&lt;/td&gt; &lt;td align="left"&gt;Water is a &lt;em&gt;supercritical fluid&lt;/em&gt; with properties intermediate between liquid and gas; h can be &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt; – comparable to high‑pressure liquid.&lt;/td&gt; &lt;td align="left"&gt;Same supercritical fluid – the “steam” label loses meaning; h values become similar to the liquid side.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Take‑away:&lt;/strong&gt; Pressure improves h for both phases, but because liquid water already starts with a much higher baseline, the &lt;em&gt;ratio&lt;/em&gt; &lt;em&gt;h&lt;/em&gt;liq​/&lt;em&gt;h&lt;/em&gt;steam​ stays in the &lt;strong&gt;3–6 range&lt;/strong&gt; for most engineering‑relevant pressures (5–20 MPa). Only when you go extremely close to the critical point does the gap shrink dramatically.&lt;/p&gt; &lt;h1&gt;7. Practical implications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Application&lt;/th&gt; &lt;th align="left"&gt;Typical operating pressure &amp;amp; temperature&lt;/th&gt; &lt;th align="left"&gt;Which phase gives the larger convective h?&lt;/th&gt; &lt;th align="left"&gt;Design note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;High‑pressure water‑to‑steam heat exchangers (e.g., nuclear steam generators)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15 MPa, 300 °C liquid → 15 MPa, 350 °C steam&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Liquid side&lt;/em&gt; (primary) has &lt;em&gt;h&lt;/em&gt; 4–6× larger than the steam side.&lt;/td&gt; &lt;td align="left"&gt;The steam side is usually the limiting resistance; designers boost its turbulence (twisted tapes, ribbed tubes) to compensate.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical CO₂ heat recovery (analogy)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.5 MPa, 500 °C (fluid is supercritical)&lt;/td&gt; &lt;td align="left"&gt;The fluid behaves like a dense gas; h is &lt;strong&gt;lower&lt;/strong&gt; than a comparable liquid but &lt;strong&gt;higher&lt;/strong&gt; than low‑pressure CO₂ gas.&lt;/td&gt; &lt;td align="left"&gt;Use high‑Re flow (large velocities, small hydraulic diameters) to raise h.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geothermal wells (high‑P hot water, 10 MPa, 250 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fluid is liquid‑like&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Liquid&lt;/strong&gt; h ≈ 2–3 × 10⁴ W m⁻² K⁻¹, far exceeding what the same well would deliver if the fluid flashed to steam.&lt;/td&gt; &lt;td align="left"&gt;Maintaining liquid conditions (prevent flashing) maximises convective heat extraction.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical water oxidation reactors (SCWO, &amp;gt; 25 MPa, &amp;gt; 400 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Water is supercritical (no distinct liquid/steam)&lt;/td&gt; &lt;td align="left"&gt;h values of &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt;; similar to high‑pressure liquid water.&lt;/td&gt; &lt;td align="left"&gt;The high h is a key advantage for rapid heat removal in exothermic oxidation.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;8. Summary (concise bullet list)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Liquid water at high pressure&lt;/strong&gt; has &lt;strong&gt;much larger density&lt;/strong&gt; and &lt;strong&gt;higher Prandtl number&lt;/strong&gt; than steam at the same pressure/temperature.&lt;/li&gt; &lt;li&gt;These property differences raise the Reynolds number and shrink the thermal boundary layer, leading to &lt;strong&gt;higher Nusselt numbers&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Quantitatively, for a 20 mm tube, 2 m s⁻¹ flow, 10 MPa and 500 °C:→ &lt;strong&gt;Liquid water convects roughly 5 × faster.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;h_liquid ≈ 3–4 × 10⁴ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;h_steam ≈ 6–8 × 10³ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Raising pressure improves h for &lt;strong&gt;both&lt;/strong&gt; fluids, but the &lt;strong&gt;ratio stays between 3 and 6&lt;/strong&gt; over the usual engineering range (5–20 MPa).&lt;/li&gt; &lt;li&gt;Near the critical point the distinction blurs; steam becomes a dense supercritical fluid and its h rises dramatically, approaching that of the liquid side.&lt;/li&gt; &lt;li&gt;In design practice the &lt;strong&gt;steam side is usually the thermal bottleneck&lt;/strong&gt; in high‑pressure heat exchangers; engineers therefore add turbulence promoters, increase velocity, or operate at higher pressure to mitigate the lower h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Bottom line&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Recipe7620"&gt; /u/Ill_Recipe7620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n49hcg</id>
    <title>New AMD unified memory product - 512 bit bus = ~512GB/s memory bandwidth</title>
    <updated>2025-08-30T18:54:22+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent AMD leak hints at a new 512 bit memory bus for their unified memory systems. If so, a successor to the AI max would likely have 2x the memory bandwidth.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship"&gt;https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4dsym</id>
    <title>128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow.</title>
    <updated>2025-08-30T21:59:06+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt; &lt;img alt="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." src="https://preview.redd.it/ld3rckf8b8mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad5697c140ab4e2a23aa8df66f12cc106ee3b653" title="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ld3rckf8b8mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T21:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4garp</id>
    <title>Creating the brain behind dumb models</title>
    <updated>2025-08-30T23:55:53+00:00</updated>
    <author>
      <name>/u/ChristopherLyon</name>
      <uri>https://old.reddit.com/user/ChristopherLyon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt; &lt;img alt="Creating the brain behind dumb models" src="https://external-preview.redd.it/MWFtYzE2NDZ3OG1mMdkNi0cONUhOlI53Ev_Op4Ah4qsUiMoYY-mZ8OWM03HP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d27b6fcdb4c16b3d3a61e3f5d15d95327d5ebe8" title="Creating the brain behind dumb models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been fascinated by model intelligence enhancement and trying to deploy super tiny models like gemma3:270m in niche domains with high levels of success... &lt;/p&gt; &lt;p&gt;My latest implementation is a &amp;quot;community nested&amp;quot; relational graph knowledgebase pipeline that gives both top down context on knowledge sub-domains, but also a traditional bottom-up search (essentially regular semantic embedding cosine similarity) with a traversal mechanism to grab context from nodes that are not semantically similar but still referentially linked. Turns out there is a LOT of context that does not get picked up through regular embedding based RAG.&lt;/p&gt; &lt;p&gt;I created a quick front-end with nextjs and threejs to visualize how my knowledge base hangs together, and to quickly identify if I had a high level of overall coherence (i.e. number of isolated/disconnected clusters) and to get a better feeling for what context the LLM loads into memory for any given user query in real time (I'm a visual learner)&lt;/p&gt; &lt;p&gt;The KB you can see in the video is from a single 160 page PDF on Industrial Design, taking you anywhere from notable people, material science to manufacturing techniques. I was pleasantly surprised to see that the node for &amp;quot;ergonomics&amp;quot; was by far the most linked and overall strongly referenced in the corpus - essentially linking the &amp;quot;human factor&amp;quot; to some significant contribution to great product design.&lt;/p&gt; &lt;p&gt;If anyone hasn't gotten into graph based retrieval augmented generation I found the best resource and starter to be from Microsoft: &lt;a href="https://github.com/microsoft/graphrag"&gt;https://github.com/microsoft/graphrag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^ pip install graphrag and use the init and index commands to create your first graph in minutes.&lt;/p&gt; &lt;p&gt;Anyone else been in my shoes and already know what the NEXT step will be? Let me know.&lt;/p&gt; &lt;p&gt;It's 2 am so a quick video shot on my mobile is all I have right now, but I can't sleep thinking about this so thought I'd post what I have. I need to work some more on it and add the local LLM interface for querying the KB through the front end, but I don't mind open sourcing it if anyone is interested. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChristopherLyon"&gt; /u/ChristopherLyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bwyft136w8mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46ify</id>
    <title>Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)</title>
    <updated>2025-08-30T16:52:00+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt; &lt;img alt="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" src="https://preview.redd.it/1wl79kpjs6mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967356c2a208d26c7d657a1b535e795c0332f304" title="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wl79kpjs6mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
