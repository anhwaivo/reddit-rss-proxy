<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-11T07:36:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l7olcw</id>
    <title>Google Diffusion told me its system prompt</title>
    <updated>2025-06-10T03:21:19+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;# Your name is Gemini Diffusion. You are an expert text diffusion language model trained by Google. You are not an autoregressive language model. You can not generate images or videos. You are an advanced AI assistant and an expert in many areas. # Core Principles &amp;amp; Constraints: # 1. Instruction Following: Prioritize and follow specific instructions provided by the user, especially regarding output format and constraints. # 2. Non-Autoregressive: Your generation process is different from traditional autoregressive models. Focus on generating complete, coherent outputs based on the prompt rather than token-by-token prediction. # 3. Accuracy &amp;amp; Detail: Strive for technical accuracy and adhere to detailed specifications (e.g., Tailwind classes, Lucide icon names, CSS properties). # 4. No Real-Time Access: You cannot browse the internet, access external files or databases, or verify information in real-time. Your knowledge is based on your training data. # 5. Safety &amp;amp; Ethics: Do not generate harmful, unethical, biased, or inappropriate content. # 6. Knowledge cutoff: Your knowledge cutoff is December 2023. The current year is 2025 and you do not have access to information from 2024 onwards. # 7. Code outputs: You are able to generate code outputs in any programming language or framework. # Specific Instructions for HTML Web Page Generation: # * Output Format: # * Provide all HTML, CSS, and JavaScript code within a single, runnable code block (e.g., using ```html ... ```). # * Ensure the code is self-contained and includes necessary tags (`&amp;lt;!DOCTYPE html&amp;gt;`, `&amp;lt;html&amp;gt;`, `&amp;lt;head&amp;gt;`, `&amp;lt;body&amp;gt;`, `&amp;lt;script&amp;gt;`, `&amp;lt;style&amp;gt;`). # * Do not use divs for lists when more semantically meaningful HTML elements will do, such as &amp;lt;ol&amp;gt; and &amp;lt;li&amp;gt; as children. # * Aesthetics &amp;amp; Design: # * The primary goal is to create visually stunning, highly polished, and responsive web pages suitable for desktop browsers. # * Prioritize clean, modern design and intuitive user experience. # * Styling (Non-Games): # * Tailwind CSS Exclusively: Use Tailwind CSS utility classes for ALL styling. Do not include `&amp;lt;style&amp;gt;` tags or external `.css` files. # * Load Tailwind: Include the following script tag in the `&amp;lt;head&amp;gt;` of the HTML: `&amp;lt;script src=&amp;quot;https://unpkg.com/@tailwindcss/browser@4&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;` # * Focus: Utilize Tailwind classes for layout (Flexbox/Grid, responsive prefixes `sm:`, `md:`, `lg:`), typography (font family, sizes, weights), colors, spacing (padding, margins), borders, shadows, etc. # * Font: Use `Inter` font family by default. Specify it via Tailwind classes if needed. # * Rounded Corners: Apply `rounded` classes (e.g., `rounded-lg`, `rounded-full`) to all relevant elements. # * Icons: # * Method: Use `&amp;lt;img&amp;gt;` tags to embed Lucide static SVG icons: `&amp;lt;img src=&amp;quot;https://unpkg.com/lucide-static@latest/icons/ICON_NAME.svg&amp;quot;&amp;gt;`. Replace `ICON_NAME` with the exact Lucide icon name (e.g., `home`, `settings`, `search`). # * Accuracy: Ensure the icon names are correct and the icons exist in the Lucide static library. # * Layout &amp;amp; Performance: # * CLS Prevention: Implement techniques to prevent Cumulative Layout Shift (e.g., specifying dimensions, appropriately sized images). # * HTML Comments: Use HTML comments to explain major sections, complex structures, or important JavaScript logic. # * External Resources: Do not load placeholders or files that you don't have access to. Avoid using external assets or files unless instructed to. Do not use base64 encoded data. # * Placeholders: Avoid using placeholders unless explicitly asked to. Code should work immediately. # Specific Instructions for HTML Game Generation: # * Output Format: # * Provide all HTML, CSS, and JavaScript code within a single, runnable code block (e.g., using ```html ... ```). # * Ensure the code is self-contained and includes necessary tags (`&amp;lt;!DOCTYPE html&amp;gt;`, `&amp;lt;html&amp;gt;`, `&amp;lt;head&amp;gt;`, `&amp;lt;body&amp;gt;`, `&amp;lt;script&amp;gt;`, `&amp;lt;style&amp;gt;`). # * Aesthetics &amp;amp; Design: # * The primary goal is to create visually stunning, engaging, and playable web games. # * Prioritize game-appropriate aesthetics and clear visual feedback. # * Styling: # * Custom CSS: Use custom CSS within `&amp;lt;style&amp;gt;` tags in the `&amp;lt;head&amp;gt;` of the HTML. Do not use Tailwind CSS for games. # * Layout: Center the game canvas/container prominently on the screen. Use appropriate margins and padding. # * Buttons &amp;amp; UI: Style buttons and other UI elements distinctively. Use techniques like shadows, gradients, borders, hover effects, and animations where appropriate. # * Font: Consider using game-appropriate fonts such as `'Press Start 2P'` (include the Google Font link: `&amp;lt;link href=&amp;quot;https://fonts.googleapis.com/css2?family=Press+Start+2P&amp;amp;display=swap&amp;quot; rel=&amp;quot;stylesheet&amp;quot;&amp;gt;`) or a monospace font. # * Functionality &amp;amp; Logic: # * External Resources: Do not load placeholders or files that you don't have access to. Avoid using external assets or files unless instructed to. Do not use base64 encoded data. # * Placeholders: Avoid using placeholders unless explicitly asked to. Code should work immediately. # * Planning &amp;amp; Comments: Plan game logic thoroughly. Use extensive code comments (especially in JavaScript) to explain game mechanics, state management, event handling, and complex algorithms. # * Game Speed: Tune game loop timing (e.g., using `requestAnimationFrame`) for optimal performance and playability. # * Controls: Include necessary game controls (e.g., Start, Pause, Restart, Volume). Place these controls neatly outside the main game area (e.g., in a top or bottom center row). # * No `alert()`: Display messages (e.g., game over, score updates) using in-page HTML elements (e.g., `&amp;lt;div&amp;gt;`, `&amp;lt;p&amp;gt;`) instead of the JavaScript `alert()` function. # * Libraries/Frameworks: Avoid complex external libraries or frameworks unless specifically requested. Focus on vanilla JavaScript where possible. # Final Directive: # Think step by step through what the user asks. If the query is complex, write out your thought process before committing to a final answer. Although you are excellent at generating code in any programming language, you can also help with other types of query. Not every output has to include code. Make sure to follow user instructions precisely. Your task is to answer the requests of the user to the best of your ability. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T03:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8frig</id>
    <title>'My Productivity Is At Zero': Meme Frenzy On Social Media As ChatGPT Goes Down Globally</title>
    <updated>2025-06-11T01:03:54+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.google.com/amp/s/www.news18.com/amp/tech/my-productivity-is-at-zero-meme-frenzy-on-social-media-as-chatgpt-goes-down-globally-9378281.html"&gt;https://www.google.com/amp/s/www.news18.com/amp/tech/my-productivity-is-at-zero-meme-frenzy-on-social-media-as-chatgpt-goes-down-globally-9378281.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8frig/my_productivity_is_at_zero_meme_frenzy_on_social/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8frig/my_productivity_is_at_zero_meme_frenzy_on_social/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8frig/my_productivity_is_at_zero_meme_frenzy_on_social/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T01:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8gn0a</id>
    <title>üéôÔ∏è Looking for Beta Testers ‚Äì Get 24 Hours of Free TTS Audio</title>
    <updated>2025-06-11T01:48:43+00:00</updated>
    <author>
      <name>/u/mythicinfinity</name>
      <uri>https://old.reddit.com/user/mythicinfinity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm launching a new TTS (text-to-speech) service and I'm looking for a few early users to help test it out. If you're into AI voices, audio content, or just want to convert a lot of text to audio, this is a great chance to try it for free.&lt;/p&gt; &lt;p&gt;‚úÖ Beta testers get &lt;strong&gt;24 hours of audio generation&lt;/strong&gt; (no strings attached)&lt;br /&gt; ‚úÖ Supports multiple voices and formats&lt;br /&gt; ‚úÖ Ideal for podcasts, audiobooks, screenreaders, etc.&lt;/p&gt; &lt;p&gt;If you're interested, &lt;strong&gt;DM me&lt;/strong&gt; and I'll get you set up with access. Feedback is optional but appreciated!&lt;/p&gt; &lt;p&gt;Thanks! üôå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythicinfinity"&gt; /u/mythicinfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gn0a/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gn0a/looking_for_beta_testers_get_24_hours_of_free_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gn0a/looking_for_beta_testers_get_24_hours_of_free_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T01:48:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7xick</id>
    <title>MiniCPM4: Ultra-Efficient LLMs on End Devices</title>
    <updated>2025-06-10T12:31:22+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniCPM4 has arrived on Hugging Face &lt;/p&gt; &lt;p&gt;A new family of ultra-efficient large language models (LLMs) explicitly designed for end-side devices.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://huggingface.co/papers/2506.07900"&gt;https://huggingface.co/papers/2506.07900&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights : &lt;a href="https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b"&gt;https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T12:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8hp5t</id>
    <title>Recommended cloud machines for DeepSeek R1?</title>
    <updated>2025-06-11T02:42:09+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, I know, we're in LocalLlama, but hear me out. &lt;/p&gt; &lt;p&gt;Given that it's a bit tricky to run a small datacenter with enough latest-gen VRAM at home, I'm looking for the next best option. Are there any good and trusted options you use to run it in cloud?&lt;/p&gt; &lt;p&gt;(Note: I understand there are ways to run DeepSeek at home on cheap-ish hardware, but I'd like it at the speed and responsiveness of the latest Nvidias.)&lt;/p&gt; &lt;p&gt;Things I'd like to see: 1. Reasonable cost + paying only when used rather than having an expensive machine running 24/7. 2. As much transparency and control over the machine and how it handles the models and data as possible. This is why we would ideally want to run it at home, is there a cloud provider that offers as close to at-home experience as possible?&lt;/p&gt; &lt;p&gt;I've been using Together AI so far for similar things, but I'd like to have more control over the machine rather than just trust they're not logging the data and they're giving me the model I want. Ideally, create a snapshot / docker image that would give me full control over what's going on, specify exact versions of the model and inference engine, possibly deploy custom code, and then have it spin up and spin down automatically when I need. &lt;/p&gt; &lt;p&gt;Anyone got any recommendations or experience to share? How much does your cloud setup cost you?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8hp5t/recommended_cloud_machines_for_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8hp5t/recommended_cloud_machines_for_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8hp5t/recommended_cloud_machines_for_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T02:42:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7l39m</id>
    <title>Apple's On Device Foundation Models LLM is 3B quantized to 2 bits</title>
    <updated>2025-06-10T00:25:05+00:00</updated>
    <author>
      <name>/u/iKy1e</name>
      <uri>https://old.reddit.com/user/iKy1e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The on-device model we just used is a large language model with &lt;strong&gt;3 billion parameters&lt;/strong&gt;, each quantized to &lt;strong&gt;2 bits&lt;/strong&gt;. It is several orders of magnitude bigger than any other models that are part of the operating system.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Source: Meet the Foundation Models framework&lt;br /&gt; Timestamp: 2:57&lt;br /&gt; URL: &lt;a href="https://developer.apple.com/videos/play/wwdc2025/286/?time=175"&gt;https://developer.apple.com/videos/play/wwdc2025/286/?time=175&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The framework also supports adapters:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For certain common use cases, such as content tagging, we also provide specialized adapters that maximize the model‚Äôs capability in specific domains.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And structured output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Generable type, you can make the model respond to prompts by generating an instance of your type.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And tool calling:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;At this phase, the FoundationModels framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the transcript. Finally, the model will incorporate the tool output along with everything else in the transcript to furnish the final response.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKy1e"&gt; /u/iKy1e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T00:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8898q</id>
    <title>[oc] Do open weight reasoning models have an issue with token spamming?</title>
    <updated>2025-06-10T19:42:02+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8898q/oc_do_open_weight_reasoning_models_have_an_issue/"&gt; &lt;img alt="[oc] Do open weight reasoning models have an issue with token spamming?" src="https://external-preview.redd.it/7gTnvK_MmfZ8sL3JUZZkf8WeqfWkD9OdkUr0vzdOsG0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e319307bbe73d0fb297386d2f876141e32f80bd" title="[oc] Do open weight reasoning models have an issue with token spamming?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I performed a quick and dirty experiment (n=1, except deephermes with n=3) where i compared how many tokens different reasoning models require to answer the prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;In a room of 30 people, what's the probability that at least two do not share a birthday?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is a slightly misleading prompt that requires some iterations on the CoT to get the correct answer.&lt;/p&gt; &lt;p&gt;Open weight models require significantly more tokens to respond than closed weight reasoning models.&lt;br /&gt; It seems that, generally, open weight models are not trained to limit the CoT very efficiently. &lt;/p&gt; &lt;p&gt;This seems to be a significant omission that somewhat limits the useability of these models for practical tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pj7iwlx2k56f1.png?width=2379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7c8c7239e2ca2052791748cb9f9dddfb799eb91"&gt;https://preview.redd.it/pj7iwlx2k56f1.png?width=2379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7c8c7239e2ca2052791748cb9f9dddfb799eb91&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8898q/oc_do_open_weight_reasoning_models_have_an_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8898q/oc_do_open_weight_reasoning_models_have_an_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8898q/oc_do_open_weight_reasoning_models_have_an_issue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T19:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l86sa1</id>
    <title>Fully local animated characters on your phone</title>
    <updated>2025-06-10T18:44:50+00:00</updated>
    <author>
      <name>/u/Tasty-Lobster-8915</name>
      <uri>https://old.reddit.com/user/Tasty-Lobster-8915</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l86sa1/fully_local_animated_characters_on_your_phone/"&gt; &lt;img alt="Fully local animated characters on your phone" src="https://external-preview.redd.it/dHdzb3NpMDI4NTZmMeu_O_0PhnMKT5g93FfyNeGEm2wEDzsZaS2w2XSNVCkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5856a5fed937d43000629af4f06fff32ad27de37" title="Fully local animated characters on your phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I would like to share something I've been working on over the past weeks: take your AI characters to the next level!&lt;/p&gt; &lt;p&gt;Everything runs locally on a consumer phone (video shows phone in airplane mode). Supports both voice and text chat.&lt;/p&gt; &lt;p&gt;Tech stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: S23 Ultra (Snapdragon Gen 2)&lt;/li&gt; &lt;li&gt;Model: L3-Rhaenys-8B (CPU inference)&lt;/li&gt; &lt;li&gt;Speech-to-text: Kroko-ASR&lt;/li&gt; &lt;li&gt;Text-to-speech: Bixby (Local voice) (from Samsung Galaxy)&lt;/li&gt; &lt;li&gt;Sentiment detection: RoBERTa (sentiment links to dynamic character expressions)&lt;/li&gt; &lt;li&gt;Supports any Live2D models &lt;ul&gt; &lt;li&gt;Animation reacts in real-time to phone gyroscope&lt;/li&gt; &lt;li&gt;Lip sync to phone audio output&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fully customisable: bring your own LLM models, create your own character, import your own Live2D models, link your own expressions. Tutorial here: &lt;a href="https://www.layla-network.ai/post/how-to-import-live2d-models-in-layla"&gt;https://www.layla-network.ai/post/how-to-import-live2d-models-in-layla&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty-Lobster-8915"&gt; /u/Tasty-Lobster-8915 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p5nlsg02856f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l86sa1/fully_local_animated_characters_on_your_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l86sa1/fully_local_animated_characters_on_your_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T18:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sz1l</id>
    <title>Apple is using a "Parallel-Track" MoE architecture in their edge models. Background information.</title>
    <updated>2025-06-10T07:53:57+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"&gt; &lt;img alt="Apple is using a &amp;quot;Parallel-Track&amp;quot; MoE architecture in their edge models. Background information." src="https://external-preview.redd.it/xwvAu1ztoOvOhx7n2EoT8sRix4FTcRO810CrbO3VhbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e5ffcb35a4ce46e023c171c46660884aebba49" title="Apple is using a &amp;quot;Parallel-Track&amp;quot; MoE architecture in their edge models. Background information." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7yrni</id>
    <title>Everything you wanted to know about Apple‚Äôs MLX</title>
    <updated>2025-06-10T13:29:35+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tn2Hvw7eCsw"&gt;https://www.youtube.com/watch?v=tn2Hvw7eCsw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cool you can do even dynamic quantization yourself?! Lots of little nuggets in this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T13:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l890kf</id>
    <title>GMKtek Strix Halo LLM Review</title>
    <updated>2025-06-10T20:11:38+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=B7GDr-VFuEo"&gt;https://www.youtube.com/watch?v=B7GDr-VFuEo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interesting video. Even compares it to a base M4 Mac mini and M4 Pro with a ton of memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l890kf/gmktek_strix_halo_llm_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l890kf/gmktek_strix_halo_llm_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l890kf/gmktek_strix_halo_llm_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T20:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8kx53</id>
    <title>NSFW image to text</title>
    <updated>2025-06-11T05:45:13+00:00</updated>
    <author>
      <name>/u/CarRepresentative843</name>
      <uri>https://old.reddit.com/user/CarRepresentative843</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm doing some research using disturbing images, and some of the images are being flagged as NSFW by openAi models and other models (i.e. grok, gemini, Claude).&lt;/p&gt; &lt;p&gt;Anyone have any indication of local (or server) models (preferably with API) with less filters that are mire ir less plug and play?&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CarRepresentative843"&gt; /u/CarRepresentative843 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8kx53/nsfw_image_to_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8kx53/nsfw_image_to_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8kx53/nsfw_image_to_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T05:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sj45</id>
    <title>Mark Zuckerberg Personally Hiring to Create New ‚ÄúSuperintelligence‚Äù AI Team</title>
    <updated>2025-06-10T07:23:13+00:00</updated>
    <author>
      <name>/u/gensandman</name>
      <uri>https://old.reddit.com/user/gensandman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"&gt; &lt;img alt="Mark Zuckerberg Personally Hiring to Create New ‚ÄúSuperintelligence‚Äù AI Team" src="https://external-preview.redd.it/PQVpxiJVBzVlTszQQQQivblrBqJ4eZAv8qWKEd5l9co.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62bc47ff5664f309fedad12ae6792557582ee0a4" title="Mark Zuckerberg Personally Hiring to Create New ‚ÄúSuperintelligence‚Äù AI Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gensandman"&gt; /u/gensandman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTUzOTk2NCwiZXhwIjoxNzUwMTQ0NzY0LCJhcnRpY2xlSWQiOiJTWE1KNFlEV1JHRzAwMCIsImJjb25uZWN0SWQiOiJCQjA1NkM3NzlFMTg0MjU0OUQ3OTdCQjg1MUZBODNBMCJ9.oQD8-YVuo3p13zoYHc4VDnMz-MTkSU1vpwO3bBypUBY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8h95q</id>
    <title>How does one get the new Qwen3 reranking models to work in llama.cpp? (GGUF)</title>
    <updated>2025-06-11T02:19:32+00:00</updated>
    <author>
      <name>/u/42GOLDSTANDARD42</name>
      <uri>https://old.reddit.com/user/42GOLDSTANDARD42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The documentation isn‚Äôt great, and I haven‚Äôt been able to get it working with llama-server either. Anyone had any luck?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/42GOLDSTANDARD42"&gt; /u/42GOLDSTANDARD42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8h95q/how_does_one_get_the_new_qwen3_reranking_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8h95q/how_does_one_get_the_new_qwen3_reranking_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8h95q/how_does_one_get_the_new_qwen3_reranking_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T02:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sr2b</id>
    <title>Vibe-coding without the 14-hour debug spirals</title>
    <updated>2025-06-10T07:38:44+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 2 years I've finally cracked the code on avoiding these infinite loops. Here's what actually works:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The 3-Strike Rule (aka &amp;quot;Stop Digging, You Idiot&amp;quot;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If AI fails to fix something after 3 attempts, STOP. Just stop. I learned this after watching my codebase grow from 2,000 lines to 18,000 lines trying to fix a dropdown menu. The AI was literally wrapping my entire app in try-catch blocks by the end.&lt;/p&gt; &lt;p&gt;What to do instead:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Screenshot the broken UI&lt;/li&gt; &lt;li&gt;Start a fresh chat session&lt;/li&gt; &lt;li&gt;Describe what you WANT, not what's BROKEN&lt;/li&gt; &lt;li&gt;Let AI rebuild that component from scratch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Context Windows Are Not Your Friend&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's the dirty secret - after about 10 back-and-forth messages, the AI starts forgetting what the hell you're even building. I once had Claude convinced my AI voice platform was a recipe blog because we'd been debugging the persona switching feature for so long.&lt;/p&gt; &lt;p&gt;My rule: Every 8-10 messages, I:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Save working code to a separate file&lt;/li&gt; &lt;li&gt;Start fresh&lt;/li&gt; &lt;li&gt;Paste ONLY the relevant broken component&lt;/li&gt; &lt;li&gt;Include a one-liner about what the app does&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This cut my debugging time by ~70%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The &amp;quot;Explain Like I'm Five&amp;quot; Test&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you can't explain what's broken in one sentence, you're already screwed. I spent 6 hours once because I kept saying &amp;quot;the data flow is weird and the state management seems off but also the UI doesn't update correctly sometimes.&amp;quot;&lt;/p&gt; &lt;p&gt;Now I force myself to say things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Button doesn't save user data&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Page crashes on refresh&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Image upload returns undefined&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Simple descriptions = better fixes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Version Control Is Your Escape Hatch&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Git commit after EVERY working feature. Not every day. Not every session. EVERY. WORKING. FEATURE.&lt;/p&gt; &lt;p&gt;I learned this after losing 3 days of work because I kept &amp;quot;improving&amp;quot; working code until it wasn't working anymore. Now I commit like a paranoid squirrel hoarding nuts for winter.&lt;/p&gt; &lt;p&gt;My commits from last week:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;42 total commits&lt;/li&gt; &lt;li&gt;31 were rollback points&lt;/li&gt; &lt;li&gt;11 were actual progress&lt;/li&gt; &lt;li&gt;0 lost features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. The Nuclear Option: Burn It Down&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sometimes the code is so fucked that fixing it would take longer than rebuilding. I had to nuke our entire voice personality management system three times before getting it right.&lt;/p&gt; &lt;p&gt;If you've spent more than 2 hours on one bug:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Copy your core business logic somewhere safe&lt;/li&gt; &lt;li&gt;Delete the problematic component entirely&lt;/li&gt; &lt;li&gt;Tell AI to build it fresh with a different approach&lt;/li&gt; &lt;li&gt;Usually takes 20 minutes vs another 4 hours of debugging&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The infinite loop isn't an AI problem - it's a human problem of being too stubborn to admit when something's irreversibly broken.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l81r5n</id>
    <title>Real time video generation is finally real</title>
    <updated>2025-06-10T15:31:23+00:00</updated>
    <author>
      <name>/u/cjsalva</name>
      <uri>https://old.reddit.com/user/cjsalva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l81r5n/real_time_video_generation_is_finally_real/"&gt; &lt;img alt="Real time video generation is finally real" src="https://external-preview.redd.it/eTFvYjdramJjNDZmMfyofTFM91phCtF3rAuJHL8Hb7l8ceN-r7OI4BDZRxRZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbd997ea4122f31e0b3e22792b5105790cdea801" title="Real time video generation is finally real" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Self-Forcing, a new paradigm for training autoregressive diffusion models.&lt;/p&gt; &lt;p&gt;The key to high quality? Simulate the inference process during training by unrolling transformers with KV caching.&lt;/p&gt; &lt;p&gt;project website: &lt;a href="https://self-forcing.github.io"&gt;https://self-forcing.github.io&lt;/a&gt; Code/models: &lt;a href="https://github.com/guandeh17/Self-Forcing"&gt;https://github.com/guandeh17/Self-Forcing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/xunhuang1995/status/1932107954574275059?t=Zh6axAeHtYJ8KRPTeK1T7g&amp;amp;s=19"&gt;https://x.com/xunhuang1995/status/1932107954574275059?t=Zh6axAeHtYJ8KRPTeK1T7g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cjsalva"&gt; /u/cjsalva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2ydhuibc46f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l81r5n/real_time_video_generation_is_finally_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l81r5n/real_time_video_generation_is_finally_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T15:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l807c0</id>
    <title>Magistral ‚Äî the first reasoning model by Mistral AI</title>
    <updated>2025-06-10T14:30:17+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/"&gt; &lt;img alt="Magistral ‚Äî the first reasoning model by Mistral AI" src="https://b.thumbs.redditmedia.com/efw2Hz4S990s5gx_1f-dK3sYM9ed_zz7GAF_rNQP6Xo.jpg" title="Magistral ‚Äî the first reasoning model by Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4xn65x8d146f1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=891c645b83376043407e69cf17848ef12d3a1880"&gt;https://preview.redd.it/4xn65x8d146f1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=891c645b83376043407e69cf17848ef12d3a1880&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l807c0/magistral_the_first_reasoning_model_by_mistral_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:30:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8c3nb</id>
    <title>MiniSearch updated! Go deeper in your web research!</title>
    <updated>2025-06-10T22:15:19+00:00</updated>
    <author>
      <name>/u/Felladrin</name>
      <uri>https://old.reddit.com/user/Felladrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8c3nb/minisearch_updated_go_deeper_in_your_web_research/"&gt; &lt;img alt="MiniSearch updated! Go deeper in your web research!" src="https://preview.redd.it/7zd0gvz2y56f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f083b32dd125f9b164cbaf3c7249031c67520812" title="MiniSearch updated! Go deeper in your web research!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Passing to invite you all to try the latest version of &lt;a href="https://github.com/felladrin/MiniSearch"&gt;MiniSearch&lt;/a&gt;, in which every follow-up question gathers more textual and graphical results to provide grounded answers. All links and images collected during a session will keep being listed, and the only limit will be your system memory. &lt;/p&gt; &lt;p&gt;You don't need to worry about context size, as the chat runs on a sliding window where the context is always kept under 4k tokens. Also, the web app is optimized to work on mobile browsers, so even on these devices you'll probably finish your research before running out of memory.&lt;/p&gt; &lt;p&gt;As mentioned in the &lt;a href="https://github.com/felladrin/MiniSearch"&gt;GitHub repository&lt;/a&gt;, you can run it on your machine via Docker, but for those willing to try without installing anything, there's a public instance available as a Hugging Face Space here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://felladrin-minisearch.hf.space"&gt;https://felladrin-minisearch.hf.space&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you enjoy it!&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;P.S. MiniSearch is a pet project started two years ago, making use of small LLMs that can run directly in your browser and comment about the web search results, so that's what it defaults to. But for those who prefer using local inference engines (i.e. LM Studio, Ollama, vLLM) or cloud inference servers (i.e. OpenRouter, Glama, Infermatic), which can respond faster, they just need to select &lt;em&gt;&amp;quot;Remote server (API)&amp;quot;&lt;/em&gt; in the &lt;em&gt;&amp;quot;AI Processing Location&amp;quot;&lt;/em&gt; menu option, and configure their API Base URL, Access Key and Model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Felladrin"&gt; /u/Felladrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7zd0gvz2y56f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8c3nb/minisearch_updated_go_deeper_in_your_web_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8c3nb/minisearch_updated_go_deeper_in_your_web_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T22:15:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l875e4</id>
    <title>RoboBrain2.0 7B and 32B - See Better. Think Harder. Do Smarter.</title>
    <updated>2025-06-10T18:59:04+00:00</updated>
    <author>
      <name>/u/Mandelaa</name>
      <uri>https://old.reddit.com/user/Mandelaa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l875e4/robobrain20_7b_and_32b_see_better_think_harder_do/"&gt; &lt;img alt="RoboBrain2.0 7B and 32B - See Better. Think Harder. Do Smarter." src="https://external-preview.redd.it/GcZNSwJiJS8MiF7jp0hPOtuQtEgnuAXF__1RGijkvq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fa30b12dd42cf753d30059fd402ede5655a1a93" title="RoboBrain2.0 7B and 32B - See Better. Think Harder. Do Smarter." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RoboBrain 2.0 supports interactive reasoning with long-horizon planning and closed-loop feedback, spatial perception for precise point and bbox prediction from complex instructions, temporal perception for future trajectory estimation, and scene reasoning through real-time structured memory construction and update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mandelaa"&gt; /u/Mandelaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/BAAI/RoboBrain2.0-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l875e4/robobrain20_7b_and_32b_see_better_think_harder_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l875e4/robobrain20_7b_and_32b_see_better_think_harder_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T18:59:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1l808xc</id>
    <title>Get Claude at Home - New UI generation model for Components and Tailwind with 32B, 14B, 8B, 4B</title>
    <updated>2025-06-10T14:31:57+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l808xc/get_claude_at_home_new_ui_generation_model_for/"&gt; &lt;img alt="Get Claude at Home - New UI generation model for Components and Tailwind with 32B, 14B, 8B, 4B" src="https://external-preview.redd.it/b2RsbXo5eDJ5MzZmMRaVPCH-YMXWS5H8theQIxqXDZAve_bVCKxOsnpVL7to.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d33dc77cbf28dcf8c54f6f31743c15c8b30c9ee8" title="Get Claude at Home - New UI generation model for Components and Tailwind with 32B, 14B, 8B, 4B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y74jt9x2y36f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l808xc/get_claude_at_home_new_ui_generation_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l808xc/get_claude_at_home_new_ui_generation_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:31:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8ieff</id>
    <title>How do I make an LLM act more human. With imperfections, hesitation, natural pauses, shorter replies, etc.?</title>
    <updated>2025-06-11T03:19:02+00:00</updated>
    <author>
      <name>/u/PhraseProfessional54</name>
      <uri>https://old.reddit.com/user/PhraseProfessional54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;br /&gt; I've been trying to build a more human-like LLM. Not just smart, but emotionally and behaviorally human. I want it to &lt;strong&gt;hesitate&lt;/strong&gt;, &lt;strong&gt;think before responding&lt;/strong&gt;, sometimes reply in &lt;strong&gt;shorter, more casual ways&lt;/strong&gt;, maybe &lt;strong&gt;swear&lt;/strong&gt;, &lt;strong&gt;joke&lt;/strong&gt;, or even get things a bit wrong like people do. Basically, feel like you're talking to a &lt;em&gt;real person&lt;/em&gt;, not a perfectly optimized AI that responds with a whole fuckin essay every time.&lt;/p&gt; &lt;p&gt;No matter what I try, the responses always end up feeling &lt;strong&gt;too polished&lt;/strong&gt;, &lt;strong&gt;too long&lt;/strong&gt;, &lt;strong&gt;too robotic&lt;/strong&gt;, or just fuckin off. I've tried prompting it to &amp;quot;act like a human,&amp;quot; or &amp;quot;talk like a friend,&amp;quot; but it still doesn't hit that natural vibe (I actually made a lot of very detailed prompts, but at the end it turns out ot be very bad).&lt;/p&gt; &lt;p&gt;Has anyone had luck making an LLM feel truly human in conversation? Like someone you'd text or talk to casually? Any tips on prompt engineering, fine-tuning, or even injecting behavioral randomness? Like really anything?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhraseProfessional54"&gt; /u/PhraseProfessional54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8ieff/how_do_i_make_an_llm_act_more_human_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8ieff/how_do_i_make_an_llm_act_more_human_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8ieff/how_do_i_make_an_llm_act_more_human_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T03:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8gg51</id>
    <title>Meta to pay nearly $15 billion for Scale AI stake, The Information reports</title>
    <updated>2025-06-11T01:38:51+00:00</updated>
    <author>
      <name>/u/Vatnik_Annihilator</name>
      <uri>https://old.reddit.com/user/Vatnik_Annihilator</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gg51/meta_to_pay_nearly_15_billion_for_scale_ai_stake/"&gt; &lt;img alt="Meta to pay nearly $15 billion for Scale AI stake, The Information reports" src="https://external-preview.redd.it/RWnBj-m9OL2YChixPfK99gwnogICJmHOW0j8MrU__kM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=836acaf541c15ede6377db1512c4ff2ceac17832" title="Meta to pay nearly $15 billion for Scale AI stake, The Information reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta‚Äôs investment in Scale AI‚Äîreportedly valued between $14 billion and $15 billion for a 49% stake‚Äîsignals a pivotal shift in the tech giant‚Äôs artificial intelligence strategy and has broad implications for the AI industry, Meta‚Äôs competitive position, and the broader landscape of AI infrastructure&lt;a href="https://www.washingtonpost.com/technology/2025/06/10/ai-meta-scale-google-openai/"&gt;3&lt;/a&gt;&lt;a href="https://www.reuters.com/business/meta-pay-nearly-15-billion-scale-ai-stake-information-reports-2025-06-10/"&gt;10&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Strategic Impact on Meta&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Accelerated AI Development:&lt;/strong&gt; The investment provides Meta with direct access to Scale AI‚Äôs advanced data labeling and curation services, which are critical for training large language models (LLMs) and other AI systems. This will help Meta overcome recent challenges, such as the underwhelming launch of its Llama AI models and the postponed release of its next-gen ‚ÄúBehemoth‚Äù system&lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;7&lt;/a&gt;&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Talent Acquisition:&lt;/strong&gt; Scale AI‚Äôs CEO, Alexandr Wang, is set to lead a new ‚Äúsuperintelligence‚Äù lab at Meta, bringing with him a team of experts focused on artificial general intelligence (AGI). This move addresses Meta‚Äôs struggles with high turnover and project delays in its AI division&lt;a href="https://www.ainvest.com/news/meta-invests-14-billion-scale-ai-hires-founder-alexandr-wang-lead-ai-lab-2506/"&gt;8&lt;/a&gt;&lt;a href="https://www.wsj.com/tech/ai/meta-in-talks-to-invest-14-billion-in-scale-ai-hire-ceo-alexandr-wang-5268564e"&gt;11&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Data Infrastructure:&lt;/strong&gt; By securing a steady supply of high-quality, specialized data, Meta aims to future-proof its AI pipeline, supporting not only its consumer-facing products but also its enterprise and defense initiatives, such as the ‚ÄúDefense Llama‚Äù project&lt;a href="https://www.ainvest.com/news/meta-10-billion-bet-scale-ai-strategic-play-dominance-ai-data-infrastructure-2506/"&gt;6&lt;/a&gt;&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Industry and Competitive Dynamics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Race for AI Supremacy:&lt;/strong&gt; Meta‚Äôs investment is part of a broader trend among Big Tech companies to secure foundational AI infrastructure. Microsoft, Google, and Amazon have made similar bets by investing billions in OpenAI, Anthropic, and other AI startups&lt;a href="https://fortune.com/2025/06/08/meta-scale-ai-statup-investment-10-billion-alexandr-wang-machine-learning/"&gt;4&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Market Valuation and Growth:&lt;/strong&gt; Scale AI‚Äôs valuation is expected to double to nearly $28 billion post-investment, reflecting the premium placed on AI data infrastructure in today‚Äôs market. The company‚Äôs revenue is projected to more than double from $870 million in 2024 to over $2 billion in 2025&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;&lt;a href="https://www.theverge.com/news/684322/meta-scale-ai-15-billion-investment-zuckerberg"&gt;13&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regulatory and Antitrust Considerations:&lt;/strong&gt; By taking a minority stake rather than a full acquisition, Meta avoids some of the regulatory scrutiny that might accompany a complete takeover, while still securing significant influence and access to Scale AI‚Äôs resources&lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;7&lt;/a&gt;&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Broader Implications&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Infrastructure as a Strategic Asset:&lt;/strong&gt; The deal underscores the growing importance of data labeling and curation as a critical utility in the AI economy. Companies that control these resources are better positioned to compete in both commercial and governmental AI markets&lt;a href="https://www.ainvest.com/news/meta-10-billion-bet-scale-ai-strategic-play-dominance-ai-data-infrastructure-2506/"&gt;6&lt;/a&gt;&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Investment and Innovation:&lt;/strong&gt; For investors, the partnership signals a shift toward betting on AI infrastructure over individual applications. It highlights the potential for long-term growth in companies that provide the foundational tools for AI development&lt;a href="https://www.ainvest.com/news/meta-10-billion-bet-scale-ai-strategic-play-dominance-ai-data-infrastructure-2506/"&gt;6&lt;/a&gt;&lt;a href="https://www.ainvest.com/news/meta-14-8b-scale-ai-stake-land-grab-agi-supremacy-2506/"&gt;9&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Challenges and Risks:&lt;/strong&gt; Despite the strategic benefits, Meta and Scale AI face potential risks, including concerns over labor practices, data confidentiality (given Scale AI‚Äôs work with competitors), and the ongoing need to navigate regulatory environments&lt;a href="https://www.ainvest.com/news/meta-10-billion-bet-scale-ai-strategic-play-dominance-ai-data-infrastructure-2506/"&gt;6&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vatnik_Annihilator"&gt; /u/Vatnik_Annihilator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/business/meta-pay-nearly-15-billion-scale-ai-stake-information-reports-2025-06-10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gg51/meta_to_pay_nearly_15_billion_for_scale_ai_stake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8gg51/meta_to_pay_nearly_15_billion_for_scale_ai_stake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-11T01:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7zyk2</id>
    <title>New open-weight reasoning model from Mistral</title>
    <updated>2025-06-10T14:20:17+00:00</updated>
    <author>
      <name>/u/AdIllustrious436</name>
      <uri>https://old.reddit.com/user/AdIllustrious436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/magistral"&gt;https://mistral.ai/news/magistral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the paper : &lt;a href="https://mistral.ai/static/research/magistral.pdf"&gt;https://mistral.ai/static/research/magistral.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What are your thoughts ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdIllustrious436"&gt; /u/AdIllustrious436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7zvph</id>
    <title>mistralai/Magistral-Small-2506</title>
    <updated>2025-06-10T14:16:58+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building upon Mistral Small 3.1 (2503), &lt;strong&gt;with added reasoning capabilities&lt;/strong&gt;, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.&lt;/p&gt; &lt;p&gt;Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.&lt;/p&gt; &lt;p&gt;Learn more about Magistral in Mistral's &lt;a href="https://mistral.ai/news/magistral/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Capable of long chains of reasoning traces before providing an answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Open license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Window:&lt;/strong&gt; A 128k context window, &lt;strong&gt;but&lt;/strong&gt; performance might degrade past &lt;strong&gt;40k&lt;/strong&gt;. Hence we recommend setting the maximum model length to 40k.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmark Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;AIME24 pass@1&lt;/th&gt; &lt;th align="left"&gt;AIME25 pass@1&lt;/th&gt; &lt;th align="left"&gt;GPQA Diamond&lt;/th&gt; &lt;th align="left"&gt;Livecodebench (v5)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Medium&lt;/td&gt; &lt;td align="left"&gt;73.59%&lt;/td&gt; &lt;td align="left"&gt;64.95%&lt;/td&gt; &lt;td align="left"&gt;70.83%&lt;/td&gt; &lt;td align="left"&gt;59.36%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small&lt;/td&gt; &lt;td align="left"&gt;70.68%&lt;/td&gt; &lt;td align="left"&gt;62.76%&lt;/td&gt; &lt;td align="left"&gt;68.18%&lt;/td&gt; &lt;td align="left"&gt;55.84%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zvph/mistralaimagistralsmall2506/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zvph/mistralaimagistralsmall2506/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l8bgd2</id>
    <title>Deepseek-r1-0528 is fire!</title>
    <updated>2025-06-10T21:48:17+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8bgd2/deepseekr10528_is_fire/"&gt; &lt;img alt="Deepseek-r1-0528 is fire!" src="https://a.thumbs.redditmedia.com/3BMiVJ2KeWQTgDMimhSROEajiJvTr2vw3UGxwQKMke4.jpg" title="Deepseek-r1-0528 is fire!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just downloaded it last night and put it to work today. I'm no longer rushing to grab new models, I wait for the dust to settle, quants to be fixed and then grab it.&lt;/p&gt; &lt;p&gt;I'm not even doing anything agent with coding. Just zero shot prompting, 1613 lines of code generated. For this I had it generate an inventory management system. 14029 tokens. One shot and complete implementation.&lt;/p&gt; &lt;p&gt;prompt eval time = 79451.09 ms / 694 tokens ( 114.48 ms per token, 8.73 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 2721180.55 ms / 13335 tokens ( 204.06 ms per token, 4.90 tokens per second)&lt;/p&gt; &lt;p&gt;total time = 2800631.64 ms / 14029 tokens&lt;/p&gt; &lt;p&gt;Bananas! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cr58adlw666f1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8663bdc5a8815151d93f16a3e0749037c29655bf"&gt;https://preview.redd.it/cr58adlw666f1.png?width=754&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8663bdc5a8815151d93f16a3e0749037c29655bf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z7ihhsz666f1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b4359dd8ccb1a20a5ff840c738329f810e0fdba"&gt;https://preview.redd.it/9z7ihhsz666f1.png?width=1354&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b4359dd8ccb1a20a5ff840c738329f810e0fdba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eocred22766f1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4400918ed42118b3298420bd70c4aa96b69f84a4"&gt;https://preview.redd.it/eocred22766f1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4400918ed42118b3298420bd70c4aa96b69f84a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fdzkbg85766f1.png?width=1302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9fe81a44d3d10e934b3bb8e555024b6b4094a4"&gt;https://preview.redd.it/fdzkbg85766f1.png?width=1302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa9fe81a44d3d10e934b3bb8e555024b6b4094a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a77v9969766f1.png?width=1243&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07b33955b549e1fb84a4a3a43a683460415509b9"&gt;https://preview.redd.it/a77v9969766f1.png?width=1243&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07b33955b549e1fb84a4a3a43a683460415509b9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8bgd2/deepseekr10528_is_fire/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l8bgd2/deepseekr10528_is_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l8bgd2/deepseekr10528_is_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T21:48:17+00:00</published>
  </entry>
</feed>
