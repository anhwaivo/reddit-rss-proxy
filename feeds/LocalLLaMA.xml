<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-14T19:05:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1janir5</id>
    <title>End of the Open LLM Leaderboard</title>
    <updated>2025-03-13T21:38:32+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt; &lt;img alt="End of the Open LLM Leaderboard" src="https://external-preview.redd.it/uNVcFTJjErtGHcv_RU8nJOqopdFi5HpQXXuBPYA8mRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78ec61fc7857881ca74251621f68697e9ed6557a" title="End of the Open LLM Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/discussions/1135"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb9ykl</id>
    <title>Where to find benchmarks/leaderboard for small llms?</title>
    <updated>2025-03-14T17:50:43+00:00</updated>
    <author>
      <name>/u/Dean_Thomas426</name>
      <uri>https://old.reddit.com/user/Dean_Thomas426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open llm leaderboard on huggingface is super slow in adding new models and livebench has usually only the bigger models. Is there a good website or source that compares smaller llms using some kind of benchmarking system?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dean_Thomas426"&gt; /u/Dean_Thomas426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9ykl/where_to_find_benchmarksleaderboard_for_small_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9ykl/where_to_find_benchmarksleaderboard_for_small_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9ykl/where_to_find_benchmarksleaderboard_for_small_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb24uz</id>
    <title>What's your favorite model for casual texting?</title>
    <updated>2025-03-14T11:54:40+00:00</updated>
    <author>
      <name>/u/HornyGooner4401</name>
      <uri>https://old.reddit.com/user/HornyGooner4401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's your favorite model to talk casually with? Most people are focused on coding, benchmarks, or roleplay but I'm just trying to find a model that I can talk to casually. Probably something that can reply in shorter sentences, have general knowledge but doesn't always have to be right, talks naturally, maybe a little joke here and there, and preferably hallucinate personal experience (how their day went, going on a trip to Italy, working as a cashier for 2 years, etc.).&lt;/p&gt; &lt;p&gt;IIRC Facebook had a model that was trained on messages and conversations which worked somewhat well, but this was yeaaars ago before ChatGPT was even a thing. I suppose there should be better models by now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HornyGooner4401"&gt; /u/HornyGooner4401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jao3fg</id>
    <title>Qwq-32b just got updated Livebench.</title>
    <updated>2025-03-13T22:03:20+00:00</updated>
    <author>
      <name>/u/Amazing_Gate_9984</name>
      <uri>https://old.reddit.com/user/Amazing_Gate_9984</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt; &lt;img alt="Qwq-32b just got updated Livebench." src="https://b.thumbs.redditmedia.com/y-9o2Am61okZEVvbetInDUVF7Va9XDotcOFbj-bL_LI.jpg" title="Qwq-32b just got updated Livebench." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the full results: &lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29"&gt;https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Gate_9984"&gt; /u/Amazing_Gate_9984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb97u5</id>
    <title>Run SesameAILabs/csm locally with Gradio UI with CUDA or CPU (if no CUDA)</title>
    <updated>2025-03-14T17:19:38+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Choose from predefined voice prompts&lt;/li&gt; &lt;li&gt;Upload or record custom voice prompts&lt;/li&gt; &lt;li&gt;Enter multi-turn conversations&lt;/li&gt; &lt;li&gt;Generate and play audio directly in browser&lt;/li&gt; &lt;li&gt;Use your own voice as custom voice&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use Cases:&lt;br /&gt; Give it text and create a discussion like NotebookLLM.&lt;br /&gt; Create podcast in your own voice.&lt;/p&gt; &lt;p&gt;Github Repo:- &lt;a href="https://github.com/akashjss/sesame-csm/tree/main"&gt;https://github.com/akashjss/sesame-csm/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb97u5/run_sesameailabscsm_locally_with_gradio_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:19:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb77kg</id>
    <title>I built a framework to train your own custom multimodal models</title>
    <updated>2025-03-14T15:55:44+00:00</updated>
    <author>
      <name>/u/insujang</name>
      <uri>https://old.reddit.com/user/insujang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen that many open source multimodal model training has been done on a manually crafted model. There is currently no unified system that provides an interface to generate a multimodal model easily with unimodal models available in HuggingFace.&lt;/p&gt; &lt;p&gt;Therefore, I implemented &lt;a href="https://github.com/cornstarch-org/Cornstarch"&gt;Cornstarch&lt;/a&gt; that provides the interface to generate &lt;em&gt;any multimodal model&lt;/em&gt; as you want; not just a vision language model, but you can also implement a multimodal model with arbitrary number of encoders, where each model is in HuggingFace transformers. I believe this should be helpful for researchers who want to build a new multimodal model.&lt;/p&gt; &lt;p&gt;If you want to attach encoders to llama (different encoders used in mllama), for example,&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vision_encoder = SiglipVisionModel.from_pretrained(&amp;quot;google/siglip-so400m-patch14-384&amp;quot;) audio_encoder = WhisperEncoder.from_pretrained(&amp;quot;openai/whisper-large-v3&amp;quot;) llm = AutoModelForCausalLM.from_pretrained(&amp;quot;meta-llama/Llama-3.2-3B-Instruct&amp;quot;) mllm = MultimodalModel( encoders={ &amp;quot;vision&amp;quot;: ModalEncoderModule(vision_encoder), &amp;quot;audio&amp;quot;: ModalEncoderModule(audio_encoder), }, language_model=llm, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Plus, Cornstarch provides distributed training of multimodal models. We have &lt;a href="https://cornstarch-org.github.io/parallelization/"&gt;tutorials&lt;/a&gt; for easy parallelization.&lt;/p&gt; &lt;p&gt;For those who wanted to train a custom multimodal model, please try and share your thought!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/insujang"&gt; /u/insujang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb77kg/i_built_a_framework_to_train_your_own_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb77kg/i_built_a_framework_to_train_your_own_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb77kg/i_built_a_framework_to_train_your_own_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jauy8d</id>
    <title>Giving "native" tool calling to Gemma 3 (or really any model)</title>
    <updated>2025-03-14T03:40:19+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 is great at following instructions, but doesn't have &amp;quot;native&amp;quot; tool/function calling. Let's change that (at least as best we can).&lt;/p&gt; &lt;p&gt;(Quick note, I'm going to be using Ollama as the example here, but this works equally well with Jinja templates, just need to change the syntax a bit.)&lt;/p&gt; &lt;h1&gt;Defining Tools&lt;/h1&gt; &lt;p&gt;Let's start by figuring out how 'native' function calling works in Ollama. Here's qwen2.5's chat template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- if or .System .Tools }}&amp;lt;|im_start|&amp;gt;system {{- if .System }} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range .Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you think this looks like the second half of your average homebrew tool calling system prompt, you're spot on. &lt;strong&gt;This is literally appending markdown-formatted instructions on what tools are available and how to call them to the end of the system prompt.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Already, Ollama will recognize the tools you give it in the &lt;code&gt;tools&lt;/code&gt; part of your OpenAI completions request, and inject them into the system prompt.&lt;/p&gt; &lt;h1&gt;Parsing Tools&lt;/h1&gt; &lt;p&gt;Let's scroll down a bit and see how tool call messages are handled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;|im_start|&amp;gt;assistant {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the &lt;strong&gt;tool call parser&lt;/strong&gt;. If the first token (or couple tokens) that the model outputs is &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;, Ollama handles the parsing of the tool calls. Assuming the model is decent at following instructions, &lt;em&gt;this means the tool calls will actually populate the&lt;/em&gt; &lt;code&gt;tool_calls&lt;/code&gt; &lt;em&gt;field rather than&lt;/em&gt; &lt;code&gt;content&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Demonstration&lt;/h1&gt; &lt;p&gt;So just for gits and shiggles, let's see if we can get Gemma 3 to call tools properly. I adapted the same concepts from qwen2.5's chat template to Gemma 3's chat template. Before I show that template, let me show you that it works.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama def add_two_numbers(a: int, b: int) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot; Add two numbers Args: a: The first integer number b: The second integer number Returns: int: The sum of the two numbers &amp;quot;&amp;quot;&amp;quot; return a + b response = ollama.chat( 'gemma3-tools', messages=[{'role': 'user', 'content': 'What is 10 + 10?'}], tools=[add_two_numbers], ) print(response) # model='gemma3-tools' created_at='2025-03-14T02:47:29.234101Z' # done=True done_reason='stop' total_duration=19211740040 # load_duration=8867467023 prompt_eval_count=79 # prompt_eval_duration=6591000000 eval_count=35 # eval_duration=3736000000 # message=Message(role='assistant', content='', images=None, # tool_calls=[ToolCall(function=Function(name='add_two_numbers', # arguments={'a': 10, 'b': 10}))]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Booyah! Native function calling with Gemma 3.&lt;/p&gt; &lt;p&gt;It's not bullet-proof, mainly because it's not strictly enforcing a grammar. But assuming the model follows instructions, it should work *most* of the time.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Here's the template I used. It's very much like qwen2.5 in terms of the structure and logic, but using the tags of Gemma 3. Give it a shot, and better yet adapt this pattern to other models that you wish had tools.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- if .Messages }} {{- if or .System .Tools }}&amp;lt;start_of_turn&amp;gt;user {{- if .System}} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range $.Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1 -}} {{- if eq .Role &amp;quot;user&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user {{ .Content }}&amp;lt;end_of_turn&amp;gt; {{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start_of_turn&amp;gt;model {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments}}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user &amp;lt;tool_response&amp;gt; {{ .Content }} &amp;lt;/tool_response&amp;gt;&amp;lt;end_of_turn&amp;gt; {{ end }} {{- if and (ne .Role &amp;quot;assistant&amp;quot;) $last }}&amp;lt;start_of_turn&amp;gt;model {{ end }} {{- end }} {{- else }} {{- if .System }}&amp;lt;start_of_turn&amp;gt;user {{ .System }}&amp;lt;end_of_turn&amp;gt; {{ end }}{{ if .Prompt }}&amp;lt;start_of_turn&amp;gt;user {{ .Prompt }}&amp;lt;end_of_turn&amp;gt; {{ end }}&amp;lt;start_of_turn&amp;gt;model {{ end }}{{ .Response }}{{ if .Response }}&amp;lt;end_of_turn&amp;gt;{{ end }}&amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T03:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb93w4</id>
    <title>LLM-docs, software documentation intended for consumption by LLMs</title>
    <updated>2025-03-14T17:15:14+00:00</updated>
    <author>
      <name>/u/dicklesworth</name>
      <uri>https://old.reddit.com/user/dicklesworth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb93w4/llmdocs_software_documentation_intended_for/"&gt; &lt;img alt="LLM-docs, software documentation intended for consumption by LLMs" src="https://external-preview.redd.it/nCFgA6Bjci-abPBdnga4OZVbnoUUAXViwnd13AzcbtM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46483ab1e42c0f62857cf32440cc56cdf564391a" title="LLM-docs, software documentation intended for consumption by LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was inspired by a recent tweet by Andrej Karpathy, as well as my own experience copying and pasting a bunch of html docs into Claude yesterday and bemoaning how long-winded and poorly formatted it was.&lt;/p&gt; &lt;p&gt;I’m trying to decide if I should make it into a full-fledged service and completely automate the process of generating the distilled documentation.&lt;/p&gt; &lt;p&gt;Problem is that it would cost a lot in API tokens and wouldn’t generate any revenue (plus it would have to be updated as documentation changes significantly). Maybe Anthropic wants to fund it as a public good? Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dicklesworth"&gt; /u/dicklesworth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Dicklesworthstone/llm-docs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb93w4/llmdocs_software_documentation_intended_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb93w4/llmdocs_software_documentation_intended_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:15:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb9bfb</id>
    <title>QwQ-32B seems useless on local ollama. Anyone have luck to escape from thinking hell?</title>
    <updated>2025-03-14T17:23:50+00:00</updated>
    <author>
      <name>/u/soteko</name>
      <uri>https://old.reddit.com/user/soteko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, trying new QwQ-32B from 2 days ago &lt;a href="https://huggingface.co/Qwen/QwQ-32B-GGUF"&gt;https://huggingface.co/Qwen/QwQ-32B-GGUF&lt;/a&gt; and simply I can't get any real code out from it. It is thinking and thinking and never stops and probably will hit some limit like Context or Max Tokens and will stop before getting any real result.&lt;/p&gt; &lt;p&gt;I am running it on CPU, with temperature 0.7, Top P 0.95, Max Tokens (num_predict) 1200, Context 2048 - 8192.&lt;/p&gt; &lt;p&gt;Anyone trying it for coding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soteko"&gt; /u/soteko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb9bfb/qwq32b_seems_useless_on_local_ollama_anyone_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T17:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoc8n</id>
    <title>QwQ on LiveBench (update) - is better than DeepSeek R1!</title>
    <updated>2025-03-13T22:14:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt; &lt;img alt="QwQ on LiveBench (update) - is better than DeepSeek R1!" src="https://preview.redd.it/sb78tt607joe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22464da14ee7a94ffe6b7ad76b52c34bab00a921" title="QwQ on LiveBench (update) - is better than DeepSeek R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb78tt607joe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqylp</id>
    <title>LLM must pass a skill check to talk to me</title>
    <updated>2025-03-14T00:13:29+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt; &lt;img alt="LLM must pass a skill check to talk to me" src="https://external-preview.redd.it/Y3U2cGt3NmNzam9lMRWrFMxzjmNxpTNLvYX1gtD81VUNlzdlH0AiqtOky6_L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45045d6f36cca8e45bf1337eccb796748268735" title="LLM must pass a skill check to talk to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w7dney6csjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahs0b</id>
    <title>OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch</title>
    <updated>2025-03-13T17:38:10+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt; &lt;img alt="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" src="https://external-preview.redd.it/YuFnFIavAP98hFeGzOxLZQ1jrf6fXSzPC6RHQ4YO4ew.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2312e28fa573cb9d493e784a1275b4624e4c905" title="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/03/13/openai-calls-deepseek-state-controlled-calls-for-bans-on-prc-produced-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1janmn8</id>
    <title>SESAME IS HERE</title>
    <updated>2025-03-13T21:43:12+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sesame just released their 1B CSM.&lt;br /&gt; Sadly parts of the pipeline are missing.&lt;/p&gt; &lt;p&gt;Try it here:&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/sesame/csm-1b"&gt;https://huggingface.co/spaces/sesame/csm-1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation steps here:&lt;br /&gt; &lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb7u4v</id>
    <title>What is the best uncensored LLM?</title>
    <updated>2025-03-14T16:21:55+00:00</updated>
    <author>
      <name>/u/Anyusername7294</name>
      <uri>https://old.reddit.com/user/Anyusername7294</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not talking about &amp;quot;I will write you a erotic story&amp;quot; type of uncensored LLM, I'm talking about &amp;quot;I will tell you how to make a bomb&amp;quot; (I won't do that) type of uncensored LLM. It seems like everyone, when talking about &amp;quot;Uncensored&amp;quot; models, talks about erotic uncensored models and not about what I want. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anyusername7294"&gt; /u/Anyusername7294 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb7u4v/what_is_the_best_uncensored_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T16:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb4jcr</id>
    <title>Difference in Gemma 3 27b performance between ai studio and ollama</title>
    <updated>2025-03-14T13:58:51+00:00</updated>
    <author>
      <name>/u/Any-Mathematician683</name>
      <uri>https://old.reddit.com/user/Any-Mathematician683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I am building an enterprise grade RAG application and looking for a open-source LLM model for Summarisation and Question-answering purposes.&lt;/p&gt; &lt;p&gt;I really liked the Gemma 3 27B model when i tried it on ai studio. It is summarising transcripts with great precision. Infact, performance on openrouter is also great.&lt;/p&gt; &lt;p&gt;But as &lt;strong&gt;I am trying it on ollama, it is giving me subpar performance compared to aistudio.&lt;/strong&gt; I have tried &lt;a href="https://ollama.com/library/gemma3:27b-it-fp16"&gt;27b-it-fp16&lt;/a&gt; model as well as I thought performance loss might be because of quantization.&lt;/p&gt; &lt;p&gt;I also went through this &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;tutorial from Unsloth&lt;/a&gt;, and &lt;strong&gt;tried with recommended settings(temperature=1.0, top-k 64, top-p 0.95) on llama.cpp.&lt;/strong&gt; I did notice little better output but it is not as compared to output on openrouter / aistudio.&lt;/p&gt; &lt;p&gt;I noticed the &lt;strong&gt;same performance gap for command r models&lt;/strong&gt; between ollama and cohere playground.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can you please help me in identifying the root cause for this?&lt;/strong&gt; I genuinely believe there has to be some reason behind it.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Mathematician683"&gt; /u/Any-Mathematician683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jazfkf</id>
    <title>I created an OpenAI TTS compatible endpoint for Sesame CSM 1B</title>
    <updated>2025-03-14T08:48:44+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a work in progress, especially around trying to normalize the voice/voices. &lt;/p&gt; &lt;p&gt;Give it a shot and let me know what you think. PR's welcomed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/phildougherty/sesame_csm_openai"&gt;https://github.com/phildougherty/sesame_csm_openai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T08:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxec3</id>
    <title>Sesame CSM 1B Voice Cloning</title>
    <updated>2025-03-14T06:18:15+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt; &lt;img alt="Sesame CSM 1B Voice Cloning" src="https://external-preview.redd.it/JaEGat2-q67uEqWoPpGo5Nx0tvU4ZMhHe5tLQNQxW9w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c285788573980e7289358fbf97c0847d9b60866" title="Sesame CSM 1B Voice Cloning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/csm-voice-cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoy9g</id>
    <title>Meme i made</title>
    <updated>2025-03-13T22:41:19+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt; &lt;img alt="Meme i made" src="https://external-preview.redd.it/a3h0bzNwMWxiam9lMWaOI-rE6YlXiP74zpe4ixbVM_QsxQQzHzv1tNet8B-Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aee2797aa64747b42b65d38774f6590f3d0a9e9d" title="Meme i made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzku6n1lbjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb3mpe</id>
    <title>Gemma 3 Function Calling Example prompt</title>
    <updated>2025-03-14T13:14:57+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt; &lt;img alt="Gemma 3 Function Calling Example prompt" src="https://preview.redd.it/xv7wwtdmnnoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53b82d556c025c9987fe92be4363ea5c1a3d97b0" title="Gemma 3 Function Calling Example prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xv7wwtdmnnoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1sgv</id>
    <title>Conclusion: Sesame has shown us a CSM. Then Sesame announced that it would publish... something. Sesame then released a TTS, which they obviously misleadingly and falsely called a CSM. Do I see that correctly?</title>
    <updated>2025-03-14T11:34:28+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It wouldn't have been a problem at all if they had simply said that it wouldn't be open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb6gk7</id>
    <title>I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good</title>
    <updated>2025-03-14T15:23:51+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt; &lt;img alt="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" src="https://preview.redd.it/uc4ktdmraooe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fee6edf8a1814ec03199e5138b163065448da" title="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4ktdmraooe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5qvy</id>
    <title>KoboldCPP 1.86 just dropped with support of Gemma-3</title>
    <updated>2025-03-14T14:53:08+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.86"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here it is. Just tried it, thank you guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1tum</id>
    <title>HowTo: Decentralized LLM on Akash, IPFS &amp; Pocket Network, could this run LLaMA?</title>
    <updated>2025-03-14T11:36:40+00:00</updated>
    <author>
      <name>/u/era_hickle</name>
      <uri>https://old.reddit.com/user/era_hickle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt; &lt;img alt="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" src="https://external-preview.redd.it/Hrj-dOCVDQmxgV_iRUOvs_Xsy9EB5pShvqJs-R97RdI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6f0426a6f366fea7132175fe3881893d8a8b01b" title="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/era_hickle"&gt; /u/era_hickle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pocket.network/case-study-building-a-decentralized-deepseek-combining-open-data-compute-and-reasoning-with-pocket-network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:36:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jba8c1</id>
    <title>Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM</title>
    <updated>2025-03-14T18:05:43+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt; &lt;img alt="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" src="https://b.thumbs.redditmedia.com/HVn5TgokJMLlPcDoL4fVZ2_Vk4oxK6Eh7mcozu3sLRs.jpg" title="Gemma 3 Fine-tuning now in Unsloth - 1.6x faster with 60% less VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Gemma 3 (12B) up to &lt;strong&gt;6x longer context lengths&lt;/strong&gt; with Unsloth than Hugging Face + FA2 on a 24GB GPU. 27B also fits in 24GB!&lt;/p&gt; &lt;p&gt;We also saw &lt;strong&gt;infinite exploding gradients&lt;/strong&gt; when using older GPUs (Tesla T4s, RTX 2080) with float16 for Gemma 3. Newer GPUs using float16 like A100s also have the same issue - I auto fix this in Unsloth!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;There are also double BOS tokens which ruin finetunes for Gemma 3 - Unsloth auto corrects for this as well!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth now supports&lt;/strong&gt; &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;everything&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; This includes &lt;strong&gt;full fine-tuning&lt;/strong&gt;, pretraining, and support for all models (like &lt;strong&gt;Mixtral&lt;/strong&gt;, MoEs, Cohere etc. models) and algorithms like DoRA&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastModel.from_pretrained( model_name = &amp;quot;unsloth/gemma-3-4B-it&amp;quot;, load_in_4bit = True, load_in_8bit = False, # [NEW!] 8bit full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Gemma 3 (27B) fits in 22GB VRAM. You can read our in depth blog post about the new changes: &lt;a href="https://unsloth.ai/blog/gemma3"&gt;unsloth.ai/blog/gemma3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tune Gemma 3 (4B) for free using our&lt;/strong&gt; &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab notebook&lt;/strong&gt;&lt;/a&gt;.ipynb)&lt;/li&gt; &lt;li&gt;We uploaded Dynamic 4-bit quants, and it's even more effective due to Gemma 3's multi modality. See all Gemma 3 Uploads including GGUF, 4-bit etc: &lt;a href="https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b"&gt;Models&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7xnidddi3poe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75c2f0fad10c4e170d1455269118d0fff4c38baf"&gt;Gemma 3 27B quantization errors&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made a &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;Guide to run Gemma 3&lt;/a&gt; properly and fixed issues with GGUFs not working with vision - reminder the correct params according to the Gemma team are &lt;strong&gt;temperature = 1.0, top_p = 0.95, top_k = 64&lt;/strong&gt;. According to the Ollama team, you should use temp = 0.1 in Ollama for now due to some backend differences. Use temp = 1.0 in llama.cpp, Unsloth, and other backends!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Gemma 3 Dynamic 4-bit instruct quants:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-1b-it-unsloth-bnb-4bit"&gt;1B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-4b-it-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-12b-it-unsloth-bnb-4bit"&gt;12B&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit"&gt;27B&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let me know if you have any questions and hope you all have a lovely Friday and weekend! :) Also to update Unsloth do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B"&gt;&lt;strong&gt;Colab Notebook&lt;/strong&gt;&lt;/a&gt;.ipynb) with free GPU to finetune, do inference, data prep on Gemma 3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jba8c1/gemma_3_finetuning_now_in_unsloth_16x_faster_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T18:05:43+00:00</published>
  </entry>
</feed>
