<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-13T15:48:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lyl697</id>
    <title>How do you make Loras for Qwen coder / devstral?</title>
    <updated>2025-07-13T05:39:00+00:00</updated>
    <author>
      <name>/u/ComprehensiveBird317</name>
      <uri>https://old.reddit.com/user/ComprehensiveBird317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am wondering if anyone did this before, at least I couldn't find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComprehensiveBird317"&gt; /u/ComprehensiveBird317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T05:39:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyjgwv</id>
    <title>[Help] Fastest model for real-time UI automation? (Browser-Use too slow)</title>
    <updated>2025-07-13T04:00:54+00:00</updated>
    <author>
      <name>/u/BulkyAd7044</name>
      <uri>https://old.reddit.com/user/BulkyAd7044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I‚Äôve been using &lt;strong&gt;Browser-Use&lt;/strong&gt;, which is solid for tracking state/actions, but execution is too slow ‚Äî especially when an LLM is in the loop at each step.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example flow (on Google settings):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://myaccount.google.com"&gt;myaccount.google.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click ‚ÄúData &amp;amp; privacy‚Äù&lt;/li&gt; &lt;li&gt;Scroll down&lt;/li&gt; &lt;li&gt;Click ‚ÄúDelete a service or your account‚Äù&lt;/li&gt; &lt;li&gt;Click ‚ÄúDelete your Google Account‚Äù&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Looking for suggestions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest models for small structured decision tasks &lt;/li&gt; &lt;li&gt;Ways to be under 1s per step (ideally &amp;lt;500ms)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don‚Äôt need full chat reasoning ‚Äî just high-confidence decisions from small JSON lists.&lt;/p&gt; &lt;p&gt;Would love to hear what setups/models have worked for you in similar low-latency UI agent tasks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BulkyAd7044"&gt; /u/BulkyAd7044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly6cg6</id>
    <title>Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!</title>
    <updated>2025-07-12T17:41:49+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.&lt;/p&gt; &lt;p&gt;It's &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/issues/64"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lymlgp</id>
    <title>Dark Arts: Speaker embedding gradient descent for local TTS models</title>
    <updated>2025-07-13T07:08:39+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.]&lt;/p&gt; &lt;p&gt;Let's say you have a local TTS model that takes a speaker embedding &lt;code&gt;spk_emb&lt;/code&gt;, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.&lt;/p&gt; &lt;p&gt;Here is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch # 1. Initialize the embedding, either randomly or nearest neighbor spk_emb = torch.randn(1, 512) # if batch size 1, dim 512 spk_emb.requires_grad = True # 2. Initialize the model and freeze its parameters model = YourModelClass.from_pretrained('TODO') device = 'cuda' if torch.cuda.is_available() else 'cpu' model.to(device).eval() for p in model.parameters(): p.requires_grad = False # 3. Optimizer and dataset, LR is up to you optimizer = torch.optim.Adam([spk_emb], lr=0.001) TODO_your_dataset_of_text_audio_pairs = [ ('This is some text.', 'corresponding_audio.wav'), # ... ] # 4. Barebones training loop. You can add a learning rate scheduler, etc. for epoch in range(10): # how many epochs is up to you for text, audio in TODO_your_dataset_of_text_audio_pairs: loss = model.forward_with_loss(text, audio, spk_emb) loss.backward() optimizer.step() optimizer.zero_grad() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.&lt;/p&gt; &lt;p&gt;And that's it. If you have any questions you can post them below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T07:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvkhr</id>
    <title>Let‚Äôs talk about models you believed are more Hyped than Hot</title>
    <updated>2025-07-13T15:27:53+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you‚Ä¶ then list one or two models and the environment you use them in daily that do a better job. &lt;/p&gt; &lt;p&gt;I had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga‚Äôs text generation UI. I wasn‚Äôt happy with either. &lt;/p&gt; &lt;p&gt;I prefer llama 3.3 70b and Gemma 27b over it in those environments ‚Ä¶ though I love Reka‚Äôs license.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvsqv</id>
    <title>Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)</title>
    <updated>2025-07-13T15:37:33+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm releasing a v1.0 of my &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt; &lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know how it works and also checkout my &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyq1yh</id>
    <title>LLM evaluation in real life?</title>
    <updated>2025-07-13T10:59:53+00:00</updated>
    <author>
      <name>/u/Plastic-Bus-7003</name>
      <uri>https://old.reddit.com/user/Plastic-Bus-7003</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;Wanted to ask a question that's been on my mind recently.&lt;/p&gt; &lt;p&gt;I've done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.&lt;/p&gt; &lt;p&gt;But how is LLM evaluation done in real life (i.e. in industry)? If I'm a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it's doing a good job?&lt;/p&gt; &lt;p&gt;Is it only product related metrics like customer satisfaction and existing benchmarks like in the industry? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plastic-Bus-7003"&gt; /u/Plastic-Bus-7003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyq22j</id>
    <title>Local LLM to back Elastic AI</title>
    <updated>2025-07-13T11:00:03+00:00</updated>
    <author>
      <name>/u/OldManCyberNinja</name>
      <uri>https://old.reddit.com/user/OldManCyberNinja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt; &lt;p&gt;I've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt; &lt;p&gt;I did look at &lt;a href="https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt; &lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt; &lt;p&gt;Looking for help with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt; &lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt; &lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have some constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must be air-gapped&lt;/li&gt; &lt;li&gt;I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt; &lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear from anyone who‚Äôs done this in production or lab.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldManCyberNinja"&gt; /u/OldManCyberNinja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T11:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyfngg</id>
    <title>How do you keep up with all these things?</title>
    <updated>2025-07-13T00:39:07+00:00</updated>
    <author>
      <name>/u/ontologicalmemes</name>
      <uri>https://old.reddit.com/user/ontologicalmemes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontologicalmemes"&gt; /u/ontologicalmemes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T00:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyonb4</id>
    <title>How are people actually able to get the system prompt of these AI companies?</title>
    <updated>2025-07-13T09:26:26+00:00</updated>
    <author>
      <name>/u/divyamchandel</name>
      <uri>https://old.reddit.com/user/divyamchandel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?&lt;/p&gt; &lt;p&gt;There are three things that come to my mind:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Using some prompt injection (re-iteratively)&lt;/em&gt;&lt;/strong&gt;: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Inspecting the client side code if possible&lt;/em&gt;&lt;/strong&gt;: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing the request server&lt;/em&gt;&lt;/strong&gt;: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divyamchandel"&gt; /u/divyamchandel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T09:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly4zh8</id>
    <title>Okay kimi-k2 is an INSANE model WTF those one-shot animations</title>
    <updated>2025-07-12T16:44:50+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt; &lt;img alt="Okay kimi-k2 is an INSANE model WTF those one-shot animations" src="https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ce744efba81890d05b5b715ce402a332366d7a" title="Okay kimi-k2 is an INSANE model WTF those one-shot animations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/74d8efoh2hcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyozcn</id>
    <title>Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples</title>
    <updated>2025-07-13T09:49:23+00:00</updated>
    <author>
      <name>/u/muthuishere2101</name>
      <uri>https://old.reddit.com/user/muthuishere2101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"&gt; &lt;img alt="Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples" src="https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6b426f7ec6446f2ee47476b0c04d150cea9b8a5" title="Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muthuishere2101"&gt; /u/muthuishere2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T09:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lytioc</id>
    <title>Looking for my next laptop soon</title>
    <updated>2025-07-13T14:00:28+00:00</updated>
    <author>
      <name>/u/robotecnik</name>
      <uri>https://old.reddit.com/user/robotecnik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all,&lt;/p&gt; &lt;p&gt;Soon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I've done... helps.&lt;/p&gt; &lt;p&gt;Sending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.&lt;/p&gt; &lt;p&gt;I am looking at the p16s Gen 4 AMD with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.&lt;/p&gt; &lt;p&gt;The first one will be more affordable than the second one...&lt;/p&gt; &lt;p&gt;Would current big models run normally on a laptop like that P16s?&lt;/p&gt; &lt;p&gt;Thank you all in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robotecnik"&gt; /u/robotecnik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T14:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8fyj</id>
    <title>This whole thing is giving me WizardLM2 vibes.</title>
    <updated>2025-07-12T19:09:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt; &lt;img alt="This whole thing is giving me WizardLM2 vibes." src="https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104" title="This whole thing is giving me WizardLM2 vibes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kn56m7cgshcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyv750</id>
    <title>How I use Gemma 3 to help me reply my texts</title>
    <updated>2025-07-13T15:12:40+00:00</updated>
    <author>
      <name>/u/sean01-eth</name>
      <uri>https://old.reddit.com/user/sean01-eth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt; &lt;img alt="How I use Gemma 3 to help me reply my texts" src="https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fae684eb96341704e9ce19cc7a34eb9eaea57f63" title="How I use Gemma 3 to help me reply my texts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.&lt;/p&gt; &lt;p&gt;The app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt; &lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt; &lt;p&gt;Here's a brief guide to make this work with ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download the app from GitHub: &lt;a href="https://github.com/coreply/coreply"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt; &lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt; &lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href="http://192.168.xxx.xxx"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt; &lt;p&gt;Let me know how's your experience with it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sean01-eth"&gt; /u/sean01-eth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/48w6qb1mincf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly42e5</id>
    <title>Interesting info about Kimi K2</title>
    <updated>2025-07-12T16:05:34+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt; &lt;img alt="Interesting info about Kimi K2" src="https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423" title="Interesting info about Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt; &lt;p&gt;Source: @rasbt on X&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/klm2b78lvgcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykf92</id>
    <title>What Causes Poor Long-Context Performance?</title>
    <updated>2025-07-13T04:54:44+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href="https://www.databricks.com/blog/long-context-rag-performance-llms"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt; &lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href="https://arxiv.org/pdf/2410.05258"&gt;this paper&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt; &lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykqbu</id>
    <title>SmolLM-3B when asked if it was Peter Griffin</title>
    <updated>2025-07-13T05:12:45+00:00</updated>
    <author>
      <name>/u/Humble_Hovercraft199</name>
      <uri>https://old.reddit.com/user/Humble_Hovercraft199</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt; &lt;img alt="SmolLM-3B when asked if it was Peter Griffin" src="https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74c41be70bfcf32292dc40a30f75326535854875" title="SmolLM-3B when asked if it was Peter Griffin" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing the &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU"&gt;SmolLM3-3B-WebGPU&lt;/a&gt; Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: &amp;quot;Are you peter griffin?&amp;quot;, it just generated a 4000-token list of &amp;quot;Key Takeaways&amp;quot; about its existence:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0"&gt;https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was only able to trigger this behavior on that specific HF Space (Although, it doesn't seem to be a one time thing. I was able to get &lt;em&gt;very&lt;/em&gt; similar responses by asking it the same question again in a new tab, after refreshing). I've since downloaded the model and wasn't able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Hovercraft199"&gt; /u/Humble_Hovercraft199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T05:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyj81f</id>
    <title>Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)</title>
    <updated>2025-07-13T03:47:18+00:00</updated>
    <author>
      <name>/u/mathsTeacher82</name>
      <uri>https://old.reddit.com/user/mathsTeacher82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia. Google Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024. Any open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/vJjgtOcXq8A"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mathsTeacher82"&gt; /u/mathsTeacher82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lysqk7</id>
    <title>[Rumor] Huawei 920 accelerator coming H2 2026</title>
    <updated>2025-07-13T13:23:53+00:00</updated>
    <author>
      <name>/u/44seconds</name>
      <uri>https://old.reddit.com/user/44seconds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So 6 months ago I discussed some information about the at the time not launched &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/"&gt;910C accelerator here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) &lt;a href="https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/"&gt;https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And semianalysis (regarding the 800 tflop bf16 performance) &lt;a href="https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/"&gt;https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.&lt;/p&gt; &lt;p&gt;On the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.&lt;/p&gt; &lt;p&gt;Recently attended a conference where senior Huawei executives verbally discussed their future plans:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They are aiming for a launch of the 920 in H2 2026 or H1 2027 &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A quote near verbatim &amp;quot;the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.&amp;quot; It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44seconds"&gt; /u/44seconds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T13:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyvah4</id>
    <title>Tried Kimi K2 for writing and reasoning, and was not impressed.</title>
    <updated>2025-07-13T15:16:28+00:00</updated>
    <author>
      <name>/u/GlompSpark</name>
      <uri>https://old.reddit.com/user/GlompSpark</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here's a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion by using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character's thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It has possibly the most restrictive content filters i have seen out of all the AI chat bots i have tried. It's very prudish.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlompSpark"&gt; /u/GlompSpark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T15:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyaozv</id>
    <title>Moonshot AI just made their moonshot</title>
    <updated>2025-07-12T20:46:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt; &lt;img alt="Moonshot AI just made their moonshot" src="https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd" title="Moonshot AI just made their moonshot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Screenshot: &lt;a href="https://openrouter.ai/moonshotai"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Announcement: &lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95q67pnr9icf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lylo75</id>
    <title>Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing</title>
    <updated>2025-07-13T06:09:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt; &lt;img alt="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" src="https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg" title="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench responses:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lylo75"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T06:09:23+00:00</published>
  </entry>
</feed>
