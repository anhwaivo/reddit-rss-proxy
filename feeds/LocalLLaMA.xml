<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-18T09:51:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m2u9n3</id>
    <title>Can you recommend something I can personally do with two H100?</title>
    <updated>2025-07-18T05:45:29+00:00</updated>
    <author>
      <name>/u/CantaloupeDismal1195</name>
      <uri>https://old.reddit.com/user/CantaloupeDismal1195</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working at a listed OCR company and am in the on-premise OCR research department based on LLM. Since I am conducting research with large models such as Qwen2.5 VL 72B, I have a lot of personal time while the models are running. Are there any things I can do on my own related to LLM with two H100s? I would appreciate it if you could recommend them. After completing my Masters in Vision and moving to LLM, it is not easy to find things to study on my own.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CantaloupeDismal1195"&gt; /u/CantaloupeDismal1195 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T05:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1i922</id>
    <title>He’s out of line but he’s right</title>
    <updated>2025-07-16T17:06:31+00:00</updated>
    <author>
      <name>/u/EstablishmentFun3205</name>
      <uri>https://old.reddit.com/user/EstablishmentFun3205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt; &lt;img alt="He’s out of line but he’s right" src="https://preview.redd.it/dqx9wlf3q9df1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d71f6c8f3707ff6aae1011b47babeb593bd890e1" title="He’s out of line but he’s right" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EstablishmentFun3205"&gt; /u/EstablishmentFun3205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dqx9wlf3q9df1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1i922/hes_out_of_line_but_hes_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-16T17:06:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2x30u</id>
    <title>RAG at the Crossroads - Mid-2025 Reflections on AI’s Incremental Evolution | RAGFlow</title>
    <updated>2025-07-18T08:44:40+00:00</updated>
    <author>
      <name>/u/Vissidarte_2021</name>
      <uri>https://old.reddit.com/user/Vissidarte_2021</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2x30u/rag_at_the_crossroads_mid2025_reflections_on_ais/"&gt; &lt;img alt="RAG at the Crossroads - Mid-2025 Reflections on AI’s Incremental Evolution | RAGFlow" src="https://external-preview.redd.it/ux7TbEUCIw9ZU5CQwxlzu6xdLJzaPaFsd-z2NOfzQXU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55ed8d5553d36208bec37c0a09fad4b76708fb55" title="RAG at the Crossroads - Mid-2025 Reflections on AI’s Incremental Evolution | RAGFlow" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vissidarte_2021"&gt; /u/Vissidarte_2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ragflow.io/blog/rag-at-the-crossroads-mid-2025-reflections-on-ai-evolution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2x30u/rag_at_the_crossroads_mid2025_reflections_on_ais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2x30u/rag_at_the_crossroads_mid2025_reflections_on_ais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T08:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2c4hz</id>
    <title>Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1</title>
    <updated>2025-07-17T16:27:53+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt; &lt;img alt="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" src="https://preview.redd.it/in8sapsyngdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f072d23b2d5e641467cb234ea0435163e5f1b18" title="Kimi K2 Fiction.liveBench: On-par with DeepSeek V3, behind GPT-4.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/in8sapsyngdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2c4hz/kimi_k2_fictionlivebench_onpar_with_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m1xqv1</id>
    <title>We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models</title>
    <updated>2025-07-17T04:04:21+00:00</updated>
    <author>
      <name>/u/NixTheFolf</name>
      <uri>https://old.reddit.com/user/NixTheFolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt; &lt;img alt="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" src="https://preview.redd.it/zfvdqak3zcdf1.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a4c2e85087da70112018731aafb9b5d409cf823" title="We have hit 500,000 members! We have come a long way from the days of the leaked LLaMA 1 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NixTheFolf"&gt; /u/NixTheFolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfvdqak3zcdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m1xqv1/we_have_hit_500000_members_we_have_come_a_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T04:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2pvwt</id>
    <title>LPOI: Listwise Preference Optimization for Vision-Language Models (ACL 2025 Main)</title>
    <updated>2025-07-18T01:54:48+00:00</updated>
    <author>
      <name>/u/Moreselflove0324</name>
      <uri>https://old.reddit.com/user/Moreselflove0324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2pvwt/lpoi_listwise_preference_optimization_for/"&gt; &lt;img alt="LPOI: Listwise Preference Optimization for Vision-Language Models (ACL 2025 Main)" src="https://preview.redd.it/ns3j3mbqgjdf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68c4faed9595d755cd4e9dfb137e0f3c613aa22" title="LPOI: Listwise Preference Optimization for Vision-Language Models (ACL 2025 Main)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2505.21061"&gt;https://arxiv.org/abs/2505.21061&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/fatemehpesaran310/lpoi"&gt;https://github.com/fatemehpesaran310/lpoi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreselflove0324"&gt; /u/Moreselflove0324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ns3j3mbqgjdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2pvwt/lpoi_listwise_preference_optimization_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2pvwt/lpoi_listwise_preference_optimization_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T01:54:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2asou</id>
    <title>Kimi-k2 on lmarena</title>
    <updated>2025-07-17T15:37:09+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt; &lt;img alt="Kimi-k2 on lmarena" src="https://b.thumbs.redditmedia.com/fy5-o3tc0GF-I3bCGJzPb2bsjXpQ9yAyleERp4yhbOw.jpg" title="Kimi-k2 on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;overall:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e"&gt;https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hard prompts:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392"&gt;https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392&lt;/a&gt;&lt;/p&gt; &lt;p&gt;coding:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a"&gt;https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T15:37:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2xh8s</id>
    <title>Run Kimi-K2 without quantization locally for under $10k?</title>
    <updated>2025-07-18T09:10:00+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a thought experiment right now, but hear me out. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct/tree/main"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct/tree/main&lt;/a&gt; the weights for Kimi K2 is about 1031GB in total. &lt;/p&gt; &lt;p&gt;You can buy 12 sticks of 96gb DDR5-6400 RAM (total 1152GB) &lt;a href="https://www.amazon.com/NEMIX-RAM-Registered-Compatible-Supermicro/dp/B0DQLVV9TK"&gt;for about $7200&lt;/a&gt;. DDR5-6400 12 channel is &lt;a href="https://chatgpt.com/share/687a0964-60cc-8012-8b2e-d98154d79691"&gt;614GB/sec&lt;/a&gt;. That's pretty close (about 75%) of the &lt;a href="https://www.apple.com/mac-studio/specs/"&gt;512GB Mac Studio which has 819GB/sec&lt;/a&gt; memory bandwidth. &lt;/p&gt; &lt;p&gt;You just need an AMD EPYC 9005 series cpu and a compatible 12 channel RAM motherboard, which &lt;a href="https://chatgpt.com/share/687a0bf0-8f00-8012-83e6-890414f2d0d1"&gt;costs around $1400 total&lt;/a&gt; these days. Throw in a Nvidia RTX 3090 or two, or maybe a RTX5090 (to handle the non MoE layers) and it should run even faster. With the 1152GB of DDR5 RAM combined with the GPU, you can run Kimi-K2 at a very reasonable speed for below $10k. &lt;/p&gt; &lt;p&gt;Do these numbers make sense? It seems like the Mac Studio 512GB has a competitor now, at least in terms of globs of RAM. The Mac Studio 512GB is still a bit faster in terms of memory bandwidth, but having 1152GB of RAM at the same price is certainly worth considering of a tradeoff for 25% of memory bandwidth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T09:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ml3n</id>
    <title>I’ll build an expert AI for your impossible challenge and give it away free - looking for the hardest technical problem you’ve got</title>
    <updated>2025-07-17T23:19:58+00:00</updated>
    <author>
      <name>/u/Prestigious-Fan118</name>
      <uri>https://old.reddit.com/user/Prestigious-Fan118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to test this on something brutal. You give me your hardest technical challenge, I’ll build a specialized AI for it this weekend and release it here for everyone.&lt;/p&gt; &lt;p&gt;What I’m looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extremely niche technical problems&lt;/li&gt; &lt;li&gt;Challenges where current LLMs completely fail&lt;/li&gt; &lt;li&gt;Tasks that normally require 10+ years of expertise&lt;/li&gt; &lt;li&gt;The more “impossible” the better &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Examples of the difficulty level I want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI that optimizes CUDA kernels for specific GPU architectures&lt;/li&gt; &lt;li&gt;AI that diagnoses and fixes race conditions in concurrent code&lt;/li&gt; &lt;li&gt;AI that ports assembly between different architectures&lt;/li&gt; &lt;li&gt;AI that generates efficient Vulkan/Metal shaders from descriptions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What happens:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Most upvoted challenge by Friday 6PM EST wins&lt;/li&gt; &lt;li&gt;I build it over the weekend&lt;/li&gt; &lt;li&gt;I come back Monday with the working system&lt;/li&gt; &lt;li&gt;You all get to stress-test it with your edge cases&lt;/li&gt; &lt;li&gt;If it works, everyone gets access to use it&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not selling anything. Just want to see if this handles your worst problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Fan118"&gt; /u/Prestigious-Fan118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ml3n/ill_build_an_expert_ai_for_your_impossible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T23:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2w5ge</id>
    <title>Did Kimi K2 train on Claude's generated code? I think yes</title>
    <updated>2025-07-18T07:43:02+00:00</updated>
    <author>
      <name>/u/Minute_Yam_1053</name>
      <uri>https://old.reddit.com/user/Minute_Yam_1053</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2w5ge/did_kimi_k2_train_on_claudes_generated_code_i/"&gt; &lt;img alt="Did Kimi K2 train on Claude's generated code? I think yes" src="https://a.thumbs.redditmedia.com/QGK5hjpUv2pPnOZnkfITtYSRm0i7TIppV4Z1eqKlUm0.jpg" title="Did Kimi K2 train on Claude's generated code? I think yes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After conducting some tests, I'm convinced that K2 either distilled from Claude or trained on Claude-generated code.&lt;/p&gt; &lt;p&gt;Every AI model has its own traits when generating code. For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4: likes gradient backgrounds, puts &amp;quot;2024&amp;quot; in footers, uses less stock photos&lt;/li&gt; &lt;li&gt;Claude Sonnet 3.7: Loves stock photos, makes everything modular&lt;/li&gt; &lt;li&gt;GPT-4.1 and Gemini 2.5 Pro: Each has their own habits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've tested some models and never seen two produce such similar outputs... until now.&lt;/p&gt; &lt;p&gt;I threw the same prompts at K2, Sonnet 4 and the results were similar.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 1&lt;/strong&gt;: &amp;quot;Generate a construction website for Ramos Construction&amp;quot;&lt;/p&gt; &lt;p&gt;Both K2 and Sonnet 4:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Picked almost identical layouts and colors&lt;/li&gt; &lt;li&gt;Used similar contact form text&lt;/li&gt; &lt;li&gt;Had that &amp;quot;2024&amp;quot; footer (Sonnet 4 habbit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jndvodyq6ldf1.png?width=2488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a2b4a8dca13e5c7164f44f2ec3fc2ff35774f3b"&gt;https://preview.redd.it/jndvodyq6ldf1.png?width=2488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a2b4a8dca13e5c7164f44f2ec3fc2ff35774f3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0yh8g4ss6ldf1.png?width=2582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc2254027f60e795460bc2cc9a55ec3040d5ab18"&gt;https://preview.redd.it/0yh8g4ss6ldf1.png?width=2582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc2254027f60e795460bc2cc9a55ec3040d5ab18&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 2&lt;/strong&gt;: &amp;quot;Generate a meme coin website for contract 87n4vtsy5CN7EzpFeeD25YtGfyJpUbqwDZtAzNFnNtRZ. Show token metadata, such as name, symbol, etc. Also include the roadmap and white paper&amp;quot;&lt;/p&gt; &lt;p&gt;Both went with similar gradient backgrounds - classic Sonnet 4 move.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fjhhrmqw6ldf1.png?width=2260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6f7258b15b3e1de78d61947ed758f2694a24825"&gt;https://preview.redd.it/fjhhrmqw6ldf1.png?width=2260&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6f7258b15b3e1de78d61947ed758f2694a24825&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nxli0aaz6ldf1.png?width=2632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c177c07e5b33a41d15f7b2102d5a3c7dade0d489"&gt;https://preview.redd.it/nxli0aaz6ldf1.png?width=2632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c177c07e5b33a41d15f7b2102d5a3c7dade0d489&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 3:&lt;/strong&gt; I generated a long PRD with LLM for &amp;quot;Melissa's Photography&amp;quot; and gave it to both models.&lt;/p&gt; &lt;p&gt;They didn't just make similar execution plans in Claude Code - some sections had very close copy that I never wrote in the PRD. That's not coincidence&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6v7vh1p77ldf1.png?width=1384&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22b5bd53affe545805338f671c61d554c287e985"&gt;https://preview.redd.it/6v7vh1p77ldf1.png?width=1384&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22b5bd53affe545805338f671c61d554c287e985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dtuvvkp87ldf1.png?width=1542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7846c86efd149c3c8d4d9127d4636558275eb3e1"&gt;https://preview.redd.it/dtuvvkp87ldf1.png?width=1542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7846c86efd149c3c8d4d9127d4636558275eb3e1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2b82n6aa7ldf1.png?width=3118&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71c780efb7581e0bc2c8c9bd749d924a5ae2c53"&gt;https://preview.redd.it/2b82n6aa7ldf1.png?width=3118&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71c780efb7581e0bc2c8c9bd749d924a5ae2c53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hlraw63b7ldf1.png?width=3028&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdafe0d284ac9667a6ca070a350157884c474f2e"&gt;https://preview.redd.it/hlraw63b7ldf1.png?width=3028&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdafe0d284ac9667a6ca070a350157884c474f2e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What This Means&lt;/h1&gt; &lt;p&gt;The Good:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;K2's code generation is actually pretty solid&lt;/li&gt; &lt;li&gt;If it learned from Claude, that's not bad - Claude writes decent code&lt;/li&gt; &lt;li&gt;K2 is way cheaper, so better bang for your buck&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Not So Good:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;K2 still screws up more (missing closing tags, suggests low quality edits in Claude Code)&lt;/li&gt; &lt;li&gt;Not as polished as Sonnet 4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I do not care much if K2 trained on Claude generated code. The ROI for the money is really appealing to me. How did it work for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minute_Yam_1053"&gt; /u/Minute_Yam_1053 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2w5ge/did_kimi_k2_train_on_claudes_generated_code_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2w5ge/did_kimi_k2_train_on_claudes_generated_code_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2w5ge/did_kimi_k2_train_on_claudes_generated_code_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T07:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2rw38</id>
    <title>Best Hardware Setup to Run DeepSeek-V3 670B Locally on $40K–$80K?</title>
    <updated>2025-07-18T03:34:18+00:00</updated>
    <author>
      <name>/u/PrevelantInsanity</name>
      <uri>https://old.reddit.com/user/PrevelantInsanity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re looking to build a local compute cluster to run DeepSeek-V3 670B (or similar top-tier open-weight LLMs) for inference only, supporting ~100 simultaneous chatbot users with large context windows (ideally up to 128K tokens).&lt;/p&gt; &lt;p&gt;Our preferred direction is an Apple Silicon cluster — likely Mac minis or studios with M-series chips — but we’re open to alternative architectures (e.g. GPU servers) if they offer significantly better performance or scalability.&lt;/p&gt; &lt;p&gt;Looking for advice on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Is it feasible to run 670B locally in that budget?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What’s the largest model realistically deployable with decent latency at 100-user scale?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can Apple Silicon handle this effectively — and if so, which exact machines should we buy within $40K–$80K?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How would a setup like this handle long-context windows (e.g. 128K) in practice?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there alternative model/infra combos we should be considering?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear from anyone who’s attempted something like this or has strong opinions on maximizing local LLM performance per dollar. Specifics about things to investigate, recommendations on what to run it on, or where to look for a quote are greatly appreciated!&lt;/p&gt; &lt;p&gt;Edit: I’ve reached the conclusion from you guys and my own research that full context window with the user county I specified isn’t feasible. Thoughts on how to appropriately adjust context window/quantization without major loss to bring things in line with budget are welcome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrevelantInsanity"&gt; /u/PrevelantInsanity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T03:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gios</id>
    <title>Given that powerful models like K2 are available cheaply on hosted platforms with great inference speed, are you regretting investing in hardware for LLMs?</title>
    <updated>2025-07-17T19:15:12+00:00</updated>
    <author>
      <name>/u/Sky_Linx</name>
      <uri>https://old.reddit.com/user/Sky_Linx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stopped running local models on my Mac a couple of months ago because with my M4 Pro I cannot run very large and powerful models. And to be honest I no longer see the point.&lt;/p&gt; &lt;p&gt;At the moment for example I am using Kimi K2 as default model for basically everything via Groq inference, which is shockingly fast for a 1T params model, and it costs me only $1 per million input tokens and $3 per million output tokens. I mean... seriously, I get the privacy concerns some might have, but if you use LLMs for serious work, not just for playing, it really doesn't make much sense to run local LLMs anymore apart from very simple tasks.&lt;/p&gt; &lt;p&gt;So my question is mainly for those of you who have recently invested quite some chunk of cash in more powerful hardware to run LLMs locally: are you regretting it at all considering what's available on hosted platforms like Groq and OpenRouter and their prices and performance?&lt;/p&gt; &lt;p&gt;Please don't downvote right away. I am not criticizing anyone and until recently I also had some fun running some LLMs locally. I am just wondering if others agree with me that it's no longer convenient when you take performance and cost into account.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sky_Linx"&gt; /u/Sky_Linx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gios/given_that_powerful_models_like_k2_are_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2vcrx</id>
    <title>mergekit LoRA extractor – how good is that?</title>
    <updated>2025-07-18T06:52:21+00:00</updated>
    <author>
      <name>/u/uhuge</name>
      <uri>https://old.reddit.com/user/uhuge</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any tests? &lt;/p&gt; &lt;p&gt;Is this integrated with llama-swap?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uhuge"&gt; /u/uhuge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/arcee-ai/mergekit?tab=readme-ov-file#lora-extraction"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2vcrx/mergekit_lora_extractor_how_good_is_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2vcrx/mergekit_lora_extractor_how_good_is_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T06:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2o3ht</id>
    <title>Help vote for improved Vulkan performance in ik_llama.cpp</title>
    <updated>2025-07-18T00:29:02+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Came across a discussion in ik_llama.cpp by accident where the main developer (ikawrakow) is soliciting feedback about whether they should focus on improving the performance of the Vulkan backend on ik_llama.cpp.&lt;/p&gt; &lt;p&gt;The discussion is 2 weeks old, but hasn't garnered much attention until now.&lt;/p&gt; &lt;p&gt;I think improved Vulkan performance in this project will benefit the community a lot. As I commented in that discussion, these are my arguments in favor of ikawrakow giving the Vulkan backend more attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This project doesn't get that much attention on reddit, etc compared to llama.cpp. So, he current userbase is a lot smaller. Having this question in the discussions, while appropriate, won't attract that much attention.&lt;/li&gt; &lt;li&gt;Vulkan is the only backend that's not tied to a specific vendor. Any optimization you make there will be useful on all GPUs, discrete or otherwise. If you can bring Vulkan close to parity with CUDA, it will be a huge win for any device that supports Vulkan, including older GPUs from Nvidia and AMD.&lt;/li&gt; &lt;li&gt;As firecoperana noted, not all quants need to be supported. A handful of the recent IQs used in recent MoE's like Qwen3-235B, DeepSeek-671B, and Kimi-K2 are more than enough. I'd even argue for supporting only power of two IQ quants only initially to limit scope and effort.&lt;/li&gt; &lt;li&gt;Inte's A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn't get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you own AMD or Intel GPUs, I'd urge you to check this discussion and vote in favor of improving Vulkan performance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/discussions/590"&gt;Link to the discussion&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gnnk</id>
    <title>Running an open source AI anime girl avatar</title>
    <updated>2025-07-17T19:20:31+00:00</updated>
    <author>
      <name>/u/mapppo</name>
      <uri>https://old.reddit.com/user/mapppo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt; &lt;img alt="Running an open source AI anime girl avatar" src="https://external-preview.redd.it/azUzamVqZ3FpaGRmMUstPxAQzeLBZZJeAt5drdnVhSzTD0UR9O7yYNnwsX72.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37334781ea18e9aad3cbc86b76fb7bd676c22989" title="Running an open source AI anime girl avatar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;after seeing a lot of posts about a certain expensive &amp;amp; cringy anime girlfriend, i wanted to see if there was a better way to get AI avatars. This is from &lt;a href="https://github.com/Open-LLM-VTuber/Open-LLM-VTuber"&gt;https://github.com/Open-LLM-VTuber/Open-LLM-VTuber&lt;/a&gt; (not my work) using 4o API and groq whisper, but it can use any API, or run entirely locally. You can use it with any live2d vtuber, I grabbed a random free one and did not configure the animations right. You can also change the personality prompt as you want. Serving it to mobile devices should work too but I don't care enough to try.&lt;/p&gt; &lt;p&gt;Thoughts? Would you pay for a Grokfriend? Are any of you crazy enough to date your computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapppo"&gt; /u/mapppo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rn1rxkgqihdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gnnk/running_an_open_source_ai_anime_girl_avatar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2lsbm</id>
    <title>#1 model on Open ASR nvidia/canary-qwen-2.5b is available now</title>
    <updated>2025-07-17T22:45:40+00:00</updated>
    <author>
      <name>/u/SummonerOne</name>
      <uri>https://old.reddit.com/user/SummonerOne</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"&gt; &lt;img alt="#1 model on Open ASR nvidia/canary-qwen-2.5b is available now" src="https://external-preview.redd.it/Nn-LD6fffringbEQZP1Qi_wM5thia6kxISdin3VAOxU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38a003a615320ccf2dd25639917dcbbb2e78d2db" title="#1 model on Open ASR nvidia/canary-qwen-2.5b is available now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It showed up on the leaderboard as #1 a couple days ago, and it's finally available now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SummonerOne"&gt; /u/SummonerOne &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/canary-qwen-2.5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2lsbm/1_model_on_open_asr_nvidiacanaryqwen25b_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T22:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2s686</id>
    <title>Amazing performance! Kimi K2 on ik_llama.cpp</title>
    <updated>2025-07-18T03:49:10+00:00</updated>
    <author>
      <name>/u/timmytimmy01</name>
      <uri>https://old.reddit.com/user/timmytimmy01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2s686/amazing_performance_kimi_k2_on_ik_llamacpp/"&gt; &lt;img alt="Amazing performance! Kimi K2 on ik_llama.cpp" src="https://external-preview.redd.it/mbqL7tVnx0USUgmIfTfH3gk6Pper9a5zZIt2Et32S4Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af7e06b1e31f22e6366e10579021d8702da59ec9" title="Amazing performance! Kimi K2 on ik_llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8685rjwu0kdf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=75899651ae7b2f3408b852ae298d78e3502b6664"&gt;https://preview.redd.it/8685rjwu0kdf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=75899651ae7b2f3408b852ae298d78e3502b6664&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found that ik_llama.cpp is faster(faster on prefill ,roughly the same on decode) and much easier to install than ktransformers. No need for conda and no more worry about dependency errors !! (If you had ever built ktransformers you know what I'm talking about)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a perfect replacement for ktransformers.&lt;/p&gt; &lt;p&gt;My hareware: epyc 7b13, 512gb 3200mhz ddr4, dual 5070ti &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timmytimmy01"&gt; /u/timmytimmy01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2s686/amazing_performance_kimi_k2_on_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2s686/amazing_performance_kimi_k2_on_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2s686/amazing_performance_kimi_k2_on_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T03:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2k480</id>
    <title>support for Ernie 4.5 MoE models has been merged into llama.cpp</title>
    <updated>2025-07-17T21:35:47+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"&gt; &lt;img alt="support for Ernie 4.5 MoE models has been merged into llama.cpp" src="https://external-preview.redd.it/Xa2nwNvQaZ79M355gwwIuuvaJFK0WjYiA5gWgioi6UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92351043d20a041609005195d5418e8e28968ed6" title="support for Ernie 4.5 MoE models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, only the tiny Ernie model was supported by llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14658"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2k480/support_for_ernie_45_moe_models_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T21:35:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ruo5</id>
    <title>Abogen: Generate Audiobooks with Synced Subtitles (Free &amp; Open Source)</title>
    <updated>2025-07-18T03:32:11+00:00</updated>
    <author>
      <name>/u/dnzsfk</name>
      <uri>https://old.reddit.com/user/dnzsfk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ruo5/abogen_generate_audiobooks_with_synced_subtitles/"&gt; &lt;img alt="Abogen: Generate Audiobooks with Synced Subtitles (Free &amp;amp; Open Source)" src="https://preview.redd.it/cgpjczuspjdf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd93ec43387880a7df02c8f0ff446863c6dfd718" title="Abogen: Generate Audiobooks with Synced Subtitles (Free &amp;amp; Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I've been working on a tool called &lt;a href="https://github.com/denizsafak/abogen"&gt;Abogen&lt;/a&gt;. It’s a free, open-source application that converts EPUB, PDF, and TXT files into high-quality audiobooks or voiceovers for Instagram, YouTube, TikTok, or any project needing natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It runs on your own hardware locally, giving you full privacy and control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;No cloud. No APIs. No nonsense.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thought this community might find it useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: EPUB, PDF, TXT&lt;/li&gt; &lt;li&gt;Output: MP3, FLAC, WAV, OPUS, M4B (with chapters)&lt;/li&gt; &lt;li&gt;Subtitle generation (SRT, ASS) - sentence- or word-level&lt;/li&gt; &lt;li&gt;Multilingual voice support (English, Spanish, French, Japanese, etc.)&lt;/li&gt; &lt;li&gt;Drag-and-drop interface - no command line required&lt;/li&gt; &lt;li&gt;Fast processing (~3.5 minutes of audio in ~11 seconds on RTX 2060 mobile)&lt;/li&gt; &lt;li&gt;Fully offline - runs on your own hardware (Windows, Linux and Mac)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I made it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most tools I found were either online-only, paywalled, or too complex to use. I wanted something that respected privacy, gave full control over the output without relying on cloud TTS services, API keys, or subscription models. So I built Abogen to be simple, fast, and completely self-contained, something I’d actually want to use myself.&lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/denizsafak/abogen"&gt;https://github.com/denizsafak/abogen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo video: &lt;a href="https://youtu.be/C9sMv8yFkps"&gt;https://youtu.be/C9sMv8yFkps&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you have any questions, suggestions, or bug reports are always welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnzsfk"&gt; /u/dnzsfk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgpjczuspjdf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ruo5/abogen_generate_audiobooks_with_synced_subtitles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ruo5/abogen_generate_audiobooks_with_synced_subtitles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T03:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2ukka</id>
    <title>UIGEN-X-8B, Hybrid Reasoning model built for direct and efficient frontend UI generation, trained on 116 tech stacks including Visual Styles</title>
    <updated>2025-07-18T06:03:29+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ukka/uigenx8b_hybrid_reasoning_model_built_for_direct/"&gt; &lt;img alt="UIGEN-X-8B, Hybrid Reasoning model built for direct and efficient frontend UI generation, trained on 116 tech stacks including Visual Styles" src="https://b.thumbs.redditmedia.com/NlwRL-m7Nhhw8rDYVqXffaIxUdV75LKvkZRbdv5xZFU.jpg" title="UIGEN-X-8B, Hybrid Reasoning model built for direct and efficient frontend UI generation, trained on 116 tech stacks including Visual Styles" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released: &lt;strong&gt;UIGEN-X-8B&lt;/strong&gt;, a hybrid reasoning UI generation model built on Qwen3-8B. This model plans, architects, and implements complete UI systems across tons of frameworks/libraries and 7 platforms, from React, React Native, HTML, Vanilla JS, Vue, Angular, and Svelte to Flutter, Tauri, and Electron. It supports modern design systems like Glassmorphism, Neumorphism, Cyberpunk, and Swiss Design, and handles technologies like Tailwind CSS, shadcn/ui, Redux, Framer Motion, and more. The model is capable of tool calling (e.g. Unsplash image fetching, content generation), step-by-step reasoning, and producing visually styled interfaces. Try it out here: &lt;a href="https://huggingface.co/Tesslate/UIGEN-X-8B"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m2ukka"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ukka/uigenx8b_hybrid_reasoning_model_built_for_direct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2ukka/uigenx8b_hybrid_reasoning_model_built_for_direct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T06:03:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2riey</id>
    <title>Seed-X by Bytedance- LLM for multilingual translation</title>
    <updated>2025-07-18T03:14:34+00:00</updated>
    <author>
      <name>/u/Maleficent_Tone4510</name>
      <uri>https://old.reddit.com/user/Maleficent_Tone4510</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2riey/seedx_by_bytedance_llm_for_multilingual/"&gt; &lt;img alt="Seed-X by Bytedance- LLM for multilingual translation" src="https://external-preview.redd.it/rgKi-znfc6SfcJB1qo2576OWO7WXYop_Pppz6dmuZ9Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e19e17167d1422b47f4e737e0b4d946caaed8a6e" title="Seed-X by Bytedance- LLM for multilingual translation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;supported language&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Languages&lt;/th&gt; &lt;th align="left"&gt;Abbr.&lt;/th&gt; &lt;th align="left"&gt;Languages&lt;/th&gt; &lt;th align="left"&gt;Abbr.&lt;/th&gt; &lt;th align="left"&gt;Languages&lt;/th&gt; &lt;th align="left"&gt;Abbr.&lt;/th&gt; &lt;th align="left"&gt;Languages&lt;/th&gt; &lt;th align="left"&gt;Abbr.&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Arabic&lt;/td&gt; &lt;td align="left"&gt;ar&lt;/td&gt; &lt;td align="left"&gt;French&lt;/td&gt; &lt;td align="left"&gt;fr&lt;/td&gt; &lt;td align="left"&gt;Malay&lt;/td&gt; &lt;td align="left"&gt;ms&lt;/td&gt; &lt;td align="left"&gt;Russian&lt;/td&gt; &lt;td align="left"&gt;ru&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Czech&lt;/td&gt; &lt;td align="left"&gt;cs&lt;/td&gt; &lt;td align="left"&gt;Croatian&lt;/td&gt; &lt;td align="left"&gt;hr&lt;/td&gt; &lt;td align="left"&gt;Norwegian Bokmal&lt;/td&gt; &lt;td align="left"&gt;nb&lt;/td&gt; &lt;td align="left"&gt;Swedish&lt;/td&gt; &lt;td align="left"&gt;sv&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Danish&lt;/td&gt; &lt;td align="left"&gt;da&lt;/td&gt; &lt;td align="left"&gt;Hungarian&lt;/td&gt; &lt;td align="left"&gt;hu&lt;/td&gt; &lt;td align="left"&gt;Dutch&lt;/td&gt; &lt;td align="left"&gt;nl&lt;/td&gt; &lt;td align="left"&gt;Thai&lt;/td&gt; &lt;td align="left"&gt;th&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;German&lt;/td&gt; &lt;td align="left"&gt;de&lt;/td&gt; &lt;td align="left"&gt;Indonesian&lt;/td&gt; &lt;td align="left"&gt;id&lt;/td&gt; &lt;td align="left"&gt;Norwegian&lt;/td&gt; &lt;td align="left"&gt;no&lt;/td&gt; &lt;td align="left"&gt;Turkish&lt;/td&gt; &lt;td align="left"&gt;tr&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;English&lt;/td&gt; &lt;td align="left"&gt;en&lt;/td&gt; &lt;td align="left"&gt;Italian&lt;/td&gt; &lt;td align="left"&gt;it&lt;/td&gt; &lt;td align="left"&gt;Polish&lt;/td&gt; &lt;td align="left"&gt;pl&lt;/td&gt; &lt;td align="left"&gt;Ukrainian&lt;/td&gt; &lt;td align="left"&gt;uk&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Spanish&lt;/td&gt; &lt;td align="left"&gt;es&lt;/td&gt; &lt;td align="left"&gt;Japanese&lt;/td&gt; &lt;td align="left"&gt;ja&lt;/td&gt; &lt;td align="left"&gt;Portuguese&lt;/td&gt; &lt;td align="left"&gt;pt&lt;/td&gt; &lt;td align="left"&gt;Vietnamese&lt;/td&gt; &lt;td align="left"&gt;vi&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Finnish&lt;/td&gt; &lt;td align="left"&gt;fi&lt;/td&gt; &lt;td align="left"&gt;Korean&lt;/td&gt; &lt;td align="left"&gt;ko&lt;/td&gt; &lt;td align="left"&gt;Romanian&lt;/td&gt; &lt;td align="left"&gt;ro&lt;/td&gt; &lt;td align="left"&gt;Chinese&lt;/td&gt; &lt;td align="left"&gt;zh&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Tone4510"&gt; /u/Maleficent_Tone4510 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ByteDance-Seed/seed-x-6878753f2858bc17afa78543"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2riey/seedx_by_bytedance_llm_for_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2riey/seedx_by_bytedance_llm_for_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T03:14:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2bigh</id>
    <title>Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat</title>
    <updated>2025-07-17T16:04:03+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt; &lt;img alt="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral announces Deep Research, Voice mode, multilingual reasoning and Projects for Le Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New in Le Chat:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Deep Research mode: Lightning fast, structured research reports on even the most complex topics.&lt;/li&gt; &lt;li&gt;Voice mode: Talk to Le Chat instead of typing with our new Voxtral model.&lt;/li&gt; &lt;li&gt;Natively multilingual reasoning: Tap into thoughtful answers, powered by our reasoning model — Magistral.&lt;/li&gt; &lt;li&gt;Projects: Organize your conversations into context-rich folders.&lt;/li&gt; &lt;li&gt;Advanced image editing directly in Le Chat, in partnership with Black Forest Labs.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Not local, but much of their underlying models (like Voxtral and Magistral) are, with permissible licenses. For me that makes it worth supporting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/le-chat-dives-deep"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2bigh/mistral_announces_deep_research_voice_mode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T16:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2nvpn</id>
    <title>Training an LLM only on books from the 1800's - Update</title>
    <updated>2025-07-18T00:18:59+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt; &lt;img alt="Training an LLM only on books from the 1800's - Update" src="https://b.thumbs.redditmedia.com/nsMpO5S0s6t0aJGmgRVTbKS-Fsyr-akDtUyycEROI9U.jpg" title="Training an LLM only on books from the 1800's - Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple days ago I made a post sharing my experiment training an LLM on only 1800's London text. That post got more attention than I expected and some people have been checking it out on GitHub. So I just wanted to share an update on this project. I trained a second version using 500 books, legal documents, journals, etc. I also expanded the time period to 1800-1875 instead of 1800-1850. This model is now able to produce semi-coherent sentences with almost no modern references. It's no where near an LLM right now, more like a sentence generator but I'm having a lot of fun doing this and gonna keep scaling up. Many people have been giving me good feedback/advice so thank you ! I'm a bit busy right now but once I find the time I will push everything to GitHub.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxh4l1irzidf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6ba605bf41a988010fd1245efe5f8dd895c4e1"&gt;Output and Hallucinations, Prompt: \&amp;quot;In the autumn of 1847,\&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T00:18:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2tjjc</id>
    <title>Lucy: A Mobile-Capable 1.7B Reasoning Model That Rivals Jan-Nano</title>
    <updated>2025-07-18T05:02:56+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2tjjc/lucy_a_mobilecapable_17b_reasoning_model_that/"&gt; &lt;img alt="Lucy: A Mobile-Capable 1.7B Reasoning Model That Rivals Jan-Nano" src="https://external-preview.redd.it/NW11ZTU4ZGRla2RmMRgI3SJPXdfuQ9Uf_Cd9X7MtqdJcOeQzkrhllhdwrzrv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ca3bf67bc56f662c5f0f8bf3bd8c15f3e4df54d" title="Lucy: A Mobile-Capable 1.7B Reasoning Model That Rivals Jan-Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, it's Alan from Menlo Research.&lt;/p&gt; &lt;p&gt;Since Jan-Nano, we've been curious about how far you can push the search capabilities of a small model. So, we decided to build a toy model named &lt;strong&gt;Lucy&lt;/strong&gt;-&lt;strong&gt;a compact but capable 1.7B model focused on search and lightweight browsing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What this model is good at:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong agentic search via MCP-enabled tools (e.g., Serper with Google Search)&lt;/li&gt; &lt;li&gt;Basic browsing capabilities through Crawl4AI (we’ll release the MCP server used in the demo)&lt;/li&gt; &lt;li&gt;Lightweight enough to run on CPU or mobile devices with decent speed, based on Qwen3-1.7B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How did we achieve this?&lt;/strong&gt;&lt;br /&gt; A paper is coming soon, but here are a few highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We heavily optimized the reward function, making it smooth across multiple categories instead of using rigid or binary rewards (like traditional &lt;code&gt;if-else&lt;/code&gt; logic)&lt;/li&gt; &lt;li&gt;We introduced a new concept called &lt;em&gt;machine-generated task vectors&lt;/em&gt;, which allows us to optimize the contents inside &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags. These serve as dynamic task vector generators, effectively fine-tuning the model's thinking process using RLVR to be more focused rather than relying on generic reasoning&lt;/li&gt; &lt;li&gt;No supervised fine-tuning (SFT) was involved, everything was done through RLVR (which is very good at keeping model degradation at bay)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We originally aimed to reach a score of 80 on SimpleQA, but during evaluation we hit a kind of “common sense” ceiling typical for 1.7B models. Even with test-time compute optimizations, we landed at 78.&lt;/p&gt; &lt;p&gt;This release purpose is only to help us sharpen our optimization technique for task vectors, we will follow up with future models that will be using this technique so we decided to release this as a experiment/ research. We are glad if you try it and like it still !!!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use-case??&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Imagine a workflow where you can talk to your phone, ask it to research something, and it seamlessly &lt;strong&gt;offloads tasks to your desktop at home browsing the web or accessing personal data.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the demo, the model is hosted on vLLM and integrated into the Jan app for demonstration purposes, but you're free to run it yourself. It connects to a Google Search API and a remote browser hosted on a desktop using Crawl4AI.&lt;/p&gt; &lt;h1&gt;Links to models&lt;/h1&gt; &lt;p&gt;There are 2 ways to run the model: with, and without YaRN. The repo with YaRN configuration can have pretty long context window (128k) and the normal repo can do 40k. Both having the same weight.If you have issues running or configuring YaRN I highly recommend use the Lucy vs Lucy-128k&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lucy:&lt;/strong&gt; &lt;a href="https://huggingface.co/Menlo/Lucy"&gt;&lt;strong&gt;https://huggingface.co/Menlo/Lucy&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Lucy-128k:&lt;/strong&gt; &lt;a href="https://huggingface.co/Menlo/Lucy-128k"&gt;&lt;strong&gt;https://huggingface.co/Menlo/Lucy-128k&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper (coming soon will be updated in collection):&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/Menlo/lucy-6879d21ab9c82dd410b231ca"&gt;&lt;strong&gt;https://huggingface.co/collections/Menlo/lucy-6879d21ab9c82dd410b231ca&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; - Lucy: edgerunning agentic web search on mobile with machine generated task vectors.&lt;/p&gt; &lt;h1&gt;Benchmark result&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;OpenAI o1: 42.6&lt;/li&gt; &lt;li&gt;Grok 3: 44.6&lt;/li&gt; &lt;li&gt;03: 49.4&lt;/li&gt; &lt;li&gt;Claude-3.7-Sonnet: 50.0&lt;/li&gt; &lt;li&gt;Gemini-2.5 pro: 52.9&lt;/li&gt; &lt;li&gt;ChatGPT-4.5: 62.5&lt;/li&gt; &lt;li&gt;deepseek-671B-with-MCP: 78.2 (we benchmark using openrouter)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;lucy-with-MCP: 78.3&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;jan-nano-with-MCP: 80.7&lt;/li&gt; &lt;li&gt;jan-nano-128k-with-MCP: 83.2&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Acknowledgement&lt;/h1&gt; &lt;p&gt;- As usual this experiment is not possible without the &lt;strong&gt;amazing Qwen contribution to open source ai community&lt;/strong&gt;. We want to give a big shoutout to Qwen team and their relentless work in pushing boundary of open research/ai. The model was RL-ed on Qwen3-1.7B base weight.&lt;/p&gt; &lt;p&gt;-----&lt;br /&gt; Note: sorry for the music in all the demos, i'm just a fan of Navjaxx, Narvent, VØJ,..... 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jsuhtdbbekdf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2tjjc/lucy_a_mobilecapable_17b_reasoning_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2tjjc/lucy_a_mobilecapable_17b_reasoning_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-18T05:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m2gp16</id>
    <title>Just a reminder that today OpenAI was going to release a SOTA open source model… until Kimi dropped.</title>
    <updated>2025-07-17T19:22:01+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nothing further, just posting this for the lulz. Kimi is amazing. Who even needs OpenAI at this point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m2gp16/just_a_reminder_that_today_openai_was_going_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-17T19:22:01+00:00</published>
  </entry>
</feed>
