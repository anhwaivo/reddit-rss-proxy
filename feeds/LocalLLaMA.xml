<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-01T09:07:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jog7rs</id>
    <title>GMK EVO-X2 mini PC with Ryzen AI Max+ 395 Strix Halo launches April 7</title>
    <updated>2025-03-31T22:32:26+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jog7rs/gmk_evox2_mini_pc_with_ryzen_ai_max_395_strix/"&gt; &lt;img alt="GMK EVO-X2 mini PC with Ryzen AI Max+ 395 Strix Halo launches April 7" src="https://external-preview.redd.it/SX4meDM_CKwjAXifdG_zeTSXufTI8ACdJxiJ-dmx-xI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9d48cd515d9dd2f764a75624a5ebbb3672104d4" title="GMK EVO-X2 mini PC with Ryzen AI Max+ 395 Strix Halo launches April 7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/gmk-introduces-evo-x2-mini-pc-with-ryzen-ai-max-395-strix-halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jog7rs/gmk_evox2_mini_pc_with_ryzen_ai_max_395_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jog7rs/gmk_evox2_mini_pc_with_ryzen_ai_max_395_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T22:32:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnvqsg</id>
    <title>why is no one talking about Qwen 2.5 omni?</title>
    <updated>2025-03-31T05:06:57+00:00</updated>
    <author>
      <name>/u/brocolongo</name>
      <uri>https://old.reddit.com/user/brocolongo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems crazy to me the first multimodal with voice, image, and text gen open sourced and no one is talking about it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brocolongo"&gt; /u/brocolongo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnvqsg/why_is_no_one_talking_about_qwen_25_omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T05:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo9q6q</id>
    <title>Assessing facial recognition performance of vision LLMs</title>
    <updated>2025-03-31T18:05:21+00:00</updated>
    <author>
      <name>/u/jordo45</name>
      <uri>https://old.reddit.com/user/jordo45</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo9q6q/assessing_facial_recognition_performance_of/"&gt; &lt;img alt="Assessing facial recognition performance of vision LLMs" src="https://b.thumbs.redditmedia.com/5azfRvOkHc3buvqQo_AQ1DG_OLzutTmTi4piyfzEkhQ.jpg" title="Assessing facial recognition performance of vision LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought it'd be interesting to assess face recognition performance of vision LLMs. Even though it wouldn't be wise to use a vision LLM to do face rec when there are dedicated models, I'll note that:&lt;/p&gt; &lt;p&gt;- it gives us a way to measure the gap between dedicated vision models and LLM approaches, to assess how close we are to 'vision is solved'.&lt;/p&gt; &lt;p&gt;- lots of jurisdictions have regulations around face rec system, so it is important to know if vision LLMs are becoming capable face rec systems.&lt;/p&gt; &lt;p&gt;I measured performance of multiple models on multiple datasets (AgeDB30, LFW, CFP). As a baseline, I used arface-resnet-100. Note that as there are 24,000 pair of images, I did not benchmark the more costly commercial APIs:&lt;/p&gt; &lt;p&gt;Results&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a3je6r1ze2se1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4a21607e5a8790a05089996fd659a116c36fbe"&gt;https://preview.redd.it/a3je6r1ze2se1.png?width=5363&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4a21607e5a8790a05089996fd659a116c36fbe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Samples&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3nh02tvze2se1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09edd4de64b313d4716ef7ff68d5ae7e9f1c0bf"&gt;https://preview.redd.it/3nh02tvze2se1.png?width=1275&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e09edd4de64b313d4716ef7ff68d5ae7e9f1c0bf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discussion&lt;/p&gt; &lt;p&gt;- Most vision LLMs are very far from even a several year old resnet-100.&lt;/p&gt; &lt;p&gt;- All models perform better than random chance.&lt;/p&gt; &lt;p&gt;- The google models (Gemini, Gemma) perform best.&lt;/p&gt; &lt;p&gt;Repo &lt;a href="https://github.com/yhenon/llm-face-vision"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jordo45"&gt; /u/jordo45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo9q6q/assessing_facial_recognition_performance_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo9q6q/assessing_facial_recognition_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo9q6q/assessing_facial_recognition_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T18:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo0wf9</id>
    <title>RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)</title>
    <updated>2025-03-31T11:25:49+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt; &lt;img alt="RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)" src="https://b.thumbs.redditmedia.com/nA7D2jhOOKLXACZ-SNAxVn1wByf8iBUa2MWnQMWgGaY.jpg" title="RTX PRO 6000 Blackwell 96GB shows up at 7623€ before VAT (8230 USD)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cgpfkci6e0se1.jpg?width=868&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8fbbd40cc6fe111c3913c2bb4f76d623a6ae9a02"&gt;https://www.proshop.fi/Naeytoenohjaimet/NVIDIA-RTX-PRO-6000-Blackwell-Bulk-96GB-GDDR7-RAM-Naeytoenohjaimet/3358883&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Proshop is a decently sized retailer and Nvidia's partner for selling Founders Edition cards in several European countries so the listing is definitely legit.&lt;/p&gt; &lt;p&gt;NVIDIA RTX PRO 5000 Blackwell 48GB listed at ~4000€ + some more listings for those curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.proshop.fi/?s=rtx+pro+blackwell&amp;amp;o=2304"&gt;https://www.proshop.fi/?s=rtx+pro+blackwell&amp;amp;o=2304&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo0wf9/rtx_pro_6000_blackwell_96gb_shows_up_at_7623/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T11:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1joheou</id>
    <title>Best free alternative to NotebookLM for RAG?</title>
    <updated>2025-03-31T23:26:19+00:00</updated>
    <author>
      <name>/u/vlodia</name>
      <uri>https://old.reddit.com/user/vlodia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NotebookLM works well for me for RAG-ing document files for free. It's been 6 months since I was using it, asking here if you have something better as a free alternative?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vlodia"&gt; /u/vlodia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joheou/best_free_alternative_to_notebooklm_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joheou/best_free_alternative_to_notebooklm_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joheou/best_free_alternative_to_notebooklm_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T23:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1joadxp</id>
    <title>Exaone Deep 2.4B Q8_0</title>
    <updated>2025-03-31T18:31:50+00:00</updated>
    <author>
      <name>/u/giant3</name>
      <uri>https://old.reddit.com/user/giant3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B-GGUF"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LG's 2.4B model is surprisingly usable. The license might be very restrictive, but for personal use it doesn't matter.&lt;/p&gt; &lt;p&gt;I get 40 tk/s on a measly RX 7600 while DeepSeek R1 distilled llama 8B is only 3 tk/s. &lt;/p&gt; &lt;p&gt;Give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/giant3"&gt; /u/giant3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joadxp/exaone_deep_24b_q8_0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joadxp/exaone_deep_24b_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joadxp/exaone_deep_24b_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T18:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jofuyc</id>
    <title>How good unsloth fine tuned models can actually get</title>
    <updated>2025-03-31T22:17:02+00:00</updated>
    <author>
      <name>/u/CautiousSand</name>
      <uri>https://old.reddit.com/user/CautiousSand</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been reading a bit about Unsloth fine-tuning and wondering how good these models can actually get.&lt;/p&gt; &lt;p&gt;I know a lot depends on the dataset, but before I go too deep into yet another rabbit hole, I want to get a sense of what’s realistically achievable—especially when it comes to fine-tuning a model to match my writing style. Is it possible to get decent results without massive datasets and expensive hardware?&lt;/p&gt; &lt;p&gt;I’ve tried searching for examples of fine-tuned Unsloth models, but all I find are tutorials—nothing I can actually try to see what kind of results are possible.&lt;/p&gt; &lt;p&gt;For those who have worked with Unsloth fine-tuning, what’s been your experience? I’m not chasing a specific use case, just experimenting, but I don’t want to sink a ton of time into this only to find out you really need a 32B+ model and a very specific setup for it to be worthwhile.&lt;/p&gt; &lt;p&gt;How big of a dataset and model would I actually need to get reasonable results? Would love to hear from anyone who’s tried.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CautiousSand"&gt; /u/CautiousSand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jofuyc/how_good_unsloth_fine_tuned_models_can_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jofuyc/how_good_unsloth_fine_tuned_models_can_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jofuyc/how_good_unsloth_fine_tuned_models_can_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T22:17:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1joolgd</id>
    <title>v0.7.3 Update: Dive, An Open Source MCP Agent Desktop</title>
    <updated>2025-04-01T05:57:19+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"&gt; &lt;img alt="v0.7.3 Update: Dive, An Open Source MCP Agent Desktop" src="https://external-preview.redd.it/dHNhenhkNHl4NXNlMTttH9zdS9ChVet3GQR3TBIWR7IxCLqFWnntBfHnnCr8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f5f2c27599200502d7f8d304adbebf35fbbe2" title="v0.7.3 Update: Dive, An Open Source MCP Agent Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is currently the easiest way to install MCP Server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r9uvwa4yx5se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joolgd/v073_update_dive_an_open_source_mcp_agent_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T05:57:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo5v3f</id>
    <title>Latent Verification Mechanism for ~10% Absolute Factual Accuracy Improvement</title>
    <updated>2025-03-31T15:26:51+00:00</updated>
    <author>
      <name>/u/Big-Helicopter-9356</name>
      <uri>https://old.reddit.com/user/Big-Helicopter-9356</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The TransMLA paper blew my mind when it came out.&lt;/p&gt; &lt;p&gt;Since then I've been playing around with manipulating pre-trained LLMs. I'm nowhere near as smart as the people behind transMLA or probably any of you, but for a self-taught guy that's been dabbling for several years now this was a really fun project.&lt;/p&gt; &lt;p&gt;here's the repo to the implementation for my architectural modification. It adds self-verification capabilities to LLMs (currently implemented in Qwen2.5 7B: &lt;a href="https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent%5C_Verification"&gt;https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent\_Verification&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;It works by adding verification adapters (lightweight modules) every few layers.&lt;/p&gt; &lt;p&gt;These modules analyze the hidden states passing through its layer, computes a confidence score indicating how reliable the states are, applies weighted correction based on the inverse of that confidence score, and returns the corrected state back to the model's processing flow.&lt;/p&gt; &lt;p&gt;Then the cross-layer verifier compares representation across different layers to ensure consistency in the model's internal reasoning.&lt;/p&gt; &lt;p&gt;It's pretty cool. You can actually see the verification happening in the PCA projection within the `results` directory.&lt;/p&gt; &lt;p&gt;Anyway, hope y'all enjoy this. Looking forward to any feedback or ideas for improvement!&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs"&gt;https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Helicopter-9356"&gt; /u/Big-Helicopter-9356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo5v3f/latent_verification_mechanism_for_10_absolute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo5v3f/latent_verification_mechanism_for_10_absolute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo5v3f/latent_verification_mechanism_for_10_absolute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T15:26:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jocz51</id>
    <title>OpenHands-LM 32B - 37.2% verified resolve rate on SWE-Bench Verified</title>
    <updated>2025-03-31T20:17:06+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All Hands (Creator of OpenHands) released a 32B model that outperforms much larger models when using their software.&lt;br /&gt; The model is research preview so YMMV , but seems quite solid.&lt;/p&gt; &lt;p&gt;Qwen 2.5 0.5B and 1.5B seems to work nicely as draft models with this model (I still need to test in OpenHands but worked nice with the model on lmstudio). &lt;/p&gt; &lt;p&gt;Link to the model: &lt;a href="https://huggingface.co/all-hands/openhands-lm-32b-v0.1"&gt;https://huggingface.co/all-hands/openhands-lm-32b-v0.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jocz51/openhandslm_32b_372_verified_resolve_rate_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jocz51/openhandslm_32b_372_verified_resolve_rate_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T20:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo81g2</id>
    <title>Best setup for $10k USD</title>
    <updated>2025-03-31T16:57:47+00:00</updated>
    <author>
      <name>/u/LedByReason</name>
      <uri>https://old.reddit.com/user/LedByReason</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are the best options if my goal is to be able to run 70B models at &amp;gt;10 tokens/s? Mac Studio? Wait for DGX Spark? Multiple 3090s? Something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LedByReason"&gt; /u/LedByReason &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo81g2/best_setup_for_10k_usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo81g2/best_setup_for_10k_usd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo81g2/best_setup_for_10k_usd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T16:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnzq51</id>
    <title>PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s</title>
    <updated>2025-03-31T10:06:50+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"&gt; &lt;img alt="PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s" src="https://external-preview.redd.it/mjqF-7sslVFdngpzQezzzTUL6oX500j7ElXyhrnVXck.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65cdc44f4fd50dd2535768214841267fd3ed7cf8" title="PC Build: Run Deepseek-V3-0324:671b-Q8 Locally 6-8 tok/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watch as I build a monster PC to run Deepseek-V3-0324:671b-Q8 locally at 6-8 tokens per second. I'm using dual EPYC 9355 processors and 768Gb of 5600mhz RDIMMs 24x32Gb on a MZ73-LM0 Gigabyte motherboard. I flash the BIOS, install Ubuntu 24.04.2 LTS, ollama, Open WebUI, and more, step by step!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/v4810MVGhog"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzq51/pc_build_run_deepseekv30324671bq8_locally_68_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T10:06:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jnzdvp</id>
    <title>Qwen3 support merged into transformers</title>
    <updated>2025-03-31T09:42:25+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/36878"&gt;https://github.com/huggingface/transformers/pull/36878&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jnzdvp/qwen3_support_merged_into_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T09:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jodbgl</id>
    <title>Orpheus TTS Local WebUI: Your Personal Text-to-Speech Studio, Gradio UI, Supports Emotive tags.</title>
    <updated>2025-03-31T20:31:17+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jodbgl/orpheus_tts_local_webui_your_personal/"&gt; &lt;img alt="Orpheus TTS Local WebUI: Your Personal Text-to-Speech Studio, Gradio UI, Supports Emotive tags." src="https://a.thumbs.redditmedia.com/lo63Kg60TJFczxkVu65z76M9rpVvyqlHxvJdIRd5Uw4.jpg" title="Orpheus TTS Local WebUI: Your Personal Text-to-Speech Studio, Gradio UI, Supports Emotive tags." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;🎧 High-quality Text-to-Speech using the Orpheus TTS model&lt;/li&gt; &lt;li&gt;💻 Completely standalone - no external services or API keys needed&lt;/li&gt; &lt;li&gt;🔊 Multiple voice options (tara, leah, jess, leo, dan, mia, zac, zoe)&lt;/li&gt; &lt;li&gt;💾 Save audio to WAV files&lt;/li&gt; &lt;li&gt;🎨 Modern Gradio web interface&lt;/li&gt; &lt;li&gt;🔧 Adjustable generation parameters (temperature, top_p, repetition penalty)&lt;/li&gt; &lt;li&gt;Supports emotive tags &lt;code&gt;&amp;lt;laugh&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;chuckle&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;sigh&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;cough&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;sniffle&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;groan&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;yawn&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;gasp&amp;gt;&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/akashjss/orpheus-tts-local-webui"&gt;https://github.com/akashjss/orpheus-tts-local-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Audio Sample &lt;a href="https://voipnuggets.wordpress.com/wp-content/uploads/2025/03/tmpxxe176lm-1.wav"&gt;https://voipnuggets.wordpress.com/wp-content/uploads/2025/03/tmpxxe176lm-1.wav&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ScreenShot:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3j46tfo8q5se1.jpg?width=3092&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f38105febc58e63294d60d0400cefe6cf23b6a5"&gt;https://preview.redd.it/3j46tfo8q5se1.jpg?width=3092&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4f38105febc58e63294d60d0400cefe6cf23b6a5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jodbgl/orpheus_tts_local_webui_your_personal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jodbgl/orpheus_tts_local_webui_your_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jodbgl/orpheus_tts_local_webui_your_personal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T20:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo78b8</id>
    <title>LM arena updated - now contains Deepseek v3.1</title>
    <updated>2025-03-31T16:23:53+00:00</updated>
    <author>
      <name>/u/Economy_Apple_4617</name>
      <uri>https://old.reddit.com/user/Economy_Apple_4617</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;scored at 1370 - even better than R1&lt;/p&gt; &lt;p&gt;I also saw following interesting models on LMarena:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Nebula - seems to turn out as gemini 2.5&lt;/li&gt; &lt;li&gt;Phantom - disappeared few days ago&lt;/li&gt; &lt;li&gt;Chatbot-anonymous - does anyone have insights?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Apple_4617"&gt; /u/Economy_Apple_4617 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo78b8/lm_arena_updated_now_contains_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo78b8/lm_arena_updated_now_contains_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo78b8/lm_arena_updated_now_contains_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T16:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jo88lg</id>
    <title>Part of Orpheus Team here - Ama + educational content</title>
    <updated>2025-03-31T17:05:34+00:00</updated>
    <author>
      <name>/u/EveryDayStonks</name>
      <uri>https://old.reddit.com/user/EveryDayStonks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I’m part of the team behind Orpheus. It’s been really exciting to see everyone’s support for Orpheus and excited to continue launching more open speech models. I wanted to clear up some of the questions about the design and data choices, and potential misconceptions about Orpheus.&lt;/p&gt; &lt;h2&gt;Background on the project&lt;/h2&gt; &lt;p&gt;We’re a pretty small team building end-to-end multimodal human motion and speech, and our mission is to create realistic realtime “humans”. We decided to we’d start working on, and open source, a TTS about 4 weeks ago, more of as an exploration into how natural and usable we could make LLM driven speech sound, without worrying about the more complex aspects of end-to-end systems. We launched the results of our experiments just over a week and a half ago in the form or a pre-trained model and a fine-tuned model as Orpheus 0.1.&lt;/p&gt; &lt;h2&gt;Why even use an LLM as the backbone?&lt;/h2&gt; &lt;p&gt;Since LLMs have already seen trillions of text tokens, they have a deep understanding of the emotion and nuance conveyed in text. This ability transfers well to speech generation. For example, if the models is trained the text and speech for “I failed my exam but I get to resit next year”, it learns sad sentences with an upbeat finish should be said in a certain way. When it’s asked to generate “I sprained my leg, but it will get better in a few weeks” it knows, thanks to its semantic understanding, that this is also a sad sentence with an upbeat finish, and it already has a good sense of how “sad sentences with upbeat finishes” roughly sound. &lt;/p&gt; &lt;p&gt;In short, using LLMs lead to more natural generations. To maintain the model’s text abilities, we also, for the first 50% of “speech pretraining”, made every other batch being a purely text based batch. &lt;/p&gt; &lt;h2&gt;Datasets&lt;/h2&gt; &lt;h3&gt;Pretraining&lt;/h3&gt; &lt;p&gt;We used a combination of publicly available and permissively licensed text and speech datasets, available on Hugging Face. We minimally cleaned the data, like removing silence, or incoherent examples. We created dataset of tokenised text-speech pairs for the speech using the same preprocessing script, provided in the GitHub for speech. I also share the text preprocessing framework in a Github Issue for anyone interested. We then packed sequences together into 8192 token length sequences. We trained for 100k hours of speech, the first 50k hours also had interleaved batches of text sequences based on QA answer datasets. This nets around 4 million steps on speech which takes around 1500 H100 hours.&lt;/p&gt; &lt;h3&gt;Finetuning&lt;/h3&gt; &lt;p&gt;We got 8 professional voice actors to record 300 lines each. These were generated using an open source LLM prompted to include tags (like &amp;lt;laugh&amp;gt;). We used full parameter fine-tuning. Spoken lines were on average 10 seconds long with a standard deviation of 6 seconds. &lt;/p&gt; &lt;h2&gt;With regards to misconceptions about training:&lt;/h2&gt; &lt;p&gt;1.⁠ ⁠Should I train over multiple epochs: all our training was done over 1 epoch - Our fine-tuned models become slightly more unstable over multiple epochs, due to overfitting. We never tested pre-training over multiple epochs but it would make more sense to scale to a bigger dataset rather scale number of epochs, as pre-training level speech data isn’t lacking or hard to obtain.&lt;/p&gt; &lt;p&gt;2.⁠ ⁠Benefits of increasing pre-training data: I predict better stability over very long sequences as the biggest downstream improvement - but we’ll find out soon :)&lt;/p&gt; &lt;h2&gt;Model Architecture Decisions&lt;/h2&gt; &lt;p&gt;Audio is typically split up into frames (like 25-100ms chunks). Each chunk is represented by a set of tokens. Often these tokens have different levels of importance. Orpheus uses a tokeniser which has 7 tokens per frame and generates all 7 auto-regressively using the LLM. Other models like Moshi or Sesame use the LLM to predict the most important token per frame and offload the other tokens to a separate smaller model. &lt;/p&gt; &lt;h3&gt;“Offloading” could be a good idea because&lt;/h3&gt; &lt;p&gt;1.⁠ ⁠You can generate tokens faster as you use a smaller model to generate most of the tokens quickly. &lt;/p&gt; &lt;p&gt;2.⁠ ⁠You train the model on fewer speech tokens so it becomes less worse (forgets less) at text reasoning.&lt;/p&gt; &lt;h3&gt;Our thoughts are:&lt;/h3&gt; &lt;p&gt;1.⁠ ⁠For speed/realtime streaming Orpheus 3b requires 83 tokens/second which is actually very easy to get on A100/H100+ models. Not to mention Orpheus quantises well, and we are going to releasing smaller faster versions … that said I apologise to everyone current trying to run Orpheus 4-bit on RTX 4090s :)&lt;/p&gt; &lt;p&gt;2.⁠ ⁠You only need to care about maintaining really good text based reasoning for end-to-end speech models, which really suffer from LLMs catastrophically forgetting text. That said if you were trying to make end-to-end speech, in my opinion, conceptually Qwen Omni is a far superior architecture to Sesame/Moshi as it doesn’t touch the LLM at all but still has the same potential for emotional upside as Orpheus or Sesame with a bit of work.&lt;/p&gt; &lt;p&gt;3.⁠ ⁠From an architectural standpoint, our general philosophy is if it can be simple, it should be simple - and having a Llama model spit out tokens without any other modules is the simplest approach we could think of. In general, I believe machine learning is moving towards simple scalable architectures that benefit from more and higher data and over engineered architectures only offer local maxima.&lt;/p&gt; &lt;h2&gt;Why did we choose SNAC (more technical section)&lt;/h2&gt; &lt;p&gt;When training multimodal LLMs (this goes for images/motion/video/speech) there are 2 important things that go into picking a good tokeniser. First is reconstruction - if your tokeniser can’t represent the underlying modality well (i.e. it can only be de-tokenised into deep voices / or pictures with oceans) it isn’t useful. This incentivises the tokeniser architect to use as many tokens as possible with as high a codebook size, so you can capture as rich nuanced details as possible. &lt;/p&gt; &lt;p&gt;Unfortunately there is a competing interest (as there always is). This is entropy of the token distribution. LLMs are worse at learning the token statistics from tokeniser distributions with higher entropy. Without getting too technical, a good heuristic for entropy is bitrate. Bitrate = codebook size * tokens/second. For SNAC this is 980 bips, for the simplest version of Mimi this is 550 bips (which is better) but suffers from inferior reconstruction. The standard version of Mimi has a bitrate of 1100 bips which is worse than SNAC. Thus, we went with SNAC for this version of Orpheus but we may switch this in the future as too much thought hasn’t been put into this and we wanted to innovate on other parts of the approach.&lt;/p&gt; &lt;h2&gt;What’s Next&lt;/h2&gt; &lt;p&gt;We have decided to prioritise multilingual as this seems to be the most sought after feature. We will then focus on releasing the pretrained and finetunes for the smaller parameter size models. After that we have a few different ideas for what could be a good second open source speech release, and we are always open to suggestions. That said, this is our current release plan, all of which is subject to being rearranged/modified, based on what seems most important.&lt;/p&gt; &lt;p&gt;Hope this was useful/interesting, happy to go into more detail in the comments/answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EveryDayStonks"&gt; /u/EveryDayStonks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo88lg/part_of_orpheus_team_here_ama_educational_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jo88lg/part_of_orpheus_team_here_ama_educational_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jo88lg/part_of_orpheus_team_here_ama_educational_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T17:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jonibh</id>
    <title>OpenWebUI Adopt OpenAPI and offer an MCP bridge</title>
    <updated>2025-04-01T04:45:39+00:00</updated>
    <author>
      <name>/u/coding_workflow</name>
      <uri>https://old.reddit.com/user/coding_workflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open Web Ui 0.6 is adoption OpenAPI instead of MCP but offer a bridge.&lt;br /&gt; Release notes: &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;br /&gt; MCO Bridge: &lt;a href="https://github.com/open-webui/mcpo"&gt;https://github.com/open-webui/mcpo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding_workflow"&gt; /u/coding_workflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T04:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jodrcx</id>
    <title>Another coding model, Achieves strong performance on software engineering tasks, including 37.2% resolve rate on SWE-Bench Verified.</title>
    <updated>2025-03-31T20:49:12+00:00</updated>
    <author>
      <name>/u/Ornery_Local_6814</name>
      <uri>https://old.reddit.com/user/Ornery_Local_6814</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jodrcx/another_coding_model_achieves_strong_performance/"&gt; &lt;img alt="Another coding model, Achieves strong performance on software engineering tasks, including 37.2% resolve rate on SWE-Bench Verified." src="https://external-preview.redd.it/2wwsANQslIq5kOZIF2w1yy3GroEQnGK-ceJDzy3yNOI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b693b9df3a16a1584b89663d17220b1fe18b36a" title="Another coding model, Achieves strong performance on software engineering tasks, including 37.2% resolve rate on SWE-Bench Verified." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ornery_Local_6814"&gt; /u/Ornery_Local_6814 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/all-hands/openhands-lm-32b-v0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jodrcx/another_coding_model_achieves_strong_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jodrcx/another_coding_model_achieves_strong_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T20:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1joonmc</id>
    <title>Do you think this will catch on? Amazon's nova models are not very good.</title>
    <updated>2025-04-01T06:01:13+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joonmc/do_you_think_this_will_catch_on_amazons_nova/"&gt; &lt;img alt="Do you think this will catch on? Amazon's nova models are not very good." src="https://external-preview.redd.it/rMr2HgzAGr1ezJuokO53hHGlcQXGOhFKnShl9es2BBk.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950c1cf21b2ffe4a1a468b9f618389ff23cec350" title="Do you think this will catch on? Amazon's nova models are not very good." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=JLLapxWmalUhttps://www.youtube.com/watch?v=JLLapxWmalU"&gt;https://www.youtube.com/watch?v=JLLapxWmalUhttps://www.youtube.com/watch?v=JLLapxWmalU&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=JLLapxWmalU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joonmc/do_you_think_this_will_catch_on_amazons_nova/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joonmc/do_you_think_this_will_catch_on_amazons_nova/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T06:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jobe0u</id>
    <title>Benchmark: Dual-GPU boosts speed, despire all common internet wisdom. 2x RTX 5090 &gt; 1x H100, 2x RTX 4070 &gt; 1x RTX 4090 for QwQ-32B-AWQ. And the RTX 6000 Ada is overpriced.</title>
    <updated>2025-03-31T19:12:45+00:00</updated>
    <author>
      <name>/u/fxtentacle</name>
      <uri>https://old.reddit.com/user/fxtentacle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After yesterday's tests, I got the suggestion to test AWQ quants. And all over the internet I had repeatedly heard that dual-GPU setups won't help because they would not increase sequential speed. But the thing is: With vLLM, dual-GPU setups work anyway. I guess nobody told them ;)&lt;/p&gt; &lt;p&gt;In this benchmark set, the Time To First Token was below 0.1s in all cases, so I'm just going to ignore that. This race is all about the Output Tokens Per Second. And let's be honest, especially with a reasoning model like QwQ, those 4000 tokens of internal monologue is what we are waiting for and skipping the wait is all we care about. And, BTW, just like with my last benchmarking set, I am looking purely at 1-user setups here.&lt;/p&gt; &lt;p&gt;To nobody's surprise, the H100 80GB HBM3 again makes for great inference card with 78 OT/s. And the RTX 5090 is a beast with 65 OT/s, although it took me almost a day to get vLLM, flashInfer, and Nccl compiled just right for it to run stable enough to survive a 30 minute benchmark ... Still, the 5090 delivers 83% of a H100 at 10% the price. &lt;/p&gt; &lt;p&gt;Where things get surprising again is that 2x RTX 4070 TI SUPER actually outperform a RTX 4090 with 46 vs 43 OT/s. In line with that, 2x RTX 4080 also do well with 52 OT/s and they reach 80% of a 5090. My old RTX 3090 TI is also still very pleasant to use at 40 OT/s - which is a respectable 61% of the speed a shiny new 5090 would deliver.&lt;/p&gt; &lt;p&gt;The pricey RTX 6000 Ada completely disappoints with 42 OT/s, so it's only marginally faster than the 3090 TI and way behind a dual-4070 setup. &lt;/p&gt; &lt;p&gt;And what's truly cool is to see how well the 5090 can use additional RAM for speeding up the attention kernels. That's why 2x RTX 5090 outperforms even the mighty H100 by a small margin. That's 30,000€ performance for 5,718€.&lt;/p&gt; &lt;p&gt;Here's the new result table: &lt;a href="https://github.com/DeutscheKI/llm-performance-tests#qwq-32b-awq"&gt;https://github.com/DeutscheKI/llm-performance-tests#qwq-32b-awq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fxtentacle"&gt; /u/fxtentacle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jobe0u/benchmark_dualgpu_boosts_speed_despire_all_common/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T19:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jopcyr</id>
    <title>GPT 4o is not actually omni-modal</title>
    <updated>2025-04-01T06:51:45+00:00</updated>
    <author>
      <name>/u/kuzheren</name>
      <uri>https://old.reddit.com/user/kuzheren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://chatgpt.com/share/67eb9fc8-458c-8007-85ad-46be9aa56519"&gt;https://chatgpt.com/share/67eb9fc8-458c-8007-85ad-46be9aa56519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wanted to share this here - I haven’t seen much discussion about it, and I hope it could be helpful to the LocalLLaMA community.&lt;/p&gt; &lt;p&gt;(Also, let’s define &lt;em&gt;omni-modal&lt;/em&gt; as multimodal models that support both understanding and generation across different modalities. This definition might not be perfect, but we need some way to distinguish models with multimodal decoding capabilities from those without)&lt;/p&gt; &lt;p&gt;As we know, the new GPT-4o model is highly context-aware. It can reference both images and previous user conversation. At first glance, it might seem like GPT-4o generates image tokens directly based on the full context, without relying on any external tools. But that’s not exactly how it works.&lt;/p&gt; &lt;p&gt;Image generation still relies on a new version of DALL·E (at least it’s still referred to by that name), and it happens through a function call like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;image_gen.text2im { &amp;quot;prompt&amp;quot;: &amp;quot;A photorealistic owl sitting on a branch at night&amp;quot;, &amp;quot;size&amp;quot;: &amp;quot;1024x1024&amp;quot;, &amp;quot;n&amp;quot;: 1, &amp;quot;referenced_image_ids&amp;quot;: [&amp;quot;file_0000000054d45230be886096390c241a&amp;quot;], // optional &amp;quot;transparent_background&amp;quot;: false // optional } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As we can see, the process still uses an explicit API-style call. GPT writes the prompt and optionally includes image references, allowing the image generator to use much more context than DALL·E 3 ever could.&lt;/p&gt; &lt;p&gt;Compare this to models like open-source OmniGen or Gemini 2.0 Flash - these do &lt;strong&gt;not&lt;/strong&gt; rely on external function calls. Instead, they generate images directly, using both text and image inputs as unified context. That’s why I’d say they’re &lt;em&gt;truly&lt;/em&gt; omni-modal.&lt;/p&gt; &lt;p&gt;One more detail: after the image is generated, GPT only sees a &lt;strong&gt;textual description&lt;/strong&gt; of the result — not the actual image itself (unless it was user-uploaded). This means GPT-4o wasn't retrained to “see” its own generated images.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; GPT-4o doesn’t generate image tokens directly. It calls a separate, more advanced image model (a new DALL·E version) that can handle reference images. The models are still modular, not unified.&lt;/p&gt; &lt;p&gt;Please don't k#ll me for this post. I know it might sound obvious, boring, or lame, but nobody seems to be talking about it, and many people assume the image generator is somehow merged into GPT itself - which is not the case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuzheren"&gt; /u/kuzheren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jopcyr/gpt_4o_is_not_actually_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T06:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jobybk</id>
    <title>OpenAI is open-sourcing a model soon</title>
    <updated>2025-03-31T19:36:01+00:00</updated>
    <author>
      <name>/u/MysteriousPayment536</name>
      <uri>https://old.reddit.com/user/MysteriousPayment536</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is taking feedback for open source model. They will probably release o3-mini based on a poll by Sam Altman in February. &lt;a href="https://x.com/sama/status/1891667332105109653"&gt;https://x.com/sama/status/1891667332105109653&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MysteriousPayment536"&gt; /u/MysteriousPayment536 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/open-model-feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T19:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1joqnp0</id>
    <title>Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)</title>
    <updated>2025-04-01T08:28:37+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt; &lt;img alt="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" src="https://preview.redd.it/lbaxwpako6se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe2ebb66027a7c9112a0c9566eaf397ca2d5a18" title="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to share something that’s blown my mind today. I just came across &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;this paper &lt;/a&gt;evaluating state-of-the-art LLMs (like O3-MINI, Claude 3.7, etc.) on the 2025 USA Mathematical Olympiad (USAMO). And let me tell you—this is &lt;em&gt;wild&lt;/em&gt; .&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;p&gt;These models were tested on &lt;strong&gt;six proof-based math problems&lt;/strong&gt; from the 2025 USAMO. Each problem was scored out of 7 points, with a max total score of 42. Human experts graded their solutions rigorously.&lt;/p&gt; &lt;p&gt;The highest average score achieved by &lt;strong&gt;any model&lt;/strong&gt; ? &lt;strong&gt;Less than 5%.&lt;/strong&gt; Yes, you read that right: &lt;strong&gt;5%.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Even worse, when these models tried grading their own work (e.g., O3-MINI and Claude 3.7), they consistently &lt;strong&gt;overestimated their scores&lt;/strong&gt; , inflating them by up to &lt;strong&gt;20x&lt;/strong&gt; compared to human graders.&lt;/p&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;These models have been trained on &lt;strong&gt;all the math data imaginable&lt;/strong&gt; —IMO problems, USAMO archives, textbooks, papers, etc. They’ve seen it all. Yet, they struggle with tasks requiring deep logical reasoning, creativity, and rigorous proofs.&lt;/p&gt; &lt;p&gt;Here are some key issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logical Failures&lt;/strong&gt; : Models made unjustified leaps in reasoning or labeled critical steps as &amp;quot;trivial.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lack of Creativity&lt;/strong&gt; : Most models stuck to the same flawed strategies repeatedly, failing to explore alternatives.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grading Failures&lt;/strong&gt; : Automated grading by LLMs inflated scores dramatically, showing they can't even evaluate their own work reliably.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given that billions of dollars have been poured into investments on these models with the hope of it can &amp;quot;generalize&amp;quot; and do &amp;quot;crazy lift&amp;quot; in human knowledge, this result is shocking. Given the models here are probably trained on all Olympiad data previous (USAMO, IMO ,... anything)&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;https://arxiv.org/abs/2503.21934v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbaxwpako6se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T08:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jojuf4</id>
    <title>Is everyone ready for all of the totally legit AI tools &amp; models being released tomorrow?</title>
    <updated>2025-04-01T01:25:55+00:00</updated>
    <author>
      <name>/u/C_Coffie</name>
      <uri>https://old.reddit.com/user/C_Coffie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard Llama 4 is finally coming tomorrow!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_Coffie"&gt; /u/C_Coffie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T01:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jogfrz</id>
    <title>Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES</title>
    <updated>2025-03-31T22:42:26+00:00</updated>
    <author>
      <name>/u/jiMalinka</name>
      <uri>https://old.reddit.com/user/jiMalinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt; &lt;img alt="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" src="https://preview.redd.it/q2nifllfs3se1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=734a6613dfcc4ffda59b820ed615cb2ac184b109" title="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sentient-agi/OpenDeepSearch"&gt;https://github.com/sentient-agi/OpenDeepSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Pretty simple to plug-and-play – nice combo of techniques (react / codeact / dynamic few-shot) integrated with search / calculator tools. I guess that’s all you need to beat SOTA billion dollar search companies :) Probably would be super interesting / useful to use with multi-agent workflows too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiMalinka"&gt; /u/jiMalinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2nifllfs3se1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T22:42:26+00:00</published>
  </entry>
</feed>
