<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-01T08:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j0sxsv</id>
    <title>What do the deepseek papers mean for local inference?</title>
    <updated>2025-03-01T06:11:13+00:00</updated>
    <author>
      <name>/u/oldschooldaw</name>
      <uri>https://old.reddit.com/user/oldschooldaw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Upfront, I can‚Äôt understand the papers. I don‚Äôt know enough to read them. But the snippets I‚Äôm seeing about them on X suggest to me a lot of the improvements are for VERY VERY VERY large players, not those with a single 4090. &lt;/p&gt; &lt;p&gt;Is there any developments in the drops I‚Äôve missed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oldschooldaw"&gt; /u/oldschooldaw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0sxsv/what_do_the_deepseek_papers_mean_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0mi30</id>
    <title>Best practice for scaling embeddings</title>
    <updated>2025-03-01T00:15:31+00:00</updated>
    <author>
      <name>/u/Ambitious-Most4485</name>
      <uri>https://old.reddit.com/user/Ambitious-Most4485</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to handle 500 non-concurrent messages a day. I have a finetuned model which I load with sentencetrasformer. I mainly use CPU to create the embeddings and then pass the retrieved documents to an LLM (yes we have a RAG in place).&lt;/p&gt; &lt;p&gt;We have a lot of CPU resources but I want to optimize the embeddings creation part (that is expose as a microservice written in python using fastapi). I didnt find in the sentencetrasformer documentation an example for what im trying to create (if i overlooked it let me know). The bottleneck with sentencetrasformer happens when i have multiple simultaneous requests. From my understanding (which can be wrong) fastapi cannot embed multiple vectors in a multithread fashion since it uses asyncio. So it waits for the CPU following a FIFO queue (maybe It's because I implmented the method wrong?)&lt;/p&gt; &lt;p&gt;Are there better ways to solve this problem? Maybe with ONNX? Can you point out some resources where i can delve and gain additional knowledge in the reasoning needed behind the scenes?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-Most4485"&gt; /u/Ambitious-Most4485 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mi30/best_practice_for_scaling_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mi30/best_practice_for_scaling_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mi30/best_practice_for_scaling_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:15:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0thtd</id>
    <title>Choosing gpu for ai developement</title>
    <updated>2025-03-01T06:48:12+00:00</updated>
    <author>
      <name>/u/LahmeriMohamed</name>
      <uri>https://old.reddit.com/user/LahmeriMohamed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;can you suggest me a desktop config i can build with budget of 600 usd for ai developement LLM and VLM training and deployement ? and do amd gpu support them ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LahmeriMohamed"&gt; /u/LahmeriMohamed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0thtd/choosing_gpu_for_ai_developement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0thtd/choosing_gpu_for_ai_developement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0thtd/choosing_gpu_for_ai_developement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0b6xn</id>
    <title>CohereForAI/c4ai-command-r7b-arabic-02-2025 ¬∑ Hugging Face</title>
    <updated>2025-02-28T16:07:08+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"&gt; &lt;img alt="CohereForAI/c4ai-command-r7b-arabic-02-2025 ¬∑ Hugging Face" src="https://external-preview.redd.it/73KAWnSegMjFP_fsyR6QSQ02fnqaTKaAAHSwlrGelw8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57eecf29ad1f482ee744a918c100afc4a28bea9f" title="CohereForAI/c4ai-command-r7b-arabic-02-2025 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0b6xn/cohereforaic4aicommandr7barabic022025_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d54k</id>
    <title>I a designing intent_blocks to solve the dreaded problem of how much context should I send to an LLM, especially in a multi-turn conversations. Your feedback would help</title>
    <updated>2025-02-28T17:26:14+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One dreaded and underrated aspect about building RAG apps is to figure out how and when to rephrase the last user query so that you can improve retrieval. For example:&lt;/p&gt; &lt;p&gt;User: Tell me about all the great accomplishments of George Washington&lt;br /&gt; Assistant: &amp;lt;some response about George Washington&amp;gt;&lt;br /&gt; User: what about his siblings?&lt;/p&gt; &lt;p&gt;Now if you only look at the last user query your retrieval system will return junk because it doesn‚Äôt under stand ‚Äúthis‚Äù. You could pass the full history then your response would at best include both the accomplishments of GW and his siblings or worse be inaccurate. The other approach is send the full context to an LLM and ask it to rephrase or re-write the last user prompt so that the intent is fully represented in it. This is generally slow, excessive in token costs, and hard to debug if things go wrong - but does have higher chances of success.&lt;/p&gt; &lt;p&gt;A couple of &lt;a href="https://github.com/katanemo/archgw"&gt;releases ago&lt;/a&gt; I added support for multi-turn detection in archgw, where I would extract critical information (relation=siblings, person=George Washington) in a multi-turn scenario and route to an endpoint that was expecting these parameters to improve retrieval. This works fine but requires developers to define usage patterns more precisely. It‚Äôs not abstract enough to handle more nuanced retrieval scenarios.&lt;/p&gt; &lt;p&gt;So now I am designing &amp;lt;intent-blocks&amp;gt;: essentially markers applied to messages history that would indicate on what messages and assistant responses are related as metadata of the conversational history, which can then be used to rephrase the last query to improve retrieval. This means you can ignore certain blocks because they are not related and improve speed, cost and retrieval accuracy&lt;/p&gt; &lt;p&gt;Would this be useful to you? How do you go about solving this problem today? How else would you like for me to improve the designs to accommodate your needs? üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d54k/i_a_designing_intent_blocks_to_solve_the_dreaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0nli0</id>
    <title>DeepSeek R1: distilled &amp; quantized models explained simply for beginners</title>
    <updated>2025-03-01T01:10:21+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"&gt; &lt;img alt="DeepSeek R1: distilled &amp;amp; quantized models explained simply for beginners" src="https://external-preview.redd.it/gk92HmQ5dLETjf3xIkaITfkJIkYIq_lnYJClNB1FntQ.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3f5dc2938e5bd646952339cc0f2189330ec1bca" title="DeepSeek R1: distilled &amp;amp; quantized models explained simply for beginners" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/AxAj16ZmanY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0nli0/deepseek_r1_distilled_quantized_models_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d2xo</id>
    <title>MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!</title>
    <updated>2025-02-28T17:23:41+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"&gt; &lt;img alt="MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!" src="https://external-preview.redd.it/nDrA4WA4Kllguv9k8ov6r62GHYnppkRZHGSgvQCCQm4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ba35156b1076a74a3437bd13d8bec19e91a359e" title="MiraConverse has been updated. Chat with any AI model using a trigger word. The client now runs on a Raspberry PI. Multilingual support for all Kokoro supported languages. Client and server are available in Docker. Tool calling now supported!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/PmJGlafRqpI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d2xo/miraconverse_has_been_updated_chat_with_any_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0mxyu</id>
    <title>The Fastest Way to Fine-Tune LLMs Locally</title>
    <updated>2025-03-01T00:37:15+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"&gt; &lt;img alt="The Fastest Way to Fine-Tune LLMs Locally" src="https://external-preview.redd.it/6UOl6M06GBFlN2fyjDaRlq770836HUnVCb2siqyuTdg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d807111a3b15fb2be2e302d215faacb851bd7a6" title="The Fastest Way to Fine-Tune LLMs Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MaxHastings/Kolo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0mxyu/the_fastest_way_to_finetune_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izvwck</id>
    <title>DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp; smallpond (A lightweight data processing framework)</title>
    <updated>2025-02-28T01:15:12+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt; &lt;img alt="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" src="https://external-preview.redd.it/HvC95tBfvHDGJxAbUH6W9PmwC54Tm2U3z7QQDPE9EaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f064ebf3053086f57b27efe553869e937081d60d" title="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't believe DeepSeek has even revolutionized storage architecture... The last time I was amazed by a network file system was with HDFS and CEPH. But those are disk-oriented distributed file systems. Now, a truly modern SSD and RDMA network-oriented file system has been born!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3FS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;smallpond&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lightweight data processing framework built on &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; and &lt;a href="https://github.com/deepseek-ai/3FS"&gt;3FS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82"&gt;https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j00v4y</id>
    <title>"Crossing the uncanny valley of conversational voice" post by Sesame - realtime conversation audio model rivalling OpenAI</title>
    <updated>2025-02-28T05:52:31+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So this is one of the craziest voice demos I've heard so far, and they apparently want to release their models under an Apache-2.0 license in the future: I've never heard of Sesame, they seem to be very new.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Our models will be available under an Apache 2.0 license&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Your thoughts? Check the demo first: &lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No public weights yet, we can only dream and hope, but this easily matches or beats OpenAI's Advanced Voice Mode. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T05:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0l2kp</id>
    <title>OpenArc upcoming olmoOCR, Qwen2-VL support</title>
    <updated>2025-02-28T23:08:30+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I have been talking about &lt;a href="https://github.com/SearchSavior"&gt;OpenArc&lt;/a&gt; a lot this week. Even have a release planned this weekend.&lt;/p&gt; &lt;p&gt;Right now I am excited because I was just able to convert &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0225-preview"&gt;olmOCR-7B-0225-preview&lt;/a&gt; to OpenVINO format!!! Weights will be posted this weekend and OpenArc will support serving eventually with the full gamut of Qwen2-VL input/output. For those who don't know, this is a pretty big deal; olmoOCR seems very powerful for document analysis and Qwen2-VL can be tooled to be a very effect edge compute vision agent on top of it's own image vision capabilities. Pair that with omniparser or paddle... we'll have capable local intel vsion agents very soon. Not sooner than text only however.&lt;/p&gt; &lt;p&gt;Holy crap this is very exciting do a lot of work with CPU only OCR and am very familair with the literature in this area- Qwen2-VL was my first major llm project. For reference, on my 2x xeon 6242 server I was getting ~7t/s for 100 dpi images with Qwen2-VL-7B which was 7x faster than full precision; I replicated their accuracy int4 results from the qwen 2 vl paper with openvino int4 in my work on dense table analysis.&lt;/p&gt; &lt;p&gt;Anyway, stay tuned for text only, a gradio dashboard with baked in documentation and openai endpoints this weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0l2kp/openarc_upcoming_olmoocr_qwen2vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T23:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j088yg</id>
    <title>RX 9070 XT Potential performance discussion</title>
    <updated>2025-02-28T13:57:36+00:00</updated>
    <author>
      <name>/u/ashirviskas</name>
      <uri>https://old.reddit.com/user/ashirviskas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As some of you might have seen, AMD just revealed the new RDNA 4 GPUS. RX 9070 XT for $599 and RX 9070 for $549&lt;/p&gt; &lt;p&gt;Looking at the numbers, 9070 XT offers &amp;quot;2x&amp;quot; in FP16 per compute unit compared to 7900 XTX [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;], so at 64U vs 96U that means RX 9070 XT would have 33% compute uplift.&lt;/p&gt; &lt;p&gt;The issue is the bandwitdh - at 256bit GDDR6 we get ~630GB/s compared to 960GB/s on a 7900 XTX.&lt;/p&gt; &lt;p&gt;BUT! According to the same presentation [&lt;a href="https://www.youtube.com/live/GZfFPI8LJrc?si=V1ApBTDRCrkInqqR&amp;amp;t=661"&gt;source&lt;/a&gt;] they mention they've added INT8 and INT8 &lt;em&gt;with sparsity&lt;/em&gt; computations to RDNA 4, which make it 4x and 8x faster than RDNA 3 &lt;em&gt;per unit&lt;/em&gt;, which would make it 2.67x and 5.33x times faster than RX 7900 XTX.&lt;/p&gt; &lt;p&gt;I wonder if newer model architectures that are less limited by memory bandwidth could use these computations and make new AMD GPUs great inference cards. What are your thoughts?&lt;/p&gt; &lt;p&gt;EDIT: Updated links after they cut the video. Both are now the same, originallly I quoted two different parts of the video.&lt;/p&gt; &lt;p&gt;EDIT2: I missed it, but hey also mention 4-bit tensor types!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashirviskas"&gt; /u/ashirviskas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j088yg/rx_9070_xt_potential_performance_discussion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0gs1g</id>
    <title>AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play</title>
    <updated>2025-02-28T20:00:12+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt; &lt;img alt="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" src="https://external-preview.redd.it/GK3DdqrCYGvJGL-l88rd-LZtTxw8isk9OD7o602ntRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20e1333c4dbdf7f381d26b7f87d3f296cf306559" title="AMD Engineer Talks Up Vulkan/SPIR-V As Part Of Their MLIR-Based Unified AI Software Play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-Vulkan-SPIR-V-Wide-AI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0gs1g/amd_engineer_talks_up_vulkanspirv_as_part_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T20:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1j09mfx</id>
    <title>is 9070xt any good for localAI on windows ?</title>
    <updated>2025-02-28T15:00:48+00:00</updated>
    <author>
      <name>/u/gfy_expert</name>
      <uri>https://old.reddit.com/user/gfy_expert</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt; &lt;img alt="is 9070xt any good for localAI on windows ?" src="https://a.thumbs.redditmedia.com/IHRW4iuZw3RS3QXGe1CwjVtw2Kt1kBPYjq-WbucEht0.jpg" title="is 9070xt any good for localAI on windows ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gfy_expert"&gt; /u/gfy_expert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j09mfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j09mfx/is_9070xt_any_good_for_localai_on_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T15:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0uoht</id>
    <title>Chain of Draft: Thinking Faster by Writing Less</title>
    <updated>2025-03-01T08:10:14+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt; &lt;img alt="Chain of Draft: Thinking Faster by Writing Less" src="https://b.thumbs.redditmedia.com/fwljouG_I7UUcaN7hDPRHRGsfe6zHb3U7bt9TnwQ_OA.jpg" title="Chain of Draft: Thinking Faster by Writing Less" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.18600"&gt;https://arxiv.org/abs/2502.18600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CoD System prompt:&lt;/p&gt; &lt;p&gt;Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j0uoht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0uoht/chain_of_draft_thinking_faster_by_writing_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T08:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j045xn</id>
    <title>I trained a reasoning model that speaks French‚Äîfor just $20! ü§Øüá´üá∑</title>
    <updated>2025-02-28T09:51:24+00:00</updated>
    <author>
      <name>/u/TheREXincoming</name>
      <uri>https://old.reddit.com/user/TheREXincoming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j045xn/video/mvudzukrpule1/player"&gt;https://reddit.com/link/1j045xn/video/mvudzukrpule1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheREXincoming"&gt; /u/TheREXincoming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T09:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0cbvs</id>
    <title>üó£Ô∏è Free &amp; Open-Source AI TTS: Kokoro Web v0.1.0</title>
    <updated>2025-02-28T16:53:25+00:00</updated>
    <author>
      <name>/u/EduardoDevop</name>
      <uri>https://old.reddit.com/user/EduardoDevop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Excited to share &lt;strong&gt;Kokoro Web&lt;/strong&gt;, a fully open-source AI text-to-speech tool that you can use for free. No paywalls, no restrictions‚Äîjust high-quality, local-friendly TTS. &lt;/p&gt; &lt;h2&gt;üî• Why It Matters:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Open-Source&lt;/strong&gt;: No locked features, no subscriptions.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Hostable&lt;/strong&gt;: Run it locally or on your own server.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI API Compatible&lt;/strong&gt;: Drop-in replacement for AI projects.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Language Support&lt;/strong&gt;: Generate speech in different accents.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built on Kokoro v1.0&lt;/strong&gt;: One of the top-ranked models in &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;TTS Arena&lt;/a&gt;, just behind ElevenLabs.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;üöÄ Try It Out:&lt;/h2&gt; &lt;p&gt;Live demo: &lt;a href="https://voice-generator.pages.dev"&gt;https://voice-generator.pages.dev&lt;/a&gt; &lt;/p&gt; &lt;h2&gt;üîß Self-Hosting:&lt;/h2&gt; &lt;p&gt;Spin it up with Docker in minutes: &lt;a href="https://github.com/eduardolat/kokoro-web"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would love to hear your thoughts‚Äîfeedback, contributions, and ideas are always welcome! üñ§ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EduardoDevop"&gt; /u/EduardoDevop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0cbvs/free_opensource_ai_tts_kokoro_web_v010/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0c53c</id>
    <title>Inference speed comparisons between M1 Pro and maxed-out M4 Max</title>
    <updated>2025-02-28T16:45:35+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently own a MacBook M1 Pro (32GB RAM, 16-core GPU) and now a maxed-out MacBook M4 Max (128GB RAM, 40-core GPU) and ran some inference speed tests. I kept the context size at the default 4096. Out of curiosity, I compared MLX-optimized models vs. GGUF. Here are my initial results!&lt;/p&gt; &lt;h4&gt;Ollama&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:7B (4bit)&lt;/td&gt; &lt;td&gt;72.50 tokens/s&lt;/td&gt; &lt;td&gt;26.85 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:14B (4bit)&lt;/td&gt; &lt;td&gt;38.23 tokens/s&lt;/td&gt; &lt;td&gt;14.66 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:32B (4bit)&lt;/td&gt; &lt;td&gt;19.35 tokens/s&lt;/td&gt; &lt;td&gt;6.95 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5:72B (4bit)&lt;/td&gt; &lt;td&gt;8.76 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h4&gt;LM Studio&lt;/h4&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;MLX models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;101.87 tokens/s&lt;/td&gt; &lt;td&gt;38.99 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;52.22 tokens/s&lt;/td&gt; &lt;td&gt;18.88 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;24.46 tokens/s&lt;/td&gt; &lt;td&gt;9.10 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (8bit)&lt;/td&gt; &lt;td&gt;13.75 tokens/s&lt;/td&gt; &lt;td&gt;Won‚Äôt Complete (Crashed)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;10.86 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;GGUF models&lt;/th&gt; &lt;th&gt;M4 Max (128 GB RAM, 40-core GPU)&lt;/th&gt; &lt;th&gt;M1 Pro (32GB RAM, 16-core GPU)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-7B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;71.73 tokens/s&lt;/td&gt; &lt;td&gt;26.12 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-14B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;39.04 tokens/s&lt;/td&gt; &lt;td&gt;14.67 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-32B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;19.56 tokens/s&lt;/td&gt; &lt;td&gt;4.53 tokens/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-72B-Instruct (4bit)&lt;/td&gt; &lt;td&gt;8.31 tokens/s&lt;/td&gt; &lt;td&gt;Didn't Test&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Some thoughts:&lt;/p&gt; &lt;p&gt;- I don't think these models are actually utilizing the CPU. But I'm not definitive on this.&lt;/p&gt; &lt;p&gt;- I chose Qwen2.5 simply because its currently my favorite local model to work with. It seems to perform better than the distilled DeepSeek models (my opinion). But I'm open to testing other models if anyone has any suggestions.&lt;/p&gt; &lt;p&gt;- Even though there's a big performance difference between the two, I'm still not sure if its worth the even bigger price difference. I'm still debating whether to keep it and sell my M1 Pro or return it.&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;EDIT: Added test results for 72B and 7B variants&lt;/p&gt; &lt;p&gt;UPDATE: I added a github repo in case anyone wants to contribute their own speed tests. Feel free to contribute here: &lt;a href="https://github.com/itsmostafa/inference-speed-tests"&gt;https://github.com/itsmostafa/inference-speed-tests&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0c53c/inference_speed_comparisons_between_m1_pro_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T16:45:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0kgyn</id>
    <title>99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900</title>
    <updated>2025-02-28T22:41:24+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt; &lt;img alt="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" src="https://external-preview.redd.it/HFws3DDkcEP1xVBovP3WDu-ptb9oIschwqz-M_5LaEc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1b52e8ca906335c7d16f3c20d0abb029388892" title="99 tk/s - Phi 4 Mini Q8 GGUF full 128k context - Chonky Boi W7900" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ZWBQPKc.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0kgyn/99_tks_phi_4_mini_q8_gguf_full_128k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T22:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0d30g</id>
    <title>There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day</title>
    <updated>2025-02-28T17:23:46+00:00</updated>
    <author>
      <name>/u/unixmachine</name>
      <uri>https://old.reddit.com/user/unixmachine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt; &lt;img alt="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" src="https://external-preview.redd.it/VGXH8e8_7pJ5Oyuiy8alPcJzC5slCQHySpimzMU8-QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c7786514b391f52128b399c1e789d18e7f6f10a" title="There Will Not Be Official ROCm Support For The Radeon RX 9070 Series On Launch Day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixmachine"&gt; /u/unixmachine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-ROCm-RX-9070-Launch-Day"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0d30g/there_will_not_be_official_rocm_support_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T17:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0muz1</id>
    <title>Phi-4-mini Bug Fixes + GGUFs</title>
    <updated>2025-03-01T00:33:13+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! llama.cpp added supported for Phi-4 mini today - we also found and fixed 4 tokenization related problems in Phi-4 mini!&lt;/p&gt; &lt;p&gt;The biggest problem with the chat template is the EOS token was set to &amp;lt;|endoftext|&amp;gt;, but it should be &amp;lt;|end|&amp;gt;!&lt;/p&gt; &lt;p&gt;GGUFs are at: &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The rest of the versions including 16-bit are&lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt; &lt;/a&gt;also on &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And the dynamic 4bit bitsandbytes version is at &lt;a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There were also tokenization problems for the larger Phi-4 14B as well, which we fixed a while back for those who missed it and Microsoft used our fixes 2 weeks ago.&lt;/p&gt; &lt;p&gt;Thank you! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0muz1/phi4mini_bug_fixes_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0o8mt</id>
    <title>The first real open source DeepResearch attempt I've seen</title>
    <updated>2025-03-01T01:43:56+00:00</updated>
    <author>
      <name>/u/Fun_Yam_6721</name>
      <uri>https://old.reddit.com/user/Fun_Yam_6721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Search-R1&lt;/strong&gt; is a reproduction of &lt;strong&gt;DeepSeek-R1(-Zero)&lt;/strong&gt; methods for &lt;em&gt;training reasoning and searching (tool-call) interleaved LLMs&lt;/em&gt;. Built upon &lt;a href="https://github.com/volcengine/verl"&gt;veRL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Through RL (rule-based outcome reward), the 3B &lt;strong&gt;base&lt;/strong&gt; LLM (both Qwen2.5-3b-base and Llama3.2-3b-base) develops reasoning and search engine calling abilities all on its own.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PeterGriffinJin/Search-R1/tree/main"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Yam_6721"&gt; /u/Fun_Yam_6721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0o8mt/the_first_real_open_source_deepresearch_attempt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T01:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0r3go</id>
    <title>Day 6: One More Thing, DeepSeek-V3/R1 Inference System Overview</title>
    <updated>2025-03-01T04:19:45+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md"&gt;https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0r3go/day_6_one_more_thing_deepseekv3r1_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T04:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0tnsr</id>
    <title>We're still waiting Sam...</title>
    <updated>2025-03-01T06:59:17+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt; &lt;img alt="We're still waiting Sam..." src="https://preview.redd.it/31jfuybv01me1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=128f969cd722b072d73b4d77393ee7c0bc1b057b" title="We're still waiting Sam..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/31jfuybv01me1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0tnsr/were_still_waiting_sam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T06:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j0n56h</id>
    <title>Finally, a real-time low-latency voice chat model</title>
    <updated>2025-03-01T00:47:24+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't seen it yet, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo"&gt;https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I tried it fow a few minutes earlier today and another 15 minutes now. I tested and it remembered our chat earlier. It is the first time that I treated AI as a person and felt that I needed to mind my manners and say &amp;quot;thank you&amp;quot; and &amp;quot;good bye&amp;quot; at the end of the conversation.&lt;/p&gt; &lt;p&gt;Honestly, I had more fun chatting with this than some of my ex-girlfriends!&lt;/p&gt; &lt;p&gt;Github here (code not yet dropped):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;``` Model Sizes: We trained three model sizes, delineated by the backbone and decoder sizes:&lt;/p&gt; &lt;p&gt;Tiny: 1B backbone, 100M decoder Small: 3B backbone, 250M decoder Medium: 8B backbone, 300M decoder Each model was trained with a 2048 sequence length (~2 minutes of audio) over five epochs. ```&lt;/p&gt; &lt;p&gt;The model sizes look friendly to local deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j0n56h/finally_a_realtime_lowlatency_voice_chat_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-01T00:47:24+00:00</published>
  </entry>
</feed>
