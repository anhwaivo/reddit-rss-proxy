<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-26T17:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n0mz9p</id>
    <title>Advice for a local OCR solution</title>
    <updated>2025-08-26T14:13:20+00:00</updated>
    <author>
      <name>/u/Old_Consideration228</name>
      <uri>https://old.reddit.com/user/Old_Consideration228</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0mz9p/advice_for_a_local_ocr_solution/"&gt; &lt;img alt="Advice for a local OCR solution" src="https://preview.redd.it/d02cd2k5edlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ed87a25eeabc990dba34687ce10af114c2a30c3" title="Advice for a local OCR solution" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have built an OCR system (with the above architecture) with the aim of having a local, fast, accurate OCR solution. Currently I run Gemini as a final post-processor for low-confidence pages and want to drop that external step.&lt;/p&gt; &lt;p&gt;Quick questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Has anyone used &lt;strong&gt;BERT / BERT-like&lt;/strong&gt; models for OCR post-processing (page / line level)? What worked better in practice: &lt;ul&gt; &lt;li&gt;sequence tagging (token/char edits) or seq2seq correction?&lt;/li&gt; &lt;li&gt;Did you feed extra signals (token confidences, box coords, font-size) and did that help?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;I’m considering replacing the classic &lt;strong&gt;2-stage pipeline (detect -&amp;gt; recognize)&lt;/strong&gt; with a &lt;strong&gt;1-stage VLM (image -&amp;gt; full page text),&lt;/strong&gt; Since the 2-stage approach is very complex specially when introducing tables, forms, etc. . Anyone tried this in production? Major pros/cons, failure modes (tables, multi-column, rotated text)?&lt;/li&gt; &lt;li&gt;How realistic is training a &lt;strong&gt;&amp;lt;1B-param VLM&lt;/strong&gt; dedicated to OCR (pretrained encoder + small decoder/projection)? Ballpark: CER/WER I should expect, data scale (real + synthetic) needed, and common pitfalls?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Consideration228"&gt; /u/Old_Consideration228 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d02cd2k5edlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0mz9p/advice_for_a_local_ocr_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0mz9p/advice_for_a_local_ocr_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T14:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzqy3z</id>
    <title>InternVL3.5 - Best OpenSource VLM</title>
    <updated>2025-08-25T13:46:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt; &lt;img alt="InternVL3.5 - Best OpenSource VLM" src="https://b.thumbs.redditmedia.com/nVzY4GlZP996KhrAM5_W8vRFK-rnOrWqnRnOhiYSBYI.jpg" title="InternVL3.5 - Best OpenSource VLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/InternVL3_5-241B-A28B"&gt;https://huggingface.co/internlm/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;InternVL3.5 with a variety of new capabilities including GUI agent, embodied agent, etc. Specifically, InternVL3.5-241B-A28B achieves the highest overall score on multimodal general, reasoning, text, and agency tasks among leading open source MLLMs, and narrows the gap with top commercial models such as GPT-5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mzqy3z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzqy3z/internvl35_best_opensource_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T13:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0pd29</id>
    <title>Best open-source tools for parsing PDFs, Office docs, and images before feeding into LLMs?</title>
    <updated>2025-08-26T15:43:45+00:00</updated>
    <author>
      <name>/u/Particular_Cake4359</name>
      <uri>https://old.reddit.com/user/Particular_Cake4359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m currently working on a chatbot project where I want users to be able to upload different types of documents (PDF, Word, Excel, PowerPoint, JPG, PNG, etc.). These files can contain plain text, tables, or even images/diagrams. The goal is to parse the content, extract structured data, and then inject it into an LLM for question answering and reasoning. &lt;/p&gt; &lt;p&gt;From my research, I see there are different approaches: tools like PyPDF, for text extraction, and OCR engines for scanned documents or images. But I’m still a bit confused about when to use OCR vs text-based extraction, and how to best handle cases like embedded tables and images. &lt;/p&gt; &lt;p&gt;Ideally, I’m looking for a fully open-source stack (no paid APIs) that can: &lt;/p&gt; &lt;p&gt;Extract clean text from PDFs and Office files &lt;/p&gt; &lt;p&gt;Parse structured tables (into dataframes or JSON) &lt;/p&gt; &lt;p&gt;Handle images or diagrams (at least extract them, or convert charts into structured text if possible) &lt;/p&gt; &lt;p&gt;Integrate with frameworks like LangChain or LangGraph &lt;/p&gt; &lt;p&gt;My questions: &lt;/p&gt; &lt;p&gt;What are the best open-source tools for multi-format document parsing (text + tables + images)? &lt;/p&gt; &lt;p&gt;When is OCR necessary vs when is a text extractor enough? &lt;/p&gt; &lt;p&gt;Are there recommended pipelines that combine text, tables, and images into a single structured representation for LLMs? &lt;/p&gt; &lt;p&gt;Do you know of any GitHub repos, open-source projects, or example implementations that already solve (or partially solve) this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular_Cake4359"&gt; /u/Particular_Cake4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pd29/best_opensource_tools_for_parsing_pdfs_office/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pd29/best_opensource_tools_for_parsing_pdfs_office/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pd29/best_opensource_tools_for_parsing_pdfs_office/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0nu79</id>
    <title>Has anyone been able to reliably to use Higgs multispeaker for longer (&gt; 5 minute audios)</title>
    <updated>2025-08-26T14:46:57+00:00</updated>
    <author>
      <name>/u/jasmeet0817</name>
      <uri>https://old.reddit.com/user/jasmeet0817</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I used Higgs audio v2 (&lt;a href="https://huggingface.co/spaces/smola/higgs%5C_audio%5C_v2"&gt;https://huggingface.co/spaces/smola/higgs\_audio\_v2&lt;/a&gt;) the other day and it was great so I donwloaded the model locally and tried creating a 5 minute audio and it was very buggy after 1 minute mark. Is there a way to configue it for longer audios ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jasmeet0817"&gt; /u/jasmeet0817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nu79/has_anyone_been_able_to_reliably_to_use_higgs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nu79/has_anyone_been_able_to_reliably_to_use_higgs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nu79/has_anyone_been_able_to_reliably_to_use_higgs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T14:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0c58h</id>
    <title>Update llama.cpp for a big speed boost with gpt-oss and cuda.</title>
    <updated>2025-08-26T04:11:45+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few cuda commits landed today that have made a big difference in performance. Testing with gpt-oss-120B, I saw a 14.5% increase in tokens per second with 2x3090 and 1xP40. It went from 51.6 tok/sec to 59.1 tok/sec. &lt;/p&gt; &lt;p&gt;With gptoss-20B I stayed at 130 tok/sec on a single 3090 power limited to 300W. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0c58h/update_llamacpp_for_a_big_speed_boost_with_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T04:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0aijh</id>
    <title>GPT OSS 120B</title>
    <updated>2025-08-26T02:47:05+00:00</updated>
    <author>
      <name>/u/vinigrae</name>
      <uri>https://old.reddit.com/user/vinigrae</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the best function calling model I’ve used, don’t think twice, just use it. &lt;/p&gt; &lt;p&gt;We gave it a multi scenario difficulty 300 tool call test, where even 4o and GPT 5 mini performed poorly. &lt;/p&gt; &lt;p&gt;Ensure you format the system properly for it, you will find the model won’t even execute things that are actually done in a faulty manner and are detrimental to the pipeline.&lt;/p&gt; &lt;p&gt;I’m &lt;strong&gt;extremely&lt;/strong&gt; impressed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vinigrae"&gt; /u/vinigrae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0aijh/gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0jnsn</id>
    <title>multi-item tryon - qwen-edit</title>
    <updated>2025-08-26T11:49:51+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt; &lt;img alt="multi-item tryon - qwen-edit" src="https://b.thumbs.redditmedia.com/KlMjzSeLuGwUjogMzIosQV-Ldt0kqb3QFW98g1pD_Ro.jpg" title="multi-item tryon - qwen-edit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;today we release a early version of our multi item - tryon for qwen-edit&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FoxBaze/Try_On_Qwen_Edit_Lora_Alpha"&gt;https://huggingface.co/FoxBaze/Try_On_Qwen_Edit_Lora_Alpha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;given the early nature - we love to hear from you how you use it / and if something doesnt work ! find us on discord &lt;a href="https://discord.gg/UXN7zFuxbk"&gt;https://discord.gg/UXN7zFuxbk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s5c31qe4qclf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62dffbc3efbea228bd2fd938e1b45100724b7756"&gt;https://preview.redd.it/s5c31qe4qclf1.png?width=1664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62dffbc3efbea228bd2fd938e1b45100724b7756&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0jnsn/multiitem_tryon_qwenedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T11:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0ndbq</id>
    <title>XBai o4 is live and claiming to beat OpenAI's o3-mini-medium in reasoning with parallel thinking, fast inference, and better web search.</title>
    <updated>2025-08-26T14:28:45+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/theMetaStoneAI/status/1959995307351323064?t=HB1JSLhY7mKE-OimGCCSxg&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ndbq/xbai_o4_is_live_and_claiming_to_beat_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ndbq/xbai_o4_is_live_and_claiming_to_beat_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T14:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n09aof</id>
    <title>[2508.15884] Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
    <updated>2025-08-26T01:48:53+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.15884"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n09aof/250815884_jetnemotron_efficient_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n09aof/250815884_jetnemotron_efficient_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T01:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0am1b</id>
    <title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title>
    <updated>2025-08-26T02:52:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;https://arxiv.org/abs/2508.18265v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0% gain in overall reasoning performance and a 4.05× inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks—narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Models:&lt;/p&gt; &lt;p&gt;1B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-2B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-4B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-14B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;38B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-38B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-38B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;20BA4B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30BA3B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;241BA28B: &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.18265v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0am1b/internvl35_advancing_opensource_multimodal_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T02:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwqj9</id>
    <title>VibeVoice (1.5B) - TTS model by Microsoft</title>
    <updated>2025-08-25T17:22:43+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;Weights on HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers&amp;quot;&lt;/li&gt; &lt;li&gt;Based on Qwen2.5-1.5B&lt;/li&gt; &lt;li&gt;7B variant &amp;quot;coming soon&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwqj9/vibevoice_15b_tts_model_by_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0j56s</id>
    <title>support for Kimi VL model has been merged into llama.cpp (mtmd)</title>
    <updated>2025-08-26T11:23:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt; &lt;img alt="support for Kimi VL model has been merged into llama.cpp (mtmd)" src="https://external-preview.redd.it/cbkhZmPayjB8ku2nI_wAWqat0X8_NNQmx76ZV3jHgSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4760e99c4781aaef8c2381cf144ea12255d3b5e8" title="support for Kimi VL model has been merged into llama.cpp (mtmd)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We present &lt;strong&gt;Kimi-VL&lt;/strong&gt;, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers &lt;strong&gt;advanced multimodal reasoning, long-context understanding, and strong agent capabilities&lt;/strong&gt;—all while activating only &lt;strong&gt;2.8B&lt;/strong&gt; parameters in its language decoder (Kimi-VL-A3B).&lt;/p&gt; &lt;p&gt;(...)&lt;/p&gt; &lt;p&gt;This is an updated version of &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking"&gt;Kimi-VL-A3B-Thinking&lt;/a&gt;, with following improved abilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It Thinks Smarter while Consuming Less Tokens&lt;/strong&gt;: The 2506 version reaches better accuracy on multimodal reasoning benchmarks: 56.9 on MathVision (+20.1), 80.1 on MathVista (+8.4), 46.3 on MMMU-Pro (+3.3), 64.0 on MMMU (+2.1), while in average requires 20% reduced thinking length.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Sees Clearer with Thinking&lt;/strong&gt;: Unlike the previous version that specializes on thinking tasks, the 2506 version can also achieve the same or even better ability on general visual perception and understanding, e.g. MMBench-EN-v1.1 (84.4), MMStar (70.4), RealWorldQA (70.0), MMVet (78.4), surpassing or matching abilties of our non-thinking model (&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Video Scenarios&lt;/strong&gt;: The new 2506 version also improves on video reasoning and understanding benchmarks. It sets new state-of-the-art for open-source models on VideoMMMU (65.2), while also retains good ability on general video understanding (71.9 on Video-MME, matching &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Higher Resolution&lt;/strong&gt;: The new 2506 version supports 3.2 million total pixels in a single image, 4X compared to the previous version. This leads to non-trivial improvements on high-resolution perception and OS-agent grounding benchmarks: 83.2 on V* Benchmark (without extra tools), 52.8 on ScreenSpot-Pro, 52.5 on OSWorld-G (full set with refusal).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T11:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0ozdf</id>
    <title>Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?</title>
    <updated>2025-08-26T15:29:22+00:00</updated>
    <author>
      <name>/u/LightBrightLeftRight</name>
      <uri>https://old.reddit.com/user/LightBrightLeftRight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"&gt; &lt;img alt="Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?" src="https://preview.redd.it/qfb5upk6tdlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c42b7465f6015a481b4b6df52f3db1aebd024a9" title="Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This printing mistake, where they mixed up text for The Far Side with Dennis the Mennis, is one of the funniest things I've ever seen.&lt;/p&gt; &lt;p&gt;I've tried all the LLMs installed on my computer (as well as ChatGPT) with various prompts, and none of them have gotten it. I think my best prompt was &amp;quot;This newspaper made a mistake printing these comics. Can you tell me what the mistake is and why the result of the mistake itself is funny?&amp;quot;&lt;/p&gt; &lt;p&gt;My favorite hallucination was from InternVLs version of Qwen 30b-a3b:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;The mistake in the comic strip is that the caption under the second panel should be &amp;quot;I see your little, perked up tail... labeled and resting on a shelf somewhere.&amp;quot; The humor comes from the fact that the father is referring to his son's tail as if it were an object that could be labeled and shelved like a toy or a piece of furniture.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightBrightLeftRight"&gt; /u/LightBrightLeftRight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfb5upk6tdlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:29:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0f4hh</id>
    <title>Trying to run offline LLM+RAG feels impossible. What am I doing wrong?</title>
    <updated>2025-08-26T07:12:51+00:00</updated>
    <author>
      <name>/u/caprazli</name>
      <uri>https://old.reddit.com/user/caprazli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been banging my head against the wall trying to get a simple offline LLM+RAG setup running on my laptop (which is plenty powerful). The idea was just a proof of concept: local model + retrieval, able to handle MS Office docs, PDFs, &lt;strong&gt;and&lt;/strong&gt; (that's important) even .eml files.&lt;/p&gt; &lt;p&gt;Instead, it’s been an absolute nightmare. Nothing works out of the box. Every “solution” I try turns into endless code-patching across multiple platforms. Half the guides are outdated, half the repos are broken, and when I finally get something running, it chokes on the files I actually need.&lt;/p&gt; &lt;p&gt;I’m not a total beginner yet I’m definitely not an expert either. Still, I feel like the bar to entry here is ridiculously high. AI is fantastic for writing, summarizing, and all the fancy cloud-based stuff, but when it comes to coding and local setups, reliability is just… not there yet.&lt;/p&gt; &lt;p&gt;Am I doing something completely wrong? Does anyone else have similar experiences? Because honestly, &lt;strong&gt;AI might be “taking over the world,” but it’s definitely&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;taking over my computer&lt;/strong&gt;. It simply cannot.&lt;/p&gt; &lt;p&gt;Curious to hear from others. What’s your experience with local LLM+RAG setups? Any success stories or lessons learned?&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: U7-155H | 32G | 2T | Arc+NPU | W11: Should theoretically be enough to run local LLMs with big context, chew through Office/PDF/&lt;/em&gt;&lt;strong&gt;&lt;em&gt;.eml&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;docs, and push AI-native pipelines with NPU boost, yet...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caprazli"&gt; /u/caprazli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T07:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0i2ln</id>
    <title>Is there any way to run 100-120B MoE models at &gt;32k context at 30 tokens/second without spending a lot?</title>
    <updated>2025-08-26T10:23:55+00:00</updated>
    <author>
      <name>/u/vtkayaker</name>
      <uri>https://old.reddit.com/user/vtkayaker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 and a good AM5 socket system. With some tweaking, this is enough to run a 4-bit Qwen3-30B-A3B-Instruct-2507 as a coding model with 32k of context. It's no Claude Sonnet, but it's a cute toy and occasionally useful as a pair programmer.&lt;/p&gt; &lt;p&gt;I can also, with heroic effort and most of my 64GB of RAM, get GLM 4.5 Air to run painfully slowly with 32k context. Adding a &lt;a href="https://huggingface.co/jukofyork/GLM-4.5-DRAFT-0.6B-v3.0-GGUF/blob/main/README.md"&gt;draft model&lt;/a&gt; speeds up diff generation quite a bit, because even an 0.6B can accurately predict 16 tokens of unchanged diff context correctly.&lt;/p&gt; &lt;p&gt;But let's say I want to run a 4-bit quant of GLM 4.5 Air with 48-64k context at 30 tokens/second? What's the cheapest option?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An NVIDIA RTX PRO 6000 Blackwell 96GB costs around $8750. That would pay for &lt;em&gt;years&lt;/em&gt; of Claude MAX.&lt;/li&gt; &lt;li&gt;Lashing together 3 or 4 3090s requires both an EPYC motherboard and buying more 3090s.&lt;/li&gt; &lt;li&gt;Apple has some unified RAM systems. How fast are they &lt;em&gt;really&lt;/em&gt; for models like GLM 4.5 Air or GPT OSS 120B with 32-64k context and a 4-bit quant?&lt;/li&gt; &lt;li&gt;There's also the Ryzen AI MAX+ 395 with 128 GB of RAM, and dedicating 96 GB for the GPU. The few benchmarks I've seen are under 4k context, or not any better than 10 tokens/second.&lt;/li&gt; &lt;li&gt;NVIDIA has the DGX Spark coming out &lt;em&gt;sometime&lt;/em&gt; soon, but it looks like it will start at $3,000 and not actually be &lt;em&gt;that&lt;/em&gt; much better than the Ryzen AI MAX+ 395?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is there some clever setup that I'm missing? Does anyone have a 4-bit quant of GLM 4.5 Air running at 30 tokens/second with 48-64k context &lt;em&gt;without&lt;/em&gt; going all the way up to a RTX 6000 or 3-4 [345]090 cards and a server motherboard? I suspect the limiting factor here is RAM speed and PCIe lanes, even with the MoE?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vtkayaker"&gt; /u/vtkayaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dl84</id>
    <title>Been working on something... A teaser</title>
    <updated>2025-08-26T05:35:04+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt; &lt;img alt="Been working on something... A teaser" src="https://b.thumbs.redditmedia.com/gJsXHozTMr8Q_MnsobeoZsA017tBJOzE3DqbO7Z1inw.jpg" title="Been working on something... A teaser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty excited about this project i have been working on lately, be back soon with more info, but in the meantime thought a teaser wouldn't hurt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n0dl84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0pkhj</id>
    <title>Wan S2V reelased : 1st open-sourced AI Video Generation model with Audio support</title>
    <updated>2025-08-26T15:51:39+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan2.2 S2V (14B params) has been dropped recently and the early samples look great. The Audio support is great and can generate sining videos, dialogue deliveries, object sounds (like eating, rain, etc). &lt;strong&gt;I&lt;/strong&gt;t intakes a static image, an audio clip, and a text prompt. Built on a diffusion-based 3D VAE architecture with audio injection via Wav2Vec and motion consistency enabled by FramePack compression, it handles full-body movement, facial expressions, and long-form scene continuity with strong identity preservation and lip-sync accuracy.&lt;/p&gt; &lt;p&gt;Demo : &lt;a href="https://youtu.be/Hw9zaXOlU7I"&gt;https://youtu.be/Hw9zaXOlU7I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-S2V-14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-S2V-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical Report : &lt;a href="https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf"&gt;https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bhd7</id>
    <title>Microsoft VibeVoice TTS : Open-Sourced, Supports 90 minutes speech, 4 distinct speakers at a time</title>
    <updated>2025-08-26T03:36:48+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. &lt;/p&gt; &lt;p&gt;Demo Video : &lt;a href="https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ"&gt;https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0kb1d</id>
    <title>InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall</title>
    <updated>2025-08-26T12:20:27+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt; &lt;img alt="InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall" src="https://external-preview.redd.it/YcFbNVrfuwRpMZYl10KfE37DrxtDi8fi-29iTcISpUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3c3b5d958987fc0ab87ba9f72507692db53ed10" title="InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InternVL 3.5 has been released, and given the benchmark, the model looks to be the best multi-model LLM, ranking 3 overall just behind Gemini 2.5 Pro and GPT-5. Multiple variants released ranging from 1B to 241B&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v5hfeg9wclf1.png?width=1787&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2b06d9da57d572ea4ab90008e2ea2763c904f33"&gt;https://preview.redd.it/5v5hfeg9wclf1.png?width=1787&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2b06d9da57d572ea4ab90008e2ea2763c904f33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The team has introduced a number of new technical inventions, including &lt;em&gt;Cascade RL, Visual Resolution Router, Decoupled Vision-Language Deployment.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech report : &lt;a href="https://arxiv.org/abs/2508.18265"&gt;https://arxiv.org/abs/2508.18265&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video summary : &lt;a href="https://www.youtube.com/watch?v=hYrdHfLS6e0"&gt;https://www.youtube.com/watch?v=hYrdHfLS6e0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T12:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0nbih</id>
    <title>Wan-AI/Wan2.2-S2V-14B · Hugging Face</title>
    <updated>2025-08-26T14:26:43+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"&gt; &lt;img alt="Wan-AI/Wan2.2-S2V-14B · Hugging Face" src="https://external-preview.redd.it/4TRGFXGIVFwdwj9_01KulvW5c-oJPbLrLYw7udu9cqc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac2ca6a3cef9ab3cfd3e94820eb94dccc92be218" title="Wan-AI/Wan2.2-S2V-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan-S2V is an AI video generation model that can transform static images and audio into high-quality videos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-S2V-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T14:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0haub</id>
    <title>I pre-trained Gemma3 270m entirely from scratch</title>
    <updated>2025-08-26T09:36:43+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained Gemma3 270m entirely from scratch" src="https://external-preview.redd.it/BE2F9tVIKL9AN2T5zS4Z4ig6RgU9hM-QoHxWkSh5XTQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6fc22120dd0f86f8e67b629bd0ad915a09ad61" title="I pre-trained Gemma3 270m entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/9tmq5sa73clf1.gif"&gt;https://i.redd.it/9tmq5sa73clf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a video on this topic here: &lt;a href="https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB"&gt;https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in this video: &lt;/p&gt; &lt;p&gt;(1) Introduction&lt;/p&gt; &lt;p&gt;(2) Dataset loading&lt;/p&gt; &lt;p&gt;(3) Tokenisation&lt;/p&gt; &lt;p&gt;(4) Creating input-output pairs&lt;/p&gt; &lt;p&gt;(5) Building the Gemma 3 270M architecture&lt;/p&gt; &lt;p&gt;(6) Pre-training&lt;/p&gt; &lt;p&gt;(7) Inference&lt;/p&gt; &lt;p&gt;Attached is a GIF showing my lecture notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T09:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0p4t2</id>
    <title>Just one more prompt bro</title>
    <updated>2025-08-26T15:35:03+00:00</updated>
    <author>
      <name>/u/analgerianabroad</name>
      <uri>https://old.reddit.com/user/analgerianabroad</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0p4t2/just_one_more_prompt_bro/"&gt; &lt;img alt="Just one more prompt bro" src="https://preview.redd.it/g97cy5d4vdlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73b8982c8f022ac42f9fb2ed74c54155bd801ac1" title="Just one more prompt bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/analgerianabroad"&gt; /u/analgerianabroad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g97cy5d4vdlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0p4t2/just_one_more_prompt_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0p4t2/just_one_more_prompt_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:35:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0iho2</id>
    <title>LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA</title>
    <updated>2025-08-26T10:48:28+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt; &lt;img alt="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" src="https://preview.redd.it/g8lwztnlfclf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b45eb7eb720e8c27adcd24d4808bef43e5cb8dad" title="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source: &lt;a href="https://arxiv.org/pdf/2508.15884v1"&gt;https://arxiv.org/pdf/2508.15884v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g8lwztnlfclf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
