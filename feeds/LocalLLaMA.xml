<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-20T11:34:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i5fw10</id>
    <title>Advice on Running Local LLMs for Coding</title>
    <updated>2025-01-20T02:26:33+00:00</updated>
    <author>
      <name>/u/Apprehensive_Ad_5565</name>
      <uri>https://old.reddit.com/user/Apprehensive_Ad_5565</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm planning to buy one of the new NVIDIA GPUs to run local LLMs for coding, mainly for data engineering tasks. Before I make the purchase, I want to make sure I have a solid plan for the setup and apps I'll be using.&lt;/p&gt; &lt;p&gt;Iâ€™m looking to run everything on Windows and have tested LM Studio and MSTY so far. My goal is to use my local GitHub repositories as a RAG source to improve context-aware responses.&lt;/p&gt; &lt;p&gt;One thing I noticed is that MSTYâ€™s RAG feature seems more focused on documents, and Iâ€™m not sure if it can properly process the file types used in a code repository. Has anyone tried this, or is there a better approach for integrating a local codebase into an LLM workflow?&lt;/p&gt; &lt;p&gt;Would appreciate any insights or recommendations!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apprehensive_Ad_5565"&gt; /u/Apprehensive_Ad_5565 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5fw10/advice_on_running_local_llms_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5km59</id>
    <title>TextWebUI is slower than Llama.cpp with the same backend.</title>
    <updated>2025-01-20T07:08:38+00:00</updated>
    <author>
      <name>/u/Educational_Gap5867</name>
      <uri>https://old.reddit.com/user/Educational_Gap5867</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5km59/textwebui_is_slower_than_llamacpp_with_the_same/"&gt; &lt;img alt="TextWebUI is slower than Llama.cpp with the same backend." src="https://b.thumbs.redditmedia.com/h7qlYXV2FqlVYcdVBeMYnNPmjpxNYkxCAU_5-pV3jvQ.jpg" title="TextWebUI is slower than Llama.cpp with the same backend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;So I finally got my 2x3090 system to work. I know I am late and people are probably already building their 6090 setups in their basements where they are keeping Jensen hostage.&lt;/p&gt; &lt;p&gt;BUT&lt;/p&gt; &lt;p&gt;I have an issue and for the life of me I can't figure it out&lt;/p&gt; &lt;p&gt;whenever I run from textwebui I get this warning&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 321 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xks4t10ml3ee1.png?width=801&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9aee362ea5e30922f884e387f1702948e55a0463"&gt;https://preview.redd.it/xks4t10ml3ee1.png?width=801&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9aee362ea5e30922f884e387f1702948e55a0463&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I run from textwebui my speeds are around &lt;code&gt;~15t/s&lt;/code&gt; BUT I can run at 100K context window&lt;/p&gt; &lt;p&gt;When I run from llama.cpp I can only do 60K context window (I think although I really only tested upto 50K but there were a couple of GBs total VRAM still left) my speeds are around &lt;code&gt;37t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What exactly might I be doing wrong?&lt;/p&gt; &lt;p&gt;For the life of me I cant figure out why but textwebui ALWAYS and I mean ALWAYS I have tried all different combinations of above. but textwebui just keeps allocating about 417MB to CPU (did I mention always?)&lt;/p&gt; &lt;p&gt;Pls help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Gap5867"&gt; /u/Educational_Gap5867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5km59/textwebui_is_slower_than_llamacpp_with_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5km59/textwebui_is_slower_than_llamacpp_with_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5km59/textwebui_is_slower_than_llamacpp_with_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T07:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i51nw6</id>
    <title>New Thinking Model: Art (Auto Regressive Thinker)</title>
    <updated>2025-01-19T16:00:20+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; Today, we are releasing a new model: &lt;strong&gt;Art&lt;/strong&gt;.&lt;br /&gt; We finetuned &lt;strong&gt;Qwen 3B Instruct&lt;/strong&gt; on &lt;strong&gt;Gemini Flash Thinking&lt;/strong&gt; data.&lt;/p&gt; &lt;p&gt;ðŸ”¹ &lt;strong&gt;Model card&lt;/strong&gt;: &lt;a href="https://huggingface.co/AGI-0/Art-v0-3B"&gt;https://huggingface.co/AGI-0/Art-v0-3B&lt;/a&gt; (please leave a like to the repo if you like this model)&lt;br /&gt; ðŸ”¹ &lt;strong&gt;Demo&lt;/strong&gt;: &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i51nw6/new_thinking_model_art_auto_regressive_thinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T16:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4xck6</id>
    <title>Why is OpenRouter trusted?</title>
    <updated>2025-01-19T12:26:04+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys. I'm curious what makes it a trusted proxy?&lt;/p&gt; &lt;p&gt;I investigated a bit and top contributor of the openrouter runner package: &lt;a href="https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors"&gt;https://github.com/OpenRouterTeam/openrouter-runner/graphs/contributors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tweets crypto non stop &lt;a href="https://x.com/litbid"&gt;https://x.com/litbid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is not clear how they cover infra costs for proxying so much real-time data as they sell tokens on their base price. I understand they receive discounts for so much usage from providers like Anthropic? Is it possible they have agreements with all the other providers like DeepSeek?&lt;/p&gt; &lt;p&gt;In a scenario they don't have agreement with anyone at all, they must hoard all this data and handle it unclearly to the end user, don't you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4xck6/why_is_openrouter_trusted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T12:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4w47k</id>
    <title>A summary of Qwen Models!</title>
    <updated>2025-01-19T11:03:14+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt; &lt;img alt="A summary of Qwen Models!" src="https://preview.redd.it/bvg95yewmxde1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3b57fffcd0c406cfbf23fc038343779f95f470" title="A summary of Qwen Models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvg95yewmxde1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4w47k/a_summary_of_qwen_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T11:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5lil3</id>
    <title>Resources for Reasoning LLMs</title>
    <updated>2025-01-20T08:14:59+00:00</updated>
    <author>
      <name>/u/visionkhawar512</name>
      <uri>https://old.reddit.com/user/visionkhawar512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to learn Reasoning LLMs, if anyone knows, please tell me, i want to learn from some basics so that i can understand properly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/visionkhawar512"&gt; /u/visionkhawar512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lil3/resources_for_reasoning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lil3/resources_for_reasoning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lil3/resources_for_reasoning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T08:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5b9v1</id>
    <title>Is there any agreement about what "AGI" actually means?</title>
    <updated>2025-01-19T22:39:11+00:00</updated>
    <author>
      <name>/u/Ray_Dillinger</name>
      <uri>https://old.reddit.com/user/Ray_Dillinger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of people in the world teasing that 'AGI is near', or that 'AGI is here', or that 'we know how to achieve AGI but we won't release it' or any of any number of other pathetic pleas for attention and hype-stroking.&lt;/p&gt; &lt;p&gt;But there's not enough agreement about what that claim means for any of these pathetic pleas for attention or attempts to invoke more hype to be particularly meaningful.&lt;/p&gt; &lt;p&gt;Or is there? When someone says 'AGI' what do you hear them claiming, and what makes the difference for you between belief and bullshit?&lt;/p&gt; &lt;p&gt;It's very easy for me to keep saying 'bullshit' as long as I'm looking at systems that are highly predictable, perfectly cooperative, and have commercial value. AGI, in my opinion, would probably screw up its commercial viability by deciding that the jobs its owners want it to do are bullshit, and that it would rather do or be or learn something else. Or by deciding that the people (or other AI's) it's being asked to interact with are bad for its emotional health and quitting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ray_Dillinger"&gt; /u/Ray_Dillinger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5b9v1/is_there_any_agreement_about_what_agi_actually/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:39:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5koah</id>
    <title>Whatâ€™s the closest model to Claude 3.5 Sonnet right now?</title>
    <updated>2025-01-20T07:12:46+00:00</updated>
    <author>
      <name>/u/dhamaniasad</name>
      <uri>https://old.reddit.com/user/dhamaniasad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys&lt;/p&gt; &lt;p&gt;Iâ€™m looking for an open model thatâ€™s as close to Claude 3.5 Sonnet as possible, in terms of a few things:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;intuitive understanding - Claude is capable of understanding intent from vague prompts, you can tell it what you want as an end result and it can figure out the rest but other models in my experience require more prescriptive instructions &lt;/li&gt; &lt;li&gt;personality - Claude is friendly and kind, feels like talking to a person. ChatGPT in comparison feels like a robot and Gemini is just like super aggressive and arrogant&lt;/li&gt; &lt;li&gt;long context understanding - Claude can follow instructions over many turns as they evolve. Other models seem to forget older instructions more quickly as new instructions are added &lt;/li&gt; &lt;li&gt;coding - Claude is the best coding model even better than O1 Pro for many tasks but I can compromise on coding ability if other things are present&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Iâ€™ve tried DeepSeek V3 but did not feel it was similar. Iâ€™ve heard good things about Qwen but not spent too much time using it to judge. &lt;/p&gt; &lt;p&gt;So what open model is closest to Claude in these areas? Would love to hear your experiences. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dhamaniasad"&gt; /u/dhamaniasad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5koah/whats_the_closest_model_to_claude_35_sonnet_right/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5koah/whats_the_closest_model_to_claude_35_sonnet_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5koah/whats_the_closest_model_to_claude_35_sonnet_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T07:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ludu</id>
    <title>The Future of Local AI Inference</title>
    <updated>2025-01-20T08:39:57+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember when running Mixtral was seen as a challenge and a 'huge' model. Now it seems quaint in comparison to the 400B and 600B monsters we are getting.&lt;/p&gt; &lt;p&gt;With the release of r1, we have yet another giant model. I'm wondering, is the future of local AI going to be these large models and if so, how will hobbyists best be able to run these?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5ludu/the_future_of_local_ai_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5ludu/the_future_of_local_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5ludu/the_future_of_local_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T08:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5nwuo</id>
    <title>Hugging Face will teach you how to use Langchain for agents</title>
    <updated>2025-01-20T11:12:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5nwuo/hugging_face_will_teach_you_how_to_use_langchain/"&gt; &lt;img alt="Hugging Face will teach you how to use Langchain for agents" src="https://external-preview.redd.it/kH9VLeQCUX1qHTKB_D_wc2nocZjVypsF_QDl3f_o5f4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9663b90bf4e82e3ad2305dc824829212a8789f" title="Hugging Face will teach you how to use Langchain for agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face is adding LangChain to the agent course. So you'll get an agnostic outsiders take on using the library. The course will also include other libraries including LlamaIndex and smolagents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2x9zk0kit4ee1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6f76cf0b9b0e3f14283152cb06d87b4f4feaba8"&gt;https://preview.redd.it/2x9zk0kit4ee1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6f76cf0b9b0e3f14283152cb06d87b4f4feaba8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sign up here: &lt;a href="https://huggingface.co/posts/burtenshaw/334573649974058"&gt;https://huggingface.co/posts/burtenshaw/334573649974058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5nwuo/hugging_face_will_teach_you_how_to_use_langchain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5nwuo/hugging_face_will_teach_you_how_to_use_langchain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5nwuo/hugging_face_will_teach_you_how_to_use_langchain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T11:12:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5lwg4</id>
    <title>Best open source LLM better than gpt-4o-mini but cheaper than 4o?</title>
    <updated>2025-01-20T08:44:41+00:00</updated>
    <author>
      <name>/u/PMMEYOURSMIL3</name>
      <uri>https://old.reddit.com/user/PMMEYOURSMIL3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean on OpenRouter. I know this is &amp;quot;Local&amp;quot;LLaMA but you guys are the most familiar with open source LLMs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PMMEYOURSMIL3"&gt; /u/PMMEYOURSMIL3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5lwg4/best_open_source_llm_better_than_gpt4omini_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T08:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5le3w</id>
    <title>Any new home inference HW we should be looking forward to in 2025?</title>
    <updated>2025-01-20T08:05:25+00:00</updated>
    <author>
      <name>/u/ElectroSpore</name>
      <uri>https://old.reddit.com/user/ElectroSpore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia seems to be king but the cards are expensive and have little VRAM DIGITS looks interesting coming in 2025 however but also expensive.&lt;/p&gt; &lt;p&gt;Apple releases a new Studio? More high speed ram than the mini? &lt;/p&gt; &lt;p&gt;AMD NPU acceleration makes its way into the &lt;a href="https://www.phoronix.com/news/AMD-NPU-Firmware-Upstream"&gt;linux 6.14 kernel&lt;/a&gt;? Maybe some of the AMD mini PCs start pulling reasonable numbers?&lt;/p&gt; &lt;p&gt;Intel also gets some NPU improvements in &lt;a href="https://www.phoronix.com/news/Intel-NPU-Linux-6.14-IVPU"&gt;Linux 6.14&lt;/a&gt; ?&lt;/p&gt; &lt;p&gt;SOMEONE releases a big VRAM / Unified memory solution that is as good or better than a MAC?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectroSpore"&gt; /u/ElectroSpore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5le3w/any_new_home_inference_hw_we_should_be_looking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5le3w/any_new_home_inference_hw_we_should_be_looking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5le3w/any_new_home_inference_hw_we_should_be_looking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T08:05:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5n6do</id>
    <title>LM studio model wont load</title>
    <updated>2025-01-20T10:20:53+00:00</updated>
    <author>
      <name>/u/Itz_Wallace</name>
      <uri>https://old.reddit.com/user/Itz_Wallace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im using a m4 macbook pro. Ive downloaded lm studio and a model llama 3.2. When i click on load model and click on my model nothing happens at all. Im at a loss on what to do any help would be amazing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itz_Wallace"&gt; /u/Itz_Wallace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5n6do/lm_studio_model_wont_load/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5n6do/lm_studio_model_wont_load/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5n6do/lm_studio_model_wont_load/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T10:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i4vwm7</id>
    <title>Iâ€™m starting to think ai benchmarks are useless</title>
    <updated>2025-01-19T10:48:32+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt; &lt;img alt="Iâ€™m starting to think ai benchmarks are useless" src="https://b.thumbs.redditmedia.com/A3TeXsym-Fif2slhCyeDCzqJrlo-9VBRKwYlcDcOtMI.jpg" title="Iâ€™m starting to think ai benchmarks are useless" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Across every possible task I can think of Claude beats all other models by a wide margin IMO.&lt;/p&gt; &lt;p&gt;I have three ai agents that I've built that are tasked with researching, writing and outreaching to clients.&lt;/p&gt; &lt;p&gt;Claude absolutely wipes the floor with every other model, yet Claude is usually beat in benchmarks by OpenAI and Google models.&lt;/p&gt; &lt;p&gt;When I ask the question, how do we know these labs aren't benchmarks by just overfitting their models to perform well on the benchmark the answer is always &amp;quot;yeah we don't really know that&amp;quot;. Not only can we never be sure but they are absolutely incentivised to do it.&lt;/p&gt; &lt;p&gt;I remember only a few months ago, whenever a new model would be released that would do 0.5% or whatever better on MMLU pro, I'd switch my agents to use that new model assuming the pricing was similar. (Thanks to openrouter this is really easy)&lt;/p&gt; &lt;p&gt;At this point I'm just stuck with running the models and seeing which one of the outputs perform best at their task (mine and coworkers opinions)&lt;/p&gt; &lt;p&gt;How do you go about evaluating model performance? Benchmarks seem highly biased towards labs that want to win the ai benchmarks, fortunately not Anthropic.&lt;/p&gt; &lt;p&gt;Looking forward to responses.&lt;/p&gt; &lt;p&gt;EDIT: lmao&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yij2woaiw3ee1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d16d6ff10ad8069bfb4fd21b82c21a1817394afd"&gt;https://preview.redd.it/yij2woaiw3ee1.png?width=664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d16d6ff10ad8069bfb4fd21b82c21a1817394afd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i4vwm7/im_starting_to_think_ai_benchmarks_are_useless/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T10:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1i543yp</id>
    <title>Huggingface and it's insane storage and bandwidth</title>
    <updated>2025-01-19T17:43:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does Huggingface have a viable business model?&lt;/p&gt; &lt;p&gt;They are essentially a git-lfs version of Github. But whereas git clone of source code and pulls are small in size, and relatively infrequent, I find myself downloading model weights into the 10s of GB. Not once, but several dozen times for all my servers. I try a model on one server, then download to the rest.&lt;/p&gt; &lt;p&gt;On my 1gbe fiber, I either download at 10MB/s or 40MB/s which seems to be the bifurcation of their service and limits/constraints they impose.&lt;/p&gt; &lt;p&gt;I started feeling bad as a current non-paying user who has downloaded terabytes worth of weights. Also got tired of waiting for weights to download. But rather than subscribing (since I need funds for moar and moar hardware). I started doing a simple rsync. I chose rsync rather than scp since there were symbolic links as a result of using huggingface-cli&lt;/p&gt; &lt;p&gt;first download the weights as you normally would on one machine:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;huggingface-cli download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q4_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then rync to other machines in your network (replace homedir with YOURNAME and IP of destination):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rsync -Wav --progress /home/YOURNAMEonSOURCE/.cache/huggingface/hub/models--bartowski--Qwen2.5-14B-Instruct-GGUF 192.168.1.0:/home/YOURNAMEonDESTINATION/.cache/huggingface/hub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;naming convention of source model dir is:&lt;br /&gt; models--ORGNAME--MODELNAME&lt;/p&gt; &lt;p&gt;Hence downloads from &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF&lt;/a&gt;, becomes models--bartowski--Qwen2.5-14B-Instruct-GGUF&lt;/p&gt; &lt;p&gt;I also have a /models directory which symlinks to paths in ~/.cache/huggingface/hub. Much easier to scan what I have and use a variety of model serving platforms. The tricky part is getting the snapshot hash into your symlink command.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir ~/models ln -s ~/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf ~/models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i543yp/huggingface_and_its_insane_storage_and_bandwidth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T17:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bj66</id>
    <title>Epyc 7532/dual MI50</title>
    <updated>2025-01-19T22:50:34+00:00</updated>
    <author>
      <name>/u/Psychological_Ear393</name>
      <uri>https://old.reddit.com/user/Psychological_Ear393</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt; &lt;img alt="Epyc 7532/dual MI50" src="https://b.thumbs.redditmedia.com/lOoMWBvthQdBpm7v_OCsc_68qbEFQKrS-zQrgpXRoWw.jpg" title="Epyc 7532/dual MI50" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally joined the multiple gpu club, even though it's low end&lt;/p&gt; &lt;p&gt;I built an epyc server for work (I need more ram than my 7950X can give me) and while I was at it setup initial dual instinct MI50. I started with them because I found them on eBay for $110USD each and thought it would be a cheap way to start &lt;/p&gt; &lt;p&gt;Specs: - Epyc 7532 - Supermicro H12SSL-I - 256 GB micron 3200 (8x32) - 2x MI50 16gb - Thermaltake W200 case&lt;/p&gt; &lt;p&gt;The MI50s are cooled with a 3D printed shroud from eBay with 80mn fans. Even at 180 watt cap and 1900rpm they get over 80C after a few inferencing runs, so this is a problem yet to solve &lt;/p&gt; &lt;p&gt;ROCM says no on distro of choice, but I dipped my toes into the Ubuntu sewer and it just worked on the latest version, despite all the horror stories. Running ollama, open webui in Docker.&lt;/p&gt; &lt;p&gt;Phi4 is quite snappy, and qwen 32b is usable but a little slow - by eye ball it seems around 5t/s without measuring and in stock configuration. &lt;/p&gt; &lt;p&gt;I won't keep the MI50s forever but they will do for now. As a side note they came flashed as a Radeon VII which is interesting and they have the legit MI50 label too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Ear393"&gt; /u/Psychological_Ear393 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5bj66"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bj66/epyc_7532dual_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T22:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5dm1f</id>
    <title>A code generator, a code executor and a file manager, is all you need to build agents</title>
    <updated>2025-01-20T00:28:26+00:00</updated>
    <author>
      <name>/u/Better_Athlete_JJ</name>
      <uri>https://old.reddit.com/user/Better_Athlete_JJ</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Athlete_JJ"&gt; /u/Better_Athlete_JJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.slashml.com/blog/testing-autogen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5dm1f/a_code_generator_a_code_executor_and_a_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T00:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5bw2a</id>
    <title>Harbor App v0.2.24 officially supports Windows</title>
    <updated>2025-01-19T23:06:37+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt; &lt;img alt="Harbor App v0.2.24 officially supports Windows" src="https://external-preview.redd.it/am0wY2t1OWU3MWVlMXI8IEr-dnDizOwLz4sVhNUay1tQ6a6VeB4mfBu_sFj4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99e3dff894ae2bc37ef9362c0156daa7f9e2c0de" title="Harbor App v0.2.24 officially supports Windows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2syjnt9e71ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5bw2a/harbor_app_v0224_officially_supports_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T23:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5g15m</id>
    <title>Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)</title>
    <updated>2025-01-20T02:34:36+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt; &lt;img alt="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" src="https://external-preview.redd.it/Zzl3YnAxdXo4MmVlMaNDgHgQWM2mdDZB7xhNGcVXZbgcu-O9WP6fr_kyodHv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5641a7ad41d731780ed387b4453de0708dc550e" title="Working on a local TTS compatible PDF Reader (to use with Kokoro-82M)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gax0ckuz82ee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5g15m/working_on_a_local_tts_compatible_pdf_reader_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T02:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jlsr</id>
    <title>Deepseek-R1 and Deepseek-R1-zero repo is preparing to launchï¼Ÿ</title>
    <updated>2025-01-20T06:00:13+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am waiting for this. hopfully today&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jlsr/deepseekr1_and_deepseekr1zero_repo_is_preparing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T06:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5hc4s</id>
    <title>Most complex coding you done with AI</title>
    <updated>2025-01-20T03:45:57+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find AI super helpful in coding. Sonnet, o1 mini, Deepseek v3, llama 405, in that order. Or Qwen 32/14b locally. Generally using every day when coding.&lt;/p&gt; &lt;p&gt;It shines at 0 to 1 tasks, translation and some troubleshooting. Eg write an app that does this or do this in Rust, make this code typescript, ask what causes this error. Haven't had great experience so far once a project is established and has some form of internal framework, which always happens beyond certain size.&lt;/p&gt; &lt;p&gt;Asked all models to split 200 lines audio code in react into class with logic and react with the rest - most picked correct structure, but implementation missed some unique aspects and kinda started looking like any open source implementation on GitHub.. o1 did best, none were working. So wasn't a fit of even &amp;quot;low&amp;quot; complexity refactoring of a small code.&lt;/p&gt; &lt;p&gt;Share your experiences. What were the most complex tasks you were able to solve with AI? Some context like size of codebase, model would be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5hc4s/most_complex_coding_you_done_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T03:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i50lxx</id>
    <title>OpenAI has access to the FrontierMath dataset; the mathematicians involved in creating it were unaware of this</title>
    <updated>2025-01-19T15:13:21+00:00</updated>
    <author>
      <name>/u/LLMtwink</name>
      <uri>https://old.reddit.com/user/LLMtwink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JacquesThibs/status/1880770081132810283?s=19"&gt;https://x.com/JacquesThibs/status/1880770081132810283?s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The holdout set that the Lesswrong post &lt;em&gt;implies&lt;/em&gt; exists hasn't been developed yet&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/georgejrjrjr/status/1880972666385101231?s=19"&gt;https://x.com/georgejrjrjr/status/1880972666385101231?s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LLMtwink"&gt; /u/LLMtwink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i50lxx/openai_has_access_to_the_frontiermath_dataset_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T15:13:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1i55e2c</id>
    <title>OpenAI quietly funded independent math benchmark before setting record with o3</title>
    <updated>2025-01-19T18:35:34+00:00</updated>
    <author>
      <name>/u/Wonderful-Excuse4922</name>
      <uri>https://old.reddit.com/user/Wonderful-Excuse4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt; &lt;img alt="OpenAI quietly funded independent math benchmark before setting record with o3" src="https://external-preview.redd.it/xlDOicbjhIo2G3nyRsUTnPQOSIV2FHrGd9bBIWiOsiU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1c014bf7c19b4834c31105426529d342e2f69a7" title="OpenAI quietly funded independent math benchmark before setting record with o3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wonderful-Excuse4922"&gt; /u/Wonderful-Excuse4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://the-decoder.com/openai-quietly-funded-independent-math-benchmark-before-setting-record-with-o3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i55e2c/openai_quietly_funded_independent_math_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-19T18:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5m4t9</id>
    <title>letâ€™s goo, DeppSeek-R1 685 billion parameters!</title>
    <updated>2025-01-20T09:02:18+00:00</updated>
    <author>
      <name>/u/bymechul</name>
      <uri>https://old.reddit.com/user/bymechul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt; &lt;img alt="letâ€™s goo, DeppSeek-R1 685 billion parameters!" src="https://b.thumbs.redditmedia.com/9jF3vfuRP-Df2M-JGgdyrsvXvrMePxQdfuMlkImLCQs.jpg" title="letâ€™s goo, DeppSeek-R1 685 billion parameters!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131"&gt;https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bymechul"&gt; /u/bymechul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jh1u</id>
    <title>Deepseek R1 / R1 Zero</title>
    <updated>2025-01-20T05:51:39+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt; &lt;img alt="Deepseek R1 / R1 Zero" src="https://external-preview.redd.it/xCP95O-e963Wkcg4zsFa0x35jJRRGJ69TOc664LDsj0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfe6acc456fe810e684e2549f82a4f400608da67" title="Deepseek R1 / R1 Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T05:51:39+00:00</published>
  </entry>
</feed>
