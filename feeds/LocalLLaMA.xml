<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-03T22:36:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jqog3h</id>
    <title>How to implement citations in Web Search</title>
    <updated>2025-04-03T17:24:31+00:00</updated>
    <author>
      <name>/u/tilmx</name>
      <uri>https://old.reddit.com/user/tilmx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm implementing web search in my app (which is like ChatGPT Desktop, but with local mode and other providers). I've got a V1 working through Tavily and plan to layer in other web search providers (SearXNG, Google, Jina, etc.) over time. But there's one point I'm stuck on:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How do providers like Perplexity or OpenAI add the 'citations' at the relevant parts of the generated responses&lt;/strong&gt;? I can &lt;em&gt;ask&lt;/em&gt; the model to do this by appending something to the end of my prompt (i.e. &amp;quot;add citations in your response&amp;quot;), but that seems to produce mixed results- stochastic at best. Does anyone know a more deterministic, programmatic way to go about this?&lt;/p&gt; &lt;p&gt;Code is &lt;a href="https://github.com/synth-inc/onit/pull/176/files"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tilmx"&gt; /u/tilmx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqog3h/how_to_implement_citations_in_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqog3h/how_to_implement_citations_in_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqog3h/how_to_implement_citations_in_web_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T17:24:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqum8l</id>
    <title>Fairly simple coding question throwing off lot of smallish models</title>
    <updated>2025-04-03T21:20:25+00:00</updated>
    <author>
      <name>/u/gamesntech</name>
      <uri>https://old.reddit.com/user/gamesntech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have this bad CUDA code below that I wanted checked and corrected. A lot of models around the 20-30B range seem to fail. Most of them identify and address some of the &amp;quot;less serious&amp;quot; issues with the code but not identify and fix the main issue, which is move the cudaHello method out of main.&lt;/p&gt; &lt;p&gt;The latest Gemma 27B fails this miserably. Gemini Flash 1.5 and above of course, work fine.&lt;/p&gt; &lt;p&gt;The smaller Qwen2.5 Coder-14B fails, but the 32B version does work well.&lt;/p&gt; &lt;p&gt;Some of the models that do work can still produce some unnecessary code. Only some of them correctly identify and eliminate the whole malloc/free parts which are not required.&lt;/p&gt; &lt;p&gt;One notable exception in this range that works perfectly is Mistral-Small-24B.&lt;/p&gt; &lt;p&gt;These results were very surprising to me. If folks have any other smallish models handy can you please try this out on some of the latest versions?&lt;/p&gt; &lt;p&gt;Any thoughts on why simple code like this seems to trump so many models after all this time?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;does this code look right? if not, can you provide the corrected version? #include &amp;lt;iostream&amp;gt; #include &amp;lt;cuda.h&amp;gt; int main() { // Allocate on device char *dev; size_t numThreads = 1024; cudaMalloc(&amp;amp;dev, numThreads); // Kernel function __global__ void cudaHello() { int i = threadIdx.x; std::cout &amp;lt;&amp;lt; &amp;quot;Hello, CUDA! from thread &amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; std::endl; } // Launch kernel cudaLaunch(&amp;amp;cudaHello, numThreads); // Cleanup cudaFree(dev); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamesntech"&gt; /u/gamesntech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqum8l/fairly_simple_coding_question_throwing_off_lot_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqum8l/fairly_simple_coding_question_throwing_off_lot_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqum8l/fairly_simple_coding_question_throwing_off_lot_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T21:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqun8o</id>
    <title>Best PYTHON coding assist for RTX5070ti?</title>
    <updated>2025-04-03T21:21:33+00:00</updated>
    <author>
      <name>/u/AIgavemethisusername</name>
      <uri>https://old.reddit.com/user/AIgavemethisusername</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good evening all,&lt;/p&gt; &lt;p&gt;I intend to learn PYTHON and will be self teaching myself with the assistance of AI running on a RTX5070ti (16gb ram), card is being delivered tomorrow.&lt;/p&gt; &lt;p&gt;System is Ryzen 9700x with 64gb ram. (currenly using CPU gfx)&lt;/p&gt; &lt;p&gt;I’ve got Ollama installed and currently running on CPU only, using &lt;a href="http://Msty.app"&gt;Msty.app&lt;/a&gt; as the front end.&lt;/p&gt; &lt;p&gt;Ive been testing out qwen2.5-coder:32b this evening, and although its running quite slow on the CPU, it seems to be giving good results so far. It is, however using about 20GB ram, which is too much to run on the 5070ti.&lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What models are recommended for coding? – or have I randomly picked a good one with qwen?&lt;/li&gt; &lt;li&gt;If a model wont fit entirely on the GPU, will it ‘split’ and use system ram also? Or does it have to entirely fit on the GPU?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any other advice is welcome, I’m entirely new to this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIgavemethisusername"&gt; /u/AIgavemethisusername &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqun8o/best_python_coding_assist_for_rtx5070ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqun8o/best_python_coding_assist_for_rtx5070ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqun8o/best_python_coding_assist_for_rtx5070ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T21:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqhff1</id>
    <title>Personal experience with local&amp;commercial LLM's</title>
    <updated>2025-04-03T12:43:11+00:00</updated>
    <author>
      <name>/u/zoom3913</name>
      <uri>https://old.reddit.com/user/zoom3913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have the luxury of having 2x 3090's at home and access to MS Copilot / 4o / 4o-mini at work. I've used a load of models extensively the past couple of months; regarding the non-reasoning models, I value the models as follows;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--10B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Not really intelligent, makes lots of basic mistakes&lt;/em&gt; &lt;/li&gt; &lt;li&gt;&lt;em&gt;Doesn't follow instructions to the letter&lt;/em&gt; &lt;em&gt;However, really good at &amp;quot;vibe check&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Writing text that sounds good&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#1 Mistral Nemo&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--30B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Semi intelligent, can follow basic tasks without major mistakes For example, here's a list of people+phone number, and another list of people+address, combine the lists, give the phone and address of each person&lt;/em&gt; &lt;/li&gt; &lt;li&gt;&lt;em&gt;Very fast generation speed&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#3 Mistral Small&lt;/p&gt; &lt;p&gt;#2 Qwen2.5B 32B&lt;/p&gt; &lt;p&gt;#1 4o-mini&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--70B +-&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follows more complex tasks without major mistakes&lt;/li&gt; &lt;li&gt;Trade-off: lower generation speed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#3 Llama3.3 70B&lt;/p&gt; &lt;p&gt;#2 4o / Copilot, considering how much these costs in corporate settings, their performance is really disappointing &lt;/p&gt; &lt;p&gt;#1 Qwen2.5 72B&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--Even better;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Follows even more complex tasks without mistakes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;#4 DeepSeek V3&lt;/p&gt; &lt;p&gt;#3 Gemini models&lt;/p&gt; &lt;p&gt;#2 Sonnet 3.7; I actually prefer 3.5 to this&lt;/p&gt; &lt;p&gt;#1 DeepSeek V3 0324&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--Peak&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;#1 Sonnet 3.5&lt;/p&gt; &lt;p&gt;I think the picture is clear, basically, for a complex coding / data task I would confidently let Sonnet 3.5 do its job and return after a couple of minutes expecting a near perfect output.&lt;/p&gt; &lt;p&gt;DeepSeekV3 would need 2 iterations +-. A note here is that I think DS V3 0324 would suffice for 99% of the cases, but it's less usable due to timeouts / low generation speed. Gemini is a good, fast and cheap tradeoff.&lt;/p&gt; &lt;p&gt;70B models, probably 5 back and forths&lt;/p&gt; &lt;p&gt;For the 30B models even more, and probably I'll have to invest some thinking in order to simplify the problem so the LLM can solve it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoom3913"&gt; /u/zoom3913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhff1/personal_experience_with_localcommercial_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T12:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jq8zfk</id>
    <title>Open-WebUI Artifacts Overhaul has been updated to v0.6.0!</title>
    <updated>2025-04-03T04:11:32+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt; &lt;img alt="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" src="https://external-preview.redd.it/wZszwQ2U6yJ7pvC1CwegCQ8kArJM8Bhaojq_gfjcMsA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=784dcdbc7977acbb264691fc234770109ef75318" title="Open-WebUI Artifacts Overhaul has been updated to v0.6.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I just wanted to let you know that the Open-WebUI Artifacts Overhaul fork has been updated to match v0.6.0 of Open-Webui!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;https://github.com/nick-tonjum/open-webui-artifacts-overhaul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Don't know what the 'Artifacts Overhaul' branch is? It adds the following to open-webui:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🖼️ &lt;strong&gt;Coding Canvas&lt;/strong&gt;: Whenever a LLM outputs code, it will appear on the right side of the page with Monaco editor, similar to VSCode. Here you can cycle through different files produced via the LLM and also different versions&lt;/li&gt; &lt;li&gt;🔍 &lt;strong&gt;Difference Checker&lt;/strong&gt;: If a LLM makes changes to code, the differences will be highlight. This can be easily disabled or enabled via a single click!&lt;/li&gt; &lt;li&gt;🎨 &lt;strong&gt;Design Viewer&lt;/strong&gt;: Easily toggle between code view and design view with the click of a button! This currently supports HTML/CSS/JavaScript like before, but now with Tailwind styles built in. React components work too!&lt;/li&gt; &lt;li&gt;⚛️ &lt;strong&gt;React Visualizer&lt;/strong&gt;: As mentioned above, React components work too. This seems to work 80% of the time and I'm working hard to get it 100% of the time! As long as the code block has an export default it should work.&lt;/li&gt; &lt;li&gt;💼 &lt;strong&gt;Compacted Code&lt;/strong&gt;: When the canvas is open, code blocks in the regular chat are compacted and visualized as an attachment.&lt;/li&gt; &lt;li&gt;🌐 &lt;strong&gt;MANY supported languages&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to check it out. Hopefully someday this will end up in the main branch :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7lewes7wojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8b3a77a94287acbf414bb38e5e5f934d147ff2df"&gt;Difference Viewer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/7wbnf7kwojse1.gif"&gt;Cycle through multiple files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/is93kyswojse1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4502d68ce62e7e656fe050b71aeb0bfdf9fec8fe"&gt;React component viewer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jq8zfk/openwebui_artifacts_overhaul_has_been_updated_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T04:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqr48r</id>
    <title>Best place to check LLM Rankings?</title>
    <updated>2025-04-03T19:05:16+00:00</updated>
    <author>
      <name>/u/Dangerous-Stress732</name>
      <uri>https://old.reddit.com/user/Dangerous-Stress732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;I only know lmarena&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Stress732"&gt; /u/Dangerous-Stress732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqr48r/best_place_to_check_llm_rankings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqr48r/best_place_to_check_llm_rankings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqr48r/best_place_to_check_llm_rankings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T19:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqpw70</id>
    <title>Build local AI Agents and RAGs over your docs/sites in minutes now.</title>
    <updated>2025-04-03T18:18:56+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqpw70/build_local_ai_agents_and_rags_over_your/"&gt; &lt;img alt="Build local AI Agents and RAGs over your docs/sites in minutes now." src="https://external-preview.redd.it/yRO6a87xXfqB5nY-EMIOnUbobv9XobH0V6QgrfkAv5A.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4969d1eda1c511814f9d96d4b12dd67fb623f31d" title="Build local AI Agents and RAGs over your docs/sites in minutes now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;Following up on Rlama – many of you were interested in how quickly you can get a local RAG system running. The key now is the new **Rlama Playground**, our web UI designed to take the guesswork out of configuration.&lt;/p&gt; &lt;p&gt;Building RAG systems often involves juggling models, data sources, chunking parameters, reranking settings, and more. It can get complex fast! The Playground simplifies this dramatically.&lt;/p&gt; &lt;p&gt;The Playground acts as a user-friendly interface to visually configure your entire Rlama RAG setup before you even touch the terminal.&lt;/p&gt; &lt;p&gt;**Here's how you build an AI solution in minutes using it:**&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;**Select Your Model:** Choose any model available via **Ollama** (like llama3, gemma3, mistral) or **Hugging Face** directly in the UI.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Choose Your Data Source:**&lt;/p&gt; &lt;p&gt;* **Local Folder:** Just provide the path to your documents (./my\_project\_docs).&lt;/p&gt; &lt;p&gt;* **Website:** Enter the URL (&lt;a href="https://rlama.dev"&gt;https://rlama.dev&lt;/a&gt;), set crawl depth, concurrency, and even specify paths to exclude (/blog, /archive). You can also leverage sitemaps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**(Optional) Fine-Tune Settings:**&lt;/p&gt; &lt;p&gt;* **Chunking:** While we offer sensible defaults (Hybrid or Auto), you can easily select different strategies (Semantic, Fixed, Hierarchical), adjust chunk size, and overlap if needed. Tooltips guide you.&lt;/p&gt; &lt;p&gt;* **Reranking:** Enable/disable reranking (improves relevance), set a score threshold, or even specify a different reranker model – all visually.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Generate Command:** This is the magic button! Based on all your visual selections, the Playground instantly generates the precise rlama CLI command needed to build this exact RAG system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Copy &amp;amp; Run:**&lt;/p&gt; &lt;p&gt;* Click &amp;quot;Copy&amp;quot;.&lt;/p&gt; &lt;p&gt;* Paste the generated command into your terminal.&lt;/p&gt; &lt;p&gt;* Hit Enter. Rlama processes your data and builds the vector index.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;**Query Your Data:** Once complete (usually seconds to a couple of minutes depending on data size), run rlama run my\_website\_rag and start asking questions!&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;**That's it!** The Playground turns potentially complex configuration into a simple point-and-click process, generating the exact command so you can launch your tailored, local AI solution in minutes. No need to memorize flags or manually craft long commands.&lt;/p&gt; &lt;p&gt;It abstracts the complexity while still giving you granular control if you want it.&lt;/p&gt; &lt;p&gt;**Try the Playground yourself:**&lt;/p&gt; &lt;p&gt;* **Playground/Website:** [&lt;a href="https://rlama.dev/%5C%5D(https://rlama.dev/)"&gt;https://rlama.dev/\](https://rlama.dev/)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* **GitHub:** [&lt;a href="https://github.com/dontizi/rlama%5C%5D(https://github.com/dontizi/rlama)"&gt;https://github.com/dontizi/rlama\](https://github.com/dontizi/rlama)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you have any questions about using the Playground!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=LJm4E5U5GvQ&amp;amp;t=1s&amp;amp;ab_channel=Dontizi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqpw70/build_local_ai_agents_and_rags_over_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqpw70/build_local_ai_agents_and_rags_over_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T18:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqn570</id>
    <title>LocalScore - Local LLM Benchmark</title>
    <updated>2025-04-03T16:34:29+00:00</updated>
    <author>
      <name>/u/sipjca</name>
      <uri>https://old.reddit.com/user/sipjca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share &lt;a href="https://localscore.ai"&gt;LocalScore&lt;/a&gt; with y'all today. I love local AI and have been writing a local LLM benchmark over the past few months. It's aimed at being a helpful resource for the community in regards to how different GPU's perform on different models. &lt;/p&gt; &lt;p&gt;You can download it and give it a try here: &lt;a href="https://localscore.ai/download"&gt;https://localscore.ai/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code for both the benchmarking client and the website are both open source. This was very intentional so together we can make a great resrouce for the community through community feedback and contributions.&lt;/p&gt; &lt;p&gt;Overall the benchmarking client is pretty simple. I chose a set of tests which hopefully are fairly representative of how people will be using LLM's locally. Each test is a combination of different prompt and text generation lengths. We definitely will be taking community feedback to make the tests even better. It runs through these tests measuring:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Prompt processing speed (tokens/sec)&lt;/li&gt; &lt;li&gt;Generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;Time to first token (ms)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We then combine these three metrics into a single score called the LocalScore. The website is a database of results from the benchmark, allowing you to explore the performance of different models and hardware configurations.&lt;/p&gt; &lt;p&gt;Right now we are only supporting single GPUs for submitting results. You can have multiple GPUs but LocalScore will only run on the one of your choosing. Personally I am skeptical of the long term viability of multi GPU setups for local AI, similar to how gaming has settled into single GPU setups. However, if this is something you really want, open a GitHub discussion so we can figure out the best way to support it!&lt;/p&gt; &lt;p&gt;Give it a try! I would love to hear any feedback or contributions!&lt;/p&gt; &lt;p&gt;If you want to learn more, here are some links: - Website: &lt;a href="https://localscore.ai"&gt;https://localscore.ai&lt;/a&gt; - Demo video: &lt;a href="https://youtu.be/De6pA1bQsHU"&gt;https://youtu.be/De6pA1bQsHU&lt;/a&gt; - Blog post: &lt;a href="https://localscore.ai/blog"&gt;https://localscore.ai/blog&lt;/a&gt; - CLI Github: &lt;a href="https://github.com/Mozilla-Ocho/llamafile/tree/main/localscore"&gt;https://github.com/Mozilla-Ocho/llamafile/tree/main/localscore&lt;/a&gt; - Website Github: &lt;a href="https://github.com/cjpais/localscore"&gt;https://github.com/cjpais/localscore&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sipjca"&gt; /u/sipjca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://localscore.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqn570/localscore_local_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqn570/localscore_local_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T16:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jptset</id>
    <title>University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy</title>
    <updated>2025-04-02T17:04:49+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt; &lt;img alt="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" src="https://b.thumbs.redditmedia.com/nIuszN8uDMIjbhUsiZdUw2NeBO5my-uVcctXiMF1pcI.jpg" title="University of Hong Kong releases Dream 7B (Diffusion reasoning model). Highest performing open-source diffusion model to date. You can adjust the number of diffusion timesteps for speed vs accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jptset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jptset/university_of_hong_kong_releases_dream_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T17:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqvgj8</id>
    <title>Tenstorrent Launches Blackhole™ Developer Products at Tenstorrent Dev Day</title>
    <updated>2025-04-03T21:54:54+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqvgj8/tenstorrent_launches_blackhole_developer_products/"&gt; &lt;img alt="Tenstorrent Launches Blackhole™ Developer Products at Tenstorrent Dev Day" src="https://external-preview.redd.it/0TFIOcmgqMII4bL3YWw8Idip-V5qlu9MQ1vQPJ86i6A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5873af5b2b24b329f1d51e823e88d4d62c00a070" title="Tenstorrent Launches Blackhole™ Developer Products at Tenstorrent Dev Day" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tenstorrent.com/vision/tenstorrent-launches-blackhole-developer-products-at-tenstorrent-dev-day"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqvgj8/tenstorrent_launches_blackhole_developer_products/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqvgj8/tenstorrent_launches_blackhole_developer_products/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T21:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jql4ia</id>
    <title>Does anyone else kinda love the coil whine noise as the LLM spins up?</title>
    <updated>2025-04-03T15:16:17+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first time I heard the faint screech as a model started doing its thing, I was afraid my GPU was fucked up... a year later, I've come to almost see it as the dial up modem tone of yesteryear - a small sound that let me know good things are coming in just a moment! Seems like every model has its own little song, and the tones during inference on a Mac are very different than the ones I get out of my nvidia GPUs. It makes me weirdly nostalgic, and now it's almost a comforting indicator that things are working rather than a warning flag. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jql4ia/does_anyone_else_kinda_love_the_coil_whine_noise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T15:16:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqawj1</id>
    <title>Open Sourcing Latent Space Guardrails that catch 43% of Hallucinations</title>
    <updated>2025-04-03T06:05:16+00:00</updated>
    <author>
      <name>/u/Cautious_Hospital352</name>
      <uri>https://old.reddit.com/user/Cautious_Hospital352</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released fully open source latent space guardrails that monitor and stop unwelcome outputs of your LLM on the latent space level. Check it out here and happy to adopt it to your use case! &lt;a href="https://github.com/wisent-ai/wisent-guard"&gt;https://github.com/wisent-ai/wisent-guard&lt;/a&gt; On hallucinations it has not been trained on in TruthfulQA, this results in a 43% detection of hallucinations just from the activation patterns. You can use them to control the brain of your LLM and block it from outputting bad code, harmful outputs or taking decisions because of gender or racial bias. This is a new approach, different from circuit breakers or SAE-based mechanistic interpretability. We will be releasing a new version of the reasoning architecture based on latent space interventions soon to not only reduce hallucinations but use this for capabilities gain as well! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cautious_Hospital352"&gt; /u/Cautious_Hospital352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqawj1/open_sourcing_latent_space_guardrails_that_catch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T06:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqcj89</id>
    <title>YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!</title>
    <updated>2025-04-03T07:53:41+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt; &lt;img alt="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" src="https://external-preview.redd.it/NTZsOHM4NDNya3NlMWK-C_FeILIR1n-iDldyZrl0VqZ3vuhsUq5jbN6auZ0Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a07a6ac257f3a744208337e975f20df2a138d814" title="YourBench: Know which model is the best for your use case in less than 5 min, no matter the topic!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! clefourrier from HF's OpenEvals team! We open sourced YourBench yesterday, a custom synthetic evaluation framework: from any document, it creates a custom made QA set, then builds a leaderboard on your specific use case.&lt;/p&gt; &lt;p&gt;It works through multiple steps of chunking, summarization, LLM single and multi hop question and answer generation, validation, and so far we've found it works really well to generate interesting QAs! &lt;/p&gt; &lt;p&gt;You can use the demo as is, or customize and download it to run it with your favorite models: Best model for diverse questions is Qwen2.5-32B, and open model generating most grounded/valid questions is Gemma3-27B (just one place below o3-mini)! You can also set several seeds to augment diversity, complexity, etc. &lt;/p&gt; &lt;p&gt;This work has been carried by our intern, Sumuk, who had a great idea on how to dynamically generate eval sets, and we wrote a paper explaining the full method here: &lt;a href="https://huggingface.co/papers/2504.01833"&gt;https://huggingface.co/papers/2504.01833&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try it out here: &lt;a href="https://huggingface.co/spaces/yourbench/demo"&gt;https://huggingface.co/spaces/yourbench/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: Document -&amp;gt; custom made evaluation set -&amp;gt; leaderboard in 5 min&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xy7fgb43rkse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqcj89/yourbench_know_which_model_is_the_best_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T07:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqj9a7</id>
    <title>Fully Featured AI Coding Agent as MCP Server (or for local model)</title>
    <updated>2025-04-03T14:02:31+00:00</updated>
    <author>
      <name>/u/Left-Orange2267</name>
      <uri>https://old.reddit.com/user/Left-Orange2267</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been working like hell on this one: a fully capable Agent, as good or better than Windsurf's Cascade, Claude Code or Cursor's agent - but can be used for free.&lt;/p&gt; &lt;p&gt;It can run as an MCP server, so you can use it for free with Claude Desktop, and it can still fully understand a code base, even a very large one. We did this by using a language server instead of RAG to analyze code.&lt;/p&gt; &lt;p&gt;Can also run it on any model, including local ones.&lt;/p&gt; &lt;p&gt;Check it out, super easy to run, GPL license:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/oraios/serena"&gt;https://github.com/oraios/serena&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Left-Orange2267"&gt; /u/Left-Orange2267 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqj9a7/fully_featured_ai_coding_agent_as_mcp_server_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T14:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqip5f</id>
    <title>Security vulnerabilities with Ryzen AI / NPU CPUs</title>
    <updated>2025-04-03T13:38:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a bunch of recent &lt;a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-7037.html"&gt;security issues&lt;/a&gt; in the driver for the NPU, as well as related software. Basically, a malicious AI model could install malware on the local machine when executed via NPU. If the developer SDK is also installed when it could even easily get administrator permissions despite running via restricted account.&lt;/p&gt; &lt;p&gt;There's a &lt;a href="https://ryzenai.docs.amd.com/en/latest/inst.html"&gt;software update&lt;/a&gt; available where the issues have been fixed, but for downloading it you need to &lt;a href="https://account.amd.com/en/forms/downloads/amd-end-user-license-xef.html?filename=NPU_RAI1.4_GA_257_WHQL.zip"&gt;log in&lt;/a&gt; first. Basic drivers for your hardware should be freely accessible, especially when it's about security updates, and not kept behind a log in wall.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqip5f/security_vulnerabilities_with_ryzen_ai_npu_cpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T13:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqtcav</id>
    <title>llama.cpp discussion - Experimenting with custom quants</title>
    <updated>2025-04-03T20:30:21+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqtcav/llamacpp_discussion_experimenting_with_custom/"&gt; &lt;img alt="llama.cpp discussion - Experimenting with custom quants" src="https://external-preview.redd.it/46Rep-nLx-3vJHhS_PeWnbGdv0vLCjLkOmttYuCisu0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0990f8b73a03b0471a3890665aa7002573c0fda" title="llama.cpp discussion - Experimenting with custom quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/12741"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqtcav/llamacpp_discussion_experimenting_with_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqtcav/llamacpp_discussion_experimenting_with_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T20:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqrnx6</id>
    <title>Quasar Alpha on OpenRouter</title>
    <updated>2025-04-03T19:26:23+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New &amp;quot;cloaked&amp;quot; model. How do you think what it is?&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/openrouter/quasar-alpha"&gt;https://openrouter.ai/openrouter/quasar-alpha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Passes initial vibe check, but not sure about more complex tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrnx6/quasar_alpha_on_openrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrnx6/quasar_alpha_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrnx6/quasar_alpha_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T19:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqhgzq</id>
    <title>Confused with Too Many LLM Benchmarks, What Actually Matters Now?</title>
    <updated>2025-04-03T12:45:15+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to make sense of the constant benchmarks for new LLM advancements in 2025.&lt;br /&gt; Since the early days of GPT‑3.5, we've witnessed countless benchmarks and competitions — MMLU, HumanEval, GSM8K, HellaSwag, MLPerf, GLUE, etc.—and it's getting overwhelming .&lt;/p&gt; &lt;p&gt;I'm curious, so its the perfect time to ask the reddit folks: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;What’s your go-to benchmark?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;How do you stay updated on benchmark trends?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What Really Matters&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Your take on benchmarking in general&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I guess my question could be summarized to what genuinely indicate better performance vs. hype?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;feel free to share your thoughts, experiences or HOT Takes.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqhgzq/confused_with_too_many_llm_benchmarks_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T12:45:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqa182</id>
    <title>Llama 4 will probably suck</title>
    <updated>2025-04-03T05:12:21+00:00</updated>
    <author>
      <name>/u/klapperjak</name>
      <uri>https://old.reddit.com/user/klapperjak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been following meta FAIR research for awhile for my phd application to MILA and now knowing that metas lead ai researcher quit, I’m thinking it happened to dodge responsibility about falling behind basically.&lt;/p&gt; &lt;p&gt;I hope I’m proven wrong of course, but the writing is kinda on the wall.&lt;/p&gt; &lt;p&gt;Meta will probably fall behind and so will Montreal unfortunately 😔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klapperjak"&gt; /u/klapperjak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqa182/llama_4_will_probably_suck/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T05:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqfnmh</id>
    <title>Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding</title>
    <updated>2025-04-03T11:13:31+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"&gt; &lt;img alt="Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding" src="https://external-preview.redd.it/KfSCldbxB1IZkgQJUxzx3I66OmXlBpPyIYpJisUrymw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7b350abd537aa130a3efa570e482b7dc669f3843" title="Gemma 3 Reasoning Finetune for Creative, Scientific, and Coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tesslate/Synthia-S1-27b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqfnmh/gemma_3_reasoning_finetune_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T11:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqrlo8</id>
    <title>OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp; 20+ Rich Interactions</title>
    <updated>2025-04-03T19:24:01+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrlo8/oasis_opensourced_social_media_simulator_that/"&gt; &lt;img alt="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" src="https://preview.redd.it/knefnw7o7ose1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c525b8329c9751d8a985c8d5d18b3008a9c8066" title="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meet Matrix (the social simulation engine for Social Media)&lt;/p&gt; &lt;p&gt;Add any account&lt;br /&gt; Drop a post&lt;br /&gt; Let agents engage&lt;/p&gt; &lt;p&gt;Try tweeting as you are billgates and watch what Elon does&lt;/p&gt; &lt;p&gt;&lt;a href="http://matrix.eigent.ai/x"&gt;matrix.eigent.ai/x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/knefnw7o7ose1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrlo8/oasis_opensourced_social_media_simulator_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqrlo8/oasis_opensourced_social_media_simulator_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T19:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqo71p</id>
    <title>Google released Gemma 3 QAT, is this going to be better than Bartowski's stuff</title>
    <updated>2025-04-03T17:14:48+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqo71p/google_released_gemma_3_qat_is_this_going_to_be/"&gt; &lt;img alt="Google released Gemma 3 QAT, is this going to be better than Bartowski's stuff" src="https://external-preview.redd.it/JFjSsjeahuvIaxPX6j1v08RlgEsxBGES9Cq1BjvJRK0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76af7c6f7f705eee53c593494d93c7e7a4a2d821" title="Google released Gemma 3 QAT, is this going to be better than Bartowski's stuff" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqo71p/google_released_gemma_3_qat_is_this_going_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqo71p/google_released_gemma_3_qat_is_this_going_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T17:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqlkfp</id>
    <title>What are you guys waiting for in the AI world this month?</title>
    <updated>2025-04-03T15:33:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For me, it’s:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama 4&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 3&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek R2&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral’s new model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Diffusion LLM model API on OpenRouter&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqlkfp/what_are_you_guys_waiting_for_in_the_ai_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T15:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqef4d</id>
    <title>China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4</title>
    <updated>2025-04-03T10:00:10+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt; &lt;img alt="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" src="https://preview.redd.it/x9zbqai7flse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c47a9c1cd7cd7f716e71f6e9161de09e2cf497a4" title="China modded 48 GB RTX 4090 training video models at 720p with excellent speed and sold cheaper than RTX 5090 (only 32 GB) - Batch size 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x9zbqai7flse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqef4d/china_modded_48_gb_rtx_4090_training_video_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T10:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jqnnfp</id>
    <title>Official Gemma 3 QAT checkpoints (3x less memory for ~same performance)</title>
    <updated>2025-04-03T16:54:08+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! We got new official checkpoints from the Gemma team.&lt;/p&gt; &lt;p&gt;Today we're releasing quantization-aware trained checkpoints. This allows you to use q4_0 while retaining much better quality compared to a naive quant. You can go and use this model with llama.cpp today!&lt;/p&gt; &lt;p&gt;We worked with the llama.cpp and Hugging Face teams to validate the quality and performance of the models, as well as ensuring we can use the model for vision input as well. Enjoy!&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jqnnfp/official_gemma_3_qat_checkpoints_3x_less_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-03T16:54:08+00:00</published>
  </entry>
</feed>
