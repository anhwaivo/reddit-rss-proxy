<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-14T15:35:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jb1ck8</id>
    <title>Recommended ways and tools to fine-tune a pretrained model from the start (raw text + model) on 24 GB or less of VRAM</title>
    <updated>2025-03-14T11:08:05+00:00</updated>
    <author>
      <name>/u/GoodSamaritan333</name>
      <uri>https://old.reddit.com/user/GoodSamaritan333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I like to use Cydonia-24B-v2-GGUF to narrate stories. I created some alien races and worlds, described in unformatted text (txt file) and want to fine-tune the Cydonia model with it.&lt;/p&gt; &lt;p&gt;I tried following chatgpt and deepseek instructions with no success, for fine-tuning from the GGUF file.&lt;/p&gt; &lt;p&gt;Since Cydonia is available as safetensors, I will try finetune from it.&lt;/p&gt; &lt;p&gt;I'll be glad if someone can give me tips or point-me to a good tutorial for this case.&lt;/p&gt; &lt;p&gt;The PC at my reach is running Win 11 on a I7 11700, with 128 GB of RAM and a RTX 3090 Ti.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodSamaritan333"&gt; /u/GoodSamaritan333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1ck8/recommended_ways_and_tools_to_finetune_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1ck8/recommended_ways_and_tools_to_finetune_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1ck8/recommended_ways_and_tools_to_finetune_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jatq0e</id>
    <title>Transformers without Normalization</title>
    <updated>2025-03-14T02:33:06+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.10622"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jatq0e/transformers_without_normalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jatq0e/transformers_without_normalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T02:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb34lf</id>
    <title>1080 Ti vs 3060 12gb</title>
    <updated>2025-03-14T12:49:23+00:00</updated>
    <author>
      <name>/u/SirTwitchALot</name>
      <uri>https://old.reddit.com/user/SirTwitchALot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No, this isn't yet another &amp;quot;which card should I get post.&amp;quot;&lt;/p&gt; &lt;p&gt;I had a 3060 12gb, which doesn't have enough vram to run QwQ fully on GPU. I found a 1080 ti with 11gb at a decent price, so I decided to add it to my setup. Performance on QwQ is much improved compared to running partially in CPU. Still, I wondered how the performance compared between the two cards. I did a quick test in Phi 4 14.7b q4_K_M. Here are the results:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1080 ti:&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;total duration: 26.909615066s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 15.119614ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 14 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 142ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 98.59 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 675 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 26.751s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 25.23 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3060 12gb:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total duration: 20.234592581s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load duration: 25.785563ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval count: 14 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval duration: 147ms&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval rate: 95.24 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval count: 657 token(s)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval duration: 20.06s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval rate: 32.75 tokens/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So, based on this simple test, a 3060, despite being 2 generations newer, is only 30% faster than the 1080 ti in basic inference. The 3060 wins on power consumption, drawing a peak of 170w while the 1080 maxed out at 250. Still, an old 1080 could make a decent entry level card for running LLMs locally. 25 tokens/s on a 14b q4 model is quite useable. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SirTwitchALot"&gt; /u/SirTwitchALot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb34lf/1080_ti_vs_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb34lf/1080_ti_vs_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb34lf/1080_ti_vs_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T12:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jal0yx</id>
    <title>There it is https://github.com/SesameAILabs/csm</title>
    <updated>2025-03-13T19:53:22+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...almost. Hugginface link is still 404ing. Let's wait some minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jal0yx/there_it_is_httpsgithubcomsesameailabscsm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T19:53:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb4lhd</id>
    <title>New RAG docs &amp; AI assistant make it easy for non-coders to build RAGs</title>
    <updated>2025-03-14T14:01:26+00:00</updated>
    <author>
      <name>/u/DonTizi</name>
      <uri>https://old.reddit.com/user/DonTizi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The documentation of rlama, including all available commands and detailed examples, is now live on our website! But that’s not all—we’ve also introduced Rlama Chat, an AI-powered assistant designed to help you with your RAG implementations. Whether you have questions, need guidance, or are brainstorming new RAG use cases, Rlama Chat is here to support your projects.Have an idea for a specific RAG? Build it.Check out the docs and start exploring today!&lt;/p&gt; &lt;p&gt;You can go throught here if you have interest to make RAGs: &lt;a href="https://rlama.dev/"&gt;Website&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see a demo of Rlama Chat here: &lt;a href="https://x.com/LeDonTizi/status/1900544052107399573"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonTizi"&gt; /u/DonTizi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4lhd/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4lhd/new_rag_docs_ai_assistant_make_it_easy_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4lhd/new_rag_docs_ai_assistant_make_it_easy_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T14:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb24uz</id>
    <title>What's your favorite model for casual texting?</title>
    <updated>2025-03-14T11:54:40+00:00</updated>
    <author>
      <name>/u/HornyGooner4401</name>
      <uri>https://old.reddit.com/user/HornyGooner4401</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's your favorite model to talk casually with? Most people are focused on coding, benchmarks, or roleplay but I'm just trying to find a model that I can talk to casually. Probably something that can reply in shorter sentences, have general knowledge but doesn't always have to be right, talks naturally, maybe a little joke here and there, and preferably hallucinate personal experience (how their day went, going on a trip to Italy, working as a cashier for 2 years, etc.).&lt;/p&gt; &lt;p&gt;IIRC Facebook had a model that was trained on messages and conversations which worked somewhat well, but this was yeaaars ago before ChatGPT was even a thing. I suppose there should be better models by now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HornyGooner4401"&gt; /u/HornyGooner4401 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb24uz/whats_your_favorite_model_for_casual_texting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqpiu</id>
    <title>Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp</title>
    <updated>2025-03-14T00:01:14+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt; &lt;img alt="Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp" src="https://a.thumbs.redditmedia.com/OTEjpiItOaaKB7vj3kGES8cMsYYXecIqD-eyJx7Qt88.jpg" title="Mac Speed Comparison: M2 Ultra vs M3 Ultra using KoboldCpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Running ggufs in Koboldcpp, the M3 is marginally... slower? Slightly faster prompt processing, but slower prompt writing across all models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; &lt;em&gt;I added a comparison Llama.cpp run at the bottom; same speed as Kobold, give or take.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Inference engine: Koboldcpp 1.85.1&lt;/li&gt; &lt;li&gt;Text: Same text on ALL models. Token size differences are due to tokenizer differences&lt;/li&gt; &lt;li&gt;Temp: 0.01; all other samplers disabled&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Computers:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;M3 Ultra 512GB 80 GPU Cores&lt;/li&gt; &lt;li&gt;M2 Ultra 192GB 76 GPU Cores&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jfhw63feojoe1.png?width=464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=792fc47a7c5a6b51d619cefa7c6f83f31a4f438a"&gt;https://preview.redd.it/jfhw63feojoe1.png?width=464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=792fc47a7c5a6b51d619cefa7c6f83f31a4f438a&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Notes:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Qwen2.5 Coder and Llama 3.1 8b are more sensitive to temp than Llama 3.3 70b&lt;/li&gt; &lt;li&gt;All inference was first prompt after model load&lt;/li&gt; &lt;li&gt;All models are q8, as on Mac q8 is the fastest gguf quant &lt;em&gt;(see my previous posts on Mac speeds)&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Llama 3.1 8b q8&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12433/32768, Amt:386/4000, Init:0.02s, Process:13.56s (1.1ms/T = 888.55T/s), Generate:14.41s (37.3ms/T = 26.79T/s), Total:27.96s (13.80T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12408/32768, Amt:361/4000, Init:0.01s, Process:12.05s (1.0ms/T = 999.75T/s), Generate:13.62s (37.7ms/T = 26.50T/s), Total:25.67s (14.06T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Mistral Small 24b q8&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13300/32768, Amt:661/4000, Init:0.07s, Process:34.86s (2.8ms/T = 362.50T/s), Generate:45.43s (68.7ms/T = 14.55T/s), Total:80.29s (8.23T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13300/32768, Amt:661/4000, Init:0.04s, Process:31.97s (2.5ms/T = 395.28T/s), Generate:46.27s (70.0ms/T = 14.29T/s), Total:78.24s (8.45T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen2.5 32b Coder q8 with 1.5b speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13215/32768, Amt:473/4000, Init:0.06s, Process:59.38s (4.7ms/T = 214.59T/s), Generate:34.70s (73.4ms/T = 13.63T/s), Total:94.08s (5.03T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13271/32768, Amt:529/4000, Init:0.05s, Process:52.97s (4.2ms/T = 240.56T/s), Generate:43.58s (82.4ms/T = 12.14T/s), Total:96.55s (5.48T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Qwen2.5 32b Coder q8 WITHOUT speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13315/32768, Amt:573/4000, Init:0.07s, Process:53.44s (4.2ms/T = 238.42T/s), Generate:64.77s (113.0ms/T = 8.85T/s), Total:118.21s (4.85T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:13285/32768, Amt:543/4000, Init:0.04s, Process:49.35s (3.9ms/T = 258.22T/s), Generate:62.51s (115.1ms/T = 8.69T/s), Total:111.85s (4.85T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Llama 3.3 70b q8 with 3b speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.04s, Process:116.18s (9.6ms/T = 103.69T/s), Generate:54.99s (116.5ms/T = 8.58T/s), Total:171.18s (2.76T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.02s, Process:103.12s (8.6ms/T = 116.77T/s), Generate:63.74s (135.0ms/T = 7.40T/s), Total:166.86s (2.83T/s) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Llama 3.3 70b q8 WITHOUT speculative decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.03s, Process:104.74s (8.7ms/T = 115.01T/s), Generate:98.15s (207.9ms/T = 4.81T/s), Total:202.89s (2.33T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:12519/32768, Amt:472/4000, Init:0.01s, Process:96.67s (8.0ms/T = 124.62T/s), Generate:103.09s (218.4ms/T = 4.58T/s), Total:199.76s (2.36T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;#####&lt;/p&gt; &lt;h1&gt;Llama.cpp Server Comparison Run :: Llama 3.3 70b q8 WITHOUT Speculative Decoding&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 105195.24 ms / 12051 tokens ( 8.73 ms per token, 114.56 tokens per second) eval time = 78102.11 ms / 377 tokens ( 207.17 ms per token, 4.83 tokens per second) total time = 183297.35 ms / 12428 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;M3 Ultra&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 96696.48 ms / 12051 tokens ( 8.02 ms per token, 124.63 tokens per second) eval time = 82026.89 ms / 377 tokens ( 217.58 ms per token, 4.60 tokens per second) total time = 178723.36 ms / 12428 tokens &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqpiu/mac_speed_comparison_m2_ultra_vs_m3_ultra_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jabmwz</id>
    <title>AMA with the Gemma Team</title>
    <updated>2025-03-13T13:12:40+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama! During the next day, the Gemma research and product team from DeepMind will be around to answer with your questions! Looking forward to them!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Technical Report: &lt;a href="https://goo.gle/Gemma3Report"&gt;https://goo.gle/Gemma3Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AI Studio: &lt;a href="https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it"&gt;https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Technical blog post &lt;a href="https://developers.googleblog.com/en/introducing-gemma3/"&gt;https://developers.googleblog.com/en/introducing-gemma3/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3"&gt;https://www.kaggle.com/models/google/gemma-3&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d"&gt;https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jabmwz/ama_with_the_gemma_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T13:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb6gk7</id>
    <title>I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good</title>
    <updated>2025-03-14T15:23:51+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt; &lt;img alt="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" src="https://preview.redd.it/uc4ktdmraooe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=013fee6edf8a1814ec03199e5138b163065448da" title="I deleted all my previous models after using (Reka flash 3 , 21B model) this one deserve more attention, tested it in coding and its so good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4ktdmraooe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb6gk7/i_deleted_all_my_previous_models_after_using_reka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T15:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1janir5</id>
    <title>End of the Open LLM Leaderboard</title>
    <updated>2025-03-13T21:38:32+00:00</updated>
    <author>
      <name>/u/clefourrier</name>
      <uri>https://old.reddit.com/user/clefourrier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt; &lt;img alt="End of the Open LLM Leaderboard" src="https://external-preview.redd.it/uNVcFTJjErtGHcv_RU8nJOqopdFi5HpQXXuBPYA8mRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78ec61fc7857881ca74251621f68697e9ed6557a" title="End of the Open LLM Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clefourrier"&gt; /u/clefourrier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard/discussions/1135"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janir5/end_of_the_open_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jao3fg</id>
    <title>Qwq-32b just got updated Livebench.</title>
    <updated>2025-03-13T22:03:20+00:00</updated>
    <author>
      <name>/u/Amazing_Gate_9984</name>
      <uri>https://old.reddit.com/user/Amazing_Gate_9984</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt; &lt;img alt="Qwq-32b just got updated Livebench." src="https://b.thumbs.redditmedia.com/y-9o2Am61okZEVvbetInDUVF7Va9XDotcOFbj-bL_LI.jpg" title="Qwq-32b just got updated Livebench." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to the full results: &lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29"&gt;https://preview.redd.it/wvsprzpa5joe1.jpg?width=766&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=198d4ef5ec0b493a9d57dae2a989bdb5039d9f29&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Gate_9984"&gt; /u/Amazing_Gate_9984 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jao3fg/qwq32b_just_got_updated_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jauy8d</id>
    <title>Giving "native" tool calling to Gemma 3 (or really any model)</title>
    <updated>2025-03-14T03:40:19+00:00</updated>
    <author>
      <name>/u/logkn</name>
      <uri>https://old.reddit.com/user/logkn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 is great at following instructions, but doesn't have &amp;quot;native&amp;quot; tool/function calling. Let's change that (at least as best we can).&lt;/p&gt; &lt;p&gt;(Quick note, I'm going to be using Ollama as the example here, but this works equally well with Jinja templates, just need to change the syntax a bit.)&lt;/p&gt; &lt;h1&gt;Defining Tools&lt;/h1&gt; &lt;p&gt;Let's start by figuring out how 'native' function calling works in Ollama. Here's qwen2.5's chat template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{- if or .System .Tools }}&amp;lt;|im_start|&amp;gt;system {{- if .System }} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range .Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you think this looks like the second half of your average homebrew tool calling system prompt, you're spot on. &lt;strong&gt;This is literally appending markdown-formatted instructions on what tools are available and how to call them to the end of the system prompt.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Already, Ollama will recognize the tools you give it in the `tools` part of your OpenAI completions request, and inject them into the system prompt.&lt;/p&gt; &lt;h1&gt;Parsing Tools&lt;/h1&gt; &lt;p&gt;Let's scroll down a bit and see how tool call messages are handled:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;|im_start|&amp;gt;assistant {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments }}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;|im_end|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the &lt;strong&gt;tool call parser&lt;/strong&gt;. If the first token (or couple tokens) that the model outputs is &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;, Ollama handles the parsing of the tool calls. Assuming the model is decent at following instructions, &lt;em&gt;this means the tool calls will actually populate the&lt;/em&gt; &lt;code&gt;tool_calls&lt;/code&gt; &lt;em&gt;field rather than&lt;/em&gt; &lt;code&gt;content&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Demonstration&lt;/h1&gt; &lt;p&gt;So just for gits and shiggles, let's see if we can get Gemma 3 to call tools properly. I adapted the same concepts from qwen2.5's chat template to Gemma 3's chat template. Before I show that template, let me show you that it works.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import ollama def add_two_numbers(a: int, b: int) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot; Add two numbers Args: a: The first integer number b: The second integer number Returns: int: The sum of the two numbers &amp;quot;&amp;quot;&amp;quot; return a + b response = ollama.chat( 'gemma3-tools', messages=[{'role': 'user', 'content': 'What is 10 + 10?'}], tools=[add_two_numbers], ) print(response) # model='gemma3-tools' created_at='2025-03-14T02:47:29.234101Z' # done=True done_reason='stop' total_duration=19211740040 # load_duration=8867467023 prompt_eval_count=79 # prompt_eval_duration=6591000000 eval_count=35 # eval_duration=3736000000 # message=Message(role='assistant', content='', images=None, # tool_calls=[ToolCall(function=Function(name='add_two_numbers', # arguments={'a': 10, 'b': 10}))]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Booyah! Native function calling with Gemma 3.&lt;/p&gt; &lt;p&gt;It's not bullet-proof, mainly because it's not strictly enforcing a grammar. But assuming the model follows instructions, it should work *most* of the time.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Here's the template I used. It's very much like qwen2.5 in terms of the structure and logic, but using the tags of Gemma 3. Give it a shot, and better yet adapt this pattern to other models that you wish had tools.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{- if .Messages }} {{- if or .System .Tools }}&amp;lt;start_of_turn&amp;gt;user {{- if .System}} {{ .System }} {{- end }} {{- if .Tools }} # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags: &amp;lt;tools&amp;gt; {{- range $.Tools }} {&amp;quot;type&amp;quot;: &amp;quot;function&amp;quot;, &amp;quot;function&amp;quot;: {{ .Function }}} {{- end }} &amp;lt;/tools&amp;gt; For each function call, return a json object with function name and arguments within &amp;lt;tool_call&amp;gt;&amp;lt;/tool_call&amp;gt; XML tags: &amp;lt;tool_call&amp;gt; {&amp;quot;name&amp;quot;: &amp;lt;function-name&amp;gt;, &amp;quot;arguments&amp;quot;: &amp;lt;args-json-object&amp;gt;} &amp;lt;/tool_call&amp;gt; {{- end }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- range $i, $_ := .Messages }} {{- $last := eq (len (slice $.Messages $i)) 1 -}} {{- if eq .Role &amp;quot;user&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user {{ .Content }}&amp;lt;end_of_turn&amp;gt; {{ else if eq .Role &amp;quot;assistant&amp;quot; }}&amp;lt;start_of_turn&amp;gt;model {{ if .Content }}{{ .Content }} {{- else if .ToolCalls }}&amp;lt;tool_call&amp;gt; {{ range .ToolCalls }}{&amp;quot;name&amp;quot;: &amp;quot;{{ .Function.Name }}&amp;quot;, &amp;quot;arguments&amp;quot;: {{ .Function.Arguments}}} {{ end }}&amp;lt;/tool_call&amp;gt; {{- end }}{{ if not $last }}&amp;lt;end_of_turn&amp;gt; {{ end }} {{- else if eq .Role &amp;quot;tool&amp;quot; }}&amp;lt;start_of_turn&amp;gt;user &amp;lt;tool_response&amp;gt; {{ .Content }} &amp;lt;/tool_response&amp;gt;&amp;lt;end_of_turn&amp;gt; {{ end }} {{- if and (ne .Role &amp;quot;assistant&amp;quot;) $last }}&amp;lt;start_of_turn&amp;gt;model {{ end }} {{- end }} {{- else }} {{- if .System }}&amp;lt;start_of_turn&amp;gt;user {{ .System }}&amp;lt;end_of_turn&amp;gt; {{ end }}{{ if .Prompt }}&amp;lt;start_of_turn&amp;gt;user {{ .Prompt }}&amp;lt;end_of_turn&amp;gt; {{ end }}&amp;lt;start_of_turn&amp;gt;model {{ end }}{{ .Response }}{{ if .Response }}&amp;lt;end_of_turn&amp;gt;{{ end }}&amp;quot;&amp;quot;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logkn"&gt; /u/logkn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jauy8d/giving_native_tool_calling_to_gemma_3_or_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T03:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb4jcr</id>
    <title>Difference in Gemma 3 27b performance between ai studio and ollama</title>
    <updated>2025-03-14T13:58:51+00:00</updated>
    <author>
      <name>/u/Any-Mathematician683</name>
      <uri>https://old.reddit.com/user/Any-Mathematician683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt; &lt;p&gt;I am building an enterprise grade RAG application and looking for a open-source LLM model for Summarisation and Question-answering purposes.&lt;/p&gt; &lt;p&gt;I really liked the Gemma 3 27B model when i tried it on ai studio. It is summarising transcripts with great precision. Infact, performance on openrouter is also great.&lt;/p&gt; &lt;p&gt;But as &lt;strong&gt;I am trying it on ollama, it is giving me subpar performance compared to aistudio.&lt;/strong&gt; I have tried &lt;a href="https://ollama.com/library/gemma3:27b-it-fp16"&gt;27b-it-fp16&lt;/a&gt; model as well as I thought performance loss might be because of quantization.&lt;/p&gt; &lt;p&gt;I also went through this &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively"&gt;tutorial from Unsloth&lt;/a&gt;, and &lt;strong&gt;tried with recommended settings(temperature=1.0, top-k 64, top-p 0.95) on llama.cpp.&lt;/strong&gt; I did notice little better output but it is not as compared to output on openrouter / aistudio.&lt;/p&gt; &lt;p&gt;I noticed the &lt;strong&gt;same performance gap for command r models&lt;/strong&gt; between ollama and cohere playground.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Can you please help me in identifying the root cause for this?&lt;/strong&gt; I genuinely believe there has to be some reason behind it.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Mathematician683"&gt; /u/Any-Mathematician683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb4jcr/difference_in_gemma_3_27b_performance_between_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaqylp</id>
    <title>LLM must pass a skill check to talk to me</title>
    <updated>2025-03-14T00:13:29+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt; &lt;img alt="LLM must pass a skill check to talk to me" src="https://external-preview.redd.it/Y3U2cGt3NmNzam9lMRWrFMxzjmNxpTNLvYX1gtD81VUNlzdlH0AiqtOky6_L.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45045d6f36cca8e45bf1337eccb796748268735" title="LLM must pass a skill check to talk to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w7dney6csjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaqylp/llm_must_pass_a_skill_check_to_talk_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T00:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoc8n</id>
    <title>QwQ on LiveBench (update) - is better than DeepSeek R1!</title>
    <updated>2025-03-13T22:14:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt; &lt;img alt="QwQ on LiveBench (update) - is better than DeepSeek R1!" src="https://preview.redd.it/sb78tt607joe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22464da14ee7a94ffe6b7ad76b52c34bab00a921" title="QwQ on LiveBench (update) - is better than DeepSeek R1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sb78tt607joe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoc8n/qwq_on_livebench_update_is_better_than_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jahs0b</id>
    <title>OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch</title>
    <updated>2025-03-13T17:38:10+00:00</updated>
    <author>
      <name>/u/Qaxar</name>
      <uri>https://old.reddit.com/user/Qaxar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt; &lt;img alt="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" src="https://external-preview.redd.it/YuFnFIavAP98hFeGzOxLZQ1jrf6fXSzPC6RHQ4YO4ew.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2312e28fa573cb9d493e784a1275b4624e4c905" title="OpenAI calls DeepSeek 'state-controlled,' calls for bans on 'PRC-produced' models | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qaxar"&gt; /u/Qaxar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/03/13/openai-calls-deepseek-state-controlled-calls-for-bans-on-prc-produced-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jahs0b/openai_calls_deepseek_statecontrolled_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T17:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1janmn8</id>
    <title>SESAME IS HERE</title>
    <updated>2025-03-13T21:43:12+00:00</updated>
    <author>
      <name>/u/Straight-Worker-4327</name>
      <uri>https://old.reddit.com/user/Straight-Worker-4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sesame just released their 1B CSM.&lt;br /&gt; Sadly parts of the pipeline are missing.&lt;/p&gt; &lt;p&gt;Try it here:&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/sesame/csm-1b"&gt;https://huggingface.co/spaces/sesame/csm-1b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Installation steps here:&lt;br /&gt; &lt;a href="https://github.com/SesameAILabs/csm"&gt;https://github.com/SesameAILabs/csm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight-Worker-4327"&gt; /u/Straight-Worker-4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1janmn8/sesame_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T21:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jazfkf</id>
    <title>I created an OpenAI TTS compatible endpoint for Sesame CSM 1B</title>
    <updated>2025-03-14T08:48:44+00:00</updated>
    <author>
      <name>/u/RandomRobot01</name>
      <uri>https://old.reddit.com/user/RandomRobot01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a work in progress, especially around trying to normalize the voice/voices. &lt;/p&gt; &lt;p&gt;Give it a shot and let me know what you think. PR's welcomed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/phildougherty/sesame_csm_openai"&gt;https://github.com/phildougherty/sesame_csm_openai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomRobot01"&gt; /u/RandomRobot01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jazfkf/i_created_an_openai_tts_compatible_endpoint_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T08:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb5qvy</id>
    <title>KoboldCPP 1.86 just dropped with support of Gemma-3</title>
    <updated>2025-03-14T14:53:08+00:00</updated>
    <author>
      <name>/u/YordanTU</name>
      <uri>https://old.reddit.com/user/YordanTU</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.86"&gt;https://github.com/LostRuins/koboldcpp/releases/tag/v1.86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here it is. Just tried it, thank you guys!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YordanTU"&gt; /u/YordanTU &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb5qvy/koboldcpp_186_just_dropped_with_support_of_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T14:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaj6gc</id>
    <title>AI2 releases OLMo 32B - Truly open source</title>
    <updated>2025-03-13T18:35:40+00:00</updated>
    <author>
      <name>/u/Initial-Image-1015</name>
      <uri>https://old.reddit.com/user/Initial-Image-1015</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt; &lt;img alt="AI2 releases OLMo 32B - Truly open source" src="https://preview.redd.it/4puob2w24ioe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebfe792d0c0462bf8dcf9f5a45f17815829f617d" title="AI2 releases OLMo 32B - Truly open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&amp;quot;OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;OLMo is a fully open model: [they] release all artifacts. Training code, pre- &amp;amp; post-train data, model weights, and a recipe on how to reproduce it yourself.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Links: - &lt;a href="https://allenai.org/blog/olmo2-32B"&gt;https://allenai.org/blog/olmo2-32B&lt;/a&gt; - &lt;a href="https://x.com/natolambert/status/1900249099343192573"&gt;https://x.com/natolambert/status/1900249099343192573&lt;/a&gt; - &lt;a href="https://x.com/allen_ai/status/1900248895520903636"&gt;https://x.com/allen_ai/status/1900248895520903636&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial-Image-1015"&gt; /u/Initial-Image-1015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4puob2w24ioe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaj6gc/ai2_releases_olmo_32b_truly_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T18:35:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaxec3</id>
    <title>Sesame CSM 1B Voice Cloning</title>
    <updated>2025-03-14T06:18:15+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt; &lt;img alt="Sesame CSM 1B Voice Cloning" src="https://external-preview.redd.it/JaEGat2-q67uEqWoPpGo5Nx0tvU4ZMhHe5tLQNQxW9w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c285788573980e7289358fbf97c0847d9b60866" title="Sesame CSM 1B Voice Cloning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/csm-voice-cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaxec3/sesame_csm_1b_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T06:18:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb3mpe</id>
    <title>Gemma 3 Function Calling Example prompt</title>
    <updated>2025-03-14T13:14:57+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt; &lt;img alt="Gemma 3 Function Calling Example prompt" src="https://preview.redd.it/xv7wwtdmnnoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53b82d556c025c9987fe92be4363ea5c1a3d97b0" title="Gemma 3 Function Calling Example prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xv7wwtdmnnoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb3mpe/gemma_3_function_calling_example_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T13:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jaoy9g</id>
    <title>Meme i made</title>
    <updated>2025-03-13T22:41:19+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt; &lt;img alt="Meme i made" src="https://external-preview.redd.it/a3h0bzNwMWxiam9lMWaOI-rE6YlXiP74zpe4ixbVM_QsxQQzHzv1tNet8B-Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aee2797aa64747b42b65d38774f6590f3d0a9e9d" title="Meme i made" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzku6n1lbjoe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jaoy9g/meme_i_made/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-13T22:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1sgv</id>
    <title>Conclusion: Sesame has shown us a CSM. Then Sesame announced that it would publish... something. Sesame then released a TTS, which they obviously misleadingly and falsely called a CSM. Do I see that correctly?</title>
    <updated>2025-03-14T11:34:28+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It wouldn't have been a problem at all if they had simply said that it wouldn't be open source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1sgv/conclusion_sesame_has_shown_us_a_csm_then_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jb1tum</id>
    <title>HowTo: Decentralized LLM on Akash, IPFS &amp; Pocket Network, could this run LLaMA?</title>
    <updated>2025-03-14T11:36:40+00:00</updated>
    <author>
      <name>/u/era_hickle</name>
      <uri>https://old.reddit.com/user/era_hickle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt; &lt;img alt="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" src="https://external-preview.redd.it/Hrj-dOCVDQmxgV_iRUOvs_Xsy9EB5pShvqJs-R97RdI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6f0426a6f366fea7132175fe3881893d8a8b01b" title="HowTo: Decentralized LLM on Akash, IPFS &amp;amp; Pocket Network, could this run LLaMA?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/era_hickle"&gt; /u/era_hickle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pocket.network/case-study-building-a-decentralized-deepseek-combining-open-data-compute-and-reasoning-with-pocket-network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jb1tum/howto_decentralized_llm_on_akash_ipfs_pocket/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-14T11:36:40+00:00</published>
  </entry>
</feed>
