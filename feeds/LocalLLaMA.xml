<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-08T14:48:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ikn434</id>
    <title>Your go to option for finet-uning LLMs</title>
    <updated>2025-02-08T13:20:40+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is your preferred go to option for fine-tuning LLMs?&lt;/p&gt; &lt;p&gt;I am currently using google colab free version but it is very limited. tried kaggle as well but facing issues with OOM error with VRAM.&lt;br /&gt; For a paid version is colab pro worth it? or are there any better options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn434/your_go_to_option_for_finetuning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn434/your_go_to_option_for_finetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn434/your_go_to_option_for_finetuning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:20:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijx1rh</id>
    <title>A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes</title>
    <updated>2025-02-07T15:05:55+00:00</updated>
    <author>
      <name>/u/umjustpassingby</name>
      <uri>https://old.reddit.com/user/umjustpassingby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"&gt; &lt;img alt="A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes" src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="A script to run a full-model GRPO training of Qwen2.5 0.5B on a free Google Colab T4. +25% on gsm8k eval in just 30 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umjustpassingby"&gt; /u/umjustpassingby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/qunash/820c86d1d267ec8051d9f68b4f4bb656"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijx1rh/a_script_to_run_a_fullmodel_grpo_training_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iknqbp</id>
    <title>Pc upgradation</title>
    <updated>2025-02-08T13:53:07+00:00</updated>
    <author>
      <name>/u/Particular_Garbage32</name>
      <uri>https://old.reddit.com/user/Particular_Garbage32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have an i7 12th generation processor, 16GB of RAM, and a GTX 1650 GPU. I'm working on projects to train a generative AI model for a specific task. Can you recommend a good upgrade for my system, perhaps for 24B&lt;/p&gt; &lt;p&gt;Sorry if I'm asking a dumb question. I'm a beginner in this field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular_Garbage32"&gt; /u/Particular_Garbage32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknqbp/pc_upgradation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknqbp/pc_upgradation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iknqbp/pc_upgradation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijw4l5</id>
    <title>Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism</title>
    <updated>2025-02-07T14:24:24+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"&gt; &lt;img alt="Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism" src="https://external-preview.redd.it/qPGhPtldrPs_tjiplZvAwSzWgSrwQ8e0HK8Z8gzfBS0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9bac89a4ad2d360d4f5ad9a4962d0d4f44fddb3d" title="Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ahmadosman.com/blog/do-not-use-llama-cpp-or-ollama-on-multi-gpus-setups-use-vllm-or-exllamav2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T14:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik9e54</id>
    <title>KokoroSharp - Local TTS in C#</title>
    <updated>2025-02-07T23:46:25+00:00</updated>
    <author>
      <name>/u/Lyrcaxis</name>
      <uri>https://old.reddit.com/user/Lyrcaxis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, to start with, I am working on a fully offline AI voice chat app, and while it's about 90% ready to release, a specific new, high-performance audio model came out (*stares at Kokoro*).&lt;/p&gt; &lt;p&gt;What did I do?&lt;/p&gt; &lt;p&gt;I dropped everything to build a local, cross-platform TTS engine! Beginner-friendly yet flexible.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;KokoroTTS tts = KokoroTTS.LoadModel(); KokoroVoice heartVoice = KokoroVoiceManager.GetVoice(&amp;quot;af_heart&amp;quot;); while (true) { tts.SpeakFast(Console.ReadLine(), heartVoice); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's available on &lt;a href="https://www.nuget.org/packages/KokoroSharp/"&gt;NuGet&lt;/a&gt;! Just install the package and you're ready!&lt;/p&gt; &lt;p&gt;I really hope people like it! And, of course, the source is open: &lt;a href="https://github.com/Lyrcaxis/KokoroSharp"&gt;https://github.com/Lyrcaxis/KokoroSharp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lyrcaxis"&gt; /u/Lyrcaxis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik9e54/kokorosharp_local_tts_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikbdwo</id>
    <title>Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems</title>
    <updated>2025-02-08T01:20:28+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"&gt; &lt;img alt="Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems" src="https://external-preview.redd.it/ekkK7Py0xlT-lqYgqhKU3OnrjxNRYtCzeCtltKuOicY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c6b1caf9d5319269f2c5edca00d06f1f8fb9382" title="Possible solution for poor token generation performance in llama.cpp on dual AMD Epyc systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/issues/11744"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikbdwo/possible_solution_for_poor_token_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T01:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikk8y5</id>
    <title>vLLM serving LLAMA 3.3 70B and Langflow: how to make my functions callable as tools from agent?</title>
    <updated>2025-02-08T10:15:24+00:00</updated>
    <author>
      <name>/u/UnlikelyBite</name>
      <uri>https://old.reddit.com/user/UnlikelyBite</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;disclaimer: 15+ years of programming background but almost a noob in llm.&lt;/p&gt; &lt;p&gt;This is the command i use to start vllm and serve llama 3.3: &lt;/p&gt; &lt;p&gt;&lt;code&gt;--model meta-llama/Llama-3.3-70B-Instruct --max-model-len 8192 --port 8000 --tensor-parallel-size 2 --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template examples/tool_chat_template_llama3.1_json.jinja&lt;/code&gt;&lt;/p&gt; &lt;p&gt;What i'm trying to do is to build on top of the llm, a multi agent workflow in langflow that can do for example: query on my sql database, execute python code, etc...&lt;/p&gt; &lt;p&gt;the &amp;quot;strange&amp;quot; thing is that when i use OpenAI (aka ClosedAI) the tool calling agent works without any issue and very well...when i change the llm to llama 3.3 the tools are not invoked in the right order or with the required arguments, making the response from llm unusable or completely hallucinated. &lt;/p&gt; &lt;p&gt;I’m curious if anyone has implemented a similar setup or has an alternative strategy for integrating agents with tool calls using open source models (for example llama or something like that). Is this approach valid or a complete mess? Are there improvements or pitfalls I should be aware of?&lt;/p&gt; &lt;p&gt;Thanks in advance for any feedback or shared experiences! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnlikelyBite"&gt; /u/UnlikelyBite &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikk8y5/vllm_serving_llama_33_70b_and_langflow_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T10:15:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iknxmq</id>
    <title>What are the major improvements from 2017 that lead to current SOTA LLM?</title>
    <updated>2025-02-08T14:03:20+00:00</updated>
    <author>
      <name>/u/Doug_Fripon</name>
      <uri>https://old.reddit.com/user/Doug_Fripon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to update my knowledge on transformers architecture since the foundational Attention is all you need paper from 2017. I'm struggling to find (or generate) a synthetic trustful resource that provides a high-level picture of the major improvements to the SOTA since then.&lt;/p&gt; &lt;p&gt;Can we identify the major LLM architectural evolutions from the last few years? I suggest we don't cover the multimodal topics unless directly applicable to LLM. &lt;/p&gt; &lt;p&gt;For example, the RoPE paper from 2021 &lt;a href="https://arxiv.org/pdf/2104.09864"&gt;https://arxiv.org/pdf/2104.09864&lt;/a&gt; that introduces rotary position embeddings seems a major update that reduces the dependency of explicit position encoding into the embeddings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doug_Fripon"&gt; /u/Doug_Fripon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iknxmq/what_are_the_major_improvements_from_2017_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iji47x</id>
    <title>All DeepSeek, all the time.</title>
    <updated>2025-02-07T00:29:14+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt; &lt;img alt="All DeepSeek, all the time." src="https://preview.redd.it/vnyyv4a93mhe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2c0ce4fb12db9cd74a7f55ee3931d93b15253d" title="All DeepSeek, all the time." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnyyv4a93mhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T00:29:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxefw</id>
    <title>Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)</title>
    <updated>2025-02-07T15:21:31+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"&gt; &lt;img alt="Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)" src="https://external-preview.redd.it/lhNDcosywXktXr0xSigp9rZjY66RKm_rrzGjuSCPQUg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef93da1c5f0005d9bc3c8006030c6226f627ddbb" title="Cerebras brings instant inference to Mistral Le Chat (Mistral Large 2 @ 1100 tokens/s)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The collaboration between Cerebras and Mistral has yielded a significant breakthrough in AI inference speed with the integration of Cerebras Inference into Mistral's Le Chat platform. The system achieves an unprecedented 1,100 tokens per second for text generation using the 123B parameter Mistral Large 2 model, representing a 10x performance improvement over competing AI assistants like ChatGPT 4o (115 tokens/s) and Claude Sonnet 3.5 (71 tokens/s). This exceptional speed is achieved through a combination of Cerebras's Wafer Scale Engine 3 technology, which utilizes an SRAM-based inference architecture, and speculative decoding techniques developed in partnership with Mistral researchers. The feature, branded as &amp;quot;Flash Answers,&amp;quot; is currently focused on text-based queries and is visually indicated by a lightning bolt icon in the chat interface.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cerebras.ai/blog/mistral-le-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxefw/cerebras_brings_instant_inference_to_mistral_le/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:21:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikba3x</id>
    <title>Simple RAG pipeline: Fully dockerized, completely open source.</title>
    <updated>2025-02-08T01:15:17+00:00</updated>
    <author>
      <name>/u/NewspaperSea9851</name>
      <uri>https://old.reddit.com/user/NewspaperSea9851</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, just built out a v0 of a fairly basic RAG implementation. The goal is to have a solid starting workflow from which to branch off and customize to your specific tasks.&lt;/p&gt; &lt;p&gt;If you're looking for a starting point for a solid production-grade RAG implementation - would love for you to check out: &lt;a href="https://github.com/Emissary-Tech/legit-rag"&gt;https://github.com/Emissary-Tech/legit-rag&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewspaperSea9851"&gt; /u/NewspaperSea9851 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikba3x/simple_rag_pipeline_fully_dockerized_completely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T01:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iko5y3</id>
    <title>How do the models always answer in correct English when much of the web has badly written and incorrect English?</title>
    <updated>2025-02-08T14:15:17+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering how the training works to achieve this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iko5y3/how_do_the_models_always_answer_in_correct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T14:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn3m4</id>
    <title>Qwen in the mac menu bar</title>
    <updated>2025-02-08T13:20:00+00:00</updated>
    <author>
      <name>/u/Zealousideal-Net1385</name>
      <uri>https://old.reddit.com/user/Zealousideal-Net1385</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt; &lt;img alt="Qwen in the mac menu bar" src="https://external-preview.redd.it/noY1h5Okrq7ZANvEw5MMNJGxiH9lTWE3nW0dM83KMT8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e4ce9351cd888be2d92a844b79e910f95cc4e2d" title="Qwen in the mac menu bar" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear all, I Developed this app for Mac OS and I need some testers; since I love Qwen family models I’ve developed this app to enrich the productivity, working as a floating window over other apps. Comments are really appreciated! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Net1385"&gt; /u/Zealousideal-Net1385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/andreaturchet/Qwen4Mac"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn3m4/qwen_in_the_mac_menu_bar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikmv9x</id>
    <title>Podcasts with TinyLlama and Kokoro on iOS</title>
    <updated>2025-02-08T13:07:14+00:00</updated>
    <author>
      <name>/u/derjanni</name>
      <uri>https://old.reddit.com/user/derjanni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Llama friends,&lt;/p&gt; &lt;p&gt;around a month ago I was on a flight back to Germany and hastily downloaded Podcasts before departure. Once airborne, I found all of them boring which had me sitting bored on a four hour flight. I had no coverage and the ones I had stored in the device turned out to be not really what I was into. That got me thiniking and I wanted to see if you could generate podcasts offline on my iPhone.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; before I get into the details, Botcast was approved by Apple an hour ago. Check it out if you are interested.&lt;/p&gt; &lt;h1&gt;The challenge of generating podcasts&lt;/h1&gt; &lt;p&gt;I wanted an app that works offline and generates podcasts with decent voices. I went with TinyLlama 1.1B Chat v1.0 Q6_K to generate the podcasts. My initial attempt was to generate each spoken line with an individual prompt, but it turned out that just prompting TinyLlama to generate a podcast transcript just worked fine. The podcasts are all chats between two people for which gender, name and voice are randomly selected.&lt;/p&gt; &lt;p&gt;The entire process of generating the transcript takes around a minute on my iPhone 14, much faster on the 16 Pro and around 3-4 minutes on the SE 2020. For the voices, I went with Kokoro 0.19 since these voices seem to be the best quality I could find that work on iOS. After some testing, I threw out the UK voices since those sounded much too robotic.&lt;/p&gt; &lt;h1&gt;Technical details of Botcast&lt;/h1&gt; &lt;p&gt;Botcast is a native iOS app built with Xcode and written in Swift and SwiftUI. However, the majority of it is C/C++ simple because of llama.cpp for iOS and the necessary inference libraries for Kokoro on iOS. A ton of bridging between Swift and the frameworks, libraries is involved. That's also why I went with 18.2 minimum as stability on earlies iOS versions is just way too much work to ensure.&lt;/p&gt; &lt;p&gt;And as with all the audio stuff I did before, the app is brutally multi-threading both on the CPU, the Metal GPU and the Neural Core Engines. The app will need around 1.3 GB of RAM and hence has the entitlement to increase up to 3GB on iPhone 14, up to 1.4GB on SE 2020. Of course it also uses the extended memory areas of the GPU. Around 80% of bugfixing was simply getting the memory issues resolved.&lt;/p&gt; &lt;p&gt;When I first got it into TestFlight it simply crashed when Apple reviewed it. It wouldn't even launch. I had to upgrade some inference libraries and fiddle around with their instanciation. It's technically hitting the limits of the iPhone 14, but anything above that is perfectly smooth from my experience. Since it's also Mac Catalyst compatible, it works like a charm on my M1 Pro.&lt;/p&gt; &lt;h1&gt;Future of Botcast&lt;/h1&gt; &lt;p&gt;Botcast is currently free and I intent to keep it like that. Next step is CarPlay support which I definitely want as well as Siri integration for &amp;quot;Generate&amp;quot;. The idea is to have it do its thing completely hands free. Further, the inference supports streaming, so exploring the option to really have the generate and the playback run instantly to provide really instant real-time podcasts is also on the list.&lt;/p&gt; &lt;p&gt;Botcast was a lot of work and I am potentially looking into maybe giving it some customizing in the future and just charge a one-time fee for a pro version (e.g. custom prompting, different flavours of podcasts with some exclusive to a pro version). Pricing wise, a pro version will probably become something like $5 one-time fee as I'm totally not a fan of subscriptions for something that people run on their devices.&lt;/p&gt; &lt;p&gt;Let me know what you think about Botcast, what features you'd like to see or any questions you have. I'm totally excited and into Ollama, llama.cpp and all the stuff around it. It's just pure magical what you can do with llama.cpp on iOS. Performance is really strong even with Q6_K quants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derjanni"&gt; /u/derjanni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikmv9x/podcasts_with_tinyllama_and_kokoro_on_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:07:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijxdue</id>
    <title>Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser.</title>
    <updated>2025-02-07T15:20:49+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt; &lt;img alt="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." src="https://external-preview.redd.it/eXpiZzdyaDVpcWhlMePeQo88FDwgFQaiUAHhHRFDa4M37cixJTBs9Mic6GzX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3934e49e8a9ed14426802e792f696c88e956d015" title="Kokoro WebGPU: Real-time text-to-speech running 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5b2t6sh5iqhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijxdue/kokoro_webgpu_realtime_texttospeech_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T15:20:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikek15</id>
    <title>My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload</title>
    <updated>2025-02-08T04:03:51+00:00</updated>
    <author>
      <name>/u/bo_peng</name>
      <uri>https://old.reddit.com/user/bo_peng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt; &lt;img alt="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" src="https://a.thumbs.redditmedia.com/Yk1sCOddUTNzMn5fOag5QoE4yeshLWIZhFYJ14-t3u0.jpg" title="My DeepSeek R1 671B @ Home plan: CPU+GPU hybrid, 4xGen5 NVMe offload" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321"&gt;https://preview.redd.it/uggf4ioi9uhe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=527eb75829f56baaaa8c207f009cb4833e5de321&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now waiting for 4060ti 16G to arrive. Requires lots of custom code to efficiently utilize this chimera setup :) So stay tuned. I think it can reach 10+ token/s for quantized 671B after optimizations.&lt;/p&gt; &lt;p&gt;You can use &amp;quot;ASUS Hyper M.2 x16 Gen5 Card&amp;quot; to host 4 NVME. And currently you need AMD CPUs to do native x4x4x4x4 bifurcation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bo_peng"&gt; /u/bo_peng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikek15/my_deepseek_r1_671b_home_plan_cpugpu_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T04:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikjrf3</id>
    <title>OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp; also xAI | iLya &amp; elon both were OG cofounders of OpenAI</title>
    <updated>2025-02-08T09:40:24+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt; &lt;img alt="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" src="https://preview.redd.it/wojbv3ufyvhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672d5dd30929a3d9877d2f0489d521aeb4f36140" title="OpenAI asks to Marginalise ILya as He asks investors for it's SSI &amp;amp; also xAI | iLya &amp;amp; elon both were OG cofounders of OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wojbv3ufyvhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikjrf3/openai_asks_to_marginalise_ilya_as_he_asks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T09:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik98l2</id>
    <title>I haven't seen many quad GPU setups so here is one</title>
    <updated>2025-02-07T23:39:23+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt; &lt;img alt="I haven't seen many quad GPU setups so here is one" src="https://preview.redd.it/lav3msh9zshe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a16836ce930a2602c2271a999be6be9f7ee678b" title="I haven't seen many quad GPU setups so here is one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lav3msh9zshe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik98l2/i_havent_seen_many_quad_gpu_setups_so_here_is_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T23:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik76bj</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-02-07T22:08:04+00:00</updated>
    <author>
      <name>/u/Singularity-42</name>
      <uri>https://old.reddit.com/user/Singularity-42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/juqrkouwishe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df7513b32ac2840f23b550e538e3f7b2c6c6ca3" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularity-42"&gt; /u/Singularity-42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/juqrkouwishe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik76bj/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T22:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikgsl6</id>
    <title>Germany: "We released model equivalent to R1 back in November, no reason to worry"</title>
    <updated>2025-02-08T06:14:46+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt; &lt;img alt="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" src="https://b.thumbs.redditmedia.com/mRAyAl4yIV32IK8PFM79kLhWfUBplZg1xnMZQcGaLLI.jpg" title="Germany: &amp;quot;We released model equivalent to R1 back in November, no reason to worry&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ikgsl6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikgsl6/germany_we_released_model_equivalent_to_r1_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikn5fg</id>
    <title>Glyphstral-24b: Symbolic Deductive Reasoning Model</title>
    <updated>2025-02-08T13:22:41+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt; &lt;img alt="Glyphstral-24b: Symbolic Deductive Reasoning Model" src="https://external-preview.redd.it/DghrhJAW-NKneHTJvXZ7IAcBmIpZ_fU36ahUXITL0bM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad5bc295749b594e323b350912e29031d02d474" title="Glyphstral-24b: Symbolic Deductive Reasoning Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone! &lt;/p&gt; &lt;p&gt;So I've been really obsessed lately with symbolic AI and the potential to improve reasoning and multi-dimensional thinking. I decided to go ahead and see if I could train a model to use a framework I am calling &amp;quot;Glyph Code Logic Flow&amp;quot;. &lt;/p&gt; &lt;p&gt;Essentially, it is a method of structured reasoning using deductive symbolic logic. You can learn more about it here &lt;a href="https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main"&gt;https://github.com/severian42/Computational-Model-for-Symbolic-Representations/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I first tried training Deepeek R1-Qwen-14 and QWQ-32 but their heavily pre-trained reasoning data seemed to conflict with my approach, which makes sense given the different concepts and ways of breaking down the problem.&lt;/p&gt; &lt;p&gt;I opted for Mistral-Small-24b to see the results, and after 7 days of pure training 24hrs a day (all locally using MLX-Dora at 4bit on my Mac M2 128GB). In all, the model trained on about 27mil tokens of my custom GCLF dataset (each example was around 30k tokens, with a total of 4500 examples)&lt;/p&gt; &lt;p&gt;I still need to get the docs and repo together, as I will be releasing it this weekend, but I felt like sharing a quick preview since this unexpectedly worked out awesomely.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player"&gt;https://reddit.com/link/1ikn5fg/video/9h2mgdg02xhe1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikn5fg/glyphstral24b_symbolic_deductive_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ik162w</id>
    <title>Trump just said “no” DeepSeek does not pose a national security threat at a press conference</title>
    <updated>2025-02-07T17:57:11+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt; &lt;img alt="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" src="https://preview.redd.it/73sost17arhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d90041a44ec176a4b5a3630a47ce33577c957522" title="Trump just said “no” DeepSeek does not pose a national security threat at a press conference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73sost17arhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ik162w/trump_just_said_no_deepseek_does_not_pose_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikguu9</id>
    <title>"Meta Torrented over 81 TB of Data Through Anna’s Archive, Despite Few Seeders"</title>
    <updated>2025-02-08T06:18:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://torrentfreak.com/meta-torrented-over-81-tb-of-data-through-annas-archive-despite-few-seeders-250206/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikguu9/meta_torrented_over_81_tb_of_data_through_annas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iklym8</id>
    <title>What fictional characters are going to get invented first; like this one⬇️‽</title>
    <updated>2025-02-08T12:13:17+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iklym8/what_fictional_characters_are_going_to_get/"&gt; &lt;img alt="What fictional characters are going to get invented first; like this one⬇️‽" src="https://external-preview.redd.it/bzVmejV4MnFwd2hlMa0OkiSb4WLV-Gm6A6FIunAxb3qUiDNWa7wR9OnjYSm8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=634753fb83b98461c09dd6e6af0fbba365a8a81e" title="What fictional characters are going to get invented first; like this one⬇️‽" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0rvfcr2qpwhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iklym8/what_fictional_characters_are_going_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iklym8/what_fictional_characters_are_going_to_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T12:13:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ikh3vz</id>
    <title>OpenAI is hiding the actual thinking tokens in o3-mini</title>
    <updated>2025-02-08T06:35:07+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt; &lt;img alt="OpenAI is hiding the actual thinking tokens in o3-mini" src="https://external-preview.redd.it/7p74lVWOq7GrqAnrRqOdU-BQFqsEnbk8XozOhsD2YQc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65833b846107b44ca5029ac644441d85516e7126" title="OpenAI is hiding the actual thinking tokens in o3-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Yesterday I discovered that in fact you can not even with the current &amp;quot;thinking&amp;quot; tag on ChatGPT UI have the actual thinking tokens. In a way, the thinking of the model.&lt;/p&gt; &lt;p&gt;How did i know it? By running one single simple prompt about &amp;quot;maze puzzle&amp;quot; over the two models. The sample can be found in this dataset we built at our company. &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;The prompt is a question about solving a maze: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 got it done in 483 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2vc5mhbqxuhe1.png?width=1690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=611419e379856111070a3094ac3fbe7bac397381"&gt;Deepseek-R1 giving the correct result&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT 53 seconds:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yoxgrs11yuhe1.png?width=1706&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f106a5f1553b8da728d9ca05dd32ce3702ecd5f"&gt;ChatGPT result also correct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Surprisingly both results are correct but the devil is in the detail. When I inspect Deepseek-R1 thought process, it's very long, with step by step imagination and testing how it would go through the maze.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek-R1 thinking tokens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/flt2emzlyuhe1.png?width=1610&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a16c88501392216f8998f3d97277996fac64a18"&gt;Deepseek-R1 thought process is literal pages and pages of thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ChatGPT reason mode thinking tokens:&lt;/strong&gt;&lt;br /&gt; I will put the entire thinking tokens of ChatGPT here for everyone to inspect because it's much much shorter&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Let's break down the maze format. From what I'm seeing, the maze is made up of rows and columns, ranging from 0 to 4. Each cell is represented by its coordinates in the form &amp;lt;|row-col|&amp;gt;, followed by wall tokens like &amp;lt;|up_down_left_wall|&amp;gt; and &amp;lt;|blank|&amp;gt;. The walls specify which sides of each cell are blocked, and the origin and target tokens mark the start and end points. The way these tokens are structured seems important for determining the possible movements. Let’s work with this! Alright, I need to figure out where the origin and target are in the maze. I will look through the maze row by row to find them. Looking at row 1, it appears that the origin is at (1,3) and the target is at (1,2). We need to navigate from the origin (1,3) to the target (1,2). So now, it's just about predicting and outputting the individual moves one by one, separated by spaces. Let's figure out the path! I've figured out the maze's wall tokens, but before moving, I need to check if there's a direct path from the origin at (1,3) to the target at (1,2). The origin has walls on the up and left sides, according to the &amp;lt;|up_left_wall|&amp;gt; token. The target is to the left of the origin, and the cell next to the origin at (1,2) seems to have a wall on the right side, marked by &amp;lt;|down_right_wall|&amp;gt;. I’m thinking I’ll need to consider these walls carefully before predicting the next move. From what I’m seeing, the origin at (1,3) has walls on the left and up sides, so I can only move right or down. But moving left to reach the target is blocked because of the wall at (1,3). I checked the adjacency between (1,3) and (1,2), and it’s blocked by the walls in both cells. So, to get from (1,3) to (1,2), I'll need to find a detour. Let’s move down from (1,3) to (2,3) and explore. Okay, it looks like a direct route from (1,3) to (1,2) isn’t possible due to the walls blocking access. I checked various alternatives, including moving down and exploring different connections, but many of those led to dead ends. I've landed on a slightly longer route: get from (1,3) through (2,3) to (2,4), then to (3,4), and eventually backtracking westward while checking valid moves. After verifying the path through multiple neighbor checks, this looks like the best possible route. ANSWER: &amp;lt;|down|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|down|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|left|&amp;gt; &amp;lt;|up|&amp;gt; &amp;lt;|right|&amp;gt; &amp;lt;|right|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Do you feel something is off about ChatGPT thought -&amp;gt; ANSWER?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It is very obvious that with the current thought process of ChatGPT reasoning model, &lt;strong&gt;it's glossing over so many details and somehow it can reach the result??&lt;/strong&gt; You basically cannot use the thinking tokens from ChatGPT to train a distilled thinking model because it is just not possible to reach the conclusion with those thinking, these are not actual thinking tokens of the models.&lt;/p&gt; &lt;p&gt;I have some hypothesis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI is only providing &amp;quot;summarized&amp;quot; version of their thinking tokens.&lt;/li&gt; &lt;li&gt;OpenAI has a model to output bullshit thinking tokens to distract everyone from training a distill model, so they can say they provide the community with the actual data, but there is no data.&lt;/li&gt; &lt;li&gt;They don't have &amp;quot;readable&amp;quot; thinking model but it's just an approximation of the &amp;quot;latent&amp;quot; thinking tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With the track record of OpenAI and ChatGPT, I am leaning towards &amp;quot;they are summarize or give bullshit thinking tokens&amp;quot; to the users more than they have more advanced model as option 3. Why? Because when I look at the UI it's obvious that the thought process is not outputting token per token but in chunk, which is either a summary, or a totally different model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;br /&gt; You can't just distill openAI model anymore, so don't assume everyone is distilling their model, &lt;strong&gt;THEY ARE CLOSED AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The full logs of both answers from ChatGPT and Deepseek-R1 can be found here:&lt;/strong&gt; &lt;a href="https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed"&gt;https://gist.github.com/tikikun/cf037180f402c5183662768045b59eed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The maze dataset we build can be found here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/jan-hq/Maze-Reasoning"&gt;https://huggingface.co/datasets/jan-hq/Maze-Reasoning&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ikh3vz/openai_is_hiding_the_actual_thinking_tokens_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-08T06:35:07+00:00</published>
  </entry>
</feed>
