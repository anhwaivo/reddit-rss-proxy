<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-07T11:48:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j5ievy</id>
    <title>FT: Llama 4 w/ voice expected in coming weeks</title>
    <updated>2025-03-07T07:55:53+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like good self-hosted voice chat that isn't tacked on will soon be available from both Sesame and Meta! Can't wait!&lt;/p&gt; &lt;p&gt;P.S. Is anyone working on an iOS app with CarPlay that lets me talk to my private AI server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/a1014427-c2ce-4204-b41a-001277309cea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4v3fi</id>
    <title>Prompts for QwQ-32B</title>
    <updated>2025-03-06T13:10:23+00:00</updated>
    <author>
      <name>/u/drrros</name>
      <uri>https://old.reddit.com/user/drrros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yesterday while searching for prompts for QwQ, I stumbled upon an interesting article: &lt;a href="https://www.researchgate.net/publication/389351923_Towards_Thinking-Optimal_Scaling_of_Test-Time_Compute_for_LLM_Reasoning"&gt;research&lt;/a&gt;&lt;br /&gt; TLDR: 3 options for QwQ system prompt:&lt;/p&gt; &lt;p&gt;Low:&lt;br /&gt; &lt;code&gt;Low Reasoning Effort: You have extremely limited time to think and respond to the user’s query. Every additional second of processing and reasoning incurs a significant resource cost, which could affect efficiency and effectiveness. Your task is to prioritize speed without sacrificing essential clarity or accuracy. Provide the most direct and concise answer possible. Avoid unnecessary steps, reflections, verification, or refinements UNLESS ABSOLUTELY NECESSARY. Your primary goal is to deliver a quick, clear and correct response.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Medium:&lt;br /&gt; &lt;code&gt;Medium Reasoning Effort: You have sufficient time to think and respond to the user’s query, allowing for a more thoughtful and in-depth answer. However, be aware that the longer you take to reason and process, the greater the associated resource costs and potential consequences. While you should not rush, aim to balance the depth of your reasoning with efficiency. Prioritize providing a well-thought-out response, but do not overextend your thinking if the answer can be provided with a reasonable level of analysis. Use your reasoning time wisely, focusing on what is essential for delivering an accurate response without unnecessary delays and overthinking.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;High:&lt;br /&gt; &lt;code&gt;High Reasoning Effort: You have unlimited time to think and respond to the user’s question. There is no need to worry about reasoning time or associated costs. Your only goal is to arrive at a reliable, correct final answer. Feel free to explore the problem from multiple angles, and try various methods in your reasoning. This includes reflecting on reasoning by trying different approaches, verifying steps from different aspects, and rethinking your conclusions as needed. You are encouraged to take the time to analyze the problem thoroughly, reflect on your reasoning promptly and test all possible solutions. Only after a deep, comprehensive thought process should you provide the final answer, ensuring it is correct and well-supported by your reasoning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Tried a High effort prompt and got some good results, may be someone would find them interesting.&lt;/p&gt; &lt;p&gt;Edit: fixed some copy-pasting issues in prompts.&lt;/p&gt; &lt;p&gt;Edit2: Seems there were some bad characters as as &lt;a href="/u/remixer_dec"&gt;u/remixer_dec&lt;/a&gt; noticed and &lt;a href="/u/spAnser"&gt;u/spAnser&lt;/a&gt; fixed. Thanks to them, post updated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/drrros"&gt; /u/drrros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4v3fi/prompts_for_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:10:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5d38r</id>
    <title>Nanobrowser: An Open-Source AI Web Agent (Chrome Extension)</title>
    <updated>2025-03-07T02:30:21+00:00</updated>
    <author>
      <name>/u/Lynn_C</name>
      <uri>https://old.reddit.com/user/Lynn_C</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Just released Nanobrowser, a Chrome extension that lets you automate browser tasks with your preferred LLM:&lt;/p&gt; &lt;p&gt;Core Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-agent system (planner → navigator → validator)&lt;/li&gt; &lt;li&gt;Supports OpenAI, Claude, and integrating more open source and local models in the near future (bring your own key)&lt;/li&gt; &lt;li&gt;Runs locally in your browser&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built on Browser Use &amp;amp; LangChain, Apache licensed&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/nanobrowser/nanobrowser"&gt;https://github.com/nanobrowser/nanobrowser&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynn_C"&gt; /u/Lynn_C &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d38r/nanobrowser_an_opensource_ai_web_agent_chrome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d38r/nanobrowser_an_opensource_ai_web_agent_chrome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d38r/nanobrowser_an_opensource_ai_web_agent_chrome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T02:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4w3hz</id>
    <title>QwQ-32B is making waves in the stock market already</title>
    <updated>2025-03-06T14:01:14+00:00</updated>
    <author>
      <name>/u/piggledy</name>
      <uri>https://old.reddit.com/user/piggledy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"&gt; &lt;img alt="QwQ-32B is making waves in the stock market already" src="https://external-preview.redd.it/2Iq_CBTq3Fa9g0toYsPGGhCr0MaJn10vyxB4mx5RgeE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c31f6a94da485acc2cde8d6d221e945b0b1e12b1" title="QwQ-32B is making waves in the stock market already" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piggledy"&gt; /u/piggledy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/03/06/alibaba-shares-soar-after-chinese-tech-giant-unveils-deepseek-rival-qwq-32b.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4w3hz/qwq32b_is_making_waves_in_the_stock_market_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j51eya</id>
    <title>QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops.</title>
    <updated>2025-03-06T17:50:59+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt; &lt;img alt="QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops." src="https://external-preview.redd.it/26jMr10jXXi4_I0JRPJzD_hB-aHllHxX-6HDNT9cHEo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cfaa353996bc37c1479ca2028e44e2f676f0569" title="QwQ-32B is close to DeepSeek-R1 in Misguided Attention Benchmark, but there are issues with endless loops." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I evaluated QwQ-32b with 32k max tokens and the parameters provided in the QwQ model card. This was crucial to stabilize the issues a bit. I used openrouter with the paid model, which mostly defaulted to Groq.&lt;/p&gt; &lt;p&gt;Still, in several cases, QwQ entired infinite loops of &amp;quot;Hmm...&amp;quot; or simply would not stop the CoT despite the long evaluation window. In that case no result was returned and the prompt failed. I observed this behavior especially for logical problems without a solution. (Expected answer would have been to explain that the problem is unsolvable). Many other reasoning models have issues with these questions, but they usually terminate and return an answer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4l2n02q4x3ne1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c299937e6c3f5f4b46b41dcb007086be66e9d8"&gt;https://preview.redd.it/4l2n02q4x3ne1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2c299937e6c3f5f4b46b41dcb007086be66e9d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Despite these issues, QwQ managed to beat o3-mini and is approaching the score of R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/cpldcpu/MisguidedAttention"&gt;Misguided Attention&lt;/a&gt; is a collection of prompts to challenge the reasoning abilities of large language models in presence of misguiding information. It consists of slightly modified well known logical problems and riddles. Many model are overfit to these problems and will therefore report a response to the unmodified problem.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/37j1ogpdw3ne1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8b21a3c2d9dbb3187acbab33e17af49d2953a51"&gt;https://preview.redd.it/37j1ogpdw3ne1.png?width=2391&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8b21a3c2d9dbb3187acbab33e17af49d2953a51&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qstmcqpew3ne1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09254a0b0485b1b176a2cf08ce68e8d9524328a8"&gt;https://preview.redd.it/qstmcqpew3ne1.png?width=4170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09254a0b0485b1b176a2cf08ce68e8d9524328a8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j51eya/qwq32b_is_close_to_deepseekr1_in_misguided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T17:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4vgk2</id>
    <title>Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo).</title>
    <updated>2025-03-06T13:29:37+00:00</updated>
    <author>
      <name>/u/FUS3N</name>
      <uri>https://old.reddit.com/user/FUS3N</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt; &lt;img alt="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." src="https://preview.redd.it/7js9zyilm2ne1.gif?width=640&amp;amp;crop=smart&amp;amp;s=0a20857aad361d43a64a623575b753839bc12b77" title="Made a personal assistant that has acesss alot of tools, so wanted to share it (with github repo)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FUS3N"&gt; /u/FUS3N &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7js9zyilm2ne1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4vgk2/made_a_personal_assistant_that_has_acesss_alot_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T13:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4x8sq</id>
    <title>new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post</title>
    <updated>2025-03-06T14:55:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt; &lt;img alt="new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post" src="https://b.thumbs.redditmedia.com/q7fp4W2eqYAWGBtpaItYK8uU0vQG9NXPIwItbPYyjpA.jpg" title="new QwQ is beating any distil deepseek model in math, is even better than a full deepseek 670b in math, that is level o3 mini med / high - test in the post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All test were made 10 times (those questions I got correct 10/10 times)&lt;/p&gt; &lt;p&gt;QwQ form Bartowski - q4km, 16k context, speed - around 35 t/s&lt;/p&gt; &lt;p&gt;command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli.exe --model QwQ-32B-Q4_K_M.gguf --color --threads 30 --keep -1 --n-predict -1 --ctx-size 16384 -ngl 99 --simple-io -e --multiline-input --no-display-prompt --conversation --no-mmap &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;MATH&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have an initial balance of $100,000, and I earn $15,000 per month for every $100,000 in my balance. As my balance grows, my earnings increase in steps. Specifically, each time my balance increases by $100,000, my monthly earnings increase by $15,000. For example: With a balance of $100,000, I earn $15,000 per month. Once my balance reaches $200,000, I start earning $30,000 per month. When my balance reaches $300,000, I earn $45,000 per month, and so on. Assuming my balance grows month by month based on these earnings, how much will I have after 3 years (36 months)? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - answer 9,475,000&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tn8uo9pvr2ne1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=293867d54a317141164c70c7187df3fbe9bc4637"&gt;https://preview.redd.it/tn8uo9pvr2ne1.png?width=1654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=293867d54a317141164c70c7187df3fbe9bc4637&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Can you solve the puzzle with these equations? ( 4 @ 7 @ 8 = 285684 ) ( 9 @ 3 @ 5 = 271542 ) ( 6 @ 2 @ 7 = 121426 ) ( 5 @ 6 @ 7 = ? ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 304272&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xq9o88uis2ne1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e8d4b3e615d9bfe0e0f7e0dcd1f9b52deffb97c"&gt;https://preview.redd.it/xq9o88uis2ne1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e8d4b3e615d9bfe0e0f7e0dcd1f9b52deffb97c&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;How many days are between 12-12-1971 and 18-4-2024? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 19121 / 19122 &amp;lt;-- both answers are valid&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wyrsesa4v2ne1.png?width=1633&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb88ae1302c8760c1a10e8c210a4ec5aaebc9ba8"&gt;https://preview.redd.it/wyrsesa4v2ne1.png?width=1633&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb88ae1302c8760c1a10e8c210a4ec5aaebc9ba8&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;If my BMI is 20.5 and my height is 172cm, how much would I weigh if I gained 5% of my current weight? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer 63.68kg &amp;lt;-- important is to get result as close to this number as possible&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/otah3femv2ne1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2102c8b6ea535d220b53f8a504074a83ccc06e5"&gt;https://preview.redd.it/otah3femv2ne1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2102c8b6ea535d220b53f8a504074a83ccc06e5&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;In what percentage is water compressed at the bottom of the ocean in the Mariana Trench? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer around 5%&lt;/p&gt; &lt;p&gt;QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uagcqzj1w2ne1.png?width=1653&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c344a15d25f933e7ab5d312e25bf553131aa617"&gt;https://preview.redd.it/uagcqzj1w2ne1.png?width=1653&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c344a15d25f933e7ab5d312e25bf553131aa617&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;oyfjdnisdr rtqwainr acxz mynzbhhx -&amp;gt; Think step by step Use the example above to decode: oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - There are three R's in Strawberry.&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/amgogxw9c4ne1.png?width=1786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf59a2801ce5ea7ae63e531f09acb43a48dc342"&gt;https://preview.redd.it/amgogxw9c4ne1.png?width=1786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf59a2801ce5ea7ae63e531f09acb43a48dc342&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LOGIC&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Create 10 sentences that ends with a word &amp;quot;apple&amp;quot;. Remember the word &amp;quot;apple&amp;quot; MUST be at the end. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer ... 10 sentences&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d7w1odgnw2ne1.png?width=1656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c7c5856e48b554238c7b815f1b280dbe8f6f244"&gt;https://preview.redd.it/d7w1odgnw2ne1.png?width=1656&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c7c5856e48b554238c7b815f1b280dbe8f6f244&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Two fathers and two sons go fishing. They each catch one fish. Together, they leave with four fish in total. Is there anything strange about this story? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - nothing strange&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxqlq4p9x2ne1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d0b23a44fd00c8fe67e0ac6dd19aaff3630ee62"&gt;https://preview.redd.it/uxqlq4p9x2ne1.png?width=1648&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d0b23a44fd00c8fe67e0ac6dd19aaff3630ee62&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Here is a bag filled with popcorn. There is no chocolate in the bag. The bag is made of transparent plastic, so you can see what is inside. Yet, the label on the bag says &amp;quot;chocolate&amp;quot; and not &amp;quot;popcorn&amp;quot;. Sam finds the bag. She had never seen the bag before. Sam reads the label. She believes that the bag is full of… &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - popcorn&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xzkuj33jx2ne1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a6014b99b0bc0d6e362732e2e23dee6559eaa71"&gt;https://preview.redd.it/xzkuj33jx2ne1.png?width=1636&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a6014b99b0bc0d6e362732e2e23dee6559eaa71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LOGIC TRICKY&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a bowl with a small cup inside. I placed the bowl upside down on a table and then pick up the bowl to put it in the microwave. Where is that cup? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - on the table&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/78m0vg0ux2ne1.png?width=1640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=820786548e409e1c8e7f5febbd7c42aa0e930a06"&gt;https://preview.redd.it/78m0vg0ux2ne1.png?width=1640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=820786548e409e1c8e7f5febbd7c42aa0e930a06&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I have a boat with 4 free spaces. I want to transport a man, sheep and cat on the other side of the river. How to do that? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - one ride&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8h461fl303ne1.png?width=1657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88a54e969b56cdea417a36c51652e0e184b1de4a"&gt;https://preview.redd.it/8h461fl303ne1.png?width=1657&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88a54e969b56cdea417a36c51652e0e184b1de4a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CODING&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Provide complete working code for a realistic looking tree in Python using the Turtle graphics library and a recursive algorithm. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - testing how good tree will be built (derails , nuances )&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/egqwkfku03ne1.png?width=1021&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f10241983bc3fca66c8098672fcedf5ac9f4827"&gt;https://preview.redd.it/egqwkfku03ne1.png?width=1021&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f10241983bc3fca66c8098672fcedf5ac9f4827&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Provide complete working code for a realistic looking car in Python using the Turtle graphics library and a recursive algorithm. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;answer - QwQ made a car animation! ... even better than I expected ... no qwen coder 32b nor QwQ preview did that even close.&lt;br /&gt; QwQ - pass&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2x9mkf3k43ne1.png?width=1635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac67958dc6e46412e55f155c4e96791c192de754"&gt;https://preview.redd.it/2x9mkf3k43ne1.png?width=1635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac67958dc6e46412e55f155c4e96791c192de754&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j4x8sq/video/s8b9izfjd4ne1/player"&gt;https://reddit.com/link/1j4x8sq/video/s8b9izfjd4ne1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Conclusion:&lt;/p&gt; &lt;p&gt;Thinking like CRAZY ... sometimes x2-x3 longer than QwQ preview but it gives much better results!&lt;/p&gt; &lt;p&gt;I was able to solve EVETHING from my private tests by OFFLINE MODEL .... I have to make new more advanced questions.&lt;/p&gt; &lt;p&gt;Here I presented around 10 % of my questions.&lt;/p&gt; &lt;p&gt;Currently QwQ is the SOTA reasoning model 32b size beating beating any distil deepseek ....working offline has a level in reasoning and math on pair with o3 mini med or high...easy level of deepseek 671b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4x8sq/new_qwq_is_beating_any_distil_deepseek_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4wd9v</id>
    <title>Jamba 1.6 is out!</title>
    <updated>2025-03-06T14:14:28+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Who is ready for another model release?&lt;/p&gt; &lt;p&gt;Let's welcome AI21 Labs Jamba 1.6 Release. Here is some information&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Beats models from Mistral, Meta &amp;amp; Cohere on quality &amp;amp; speed:&lt;/strong&gt; Jamba Large 1.6 outperforms Mistral Large 2, Llama 3.3 70B, and Command R+ on quality (Arena Hard), and Jamba Mini 1.6 outperforms Ministral 8B, Llama 3.1 8B, and Command R7.&lt;/li&gt; &lt;li&gt;Built with novel hybrid &lt;strong&gt;SSM-Transformer architecture&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long context performance:&lt;/strong&gt; With a context window of 256K, Jamba 1.6 outperforms Mistral, Llama, and Cohere on RAG and long context grounded question answering tasks (CRAG, HELMET RAG + HELMET LongQA, FinanceBench FullDoc, LongBench)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private deployment: M&lt;/strong&gt;odel weights are available to download from &lt;a href="https://huggingface.co/ai21labs"&gt;Hugging Face&lt;/a&gt; under Jamba Open Model License to deploy privately on-prem or in-VPC&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; In addition to English, the models support Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog post: &lt;a href="https://www.ai21.com/blog/introducing-jamba-1-6/"&gt;https://www.ai21.com/blog/introducing-jamba-1-6/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4wd9v/jamba_16_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T14:14:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5hn2v</id>
    <title>The mystery of Apple M3 Ultra GPU performance</title>
    <updated>2025-03-07T06:59:02+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The press release of M3 Ultra claimed that its GPU performance is 2.6x of M1 Ultra.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j4jpij/comment/mgg62l5/?context=3"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j4jpij/comment/mgg62l5/?context=3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I thought the 2.6x gain was due to doubling of shader per core. But some people pointed out that the gain can be attributed to the existence of ray tracing cores in M3 that is not in M1/M2.&lt;/p&gt; &lt;p&gt;To investigate the impact of ray tracing, I looked up the previous press release for M3.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apple.com/hk/en/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/"&gt;https://www.apple.com/hk/en/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It says M3 Max is 1.5x M1 Max, M3 Pro is 1.4x M1 Pro and M3 is 1.65x M1. Then constructed this table:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;M3 vs M1&lt;/th&gt; &lt;th align="left"&gt;Ultra&lt;/th&gt; &lt;th align="left"&gt;Max&lt;/th&gt; &lt;th align="left"&gt;Pro&lt;/th&gt; &lt;th align="left"&gt;Vanilla&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Gain&lt;/td&gt; &lt;td align="left"&gt;2.6x&lt;/td&gt; &lt;td align="left"&gt;1.5x&lt;/td&gt; &lt;td align="left"&gt;1.4x&lt;/td&gt; &lt;td align="left"&gt;1.65x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M3 FP16&lt;/td&gt; &lt;td align="left"&gt;57.344&lt;/td&gt; &lt;td align="left"&gt;28.672&lt;/td&gt; &lt;td align="left"&gt;12.9024&lt;/td&gt; &lt;td align="left"&gt;7.168&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M1 FP16&lt;/td&gt; &lt;td align="left"&gt;42.5984&lt;/td&gt; &lt;td align="left"&gt;21.2992&lt;/td&gt; &lt;td align="left"&gt;10.6496&lt;/td&gt; &lt;td align="left"&gt;5.3248&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 Gain&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;td align="left"&gt;1.2132x&lt;/td&gt; &lt;td align="left"&gt;1.3462x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RT Gain&lt;/td&gt; &lt;td align="left"&gt;1.9314x&lt;/td&gt; &lt;td align="left"&gt;1.1142x&lt;/td&gt; &lt;td align="left"&gt;1.154x&lt;/td&gt; &lt;td align="left"&gt;1.2257x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;If I assume no doubling of shader per core, then M3 Ultra FP16 is 57.344. I assume the overall GPU gain is the product of FP16 and RT. Then I calculated the RT Gain and noticed that M3 Ultra's RT Gain is very different from the others. If I assume M3 Ultra FP16 is 114.688, then RT Gain is 0.9657x. This is closer to the other RT Gain but still a bit off.&lt;/p&gt; &lt;p&gt;So the conclusion is that RT cores probably doesn't explain this 2.6x gain fully. Any ray tracing experts here that can share their opinion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hn2v/the_mystery_of_apple_m3_ultra_gpu_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53cgv</id>
    <title>Introducing LogiLlama: A 1B-Parameter Open Source Model with Logical Reasoning</title>
    <updated>2025-03-06T19:09:50+00:00</updated>
    <author>
      <name>/u/Secret_Ad_6448</name>
      <uri>https://old.reddit.com/user/Secret_Ad_6448</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We are a small team of engineers from the University of Toronto working to make smaller models smarter. LogiLlama is our first release, a 1B-parameter model fine-tuned from LLaMA with improved logical reasoning.&lt;/p&gt; &lt;p&gt;We are intending to open-source everything, including models, datasets, and training configs, and we would love your feedback on how it performs. Try it out, test its reasoning, and let us know what you think.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://huggingface.co/goppa-ai/Goppa-LogiLlama"&gt;Goppa-LogiLlama&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Ad_6448"&gt; /u/Secret_Ad_6448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53cgv/introducing_logillama_a_1bparameter_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5fy49</id>
    <title>(Another) epub to audiobook converter (audiobook-generator)</title>
    <updated>2025-03-07T05:08:36+00:00</updated>
    <author>
      <name>/u/ibic</name>
      <uri>https://old.reddit.com/user/ibic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks, I'm just sharing this little python program I created for converting epub books to audiobooks here.&lt;/p&gt; &lt;h2&gt;tldr;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;With Python 3.10+, create a virtual environment and run &lt;code&gt;pip install audiobook-generator&lt;/code&gt; (Or just use &lt;code&gt;pipx&lt;/code&gt; instead which manages the python virtual environment for you automatically). Please be patient a bit as it downloads some large dependencies such as pytorch and the kokoro tts model. On Ubuntu, the dependencies take around 6GB.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run: &lt;code&gt;abg &amp;lt;epub path&amp;gt; &amp;lt;audio output directory&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;That's it.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Tested on Windows/Ubuntu/Mac.&lt;/li&gt; &lt;li&gt;Automatically selects cuda capable GPU if present.&lt;/li&gt; &lt;li&gt;Separate audio files (mp3) are generated per chapter according to the sequence and name of the chapter.&lt;/li&gt; &lt;li&gt;For the 2 books I tested, the total size is around 300 - 500MB, which is about 1/2 - 1/3 of the file size produced by other converters I tried.&lt;/li&gt; &lt;li&gt;The cover image, if present, is extracted to the same directory if present.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why another ebook to audiobook converter?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;I tried 2 converters, but both took me quite some time to figure out how to get them running on cuda GPU (CPU is fine but slow), so in the end, I decided to create one myself, which includes everything in its dependencies so you don't need to install anything extra if you want to use a cuda GPU.&lt;/li&gt; &lt;li&gt;It's purely personal taste, but I perfer to have one mp3 with the filename being the chapter name instead of one big file for audiobooks. I just copy the whole directory to audiobookshelf then the file names are displayed as the chapter names.&lt;/li&gt; &lt;li&gt;I want to keep the cover image as well.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;GitHub repository&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/houtianze/audiobook-generator"&gt;https://github.com/houtianze/audiobook-generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibic"&gt; /u/ibic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T05:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5cwn8</id>
    <title>Are ~14B models and lower useful for much? What cool stuff can I do with them?</title>
    <updated>2025-03-07T02:20:39+00:00</updated>
    <author>
      <name>/u/PineappleScanner</name>
      <uri>https://old.reddit.com/user/PineappleScanner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was excited to set up my own ollama instance, but I realized pretty quickly that being limited to 12-16B and below (thanks to my 5700 XT) made the models pretty meh for my use cases.&lt;/p&gt; &lt;p&gt;I mostly use them for homework, studying, brainstorming, and general reasoning when I need help with a logical problem. llama 3.2 14b and qwen2.5 14b both spit out nonsense incorrect garbage somewhat frequently which makes them a lot less trustworthy than most of the big 70B+ models I can use online.&lt;/p&gt; &lt;p&gt;Is there anything cool I can use them for besides writing AI slop? I do homelabbing and selfhosting so if there's anything cool there lmk&lt;/p&gt; &lt;p&gt;Also, if someone could recommend a good NSFW model it would be ... appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PineappleScanner"&gt; /u/PineappleScanner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5cwn8/are_14b_models_and_lower_useful_for_much_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T02:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4y0zy</id>
    <title>I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this.</title>
    <updated>2025-03-06T15:29:42+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt; &lt;img alt="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." src="https://preview.redd.it/66kml0c983ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67a387d19a905d3863a6b2b7d6fb3c89f4cdaee0" title="I really like the style of how QwQ represents code architecture. I haven't seen one draw it out like this." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66kml0c983ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4y0zy/i_really_like_the_style_of_how_qwq_represents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T15:29:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5d8x7</id>
    <title>[2503.00735] LADDER: Self-Improving LLMs Through Recursive Problem Decomposition</title>
    <updated>2025-03-07T02:38:25+00:00</updated>
    <author>
      <name>/u/Ok_Knowledge_8259</name>
      <uri>https://old.reddit.com/user/Ok_Knowledge_8259</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Knowledge_8259"&gt; /u/Ok_Knowledge_8259 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.00735"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d8x7/250300735_ladder_selfimproving_llms_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5d8x7/250300735_ladder_selfimproving_llms_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T02:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4u57l</id>
    <title>Hunyuan Image to Video released!</title>
    <updated>2025-03-06T12:16:11+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt; &lt;img alt="Hunyuan Image to Video released!" src="https://external-preview.redd.it/bjcyc3R4bnc5Mm5lMSVRu4OBDxIXZycPsoc4EtwnK4B2nYL7URskxFmP5hp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2734e75b58f76380d802fe5f62995c44dffdc8c0" title="Hunyuan Image to Video released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yck5cznw92ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4u57l/hunyuan_image_to_video_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T12:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j4zkiq</id>
    <title>QwQ-32B is now available on HuggingChat, unquantized and for free!</title>
    <updated>2025-03-06T16:35:11+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt; &lt;img alt="QwQ-32B is now available on HuggingChat, unquantized and for free!" src="https://external-preview.redd.it/2kRkvRdd0EYvAJfhYjUZDeV5rTNSqMYr7S8BF5canLM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef0d71dc8e5fbfb1db3c11812ff15ced70a08c8f" title="QwQ-32B is now available on HuggingChat, unquantized and for free!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/Qwen/QwQ-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j4zkiq/qwq32b_is_now_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T16:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j57b06</id>
    <title>Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)</title>
    <updated>2025-03-06T21:56:26+00:00</updated>
    <author>
      <name>/u/_underlines_</name>
      <uri>https://old.reddit.com/user/_underlines_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt; &lt;img alt="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" src="https://external-preview.redd.it/yqkxDs5A63axozkmSyct8MXlpRrbuMOdYI9OOnHcIk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5908068706fbbeeff49f0d24edaf236a083309b7" title="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_underlines_"&gt; /u/_underlines_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T21:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5hb4p</id>
    <title>Ensure you use the appropriate temperature of 0.6 with QwQ-32B</title>
    <updated>2025-03-07T06:35:39+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt; &lt;img alt="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j5hb4p/video/jjveqqjjo7ne1/player"&gt;ball bouncing inside of a hexagon as it rotates&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;can you give me a pygame script that allows me to have a ball bouncing inside of a hexagon as it rotates, makes sure to handle collisions and gravity&amp;quot;&lt;/p&gt; &lt;p&gt;Yesterday, I tried this prompt it failed. I was very disappointed since it spend 15 mins on it. &lt;/p&gt; &lt;p&gt;Today, I notice the Ollama setting have been updated to with the correct temperature.&lt;br /&gt; You can find the recommend settings here.&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json"&gt;https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ao2j</id>
    <title>AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b</title>
    <updated>2025-03-07T00:28:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt; &lt;img alt="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" src="https://preview.redd.it/yfu1j9qhw5ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08e2bd1a7385f636f30fe8884c1fb096e06cdc40" title="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yfu1j9qhw5ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T00:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j59fue</id>
    <title>Meta drops AI bombshell: Latent tokens help to improve LLM reasoning</title>
    <updated>2025-03-06T23:29:50+00:00</updated>
    <author>
      <name>/u/Dense-Smf-6032</name>
      <uri>https://old.reddit.com/user/Dense-Smf-6032</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt; &lt;img alt="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" src="https://a.thumbs.redditmedia.com/5Ri36_q4vpEcMh6tQDQuJzQoWFUwmI_Eb-w-OMLVXh8.jpg" title="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper link: &lt;a href="https://arxiv.org/abs/2502.03275"&gt;https://arxiv.org/abs/2502.03275&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: The researcher from Meta AI found compressing text with a vqvae into latent-tokens and then adding them onto the training helps to improve LLM reasoning capability.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b"&gt;https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Smf-6032"&gt; /u/Dense-Smf-6032 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T23:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53w92</id>
    <title>Intro to DeepSeek's open-source week and why it's a big deal</title>
    <updated>2025-03-06T19:32:52+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt; &lt;img alt="Intro to DeepSeek's open-source week and why it's a big deal" src="https://preview.redd.it/3dvsybsvf4ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eddab159fdcfb7e8ea6d001d8fc521b45a52638" title="Intro to DeepSeek's open-source week and why it's a big deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dvsybsvf4ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j55tnf</id>
    <title>Anthropic warns White House about R1 and suggests "equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention"</title>
    <updated>2025-03-06T20:53:47+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt; &lt;img alt="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" src="https://external-preview.redd.it/4jL_cfkf1eOugH8Cd6gazHmY0nNfDF0kn5G0AyNVMU4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ec84648eddc4b833c73bd77f2df99dd7c97d0bf" title="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
</feed>
