<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-22T20:50:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m6eggp</id>
    <title>Considering 5xMI50 for Qwen 3 235b</title>
    <updated>2025-07-22T13:43:21+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;**TL;DR** Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp;amp; power, which I'm attempting to solve with bifurcation cards and a new PSU. Looking for feedback!&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Lately I've been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I'm seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?&lt;/p&gt; &lt;p&gt;If its as good as some posts claim, then I'd be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I'm going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that's why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.&lt;/p&gt; &lt;p&gt;Current system:&lt;br /&gt; * **CPU:** Ryzen 5 7600&lt;br /&gt; * **RAM:** 48GB DDR5 5200MHz&lt;br /&gt; * **Motherboard:** MSI Mortar AM5&lt;br /&gt; * **SSD (Primary):** 1TB SSD&lt;br /&gt; * **SSD (Secondary):** 2TB SSD&lt;br /&gt; * **PSU:** 850W&lt;br /&gt; * **GPU(s):** 2x AMD RX6800 &lt;/p&gt; &lt;p&gt;Prospective system:&lt;br /&gt; * **CPU:** Ryzen 5 7600&lt;br /&gt; * **RAM:** 48GB DDR5 5200MHz&lt;br /&gt; * **Motherboard:** MSI Mortar AM5(with bifurcation enabled)&lt;br /&gt; * **SSD (Primary):** 1TB SSD&lt;br /&gt; * **SSD (Secondary):** 2TB SSD&lt;br /&gt; * **GPUs (New):** 5 x MI50 32GB ($130 each + $100 shipping = $750 total)&lt;br /&gt; * **PSU (New):** 1600W PSU - $200&lt;br /&gt; * **Bifurcation Cards:** Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)&lt;br /&gt; * **Riser Cables:** Four PCIe 4.0 8x Cables - $100 ($25 each)&lt;br /&gt; * **Cooling Shrouds:** DIY MI50 GPU Cooling Shrouds (DIY)&lt;/p&gt; &lt;p&gt;* **Total Cost of New Hardware:** $1,125&lt;/p&gt; &lt;p&gt;Which doesn't seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I've been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T13:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6172l</id>
    <title>New qwen tested on Fiction.liveBench</title>
    <updated>2025-07-22T01:33:20+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt; &lt;img alt="New qwen tested on Fiction.liveBench" src="https://preview.redd.it/9rynne03xbef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e" title="New qwen tested on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rynne03xbef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T01:33:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6dco7</id>
    <title>Jamba 1.7 is now available on Kaggle</title>
    <updated>2025-07-22T12:55:54+00:00</updated>
    <author>
      <name>/u/NullPointerJack</name>
      <uri>https://old.reddit.com/user/NullPointerJack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI21 has just made Jamba 1.7 available on Kaggle:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7"&gt;https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7&lt;/a&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can run and test the model without needing to install it locally&lt;/li&gt; &lt;li&gt;No need to harness setup, hardware and engineering knowledge via Hugging Face anymore&lt;/li&gt; &lt;li&gt;Now you can run sample tasks, benchmark against other models and share public notebooks with results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Pretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences&lt;/li&gt; &lt;li&gt;256k context window - well-suited for long document summarization and memory-heavy chat agents&lt;/li&gt; &lt;li&gt;Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Who is going to try it out? What use cases do you have in mind?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullPointerJack"&gt; /u/NullPointerJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5ox8z</id>
    <title>Qwen3-235B-A22B-2507</title>
    <updated>2025-07-21T17:18:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt; &lt;img alt="Qwen3-235B-A22B-2507" src="https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5242a889814e823acb6da0b1179758e2947ea2a7" title="Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w2uh7h5lg9ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6fvd5</id>
    <title>Looking for LLMs Study Buddy</title>
    <updated>2025-07-22T14:40:08+00:00</updated>
    <author>
      <name>/u/KaiKawaii0</name>
      <uri>https://old.reddit.com/user/KaiKawaii0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I’m looking for a study buddy (or a small group) to go through &lt;a href="https://github.com/mlabonne/llm-course"&gt;Maxime Labonne’s “LLM From Scratch” course&lt;/a&gt; together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together&lt;/p&gt; &lt;h1&gt;My plan:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Set weekly goals&lt;/strong&gt; based on the course structure&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meet once a week&lt;/strong&gt; (probably one evening over the weekend) for a &lt;strong&gt;voice call&lt;/strong&gt; to review what we’ve learned, share insights, and help each other with anything confusing&lt;/li&gt; &lt;li&gt;Stay accountable and motivated through shared progress&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Drop a comment or DM me if you’re interested! Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaiKawaii0"&gt; /u/KaiKawaii0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T14:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6h67y</id>
    <title>Epyc Qwen3 235B Q8 speed?</title>
    <updated>2025-07-22T15:29:26+00:00</updated>
    <author>
      <name>/u/MidnightProgrammer</name>
      <uri>https://old.reddit.com/user/MidnightProgrammer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation? Ideally with a 3090 or better for prompt processing.&lt;/p&gt; &lt;p&gt;I've been looking at Kimi, but I've been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.&lt;/p&gt; &lt;p&gt;Was wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MidnightProgrammer"&gt; /u/MidnightProgrammer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T15:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6nvhs</id>
    <title>What is the cheapest option for hosting llama cpp with Qwen Coder at Q8?</title>
    <updated>2025-07-22T19:38:31+00:00</updated>
    <author>
      <name>/u/Available_Driver6406</name>
      <uri>https://old.reddit.com/user/Available_Driver6406</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What options do we have for Qwen3 Coder, either local or cloud services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Driver6406"&gt; /u/Available_Driver6406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5owi8</id>
    <title>Qwen3-235B-A22B-2507 Released!</title>
    <updated>2025-07-21T17:17:27+00:00</updated>
    <author>
      <name>/u/pseudoreddituser</name>
      <uri>https://old.reddit.com/user/pseudoreddituser</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-2507 Released!" src="https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00" title="Qwen3-235B-A22B-2507 Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudoreddituser"&gt; /u/pseudoreddituser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1947344511988076547"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T17:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1m66qks</id>
    <title>Private Eval result of Qwen3-235B-A22B-Instruct-2507</title>
    <updated>2025-07-22T06:28:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt; &lt;img alt="Private Eval result of Qwen3-235B-A22B-Instruct-2507" src="https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg" title="Private Eval result of Qwen3-235B-A22B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;. So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt; &lt;p&gt;The score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"&gt;logic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"&gt;coding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt; &lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt; &lt;p&gt;Source:&lt;/p&gt; &lt;p&gt;&lt;a href="Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T06:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1m67zde</id>
    <title>AI should just be open-source</title>
    <updated>2025-07-22T07:47:52+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.&lt;/p&gt; &lt;p&gt;That said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt; &lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt; &lt;p&gt;Like how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt; &lt;p&gt;It’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt; &lt;p&gt;What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T07:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6m0f7</id>
    <title>A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically.</title>
    <updated>2025-07-22T18:29:33+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/"&gt; &lt;img alt="A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically." src="https://b.thumbs.redditmedia.com/EFJzlTp5mgKfQzJuDpD-TjIuXrwPWnGGidBsBvFKiPg.jpg" title="A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/bazaar"&gt;https://github.com/lechmazur/bazaar&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.&lt;/p&gt; &lt;p&gt;Four market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.&lt;/p&gt; &lt;p&gt;Key Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a &amp;quot;truthful&amp;quot; baseline (bid your exact value).&lt;/p&gt; &lt;p&gt;All agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.&lt;/p&gt; &lt;p&gt;BAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.&lt;/p&gt; &lt;p&gt;With chat enabled, LLMs form illegal cartels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m6m0f7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6b151</id>
    <title>Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results</title>
    <updated>2025-07-22T11:00:04+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt; &lt;img alt="Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results" src="https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg" title="Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back I posted some &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt; &lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt; &lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt; &lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href="https://github.com/lhl/strix-halo-testing/tree/main/llm-bench"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt; &lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt; &lt;p&gt;All testing was done on pre-production &lt;a href="https://frame.work/desktop"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt; &lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Close to production BIOS/EC&lt;/li&gt; &lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt; &lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt; &lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt; &lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Architecture&lt;/th&gt; &lt;th align="left"&gt;Weights (B)&lt;/th&gt; &lt;th align="left"&gt;Active (B)&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Flags&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;th align="left"&gt;Memory (Max MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;998.0&lt;/td&gt; &lt;td align="left"&gt;46.5&lt;/td&gt; &lt;td align="left"&gt;4237&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;906.1&lt;/td&gt; &lt;td align="left"&gt;40.8&lt;/td&gt; &lt;td align="left"&gt;4720&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;878.2&lt;/td&gt; &lt;td align="left"&gt;37.2&lt;/td&gt; &lt;td align="left"&gt;5308&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 MoE&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;604.8&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;17527&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Mistral 3&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;316.9&lt;/td&gt; &lt;td align="left"&gt;13.6&lt;/td&gt; &lt;td align="left"&gt;14638&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;Hunyuan MoE&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;270.5&lt;/td&gt; &lt;td align="left"&gt;17.1&lt;/td&gt; &lt;td align="left"&gt;68785&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Llama 4 MoE&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;hipBLASLt&lt;/td&gt; &lt;td align="left"&gt;264.1&lt;/td&gt; &lt;td align="left"&gt;17.2&lt;/td&gt; &lt;td align="left"&gt;59720&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;HIP rocWMMA&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;94.7&lt;/td&gt; &lt;td align="left"&gt;4.5&lt;/td&gt; &lt;td align="left"&gt;41522&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;dots1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;dots1 MoE&lt;/td&gt; &lt;td align="left"&gt;142&lt;/td&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;20.6&lt;/td&gt; &lt;td align="left"&gt;84077&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Architecture&lt;/th&gt; &lt;th align="left"&gt;Weights (B)&lt;/th&gt; &lt;th align="left"&gt;Active (B)&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Flags&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;th align="left"&gt;Memory (Max MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Qwen 3 MoE&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;b=256&lt;/td&gt; &lt;td align="left"&gt;591.1&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;17377&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;620.9&lt;/td&gt; &lt;td align="left"&gt;47.9&lt;/td&gt; &lt;td align="left"&gt;4463&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 2 7B Q4_0&lt;/td&gt; &lt;td align="left"&gt;Llama 2&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;1014.1&lt;/td&gt; &lt;td align="left"&gt;45.8&lt;/td&gt; &lt;td align="left"&gt;4219&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;614.2&lt;/td&gt; &lt;td align="left"&gt;42.0&lt;/td&gt; &lt;td align="left"&gt;5333&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;dots1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;dots1 MoE&lt;/td&gt; &lt;td align="left"&gt;142&lt;/td&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;63.1&lt;/td&gt; &lt;td align="left"&gt;20.6&lt;/td&gt; &lt;td align="left"&gt;84077&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Llama 4 MoE&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;146.1&lt;/td&gt; &lt;td align="left"&gt;19.3&lt;/td&gt; &lt;td align="left"&gt;59917&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;Hunyuan MoE&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1 b=256&lt;/td&gt; &lt;td align="left"&gt;223.9&lt;/td&gt; &lt;td align="left"&gt;17.1&lt;/td&gt; &lt;td align="left"&gt;68608&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;Mistral 3&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;119.6&lt;/td&gt; &lt;td align="left"&gt;14.3&lt;/td&gt; &lt;td align="left"&gt;14540&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;70&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;fa=1&lt;/td&gt; &lt;td align="left"&gt;26.4&lt;/td&gt; &lt;td align="left"&gt;5.0&lt;/td&gt; &lt;td align="left"&gt;41456&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Testing Notes&lt;/h1&gt; &lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt; &lt;p&gt;There's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).&lt;/p&gt; &lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt; &lt;p&gt;Unlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt; &lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6orbr</id>
    <title>Anyone here who has been able to reproduce their results yet?</title>
    <updated>2025-07-22T20:11:38+00:00</updated>
    <author>
      <name>/u/Original_Log_9899</name>
      <uri>https://old.reddit.com/user/Original_Log_9899</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt; &lt;img alt="Anyone here who has been able to reproduce their results yet?" src="https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5" title="Anyone here who has been able to reproduce their results yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://x.com/makingAGI/status/1947286324735856747"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Original_Log_9899"&gt; /u/Original_Log_9899 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cfffg12fghef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T20:11:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6gq8e</id>
    <title>I wrote 2000 LLM test cases so you don't have to</title>
    <updated>2025-07-22T15:12:44+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt; &lt;h1&gt;The problem: too many options&lt;/h1&gt; &lt;p&gt;I've been building &lt;a href="https://github.com/kiln-ai/kiln"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt; &lt;p&gt;Here's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt; &lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt; &lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt; &lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt; &lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt; &lt;h1&gt;Wait, doesn't that cost a lot of money and take forever?&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.&lt;/p&gt; &lt;p&gt;Our blog has some details on the &lt;a href="https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;The Result&lt;/h1&gt; &lt;p&gt;The end result is that it's much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt; &lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, you're in control. You can always override any suggestion.&lt;/p&gt; &lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt; &lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt; &lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt; &lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt; &lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.&lt;/p&gt; &lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.getkiln.ai/docs/quickstart"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T15:12:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1m641zg</id>
    <title>MegaTTS 3 Voice Cloning is Here</title>
    <updated>2025-07-22T03:53:37+00:00</updated>
    <author>
      <name>/u/mrfakename0</name>
      <uri>https://old.reddit.com/user/mrfakename0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt; &lt;img alt="MegaTTS 3 Voice Cloning is Here" src="https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13bd3c86a79666218395f17439b714df6a5fc52c" title="MegaTTS 3 Voice Cloning is Here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt; &lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt; &lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href="https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt; &lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href="https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt; &lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mrfakename0"&gt; /u/mrfakename0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T03:53:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6bddm</id>
    <title>AMD's Strix Halo "Ryzen AI MAX" APUs Come To DIY PC Builders With New MoDT "Mini-ITX" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory</title>
    <updated>2025-07-22T11:18:22+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt; &lt;img alt="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" src="https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01" title="AMD's Strix Halo &amp;quot;Ryzen AI MAX&amp;quot; APUs Come To DIY PC Builders With New MoDT &amp;quot;Mini-ITX&amp;quot; Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T11:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ct7u</id>
    <title>Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000</title>
    <updated>2025-07-22T12:31:02+00:00</updated>
    <author>
      <name>/u/aidanjustsayin</name>
      <uri>https://old.reddit.com/user/aidanjustsayin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt; &lt;img alt="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" src="https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47d0469ef510365b725dc72ec2ab1d98d266e09a" title="Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt; &lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context: 4096 (default)&lt;/li&gt; &lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt; &lt;li&gt;CPU Thread Pool: 16&lt;/li&gt; &lt;li&gt;... all else default besides ...&lt;/li&gt; &lt;li&gt;Flash Attention: On&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt; &lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aidanjustsayin"&gt; /u/aidanjustsayin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1x5u9hrp5fef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6cfzi</id>
    <title>The ik_llama.cpp repository is back! \o/</title>
    <updated>2025-07-22T12:13:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mfic</id>
    <title>Qwen3-Coder Available on chat.qwen.ai</title>
    <updated>2025-07-22T18:44:49+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt; &lt;img alt="Qwen3-Coder Available on chat.qwen.ai" src="https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83" title="Qwen3-Coder Available on chat.qwen.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1M token context length&lt;/p&gt; &lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href="https://chat.qwen.ai"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8xj4raow0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6ny2q</id>
    <title>Qwen3-Coder Web Development</title>
    <updated>2025-07-22T19:41:12+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt; &lt;img alt="Qwen3-Coder Web Development" src="https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=113bee11066829bd35da182aa0ce00847ecb4ea0" title="Qwen3-Coder Web Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt; &lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt; &lt;p&gt;Creds &lt;a href="https://www.youtube.com/@TheFeatureCrew"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ob9yhvcjahef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:41:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6medy</id>
    <title>Qwen3-Coder is imminent</title>
    <updated>2025-07-22T18:43:38+00:00</updated>
    <author>
      <name>/u/Dudensen</name>
      <uri>https://old.reddit.com/user/Dudensen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt; &lt;img alt="Qwen3-Coder is imminent" src="https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809" title="Qwen3-Coder is imminent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dudensen"&gt; /u/Dudensen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mruaiodv0hef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mlbk</id>
    <title>Qwen3-Coder-480B-A35B-Instruct</title>
    <updated>2025-07-22T18:50:48+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;hyperolic already has it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6nxh2</id>
    <title>Everyone brace up for qwen !!</title>
    <updated>2025-07-22T19:40:36+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt; &lt;img alt="Everyone brace up for qwen !!" src="https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=855c907a55cf3f70afe582932d52350878ef5e68" title="Everyone brace up for qwen !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mn8auem2bhef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T19:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6lf9s</id>
    <title>Could this be Deepseek?</title>
    <updated>2025-07-22T18:07:46+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt; &lt;img alt="Could this be Deepseek?" src="https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e224ff9a214f929b3917304102fe92d67371e639" title="Could this be Deepseek?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qzkjkgegugef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:07:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m6mew9</id>
    <title>Qwen3- Coder 👀</title>
    <updated>2025-07-22T18:44:10+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt; &lt;img alt="Qwen3- Coder 👀" src="https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1" title="Qwen3- Coder 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Available in &lt;a href="https://chat.qwen.ai"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnhuwe801hef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-22T18:44:10+00:00</published>
  </entry>
</feed>
