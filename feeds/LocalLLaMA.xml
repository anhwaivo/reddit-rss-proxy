<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-31T22:23:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ieq5yp</id>
    <title>o3-mini?</title>
    <updated>2025-01-31T21:41:39+00:00</updated>
    <author>
      <name>/u/jalbanesi</name>
      <uri>https://old.reddit.com/user/jalbanesi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq5yp/o3mini/"&gt; &lt;img alt="o3-mini?" src="https://preview.redd.it/aior4ixvfege1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=922866aa5e68851dea76b3264380aa7296c6f4f1" title="o3-mini?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was about to ask a trivial question so I open the model selection to go with 4o, when a wild o3-mini appeared. Btw, this is using the android app. This is getting exciting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jalbanesi"&gt; /u/jalbanesi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aior4ixvfege1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq5yp/o3mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq5yp/o3mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T21:41:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie4brg</id>
    <title>DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked</title>
    <updated>2025-01-31T02:16:09+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt; &lt;img alt="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" src="https://external-preview.redd.it/FoBRfbFJiqbPvZWr-1-_kZti4liFY86vCxy63rbFeaE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8563af2625a34ede2f3818cc4a25bab8f7cf54c0" title="DeepSeek AI Database Exposed: Over 1 Million Log Lines, Secret Keys Leaked" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thehackernews.com/2025/01/deepseek-ai-database-exposed-over-1.html?m=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie4brg/deepseek_ai_database_exposed_over_1_million_log/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T02:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1idseqb</id>
    <title>DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!</title>
    <updated>2025-01-30T17:33:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt; &lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt; &lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt; &lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over ~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.&lt;/p&gt; &lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt; &lt;p&gt;If anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt; &lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt; &lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T17:33:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie94yy</id>
    <title>Tool calling support landed in llama.cpp today!</title>
    <updated>2025-01-31T06:59:16+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of the popular open models are supported: generic + native for Llama, Functionary, Hermes, Mistral, Firefunction, DeepSeek&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp/pull/9639"&gt;https://github.com/ggerganov/llama.cpp/pull/9639&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie94yy/tool_calling_support_landed_in_llamacpp_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T06:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieq0di</id>
    <title>The future of LLM performance isn't "self-reasoning" it's "self-loathing".</title>
    <updated>2025-01-31T21:34:56+00:00</updated>
    <author>
      <name>/u/cmndr_spanky</name>
      <uri>https://old.reddit.com/user/cmndr_spanky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq0di/the_future_of_llm_performance_isnt_selfreasoning/"&gt; &lt;img alt="The future of LLM performance isn't &amp;quot;self-reasoning&amp;quot; it's &amp;quot;self-loathing&amp;quot;." src="https://preview.redd.it/5zcwqw8ieege1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9e802a89047d8ec7165e10e50edd05126138752" title="The future of LLM performance isn't &amp;quot;self-reasoning&amp;quot; it's &amp;quot;self-loathing&amp;quot;." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmndr_spanky"&gt; /u/cmndr_spanky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5zcwqw8ieege1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq0di/the_future_of_llm_performance_isnt_selfreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieq0di/the_future_of_llm_performance_isnt_selfreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T21:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie0a8u</id>
    <title>QWEN just launched their chatbot website</title>
    <updated>2025-01-30T23:03:37+00:00</updated>
    <author>
      <name>/u/Vegetable-Practice85</name>
      <uri>https://old.reddit.com/user/Vegetable-Practice85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt; &lt;img alt="QWEN just launched their chatbot website" src="https://preview.redd.it/vzgzfrhlp7ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa67cbeae08d4c800e7b5dc088c0330556268f" title="QWEN just launched their chatbot website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link: &lt;a href="https://chat.qwenlm.ai/"&gt;https://chat.qwenlm.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable-Practice85"&gt; /u/Vegetable-Practice85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vzgzfrhlp7ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie0a8u/qwen_just_launched_their_chatbot_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T23:03:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1idv7yb</id>
    <title>Marc Andreessen on Anthropic CEO's Call for Export Controls on China</title>
    <updated>2025-01-30T19:29:13+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt; &lt;img alt="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" src="https://preview.redd.it/wlsi25dcn6ge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d695bb3258d357570ad11762d15df689f13fe2a8" title="Marc Andreessen on Anthropic CEO's Call for Export Controls on China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wlsi25dcn6ge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idv7yb/marc_andreessen_on_anthropic_ceos_call_for_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T19:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1idtkll</id>
    <title>Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more.</title>
    <updated>2025-01-30T18:20:59+00:00</updated>
    <author>
      <name>/u/deoxykev</name>
      <uri>https://old.reddit.com/user/deoxykev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt; &lt;img alt="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." src="https://external-preview.redd.it/VCPkBGJsVaggWY7c9V20KQQGCJhrF411vyVYUsHeuns.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=495bbbb03e5ebeff92050c2a71f7e340cb4bbebc" title="Interview with Deepseek Founder: We won’t go closed-source. We believe that establishing a robust technology ecosystem matters more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deoxykev"&gt; /u/deoxykev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thechinaacademy.org/interview-with-deepseek-founder-were-done-following-its-time-to-lead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idtkll/interview_with_deepseek_founder_we_wont_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T18:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ielupk</id>
    <title>I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API</title>
    <updated>2025-01-31T18:39:31+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"&gt; &lt;img alt="I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API" src="https://preview.redd.it/0pguliybjdge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9348eced5b7103939d1b6beb8b2ee81e5d8c3c83" title="I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pguliybjdge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T18:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iebs6c</id>
    <title>Coauthors of DeepSeek researchers. Can u spot Meta?</title>
    <updated>2025-01-31T10:22:53+00:00</updated>
    <author>
      <name>/u/osint_for_good</name>
      <uri>https://old.reddit.com/user/osint_for_good</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"&gt; &lt;img alt="Coauthors of DeepSeek researchers. Can u spot Meta?" src="https://preview.redd.it/9gdffiyl2bge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2aa09755c24832c4f07acbe500235ee5b176845" title="Coauthors of DeepSeek researchers. Can u spot Meta?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/osint_for_good"&gt; /u/osint_for_good &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9gdffiyl2bge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iebs6c/coauthors_of_deepseek_researchers_can_u_spot_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T10:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieosbx</id>
    <title>Relatively budget 671B R1 CPU inference workstation setup, 2-3T/s</title>
    <updated>2025-01-31T20:43:05+00:00</updated>
    <author>
      <name>/u/xinranli</name>
      <uri>https://old.reddit.com/user/xinranli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a post going over how to do Q2 R1 inference with a gaming rig by reading the weights directly from SSDs. It's a very neat technique and I would also like to share my experiences with CPU inference with a regular EPYC workstation setup. This setup has good memory capacity and relatively decent CPU inference performance, while also providing a great backbone for GPU or SSD expansions. Being a workstation rather than a server means this rig should be rather easily worked with and integrated into your bedroom. &lt;/p&gt; &lt;p&gt;I am using a Q4KM GGUF and still experimenting with turning cores/CCDs/SMT on and off on my 7773X and trying different context lengths to better understand where the limit is at, but 3T/s seems to be the limit as everything is still extremely memory bandwidth starved. &lt;/p&gt; &lt;p&gt;CPU: Any Milan EPYC over 32 cores should be okay. The price of these things varies greatly depending on the part number and if they are ES/QS/OEM/Production chips. I recommend buying an ES or OEM 64-core variant, some of them go for $500-$600. Some cheapest 32-core OEM models can go as low as $200-$300. Make sure you ask the seller CPU/board/BIOSver compatibility before purchasing. &lt;strong&gt;Never buy Lenovo or DELL locked EPYC chips unless you know what you are doing!&lt;/strong&gt; They are never going to work on consumer motherboards. Rome EPYCs can also work since they also support DDR4 3200, but they aren't too much cheaper and have quite a bit lower CPU performance compared to Milan. There are several overclockable ES/OEM Rome chips out here such as 32 core ZS1711E3VIVG5 and 100-000000054-04. 64 core ZS1406E2VJUG5 and 100-000000053-04. I had both ZS1711 and 54-04 and it was super fun to tweak around and OC them to 3.7GHz all core, if you can find one at a reasonable price, they are also great options. &lt;/p&gt; &lt;p&gt;Motherboard: H12SSL goes for around $500-600, and ROMED8-2T goes for $600-700. I recommend ROMED8-2T over H12SSL for the total 7x16 PCIe connectors rather than H12SSL's 5x16 + 2x8. &lt;/p&gt; &lt;p&gt;DRAM: This is where most money should be spent. You will want to get 8 sticks of 64GB DDR4 3200MT/s RDIMM. &lt;strong&gt;It has to be RDIMM (Registered DIMM), and it also has to be the same model of memory.&lt;/strong&gt; Each stick costs around $100-125, so in total you should spend $800-1000 on memory. This will give you 512GB capacity and 200GB/s bandwidth. The stick I got is HMAA8GR7AJR4N-XN, which works well with my ROMED8-2T. You don't have to pick from the QVL list of the motherboard vendor, just use it as a reference. 3200MT/s is not a strict requirement, if your budget is tight, you can go down to 2933 or 2666. Also, I would avoid 64GB LRDIMMs (Load Reduced DIMM). They are earlier DIMMs in DDR4 era when per DRAM chip density was still low, so each DRAM package has 2 or 4 chips packed inside (DDP or 3DS), the buffers on them are also additional points of failure. 128GB and 256GB LRDIMMs are the cutting edge for DDR4, but they are outrageously expensive and hard to find. 8x64GB is enough for Q4 inference.&lt;/p&gt; &lt;p&gt;CPU cooler: I would limit the spending here to around $50. Any SP3 heatsink should be OK. If you bought 280W TDP CPUs, consider maybe getting better ones but there is no need to go above $100.&lt;/p&gt; &lt;p&gt;PSU: This system should be a backbone for more GPUs to one day be installed. I would start with a pretty beefy one, maybe around 1200W ish. I think around $200 is a good spot to shop for. &lt;/p&gt; &lt;p&gt;Storage: Any 2TB+ NVME SSD should be fairly flexible, they are fairly cheap these days. $100&lt;/p&gt; &lt;p&gt;Case: I recommend a full-tower with dual PSU support. I highly recommend Lianli's o11 and o11 XL family. They are quite pricy but done really well. $200&lt;/p&gt; &lt;p&gt;In conclusion, this whole setup should cost around $2000-2500 from scratch, not too much more expensive than a single 4090 nowadays. It can do Q4 R1 inference with usable context length and it's going to be a good starting point for future local inference. The 7 x16 PCIe gen 4 expansion provided is really handy and can do so much more once you can afford more GPUs. &lt;/p&gt; &lt;p&gt;I am also looking into testing some old Xeons such as running dual E5v4s, they are dirt cheap right now. Will post some results once I have them running!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xinranli"&gt; /u/xinranli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ielhyu</id>
    <title>Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI</title>
    <updated>2025-01-31T18:24:36+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt; &lt;img alt="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Daniel &amp;amp; I (Mike) at &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; collabed with Tim from &lt;a href="https://github.com/open-webui/open-webui"&gt;Open WebUI&lt;/a&gt; to bring you this step-by-step on how to run the non-distilled DeepSeek-R1 Dynamic 1.58-bit model locally!&lt;/p&gt; &lt;p&gt;This guide is summarized so I highly recommend you read the full guide (with pics) here: &lt;a href="https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/"&gt;https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Expect 2 tokens/s with 96GB RAM (without GPU).&lt;/p&gt; &lt;h1&gt;To Run DeepSeek-R1:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download prebuilt binaries or build from source following &lt;a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"&gt;this guide&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Download the Model (1.58-bit, 131GB) from&lt;/strong&gt; &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get the model from &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Use Python to download it programmatically:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download snapshot_download( repo_id=&amp;quot;unsloth/DeepSeek-R1-GGUF&amp;quot;, local_dir=&amp;quot;DeepSeek-R1-GGUF&amp;quot;, allow_patterns=[&amp;quot;*UD-IQ1_S*&amp;quot;] ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Once the download completes, you’ll find the model files in a directory structure like this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-GGUF/ ├── DeepSeek-R1-UD-IQ1_S/ │ ├── DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf │ ├── DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf │ ├── DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Ensure you know the path where the files are stored.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Install and Run Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you don’t already have it installed, no worries! It’s a simple setup. Just follow the Open WebUI docs here: &lt;a href="https://docs.openwebui.com/"&gt;https://docs.openwebui.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Once installed, start the application - we’ll connect it in a later step to interact with the DeepSeek-R1 model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Start the Model Server with Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now that the model is downloaded, the next step is to run it using Llama.cpp’s server mode.&lt;/p&gt; &lt;h1&gt;🛠️Before You Begin:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Locate the llama-server Binary&lt;/li&gt; &lt;li&gt;If you built Llama.cpp from source, the llama-server executable is located in:llama.cpp/build/bin Navigate to this directory using:cd [path-to-llama-cpp]/llama.cpp/build/bin Replace [path-to-llama-cpp] with your actual Llama.cpp directory. For example:cd ~/Documents/workspace/llama.cpp/build/bin&lt;/li&gt; &lt;li&gt;Point to Your Model Folder&lt;/li&gt; &lt;li&gt;Use the full path to the downloaded GGUF files.When starting the server, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;🚀Start the Server&lt;/h1&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Example (If Your Model is in /Users/tim/Documents/workspace):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;✅ Once running, the server will be available at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://127.0.0.1:10000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;🖥️ Llama.cpp Server Running&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/erjbg5v5cbge1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fff4de133562bb6f67076db17285860b7294f2ad"&gt;After running the command, you should see a message confirming the server is active and listening on port 10000.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Step 5: Connect Llama.cpp to Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open Admin Settings in Open WebUI.&lt;/li&gt; &lt;li&gt;Go to Connections &amp;gt; OpenAI Connections.&lt;/li&gt; &lt;li&gt;Add the following details:&lt;/li&gt; &lt;li&gt;URL → &lt;a href="http://127.0.0.1:10000/v1API"&gt;http://127.0.0.1:10000/v1API&lt;/a&gt; Key → none&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Adding Connection in Open WebUI&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2"&gt;https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You don't need a GPU to run this model but it will make it faster especially when you have at least 24GB of VRAM.&lt;/li&gt; &lt;li&gt;Try to have a sum of RAM + VRAM = 120GB+ to get decent tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have any questions please let us know and also - any suggestions are also welcome! Happy running folks! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T18:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie5tls</id>
    <title>If you can't afford to run R1 locally, then being patient is your best action.</title>
    <updated>2025-01-31T03:35:25+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pause for a minute and read &lt;a href="https://simonwillison.net/2024/Dec/9/llama-33-70b/"&gt;I can now run a GPT-4 class model on my laptop&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It only take &lt;em&gt;20 months&lt;/em&gt; for smaller model that can run on consumer hardware to surpass bigger older models.&lt;/p&gt; &lt;p&gt;Yes, it feels like an eternity for internet user. But 1.5 years is small for human lifespan. Don't believe me? Llama 1 is almost 2 years old! (Released on February 24, 2023)&lt;/p&gt; &lt;p&gt;In the next 20 months, there will be small model that are better than R1.&lt;/p&gt; &lt;p&gt;Just like patient gamer save money waiting for steam sale, we save money by waiting for better, more efficient smaller model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie5tls/if_you_cant_afford_to_run_r1_locally_then_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T03:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieh01f</id>
    <title>Fully open source codebase to train SOTA VLMs</title>
    <updated>2025-01-31T15:15:11+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Andi from multimodal team at Hugging Face.&lt;/p&gt; &lt;p&gt;Today we're open-sourcing the codebase used to train SmolVLM from scratch on 256 H100s&lt;br /&gt; Inspired by our team's effort to open-source DeepSeek's R1 training, we are releasing the training and evaluation code on top of the weights&lt;br /&gt; Now you can train any of our SmolVLMs—or create your own custom VLMs!&lt;/p&gt; &lt;p&gt;Go check it out:&lt;br /&gt; &lt;a href="https://github.com/huggingface/smollm/tree/main/vision"&gt;https://github.com/huggingface/smollm/tree/main/vision&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1idz487</id>
    <title>'we're in this bizarre world where the best way to learn about llms... is to read papers by chinese companies. i do not think this is a good state of the world' - us labs keeping their architectures and algorithms secret is ultimately hurting ai development in the us.' - Dr Chris Manning</title>
    <updated>2025-01-30T22:13:22+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700560500416881"&gt;https://x.com/atroyn/status/1884700560500416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieaiq4</id>
    <title>Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source.</title>
    <updated>2025-01-31T08:44:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt; &lt;img alt="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." src="https://external-preview.redd.it/eMItb2dkatZwZPt-N-o-ODncWuvwGgn8w91JWJRsEcg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8507bb5e6a8a03ef08478289b8275d62d0245cc" title="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/lightblue/r1-multilingual-679c890166ac0a84e83e38fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefhfj</id>
    <title>Mistral Small 3 24B GGUF quantization Evaluation results</title>
    <updated>2025-01-31T14:03:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt; &lt;img alt="Mistral Small 3 24B GGUF quantization Evaluation results" src="https://external-preview.redd.it/6gL76ZMrrBOOoaB0ogD-JcHTREOFmGNz2SY3hi5tJtE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=506e8acadec458fdbc5263aab62e7fe23bff3e73" title="Mistral Small 3 24B GGUF quantization Evaluation results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c"&gt;https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1"&gt;https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab"&gt;https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please note that the purpose of this test is to check if the model's intelligence will be significantly affected at low quantization levels, rather than evaluating which gguf is the best.&lt;/p&gt; &lt;p&gt;Regarding Q6_K-lmstudio: This model was downloaded from the lmstudio hf repo and uploaded by bartowski. However, this one is a static quantization model, while others are dynamic quantization models from bartowski's own repo.&lt;/p&gt; &lt;p&gt;gguf: &lt;a href="https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF"&gt;https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Backend: &lt;a href="https://www.ollama.com/"&gt;https://www.ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation tool: &lt;a href="https://github.com/chigkim/Ollama-MMLU-Pro"&gt;https://github.com/chigkim/Ollama-MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation config: &lt;a href="https://pastebin.com/mqWZzxaH"&gt;https://pastebin.com/mqWZzxaH&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T14:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6gv0</id>
    <title>It’s time to lead guys</title>
    <updated>2025-01-31T04:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt; &lt;img alt="It’s time to lead guys" src="https://preview.redd.it/4r69mh9f89ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f3c997deb132531af541fbe7a279f1544512cbb" title="It’s time to lead guys" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4r69mh9f89ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T04:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieooqe</id>
    <title>DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark</title>
    <updated>2025-01-31T20:38:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt; &lt;img alt="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" src="https://preview.redd.it/i2p0m8em4ege1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4859ed3af650610750eb873e1231f2d526388ec" title="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i2p0m8em4ege1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iep1i4</id>
    <title>DeepSeek AI blocked by Italian authorities</title>
    <updated>2025-01-31T20:54:04+00:00</updated>
    <author>
      <name>/u/ApprehensiveCook2236</name>
      <uri>https://old.reddit.com/user/ApprehensiveCook2236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt; &lt;img alt="DeepSeek AI blocked by Italian authorities" src="https://external-preview.redd.it/cwVbdtxL_MOCraBvILhveGZjsoXBHPHOS4Ik8eBEAT4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74e97f5b3d89f2b950da588950bdaa1d7f71e9d" title="DeepSeek AI blocked by Italian authorities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveCook2236"&gt; /u/ApprehensiveCook2236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/01/31/deepseek-ai-blocked-by-italian-authorities-as-others-member-states-open-probes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieihjr</id>
    <title>What the hell do people expect?</title>
    <updated>2025-01-31T16:20:03+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release of R1 I saw so many &amp;quot;But it can't talk about tank man!&amp;quot;, &amp;quot;But it's censored!&amp;quot;, &amp;quot;But it's from the chinese!&amp;quot; posts. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are all censored. And for R1 in particular... I don't want to discuss chinese politics (or politics at all) with my LLM. That's not my use-case and I don't think I'm in a minority here.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would happen if it was not censored the way it is? The guy behind it would probably have disappeared by now.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They all give a fuck about data privacy as much as they can. Else we wouldn't have ever read about samsung engineers not being allowed to use GPT for processor development anymore.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The model itself is much less censored than the web chat&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;IMHO it's not worse or better than the rest (non self-hosted) and the negative media reports are 1:1 the same like back in the days when Zen was released by AMD and all Intel could do was cry like &amp;quot;But it's just cores they glued together!&amp;quot;&lt;/p&gt; &lt;p&gt;Edit: Added clarification that the web chat is more censored than the model itself (self-hosted)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iejazu</id>
    <title>DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it</title>
    <updated>2025-01-31T16:54:33+00:00</updated>
    <author>
      <name>/u/Fusseldieb</name>
      <uri>https://old.reddit.com/user/Fusseldieb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt; &lt;img alt="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" src="https://preview.redd.it/oemawg4i0dge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c75d540a6d15cd68cdeabc673be92b5e657f0e0" title="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fusseldieb"&gt; /u/Fusseldieb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oemawg4i0dge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefan2</id>
    <title>Idea: "Can I Run This LLM?" Website</title>
    <updated>2025-01-31T13:55:03+00:00</updated>
    <author>
      <name>/u/Dangerous_Bunch_3669</name>
      <uri>https://old.reddit.com/user/Dangerous_Bunch_3669</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt; &lt;img alt="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" src="https://preview.redd.it/l344q42n4cge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d54b955e3b1b2cf6f6d117e19782d25f8f4603c8" title="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have and idea. You know how websites like Can You Run It let you check if a game can run on your PC, showing FPS estimates and hardware requirements?&lt;/p&gt; &lt;p&gt;What if there was a similar website for LLMs? A place where you could enter your hardware specs and see:&lt;/p&gt; &lt;p&gt;Tokens per second, VRAM &amp;amp; RAM requirements etc.&lt;/p&gt; &lt;p&gt;It would save so much time instead of digging through forums or testing models manually. &lt;/p&gt; &lt;p&gt;Does something like this exist already? 🤔&lt;/p&gt; &lt;p&gt;I would pay for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Bunch_3669"&gt; /u/Dangerous_Bunch_3669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l344q42n4cge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T13:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ienetu</id>
    <title>Deepseek R1 is now hosted by Nvidia</title>
    <updated>2025-01-31T19:44:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt; &lt;img alt="Deepseek R1 is now hosted by Nvidia" src="https://preview.redd.it/1zufl131vdge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c70d8c80da395577b63301493ed66fac0dc6c408" title="Deepseek R1 is now hosted by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA just brought DeepSeek-R1 671-bn param model to NVIDIA NIM microservice on build.nvidia .com&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The DeepSeek-R1 NIM microservice can deliver up to 3,872 tokens per second on a single NVIDIA HGX H200 system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using NVIDIA Hopper architecture, DeepSeek-R1 can deliver high-speed inference by leveraging FP8 Transformer Engines and 900 GB/s NVLink bandwidth for expert communication.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;As usual with NVIDIA's NIM, its a enterprise-scale setu to securely experiment, and deploy AI agents with industry-standard APIs. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zufl131vdge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T19:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iehstw</id>
    <title>GPU pricing is spiking as people rush to self-host deepseek</title>
    <updated>2025-01-31T15:50:54+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt; &lt;img alt="GPU pricing is spiking as people rush to self-host deepseek" src="https://preview.redd.it/599a10y9pcge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60ac26bd7a2d3395eefcaee8fbf07a28102792f" title="GPU pricing is spiking as people rush to self-host deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/599a10y9pcge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:50:54+00:00</published>
  </entry>
</feed>
