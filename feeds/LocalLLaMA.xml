<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-01T14:06:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lp2jhr</id>
    <title>I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory</title>
    <updated>2025-07-01T14:04:51+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"&gt; &lt;img alt="I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory" src="https://external-preview.redd.it/4uXVV_gIKvEbP6L8sZKIJfYeWwmBsgdPdD9fj0WIUdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a9015fee8d0b44e9f420a04ef0902737f402d5" title="I created a script to allow running commands in an ephemeral VM to allow tool calling full access to a local directory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using `gemini` and `claude` commandline AI tools, and I wanted to have something that allowed my AI full and unrestricted access to a VM.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Mounts the local directory so it can read files&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Spawns a QEMU VM with access to those files&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Runs a command&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Returns&lt;/p&gt; &lt;p&gt;node ./scratchpad-cli --verbose --vm myvm run &amp;quot;python3 --version&amp;quot; ‚úì Found VM 'myvm' üöÄ Starting VM 'myvm'... Acceleration: kvm Work directory: /home/bigattichouse/workspace/Scratchpad/node SSH port: 2385 Mode: Ephemeral (changes discarded) Command: qemu-system-x86_64 -name myvm-session -machine pc -m 512M -accel kvm -cpu host -smp 2 -drive file=/home/bigattichouse/.scratchpad/vms/myvm/disk.qcow2,format=qcow2,if=virtio,snapshot=on -netdev user,id=net0,hostfwd=tcp::2385-:22 -device virtio-net-pci,netdev=net0 -virtfs local,path=/home/bigattichouse/workspace/Scratchpad/node,mount_tag=workdir,security_model=mapped-xattr,id=workdir -display none -serial null -monitor none ‚è≥ Connecting to VM... ‚úì Connected to VM ‚úì Mounted work directory&lt;/p&gt; &lt;p&gt;üìù Executing command... Command: cd /mnt/work 2&amp;gt;/dev/null || cd ~ &amp;amp;&amp;amp; python3 --version Python 3.10.12&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bigattichouse/scratchpad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2jhr/i_created_a_script_to_allow_running_commands_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp2ji0</id>
    <title>Reasoning models are risky. Anyone else experiencing this?</title>
    <updated>2025-07-01T14:04:52+00:00</updated>
    <author>
      <name>/u/interviuu</name>
      <uri>https://old.reddit.com/user/interviuu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a job application tool and have been testing pretty much every LLM model out there for different parts of the product. One thing that's been driving me crazy: reasoning models seem particularly dangerous for business applications that need to go from A to B in a somewhat rigid way.&lt;/p&gt; &lt;p&gt;I wouldn't call it &amp;quot;deterministic output&amp;quot; because that's not really what LLMs do, but there are definitely use cases where you need a certain level of consistency and predictability, you know?&lt;/p&gt; &lt;p&gt;Here's what I keep running into with reasoning models:&lt;/p&gt; &lt;p&gt;During the reasoning process (and I know Anthropic has shown that what we read isn't the &amp;quot;real&amp;quot; reasoning happening), the LLM tends to ignore guardrails and specific instructions I've put in the prompt. The output becomes way more unpredictable than I need it to be.&lt;/p&gt; &lt;p&gt;Sure, I can define the format with JSON schemas (or objects) and that works fine. But the actual content? It's all over the place. Sometimes it follows my business rules perfectly, other times it just doesn't. And there's no clear pattern I can identify.&lt;/p&gt; &lt;p&gt;For example, I need the model to extract specific information from resumes and job posts, then match them according to pretty clear criteria. With regular models, I get consistent behavior most of the time. With reasoning models, it's like they get &amp;quot;creative&amp;quot; during their internal reasoning and decide my rules are more like suggestions.&lt;/p&gt; &lt;p&gt;I've tested almost all of them (from Gemini to DeepSeek) and honestly, none have convinced me for this type of structured business logic. They're incredible for complex problem-solving, but for &amp;quot;follow these specific steps and don't deviate&amp;quot; tasks? Not so much.&lt;/p&gt; &lt;p&gt;Anyone else dealing with this? Am I missing something in my prompting approach, or is this just the trade-off we make with reasoning models? I'm curious if others have found ways to make them more reliable for business applications.&lt;/p&gt; &lt;p&gt;What's been your experience with reasoning models in production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/interviuu"&gt; /u/interviuu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp2ji0/reasoning_models_are_risky_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T14:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lox1f7</id>
    <title>Local models not following instructions</title>
    <updated>2025-07-01T09:16:05+00:00</updated>
    <author>
      <name>/u/thecookingsenpai</name>
      <uri>https://old.reddit.com/user/thecookingsenpai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some problems on applying local LLMs to structured workflows.&lt;/p&gt; &lt;p&gt;I use 8b to 24b models on my 16GB 4070 Super TI&lt;/p&gt; &lt;p&gt;I have no problems in chatting or doing web rag with my models, either using open webui or AnythingLLM or custom solutions in python or nodejs. What I am unable to do is doing some more structured work. &lt;/p&gt; &lt;p&gt;Specifically, but this is just an example, I am trying to have my models output a specific JSON format. &lt;/p&gt; &lt;p&gt;I am trying almost everything in the system prompt and even in forcing json responses from ollama, but 70% of the times the models just produce wrong outputs. &lt;/p&gt; &lt;p&gt;Now, my question is more generic than having this specific json so I am not sure about posting the prompt etc. &lt;/p&gt; &lt;p&gt;My question is: are there models that are more suited to follow instructions than others? &lt;/p&gt; &lt;p&gt;Mistral 3.2 is almost always a failure in producing a decent json, so is Gemma 12b&lt;/p&gt; &lt;p&gt;Any specific tips and tricks or models to test? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecookingsenpai"&gt; /u/thecookingsenpai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T09:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovqjc</id>
    <title>Best Local Model for Vision?</title>
    <updated>2025-07-01T07:46:16+00:00</updated>
    <author>
      <name>/u/xukecheng</name>
      <uri>https://old.reddit.com/user/xukecheng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe Gemma3 is the best model for vision tasks? Each image uses only 256 tokens. In my own hardware tests, it was the only model capable of processing 60 images simultaneously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xukecheng"&gt; /u/xukecheng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1loza95</id>
    <title>Dual RX580 2048SP (16GB) llama.cpp(vulkan)</title>
    <updated>2025-07-01T11:33:27+00:00</updated>
    <author>
      <name>/u/IVequalsW</name>
      <uri>https://old.reddit.com/user/IVequalsW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I have a server in my house with dual rx580 (16gb) in it, running llama.cpp via Vulkan. it runs the Qwen-3-32B-q5 (28GB total) at about 4.5 - 4.8 t/s. &lt;/p&gt; &lt;p&gt;does anyone want me to test any other ggufs? I could test it with 1 or both of the GPUs. &lt;/p&gt; &lt;p&gt;they work relatively well and are really cheap for a large amount of vram. Memory bus speed is about 256GB/s. &lt;/p&gt; &lt;p&gt;Give ideas in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IVequalsW"&gt; /u/IVequalsW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loza95/dual_rx580_2048sp_16gb_llamacppvulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lozhqc</id>
    <title>Cannot Load any GGUF model using tools like LM Studio or Jan Ai etc</title>
    <updated>2025-07-01T11:44:42+00:00</updated>
    <author>
      <name>/u/Physical-Citron5153</name>
      <uri>https://old.reddit.com/user/Physical-Citron5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So everything was okay until I upgraded from Windows 10 to 11 and suddenly I couldn‚Äôt load any local model through these GUI interfaces. I don‚Äôt see any error; it just loads indefinitely, no VRAM will also get occupied. &lt;/p&gt; &lt;p&gt;I checked with llama cpp and it worked fine, no errors.&lt;/p&gt; &lt;p&gt;I have 2x RTX 3090 and I am just confused why this is happening. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical-Citron5153"&gt; /u/Physical-Citron5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0j7f</id>
    <title>Best open source Arabic tts</title>
    <updated>2025-07-01T12:36:32+00:00</updated>
    <author>
      <name>/u/Spiritual_Button827</name>
      <uri>https://old.reddit.com/user/Spiritual_Button827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I‚Äôve been trying to find the best TTS options to fine tune for Arabic and I‚Äôve kinda hit a wall with Fish audio after their release of the new S1 model, as they‚Äôve removed the fine tuning code for older models like v1.5.&lt;/p&gt; &lt;p&gt;I tried coqui‚Äôs XTTS fork by Idap: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;https://github.com/idiap/coqui-ai-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And got good results, but I would like to try other good options.&lt;/p&gt; &lt;p&gt;I looked at &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;https://huggingface.co/spaces/TTS-AGI/TTS-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I see that not many options support Arabic.&lt;/p&gt; &lt;p&gt;My use case is: real time inference of Arabic text for an interactive chatbot&lt;/p&gt; &lt;p&gt;I‚Äôm kinda new to TTS and would appreciate any help/advice.&lt;/p&gt; &lt;p&gt;I have a good server in hand with lots of compute to test anything so any open source model with fine tuning code available and can support Arabic is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Button827"&gt; /u/Spiritual_Button827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotzy4</id>
    <title>Video Cards &amp; GPUs SPARKLE intros new Arc Pro B60 cards: one is a dual-GPU workstation card with 48GB of VRAM</title>
    <updated>2025-07-01T05:52:37+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/106121/sparkle-intros-new-arc-pro-b60-cards-one-is-dual-gpu-workstation-card-with-48gb-of-vram/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0o3i</id>
    <title>Resources to learn about samplers?</title>
    <updated>2025-07-01T12:43:05+00:00</updated>
    <author>
      <name>/u/Black-Mack</name>
      <uri>https://old.reddit.com/user/Black-Mack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could you share how to learn more about samplers?&lt;/p&gt; &lt;p&gt;Anything is fine: blogs, articles, videos, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Black-Mack"&gt; /u/Black-Mack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lomilz</id>
    <title>[Tool] Run GPT-style models from a USB stick ‚Äì no install, no internet, no GPU ‚Äì meet Local LLM Notepad üöÄ</title>
    <updated>2025-06-30T23:22:23+00:00</updated>
    <author>
      <name>/u/Awkward-Dare-1127</name>
      <uri>https://old.reddit.com/user/Awkward-Dare-1127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt; &lt;img alt="[Tool] Run GPT-style models from a USB stick ‚Äì no install, no internet, no GPU ‚Äì meet Local LLM Notepad üöÄ" src="https://external-preview.redd.it/4LxPZe7g9FzOelbXz3aOGVDeWd4mgBA3OWlLsXGi_Bc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61aa76d902ab96a1963a6d4338aa8b21a38657e" title="[Tool] Run GPT-style models from a USB stick ‚Äì no install, no internet, no GPU ‚Äì meet Local LLM Notepad üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Copy one portable&lt;/em&gt; &lt;code&gt;.exe&lt;/code&gt; &lt;em&gt;+ a&lt;/em&gt; &lt;code&gt;.gguf&lt;/code&gt; &lt;em&gt;model to a flash drive ‚Üí double-click on any Windows PC ‚Üí start chatting offline in seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;GitHub ‚ñ∂Ô∏é &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad"&gt;&lt;strong&gt;https://github.com/runzhouye/Local_LLM_Notepad&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8"&gt;https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lz6e4zmpd5af1.gif"&gt;https://i.redd.it/lz6e4zmpd5af1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;30-second Quick-Start&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Grab &lt;strong&gt;Local_LLM_Notepad-portable.exe&lt;/strong&gt; from the &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad/releases"&gt;latest release&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download a small CPU model like &lt;strong&gt;gemma-3-1b-it-Q4_K_M.gguf&lt;/strong&gt; (‚âà0.8 GB) from &lt;a href="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/tree/main"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Copy both files onto a USB stick.&lt;/li&gt; &lt;li&gt;Double-click the EXE on any Windows box ‚Üí first run loads the model.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;‚úÖ&lt;/th&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What it means&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Plug-and-play&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Single 45 MB EXE runs without admin rights&lt;/td&gt; &lt;td align="left"&gt;Run on any computer‚Äîno install needed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Source-word highlighting&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Bold-underlines every word/number from your prompt&lt;/td&gt; &lt;td align="left"&gt;Ctrl-click to trace facts &amp;amp; tables for quick fact-checking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hotkeys&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Ctrl + SCtrl + ZCtrl + FCtrl + X&lt;/code&gt; send, stop, search, clear, etc.&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Portable chat logs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;One-click JSON export&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Dare-1127"&gt; /u/Awkward-Dare-1127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokkpc</id>
    <title>A Meta-Framework for Self-Improving LLMs with Transparent Reasoning</title>
    <updated>2025-06-30T21:59:34+00:00</updated>
    <author>
      <name>/u/henryb213</name>
      <uri>https://old.reddit.com/user/henryb213</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt; &lt;img alt="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" src="https://external-preview.redd.it/GF7LOLNV1EkT3j_WQj3wN6pKRBc62ktaNGoxeqmHjug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31f4c15b33f9e40cd80aee5e1468225b045437e8" title="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Framework overview:&lt;/strong&gt; LLMs iteratively refine their own outputs‚Äîtypically through a three‚Äëphase cycle &lt;strong&gt;draft ‚Üí critique ‚Üí revision&lt;/strong&gt;, repeat until convergence (all phases &amp;amp; stop rules are configurable). I started coding three weeks ago after an eight‚Äëyear break and zero professional dev experience.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;The classes work as Python callables with built in observability: instances are callable -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Python,tabs=4 from recursive_companion.base import MarketingCompanion agent = MarketingCompanion() answer = agent(&amp;quot;question or problem‚Ä¶&amp;quot;) # final refined output print(answer) print(agent.run_log) # list[dict] of every draft, critique &amp;amp; revision &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Why it stays clean &amp;amp; modular&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Templates are plain text files (system prompts, user prompts, protocol). &lt;em&gt;Swap harsh critiques for creative ones by swapping files.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;build_templates()&lt;/code&gt; lets you compose any combination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol injection&lt;/strong&gt; cleanly separates reasoning patterns from implementation.&lt;/li&gt; &lt;li&gt;New agents in &lt;strong&gt;3 lines&lt;/strong&gt;‚Äîjust inherit from &lt;code&gt;BaseCompanion&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Convergence uses &lt;strong&gt;embedding‚Äëbased cosine similarity&lt;/strong&gt; by default, but the metric is fully pluggable.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How it came together&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The design emerged from recursive dialogues with multiple LLMs‚Äîthe same iterative process the framework now automates. No legacy assumptions meant every piece became independent: swap models, add phases, change convergence logic‚Äîno rewiring required.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streamlit app&lt;/strong&gt; shows the thinking live as it happens.&lt;/li&gt; &lt;li&gt;Demos cover raw orchestration &lt;em&gt;and&lt;/em&gt; LangGraph integration (agents as graph nodes).&lt;/li&gt; &lt;li&gt;Full architecture docs, comprehensive docstrings, commenting, and worked examples included.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT)&lt;/strong&gt; &lt;a href="https://github.com/hankbesser/recursive-companion"&gt;https://github.com/hankbesser/recursive-companion&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built by questioning everything. Learning by building, built for learning.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for reading and really looking for any feedback and open to contributors, no question or discussion is too big or small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryb213"&gt; /u/henryb213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hankbesser/recursive-companion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1loxw8f</id>
    <title>What is night forge?</title>
    <updated>2025-07-01T10:12:17+00:00</updated>
    <author>
      <name>/u/Professional-Ad-4376</name>
      <uri>https://old.reddit.com/user/Professional-Ad-4376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt; &lt;img alt="What is night forge?" src="https://a.thumbs.redditmedia.com/93U5v10Vycvd1XA2AyyAUDnfoGNgsP5NzRsHUeD4F_0.jpg" title="What is night forge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec"&gt;https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a webdev arena, and one was very distinct in its style but I preferred it.&lt;/p&gt; &lt;p&gt;after voting for it, it said it was nightforge? I tried googling but couldn't find anything. Am I on the moon or whats going on?&lt;/p&gt; &lt;p&gt;Does anyone know what this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Ad-4376"&gt; /u/Professional-Ad-4376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T10:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1loo2u3</id>
    <title>Struggling with vLLM. The instructions make it sound so simple to run, but it‚Äôs like my Kryptonite. I give up.</title>
    <updated>2025-07-01T00:35:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm normally the guy they call in to fix the IT stuff nobody else can fix. I‚Äôll laser focus on whatever it is and figure it out probably 99% of the time. I‚Äôve been in IT for over 28+ years. I‚Äôve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I‚Äôve never encountered a more difficult software package to run than trying to get vLLM working in Docker. I can run nearly anything else in Docker except for vLLM. I feel like I‚Äôm really close, but every time I think it‚Äôs going to run, BAM! some new error that i find very little information on. - I‚Äôm running Ubuntu 24.04 - I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. - Yes I have the Nvidia runtime container working - Yes I have the hugginface token generated is there an easy button somewhere that I‚Äôm missing? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T00:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1loswvr</id>
    <title>New to the scene. Yesterday, got 4 t/s on R1 671b q4. Today, I'm getting about 0.15 t/s... What did I break lol</title>
    <updated>2025-07-01T04:46:12+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5975wx, 512gb DDR4 3200, dual 3090s. Ollama + OpenWebUI. Running on LMDE.&lt;/p&gt; &lt;p&gt;Idk what went wrong now but I'm struggling to get it back to 4 t/s... I can work with 4 t/s, but 0.15 t/s is just terrible.&lt;/p&gt; &lt;p&gt;Any ideas? Happy to provide information upon request.&lt;/p&gt; &lt;p&gt;Total noob here, just built this a few days ago and very little terminal experience lol but have an open mind and a will to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovuxp</id>
    <title>Current state of Intel A770 16GB GPU for Inference?</title>
    <updated>2025-07-01T07:55:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I could only find old posts regarding how the Intel A770 fares with LLMs, specifically people notice the high idle power consumption and difficult setup depending on what framework you use. At least a year ago, it was supposed to be a pain to use with Ollama.&lt;/p&gt; &lt;p&gt;Here in Germany, it is by far the cheapest 16GB card, in summary:&lt;br /&gt; - Intel A770, prices starting at 280-300‚Ç¨&lt;br /&gt; - AMD 9060 XT starting at 370‚Ç¨ (+32%)&lt;br /&gt; - Nvidia RTX 5060 Ti starting at 440‚Ç¨ (+57%)&lt;/p&gt; &lt;p&gt;Price-wise the A770 is a no-brainer, but what is your current experience? Currently using an RTX 4060 8GB and LMStudio on Windows 11 (+32GB DDR5).&lt;/p&gt; &lt;p&gt;Thanks for any insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta‚Äôs ‚ÄòSuperintelligence‚Äô Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotza5</id>
    <title>KrunchWrapper - a LLM compression proxy (beta)</title>
    <updated>2025-07-01T05:51:25+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt; &lt;img alt="KrunchWrapper - a LLM compression proxy (beta)" src="https://preview.redd.it/c4bjroisb7af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b39a5201c024ced3ca9aba3ebe3b3090ade2d9" title="KrunchWrapper - a LLM compression proxy (beta)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that &amp;quot;compresses&amp;quot; requests sent to models as a proof of concept. I've seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a &amp;quot;decoder&amp;quot; to the LLM via a system prompt.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Github Link&lt;/strong&gt;: &lt;a href="https://github.com/thad0ctor/KrunchWrapper"&gt;https://github.com/thad0ctor/KrunchWrapper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.&lt;/p&gt; &lt;p&gt;Between compression and (optional) comment stripping, &lt;strong&gt;I have been able to acheive &amp;gt;40% compression when passing code files to the LLM that contain lots of repetition.&lt;/strong&gt; So far I haven't had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.&lt;/p&gt; &lt;p&gt;At its core, what KrunchWrapper essentially does is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Receive:&lt;/strong&gt; Establishes a proxy server that &amp;quot;intercepts&amp;quot; prompts going to a LLM server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Analyzes those prompts for common patterns of text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assign:&lt;/strong&gt; Maps a unicode symbol (known to use fewer tokens) to that pattern of text &lt;ol&gt; &lt;li&gt;Analyzes whether savings &amp;gt; system prompt overhead&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compress:&lt;/strong&gt; Replaces all identified patterns of text with the selected symbol(s) &lt;ol&gt; &lt;li&gt; Preserves JSON, markdown, tool calls&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Passes a system prompt with the compression decoder to the LLM along with the compressed message&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct:&lt;/strong&gt; Instucts the LLM to use the compressed symbols in any response&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decompress:&lt;/strong&gt; Decodes any responses received from the LLM that contain the compressed symbols&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat:&lt;/strong&gt; Intilligently adds to and re-uses any compression dictionaries in follow-on messages&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Beyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not &amp;quot;waste&amp;quot; compression tokens on minimal impact compression opportunities.&lt;/p&gt; &lt;p&gt;Looking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; I am not a programmer by trade. I refuse to use the v-word I so often see on here but let's just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.&lt;/p&gt; &lt;p&gt;This type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4bjroisb7af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp01c7</id>
    <title>Deepseek R1 at 6,5 tk/s on an Nvidia Tesla P40</title>
    <updated>2025-07-01T12:12:19+00:00</updated>
    <author>
      <name>/u/dc740</name>
      <uri>https://old.reddit.com/user/dc740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I figured I'd post my final setup since many people asked about the P40 and assumed you couldn't do much with it (but you can!).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./ik_llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \ --threads 40 \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --top-p 0.95 \ --temp 0.6 \ --ctx-size 32768 \ --seed 3407 \ --n-gpu-layers 62 \ -ot &amp;quot;exps=CPU&amp;quot; \ --mlock \ --no-mmap \ -mla 2 -fa -fmoe \ -ser 5,1 \ -amb 512 \ --prompt &amp;quot;&amp;lt;ÔΩúUserÔΩú&amp;gt;Create a Flappy Bird game in Python.&amp;lt;ÔΩúAssistantÔΩú&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result at the end of the run is around 6.5tk/s. &amp;lt;EDIT: Did another run and added the results. 7tk/s!&amp;gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_print_timings: load time = 896376.08 ms llama_print_timings: sample time = 594.81 ms / 2549 runs ( 0.23 ms per token, 4285.42 tokens per second) llama_print_timings: prompt eval time = 1193.93 ms / 12 tokens ( 99.49 ms per token, 10.05 tokens per second) llama_print_timings: eval time = 363871.92 ms / 2548 runs ( 142.81 ms per token, 7.00 tokens per second) llama_print_timings: total time = 366975.53 ms / 2560 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm open to ideas on how to improve it.&lt;/p&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully populated Dell R740 (in performance profile)&lt;/li&gt; &lt;li&gt;Nvidia Tesla P40 (24GB vram)&lt;/li&gt; &lt;li&gt;Xeon Gold 6138&lt;/li&gt; &lt;li&gt;1.5TB of ram (all ram slots populated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For other models, like Mistral or QwQ I get around 10tk/s&lt;/p&gt; &lt;p&gt;These are my QwQ settings (I use the regular llama.cpp for this one)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 40 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --temp 0.6 \ --repeat-penalty 1.1 \ --min-p 0.01 \ --top-k 40 \ --top-p 0.95 \ --dry-multiplier 0.5 \ --mlock \ --no-mmap \ --prio 3 \ -no-cnv \ -fa \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The details on the selected quants are in the model path. Surprisingly, using ik_llama.cpp optimized models from &lt;em&gt;ubergarm&lt;/em&gt; did not speed up Deepseek, but it slowed it down greatly.&lt;/p&gt; &lt;p&gt;Feel free to suggest improvements. For models different than deepseek, ik_llama.cpp was giving me a lot of gibberish output if I enabled fast attention. And some models I couldn't even run on it, so that's why I still use the regular llama.cpp for some of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dc740"&gt; /u/dc740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:12:19+00:00</published>
  </entry>
</feed>
