<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-08T11:22:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kh5vrx</id>
    <title>Trying out the Ace-Step Song Generation Model</title>
    <updated>2025-05-07T19:15:23+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt; &lt;img alt="Trying out the Ace-Step Song Generation Model" src="https://external-preview.redd.it/bDY3Zm5xNjd0ZXplMfhj0slUHjrXE-UilZ0dRUIJzmh3kn39RuiWSQcdWvp9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0b49332e389d6104e19b726896e9784113cd99d" title="Trying out the Ace-Step Song Generation Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I got Gemini to whip up some lyrics for an alphabet song, and then I used ACE-Step-v1-3.5B to generate a rock-style track at 105bpm. &lt;/p&gt; &lt;p&gt;Give it a listen – how does it sound to you?&lt;/p&gt; &lt;p&gt;My feeling is that some of the transitions are still a bit off, and there are issues with the pronunciation of individual lyrics. But on the whole, it's not bad! I reckon it'd be pretty smooth for making those catchy, repetitive tunes (like that &amp;quot;Shawarma Legend&amp;quot; kind of vibe).&lt;br /&gt; This was generated on HuggingFace, took about 50 seconds.&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dfm1hq67teze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5vrx/trying_out_the_acestep_song_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh5f4q</id>
    <title>Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM</title>
    <updated>2025-05-07T18:57:06+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"&gt; &lt;img alt="Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM" src="https://external-preview.redd.it/5PkORNAHF4mCCL5YAzubG_UIVEwPjYPSSg6fvkNboCI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0822aa3d4e06f9272257151f7579577e8ecd98ba" title="Beelink Launches GTR9 Pro And GTR9 AI Mini PCs, Featuring AMD Ryzen AI Max+ 395 And Up To 128 GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/beelink-launches-gtr9-pro-and-gtr9-mini-pcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh5f4q/beelink_launches_gtr9_pro_and_gtr9_ai_mini_pcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9ape</id>
    <title>Collection of LLM System Prompts</title>
    <updated>2025-05-07T21:34:34+00:00</updated>
    <author>
      <name>/u/Haunting-Stretch8069</name>
      <uri>https://old.reddit.com/user/Haunting-Stretch8069</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"&gt; &lt;img alt="Collection of LLM System Prompts" src="https://external-preview.redd.it/mXIp6Z--Pl_AS_wcxpDFLh4ENIm93uSILhbV_jZ63CM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06835e2d945227966f4734fc5037151786b32e51" title="Collection of LLM System Prompts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting-Stretch8069"&gt; /u/Haunting-Stretch8069 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/guy915/LLM-System-Prompts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9ape/collection_of_llm_system_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1khfhoh</id>
    <title>Final verdict on LLM generated confidence scores?</title>
    <updated>2025-05-08T02:32:59+00:00</updated>
    <author>
      <name>/u/sg6128</name>
      <uri>https://old.reddit.com/user/sg6128</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember earlier hearing the confidence scores associated with a prediction from an LLM (e.g. classify XYZ text into A,B,C categories and provide a confidence score from 0-1) are gibberish and not really useful. &lt;/p&gt; &lt;p&gt;I see them used widely though and have since seen some mixed opinions on the idea.&lt;/p&gt; &lt;p&gt;While the scores are not useful in the same way a propensity is (after all it’s just tokens), they are still indicative of some sort of confidence &lt;/p&gt; &lt;p&gt;I’ve also seen that using qualitative confidence e.g. Level of confidence: low, medium, high, is better than using numbers. &lt;/p&gt; &lt;p&gt;Just wondering what’s the latest school of thought on this and whether in practice you are using confidence scores in this way, and your observations about them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sg6128"&gt; /u/sg6128 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T02:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzskq</id>
    <title>Mistral-Medium 3 (unfortunately no local support so far)</title>
    <updated>2025-05-07T15:12:17+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt; &lt;img alt="Mistral-Medium 3 (unfortunately no local support so far)" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral-Medium 3 (unfortunately no local support so far)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzskq/mistralmedium_3_unfortunately_no_local_support_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kguqmd</id>
    <title>Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp; Servicenow)</title>
    <updated>2025-05-07T11:14:13+00:00</updated>
    <author>
      <name>/u/Temporary-Size7310</name>
      <uri>https://old.reddit.com/user/Temporary-Size7310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt; &lt;img alt="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" src="https://external-preview.redd.it/EuRAXuyDLNOAu2-1ktV_-X31N5aAiqTZTPHiWEhPj-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d4cc697d8897122cd61888ad7f02b892afd49c4" title="Apriel-Nemotron-15b-Thinker - o1mini level with MIT licence (Nvidia &amp;amp; Servicenow)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Service now and Nvidia brings a new 15B thinking model with comparable performance with 32B&lt;br /&gt; Model: &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker"&gt;https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker&lt;/a&gt; (MIT licence)&lt;br /&gt; It looks very promising (resumed by Gemini) : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; Claimed to be half the size of some SOTA models (like QWQ-32b, EXAONE-32b) and consumes significantly fewer tokens (~40% less than QWQ-32b) for comparable tasks, directly impacting VRAM requirements and inference costs for local or self-hosted setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning/Enterprise:&lt;/strong&gt; Reports strong performance on benchmarks like MBPP, BFCL, Enterprise RAG, IFEval, and Multi-Challenge. The focus on Enterprise RAG is notable for business-specific applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; Competitive results on coding tasks like MBPP and HumanEval, important for development workflows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic:&lt;/strong&gt; Holds competitive scores on academic reasoning benchmarks (AIME, AMC, MATH, GPQA) relative to its parameter count.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; We need to test it&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Size7310"&gt; /u/Temporary-Size7310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kguqmd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kguqmd/aprielnemotron15bthinker_o1mini_level_with_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T11:14:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1khmaah</id>
    <title>5 commands to run Qwen3-235B-A22B Q3 inference on 4x3090 + 32-core TR + 192GB DDR4 RAM</title>
    <updated>2025-05-08T09:59:25+00:00</updated>
    <author>
      <name>/u/EmilPi</name>
      <uri>https://old.reddit.com/user/EmilPi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, thanks Qwen team for the generosity, and Unsloth team for quants.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;: optimized for my build, your options may vary (e.g. I have slow RAM, which does not work above 2666MHz, and only 3 channels of RAM available). This set of commands downloads GGUFs into llama.cpp's folder build/bin folder. If unsure, use full paths. I don't know why, but llama-server may not work if working directory is different.&lt;/p&gt; &lt;p&gt;End result: 125-180 tokens per second read speed (prompt processing), 12-15 tokens per second write speed (generation) - depends on prompt/response/context length. I use 8k context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;0. You need CUDA installed (so, I kinda lied) and available in your PATH:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/"&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Download &amp;amp; Compile llama.cpp:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp rm -rf build ; cmake -B build -DBUILD_SHARED_LIBS=ON -DLLAMA_CURL=OFF -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_USE_GRAPHS=ON ; cmake --build build --config Release --parallel 32 cd build/bin &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;2. Download quantized model (that almost fits into 96GB VRAM) files:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for i in {1..3} ; do curl -L --remote-name &amp;quot;https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q3_K_XL/Qwen3-235B-A22B-UD-Q3_K_XL-0000${i}-of-00003.gguf?download=true&amp;quot; ; done &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;3. Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --port 1234 \ --model ./Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf \ --alias Qwen3-235B-A22B-Thinking \ --temp 0.6 --top-k 20 --min-p 0.0 --top-p 0.95 \ -ngl 95 --split-mode layer -ts 22,23,24,26 \ -c 8192 -ctk q8_0 -ctv q8_0 -fa \ --main-gpu 3 \ --no-mmap \ -ot 'blk\.[2-3]1\.ffn.*=CPU' \ -ot 'blk\.[5-8]1\.ffn.*=CPU' \ -ot 'blk\.9[0-1]\.ffn.*=CPU' \ --threads 32 --numa distribute &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmilPi"&gt; /u/EmilPi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:59:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzey8</id>
    <title>Run FLUX.1 losslessly on a GPU with 20GB VRAM</title>
    <updated>2025-05-07T14:57:14+00:00</updated>
    <author>
      <name>/u/arty_photography</name>
      <uri>https://old.reddit.com/user/arty_photography</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've released &lt;strong&gt;losslessly compressed versions&lt;/strong&gt; of the &lt;strong&gt;12B FLUX.1-dev&lt;/strong&gt; and &lt;strong&gt;FLUX.1-schnell&lt;/strong&gt; models using &lt;strong&gt;DFloat11&lt;/strong&gt;, a compression method that applies entropy coding to BFloat16 weights. This reduces model size by &lt;strong&gt;~30%&lt;/strong&gt; &lt;em&gt;without changing outputs&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;This brings the models down from &lt;strong&gt;24GB to ~16.3GB&lt;/strong&gt;, enabling them to run on a &lt;strong&gt;single GPU with 20GB or more of VRAM&lt;/strong&gt;, with only a &lt;strong&gt;few seconds of extra overhead per image&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;🔗 Downloads &amp;amp; Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-dev&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-dev-DF11"&gt;huggingface.co/DFloat11/FLUX.1-dev-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed FLUX.1-schnell&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11/FLUX.1-schnell-DF11"&gt;huggingface.co/DFloat11/FLUX.1-schnell-DF11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Example Code&lt;/strong&gt;: &lt;a href="https://github.com/LeanModels/DFloat11/tree/master/examples/flux.1"&gt;github.com/LeanModels/DFloat11/tree/master/examples/flux.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compressed LLMs (Qwen 3, Gemma 3, etc.)&lt;/strong&gt;: &lt;a href="https://huggingface.co/DFloat11"&gt;huggingface.co/DFloat11&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2504.11651"&gt;arxiv.org/abs/2504.11651&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feedback welcome&lt;/strong&gt;! Let me know if you try them out or run into any issues!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arty_photography"&gt; /u/arty_photography &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzey8/run_flux1_losslessly_on_a_gpu_with_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T14:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1khlxzj</id>
    <title>If you could make a MoE with as many active and total parameters as you wanted. What would it be?</title>
    <updated>2025-05-08T09:35:09+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khlxzj/if_you_could_make_a_moe_with_as_many_active_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh3g7f</id>
    <title>Did anyone try out Mistral Medium 3?</title>
    <updated>2025-05-07T17:38:36+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt; &lt;img alt="Did anyone try out Mistral Medium 3?" src="https://external-preview.redd.it/Z3k2eW51bjJiZXplMcUDN_ixsC3ErmhxAmaSJ8XxFt_ddYdhD_A2seyNDJhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=380661f2e13d59675a91c29257e8bd38b702503f" title="Did anyone try out Mistral Medium 3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I briefly tried Mistral Medium 3 on OpenRouter, and I feel its performance might not be as good as Mistral's blog claims. (The video shows the best result out of the 5 shots I ran. ) &lt;/p&gt; &lt;p&gt;Additionally, I tested having it recognize and convert the benchmark image from the blog into JSON. However, it felt like it was just randomly converting things, and not a single field matched up. Could it be that its input resolution is very low, causing compression and therefore making it unable to recognize the text in the image? &lt;/p&gt; &lt;p&gt;Also, I don't quite understand why it uses 5-shot in the GPTQ diamond and MMLU Pro benchmarks. Is that the default number of shots for these tests?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6w9w0rl2beze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh3g7f/did_anyone_try_out_mistral_medium_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T17:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgrjor</id>
    <title>New ""Open-Source"" Video generation model</title>
    <updated>2025-05-07T07:32:32+00:00</updated>
    <author>
      <name>/u/topiga</name>
      <uri>https://old.reddit.com/user/topiga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt; &lt;img alt="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" src="https://external-preview.redd.it/ZHdlOHlodmQ5YnplMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6db55c8236c9c875dc6ec641feb87d228687bd65" title="New &amp;quot;&amp;quot;Open-Source&amp;quot;&amp;quot; Video generation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216×704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &lt;p&gt;To be honest, I don't view it as open-source, not even open-weight. The license is weird, not a license we know of, and there's &amp;quot;Use Restrictions&amp;quot;. By doing so, it is NOT open-source.&lt;br /&gt; Yes, the restrictions are honest, and I invite you to read them, &lt;a href="https://static.lightricks.com/legal/LTXV-13b-0.9.7-dev.pdf"&gt;here is an example&lt;/a&gt;, but I think they're just doing this to protect themselves.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;https://github.com/Lightricks/LTX-Video&lt;/a&gt;&lt;br /&gt; HF: &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;https://huggingface.co/Lightricks/LTX-Video&lt;/a&gt; (FP8 coming soon)&lt;br /&gt; Documentation: &lt;a href="https://www.lightricks.com/ltxv-documentation"&gt;https://www.lightricks.com/ltxv-documentation&lt;/a&gt;&lt;br /&gt; Tweet: &lt;a href="https://x.com/LTXStudio/status/1919751150888239374"&gt;https://x.com/LTXStudio/status/1919751150888239374&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topiga"&gt; /u/topiga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i4ioviud9bze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgrjor/new_opensource_video_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh6kh3</id>
    <title>Qwen3 MMLU-Pro Computer Science LLM Benchmark Results</title>
    <updated>2025-05-07T19:43:27+00:00</updated>
    <author>
      <name>/u/WolframRavenwolf</name>
      <uri>https://old.reddit.com/user/WolframRavenwolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt; &lt;img alt="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" src="https://preview.redd.it/3yuv5m5qxeze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ed0dfa07bd3b2e9b138176b2104e26c7a51e6e4" title="Qwen3 MMLU-Pro Computer Science LLM Benchmark Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive &lt;strong&gt;Qwen 3 evaluations&lt;/strong&gt; across a range of formats and quantisations, focusing on &lt;strong&gt;MMLU-Pro&lt;/strong&gt; (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Qwen3-235B-A22B&lt;/strong&gt; (via Fireworks API) tops the table at &lt;strong&gt;83.66%&lt;/strong&gt; with ~55 tok/s.&lt;/li&gt; &lt;li&gt;But the &lt;strong&gt;30B-A3B Unsloth&lt;/strong&gt; quant delivered &lt;strong&gt;82.20%&lt;/strong&gt; while running locally at ~45 tok/s and with zero API spend.&lt;/li&gt; &lt;li&gt;The same Unsloth build is ~5x faster than Qwen's &lt;strong&gt;Qwen3-32B&lt;/strong&gt;, which scores &lt;strong&gt;82.20%&lt;/strong&gt; as well yet crawls at &amp;lt;10 tok/s.&lt;/li&gt; &lt;li&gt;On Apple silicon, the &lt;strong&gt;30B MLX&lt;/strong&gt; port hits &lt;strong&gt;79.51%&lt;/strong&gt; while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;0.6B&lt;/strong&gt; micro-model races above 180 tok/s but tops out at &lt;strong&gt;37.56%&lt;/strong&gt; - that's why it's not even on the graph (50 % performance cut-off).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, Alibaba/Qwen - you really whipped the llama's ass! And to OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. &lt;em&gt;This&lt;/em&gt; is the future!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WolframRavenwolf"&gt; /u/WolframRavenwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3yuv5m5qxeze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T19:43:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1khmh5m</id>
    <title>Anyone get speculative decoding to work for Qwen 3 on LM Studio?</title>
    <updated>2025-05-08T10:11:47+00:00</updated>
    <author>
      <name>/u/jaxchang</name>
      <uri>https://old.reddit.com/user/jaxchang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got it working in llama.cpp, but it's being slower than running Qwen 3 32b by itself in LM Studio. Anyone tried this out yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaxchang"&gt; /u/jaxchang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khmh5m/anyone_get_speculative_decoding_to_work_for_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T10:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1khb7rs</id>
    <title>The new MLX DWQ quant is underrated, it feels like 8bit in a 4bit quant.</title>
    <updated>2025-05-07T22:59:41+00:00</updated>
    <author>
      <name>/u/mzbacd</name>
      <uri>https://old.reddit.com/user/mzbacd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed it was added to MLX a few days ago and started using it since then. It's very impressive, like running an 8bit model in a 4bit quantization size without much performance loss, and I suspect it might even finally make the 3bit quantization usable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;edit:&lt;br /&gt; just made a DWQ quant one from unquantized version:&lt;br /&gt; &lt;a href="https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mzbacd"&gt; /u/mzbacd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khb7rs/the_new_mlx_dwq_quant_is_underrated_it_feels_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T22:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbxg4</id>
    <title>QwQ Appreciation Thread</title>
    <updated>2025-05-07T23:33:37+00:00</updated>
    <author>
      <name>/u/OmarBessa</name>
      <uri>https://old.reddit.com/user/OmarBessa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt; &lt;img alt="QwQ Appreciation Thread" src="https://external-preview.redd.it/iUbtHN7RzxrcJ1LnOytJyYZIsd6RNnT0J4eou-hgYFg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8377a20d6ad8107b227ddbef333fbae642705" title="QwQ Appreciation Thread" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561"&gt;https://preview.redd.it/rpteerax2gze1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfa6469374a7c33544022408885845c33043e561&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Taken from: &lt;a href="https://fiction.live/stories/Fiction-liveBench-May-06-2025/oQdzQvKHw8JyXbN87"&gt;Regarding-the-Table-Design - Fiction-liveBench-May-06-2025 - Fiction.live&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mean guys, don't get me wrong. The new Qwen3 models are great, but QwQ still holds quite decently. If it weren't for its overly verbose thinking...yet look at this. &lt;strong&gt;It is still basically sota in long context comprehension among open-source models.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OmarBessa"&gt; /u/OmarBessa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbxg4/qwq_appreciation_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh0hcd</id>
    <title>Cracking 40% on SWE-bench verified with open source models &amp; agents &amp; open-source synth data</title>
    <updated>2025-05-07T15:39:46+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt; &lt;img alt="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" src="https://preview.redd.it/4lwtc2sgpdze1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f581dfebc0968cbf87949bad4b08918a6afa989" title="Cracking 40% on SWE-bench verified with open source models &amp;amp; agents &amp;amp; open-source synth data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know that finetuning &amp;amp; RL work great for getting great LMs for agents -- the problem is where to get the training data!&lt;/p&gt; &lt;p&gt;We've generated 50k+ task instances for 128 popular GitHub repositories, then trained our own LM for SWE-agent. The result? We achieve 40% pass@1 on SWE-bench Verified -- a new SoTA among open source models.&lt;/p&gt; &lt;p&gt;We've open-sourced &lt;em&gt;everything&lt;/em&gt;, and we're excited to see what you build with it! This includes the agent (SWE-agent), the framework used to generate synthetic task instances (SWE-smith), and our fine-tuned LM (SWE-agent-LM-32B)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4lwtc2sgpdze1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh0hcd/cracking_40_on_swebench_verified_with_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:39:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9018</id>
    <title>OpenCodeReasoning - new Nemotrons by NVIDIA</title>
    <updated>2025-05-07T21:22:11+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9018/opencodereasoning_new_nemotrons_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1khl779</id>
    <title>Auto Thinking Mode Switch for Qwen3 / Open Webui Function</title>
    <updated>2025-05-08T08:40:24+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt; &lt;img alt="Auto Thinking Mode Switch for Qwen3 / Open Webui Function" src="https://external-preview.redd.it/Aave_1tyS2RPiuZJIgxgvsnWGpciBGyuudbUEh0fSQA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d772752d670d6ca5d635248fb38c7dc6830a14b" title="Auto Thinking Mode Switch for Qwen3 / Open Webui Function" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/AaronFeng753/Better-Qwen3"&gt;&lt;strong&gt;https://github.com/AaronFeng753/Better-Qwen3&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is an open webui function for Qwen3 models, it has the following features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Automatically turn on/off the thinking process by using the LLM itself to evaluate the difficulty of your request.&lt;/li&gt; &lt;li&gt;Remove model's old thoughts in multi-turn conversation, from Qwen3 model's hugging face README: &lt;code&gt;In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content.&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You will need to edit the code to config the OpenAI compatible API URL and the Model name. &lt;/p&gt; &lt;p&gt;(And yes, it works with local LLM, I'm using one right now, ollama and lm studio both has OpenAI compatible API)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cntv1xrcsize1.png?width=846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef5af10d3badbdeaa6d9558fdce7895de4f5c3"&gt;https://preview.redd.it/cntv1xrcsize1.png?width=846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9cef5af10d3badbdeaa6d9558fdce7895de4f5c3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khl779/auto_thinking_mode_switch_for_qwen3_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T08:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1khgir9</id>
    <title>Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?</title>
    <updated>2025-05-08T03:28:06+00:00</updated>
    <author>
      <name>/u/GrungeWerX</name>
      <uri>https://old.reddit.com/user/GrungeWerX</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt; &lt;img alt="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" src="https://external-preview.redd.it/eNoMp-cBhW5B3lKr_XZAgD1Qku1SepqMDIM_aLwv22o.png?width=140&amp;amp;height=82&amp;amp;crop=140:82,smart&amp;amp;auto=webp&amp;amp;s=b758fb52ae557d8179140e36108651b61ff7d08a" title="Is GLM-4 actually a hacked GEMINI? Or just Copying their Style?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only person that's noticed that GLM-4's outputs are eerily similar to Gemini Pro 2.5 in formatting? I copy/pasted a prompt in several different SOTA LLMs - GPT-4, DeepSeek, Gemini 2.5 Pro, Claude 2.7, and Grok. Then I tried it in GLM-4, and was like, wait a minute, where have I seen this formatting before? Then I checked - it was in &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;. Now, I'm not saying that GLM-4 is Gemini 2.5 Pro, of course not, but could it be a hacked earlier version? Or perhaps (far more likely) they used it as a template for how GLM does its outputs? Because Gemini is the &lt;strong&gt;only&lt;/strong&gt; LLM that does it this way where it gives you three Options w/parentheticals describing tone, and then finalizes it by saying &amp;quot;Choose the option that best fits your tone&amp;quot;. Like, &lt;strong&gt;almost exactly the same.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just tested it out on Gemini 2.0 and Gemini Flash. Neither of these versions do this. This is only done by Gemini 2.5 Pro and GLM-4. None of the other Closed-source LLMs do this either, like chat-gpt, grok, deepseek, or claude.&lt;/p&gt; &lt;p&gt;I'm not complaining. And if the Chinese were to somehow hack their LLM and released a quantized open source version to the world - despite how unlikely this is - I wouldn't protest...much. &amp;gt;.&amp;gt;&lt;/p&gt; &lt;p&gt;But jokes aside, anyone else notice this?&lt;/p&gt; &lt;p&gt;Some samples:&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d"&gt;https://preview.redd.it/xjw45f988hze1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85206ec3f5ebc5288c1e559c3ac2e50acb26b9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e"&gt;https://preview.redd.it/alnqooqa8hze1.png?width=976&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc68a36fc81af2110ac82d979e493c2889eae93e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemini Pro 2.5&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05"&gt;https://preview.redd.it/0ofz0ygd8hze1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc72427d8b0b60d02c5aca3f54ac0f1e287d1e05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f"&gt;https://preview.redd.it/igddncjf8hze1.png?width=895&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab4b4fa2f7ebdcc3a3feff0f9abbf6ce4428b4f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GrungeWerX"&gt; /u/GrungeWerX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khgir9/is_glm4_actually_a_hacked_gemini_or_just_copying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T03:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1khlw98</id>
    <title>ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation</title>
    <updated>2025-05-08T09:31:33+00:00</updated>
    <author>
      <name>/u/searcher1k</name>
      <uri>https://old.reddit.com/user/searcher1k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"&gt; &lt;img alt="ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation" src="https://external-preview.redd.it/CFr9_cLjVeZwkAhQkscmJVhOJEQi2DmpjFaEwPPrd7A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=837ef81c969d8d59aca68353cf9f08b45dab3dd4" title="ComfyGPT: A Self-Optimizing Multi-Agent System for Comprehensive ComfyUI Workflow Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2503.17671"&gt;https://arxiv.org/abs/2503.17671&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/searcher1k"&gt; /u/searcher1k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1khlw98"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khlw98/comfygpt_a_selfoptimizing_multiagent_system_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T09:31:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh579e</id>
    <title>Qwen 3 evaluations</title>
    <updated>2025-05-07T18:48:14+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt; &lt;img alt="Qwen 3 evaluations" src="https://preview.redd.it/8f8g366goeze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd68bf0ab81adb00446d201fbee1d90070c68389" title="Qwen 3 evaluations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally finished my extensive Qwen 3 evaluations across a range of formats and quantisations, focusing on MMLU-Pro (Computer Science).&lt;/p&gt; &lt;p&gt;A few take-aways stood out - especially for those interested in local deployment and performance trade-offs:&lt;/p&gt; &lt;p&gt;1️⃣ Qwen3-235B-A22B (via Fireworks API) tops the table at 83.66% with ~55 tok/s.&lt;/p&gt; &lt;p&gt;2️⃣ But the 30B-A3B Unsloth quant delivered 82.20% while running locally at ~45 tok/s and with zero API spend.&lt;/p&gt; &lt;p&gt;3️⃣ The same Unsloth build is ~5x faster than Qwen's Qwen3-32B, which scores 82.20% as well yet crawls at &amp;lt;10 tok/s.&lt;/p&gt; &lt;p&gt;4️⃣ On Apple silicon, the 30B MLX port hits 79.51% while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups.&lt;/p&gt; &lt;p&gt;5️⃣ The 0.6B micro-model races above 180 tok/s but tops out at 37.56% - that's why it's not even on the graph (50 % performance cut-off).&lt;/p&gt; &lt;p&gt;All local runs were done with @lmstudio on an M4 MacBook Pro, using Qwen's official recommended settings.&lt;/p&gt; &lt;p&gt;Conclusion: Quantised 30B models now get you ~98 % of frontier-class accuracy - at a fraction of the latency, cost, and energy. For most local RAG or agent workloads, they're not just good enough - they're the new default.&lt;/p&gt; &lt;p&gt;Well done, @Alibaba_Qwen - you really whipped the llama's ass! And to @OpenAI: for your upcoming open model, please make it MoE, with toggleable reasoning, and release it in many sizes. This is the future!&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/wolframrvnwlf/status/1920186645384478955?s=46"&gt;https://x.com/wolframrvnwlf/status/1920186645384478955?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8f8g366goeze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh579e/qwen_3_evaluations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T18:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kgzwe9</id>
    <title>New mistral model benchmarks</title>
    <updated>2025-05-07T15:16:25+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt; &lt;img alt="New mistral model benchmarks" src="https://preview.redd.it/hrtrvrvnmdze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a47a4a215c33b3670819e5b09e20d25a73074d7" title="New mistral model benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hrtrvrvnmdze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kgzwe9/new_mistral_model_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T15:16:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1khbz70</id>
    <title>Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)</title>
    <updated>2025-05-07T23:35:56+00:00</updated>
    <author>
      <name>/u/eding42</name>
      <uri>https://old.reddit.com/user/eding42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt; &lt;img alt="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" src="https://external-preview.redd.it/qU1K5b6t8nYIjVW3n7kpmgcB3rS2YYANfyJcs8RztyA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4675a9fdccab6a8f9da5be381fa2c1bd1fe534bf" title="Intel to announce new Intel Arc Pro GPUs at Computex 2025 (May 20-23)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe the 24 GB Arc B580 model that got leaked will be announced? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eding42"&gt; /u/eding42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/intel/status/1920241029804064796"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khbz70/intel_to_announce_new_intel_arc_pro_gpus_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T23:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1khjrtj</id>
    <title>Building LLM Workflows - - some observations</title>
    <updated>2025-05-08T06:54:56+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on some relatively complex LLM workflows for the past year (not continuously, on and off). Here are some conclusions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Decomposing each task to the smallest steps and prompt chaining works far better than just using a single prompt with CoT. turning each step of the CoT into its own prompt and checking/sanitizing outputs reduces errors.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using XML tags to structure the system prompt, prompt etc works best (IMO better than JSON structure but YMMV)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You have to remind the LLM that its only job is to work as a semantic parser of sorts, to merely understand and transform the input data and NOT introduce data from its own &amp;quot;knowledge&amp;quot; into the output.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NLTK, SpaCY, FlairNLP are often good ways to independently verify the output of an LLM (eg: check if the LLM's output has a sequence of POS tags you want etc). The great thing about these libraries is they're fast and reliable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ModernBERT classifiers are often just as good at LLMs if the task is small enough. Fine-tuned BERT-style classifiers are usually better than LLM for focused, narrow tasks.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM-as-judge and LLM confidence scoring is extremely unreliable, especially if there's no &amp;quot;grounding&amp;quot; for how the score is to be arrived at. Scoring on vague parameters like &amp;quot;helpfulness&amp;quot; is useless - -eg: LLMs often conflate helpfulness with professional tone and length of response. Scoring has to either be grounded in multiple examples (which has its own problems - - LLMs may make the wrong inferences from example patterns), or a fine-tuned model is needed. If you're going to fine-tune for confidence scoring, might as well use a BERT model or something similar.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;In Agentic loops, the hardest part is setting up the conditions where the LLM exits the loop - - using the LLM to decide whether or not to exit is extremely unreliable (same reason as LLM-as-judge issues).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Performance usually degrades past 4k tokens (input context window) ... this is often only seen once you've run thousands of iterations. If you have a low error threshold, even a 5% failure rate in the pipeline is unacceptable, keeping all prompts below 4k tokens helps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32B models are good enough and reliable enough for most tasks, if the task is structured properly.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured CoT (with headings and bullet points) is often better than unstructured &lt;code&gt;&amp;lt;thinking&amp;gt;Okay, so I must...etc&lt;/code&gt; tokens. Structured and concise CoT stays within the context window (in the prompt as well as examples), and doesn't waste output tokens.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Self-consistency helps, but that also means running each prompt multiple times - - forces you to use smaller models and smaller prompts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Writing your own CoT is better than relying on a reasoning model. Reasoning models are a good way to collect different CoT paths and ideas, and then synthesize your own.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The long-term plan is always to fine-tune everything. Start with a large API-based model and few-shot examples, and keep tweaking. Once the workflows are operational, consider creating fine-tuning datasets for some of the tasks so you can shift to a smaller local LLM or BERT. Making balanced datasets isn't easy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;when making a dataset for fine-tuning, make it balanced by setting up a categorization system/orthogonal taxonomy so you can get complete coverage of the task. Use MECE framework.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've probably missed many points, these were the first ones that came to mind.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khjrtj/building_llm_workflows_some_observations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T06:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kh9qlx</id>
    <title>No local, no care.</title>
    <updated>2025-05-07T21:53:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt; &lt;img alt="No local, no care." src="https://preview.redd.it/f0l4hjmklfze1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ceb732f3829a0007aaaa683f507cd9116cadc51" title="No local, no care." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f0l4hjmklfze1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kh9qlx/no_local_no_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-07T21:53:52+00:00</published>
  </entry>
</feed>
