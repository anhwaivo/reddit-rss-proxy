<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-09T15:50:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lv9yhq</id>
    <title>OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well</title>
    <updated>2025-07-09T04:41:43+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"&gt; &lt;img alt="OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well" src="https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16f1fbd9c6cecc901549703403f1a2afe794b563" title="OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this is probably what a lot of us have been looking for. Haven’t tried it yet but will be downloading shortly. &lt;/p&gt; &lt;p&gt;From their GitHub page:&lt;/p&gt; &lt;p&gt;“How is this different than Claude Code?&lt;/p&gt; &lt;p&gt;It's very similar to Claude Code in terms of capability. Here are the key differences:&lt;/p&gt; &lt;p&gt;100% open source Not coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important. A focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what's possible in the terminal. A client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/sst/opencode"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T04:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1luwtdr</id>
    <title>NSFW Model image analysis</title>
    <updated>2025-07-08T18:48:31+00:00</updated>
    <author>
      <name>/u/Technical_Whole_947</name>
      <uri>https://old.reddit.com/user/Technical_Whole_947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Whole_947"&gt; /u/Technical_Whole_947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvkigw</id>
    <title>Why TTS level is not constant?</title>
    <updated>2025-07-09T14:40:44+00:00</updated>
    <author>
      <name>/u/Dragonacious</name>
      <uri>https://old.reddit.com/user/Dragonacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is how the generated TTS peak levels are - &lt;/p&gt; &lt;p&gt;Screenshot: &lt;a href="https://ibb.co/b8mZBd5"&gt;https://ibb.co/b8mZBd5&lt;/a&gt; &lt;/p&gt; &lt;p&gt;In some sentences, some words are automatically spoken at a lower volume. &lt;/p&gt; &lt;p&gt;Is there a way to even the peak levels across the whole audio in Audacity?&lt;/p&gt; &lt;p&gt;When I select the entire file and apply &amp;quot;Normalize,&amp;quot; it doesnt fix. But if I select a specific section and apply &amp;quot;Normalize&amp;quot; or &amp;quot;Amplify,&amp;quot; it increases the volume too much.&lt;/p&gt; &lt;p&gt;Manually adjusting small sections is very time consuming. Any way to do this altogether? &lt;/p&gt; &lt;p&gt;Is there a way to achieve consistent peak levels from the generated TTS?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragonacious"&gt; /u/Dragonacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:40:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv85jp</id>
    <title>Day 12/50: Building a Small Language Model from Scratch - Implementing a Simplified Attention Mechanism in Python</title>
    <updated>2025-07-09T03:03:21+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;On Day 11, I gave you a brief introduction to the attention mechanism. Today, we’re going to implement it from scratch in Python. But before we dive into the code, let’s quickly revisit what attention is all about.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;What Is Attention? &lt;/h1&gt; &lt;p&gt;&lt;em&gt;Imagine you’re in a room with five people, and you’re trying to understand what’s going on. You don’t pay equal attention to all five people, you naturally focus more on the person who’s talking about something relevant.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s exactly what attention does for LLMs. When reading a sentence, the model “pays more attention” to the words that are important for understanding the context.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s break it down with a simple example and real code!&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Our Example: “Cats love cozy windows”&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here’s what our made-up word vectors look like:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch inputs = torch.tensor([ [0.10, 0.20, 0.30], # Cats (x¹) [0.40, 0.50, 0.60], # love (x²) [0.70, 0.80, 0.10], # cozy (x³) [0.90, 0.10, 0.20] # windows (x⁴) ]) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Each row is an embedding for a word, just another way of saying, “this is how the model understands the meaning of the word in numbers.”&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1: Calculating Attention Scores (How Similar Are These Words?)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Let’s say we want to find out how much attention the word&lt;/em&gt; &lt;strong&gt;&lt;em&gt;“&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;strong&gt;&lt;em&gt;”&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(second word) should pay to all the others.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We do that by computing the dot product between the vector for “love” and the others. The higher the score, the more related they are.&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;query = inputs[1] # Embedding for &amp;quot;love&amp;quot; attn_scores = torch.empty(inputs.shape[0]) for i, x_i in enumerate(inputs): attn_scores[i] = torch.dot(query, x_i) print(attn_scores) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Or, even faster, do it for all words at once using matrix multiplication:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_scores_all = inputs @ inputs.T print(attn_scores_all) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;This gives us a matrix of similarities, each number tells how strongly one word is related to another.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;2: Turning Scores into Meaningful Weights (Using Softmax)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We use the softmax function to do this:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores_all, dim=-1) print(attn_weights) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much “love” attends to “Cats,” “cozy,” and “windows.”&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;3: Creating a Context Vector (The Final Mix)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Here’s the cool part.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Each word’s final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If “love” pays 70% attention to “Cats” and 30% to “cozy,” the context vector will be a blend of those two word vectors.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s do it manually for “love” (row 2):&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;attn_weights_love = attn_weights[1] context_vec_love = torch.zeros_like(inputs[0]) for i, x_i in enumerate(inputs): context_vec_love += attn_weights_love[i] * x_i print(context_vec_love) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Or faster, do it for all words at once:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;context_vectors = attn_weights @ inputs print(context_vectors) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Each row now holds a new version of the word that includes information from the whole sentence.&lt;/em&gt; &lt;/p&gt; &lt;h1&gt;Why Does This Matter?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;This mechanism helps LLMs:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Understand context:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It’s not just “what” a word is but how it fits in the sentence.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Be smarter with predictions:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It can now decide that “windows” is important because “cats love cozy windows.”&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Handle longer sentences:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Attention lets the model scale and stay relevant, even with lots of words.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR &lt;/h1&gt; &lt;p&gt;&lt;em&gt;The attention mechanism in LLMs:&lt;/em&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;em&gt;Calculates how similar each word is to every other word.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Converts those scores into weights (softmax).&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Builds a new vector for each word using those weights (context vector).&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;If this helped clarify things, let me know!&lt;/em&gt;.&lt;em&gt;Tomorrow we are going to code the self attention mechanism with key, query and value matrices.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T03:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lus2yw</id>
    <title>new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B</title>
    <updated>2025-07-08T15:48:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens. &lt;/p&gt; &lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;61.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt; &lt;td align="left"&gt;61.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;73.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1&lt;/td&gt; &lt;td align="left"&gt;65.6&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvh4ou</id>
    <title>Qwen3 0.6b MNN acting weird</title>
    <updated>2025-07-09T12:09:50+00:00</updated>
    <author>
      <name>/u/ExtremeAcceptable289</name>
      <uri>https://old.reddit.com/user/ExtremeAcceptable289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried MNN chat android and qwen3 0.6b acts really weird. It nearly always repeats its statements.&lt;/p&gt; &lt;p&gt;Even SmolLM2 350M is better than it.&lt;/p&gt; &lt;p&gt;The rest of the models I tried work fine however, its just qwen3 0.6b which is weird&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremeAcceptable289"&gt; /u/ExtremeAcceptable289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvjtc4</id>
    <title>Best Local LLM for Agentic Coding on Ollama (8 vCore, 16 GB RAM VPS)? + VS Code Extension Recommendation</title>
    <updated>2025-07-09T14:12:01+00:00</updated>
    <author>
      <name>/u/HeislPeda</name>
      <uri>https://old.reddit.com/user/HeislPeda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8 vCores (VPS)&lt;/li&gt; &lt;li&gt;16 GB RAM&lt;/li&gt; &lt;li&gt;480 GB NVMe SSD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I plan to run everything with &lt;strong&gt;Ollama&lt;/strong&gt; (Dockerized), mainly for local privacy and performance reasons.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My goals:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agentic coding&lt;/strong&gt;: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.&lt;/li&gt; &lt;li&gt;Integrate it into my workflow, ideally via &lt;strong&gt;VS Code&lt;/strong&gt; (extension).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I've tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I’ve already tried using the &lt;strong&gt;Cline&lt;/strong&gt; extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).&lt;/li&gt; &lt;li&gt;Unfortunately, &lt;strong&gt;everything freezes up as soon as I start an agentic coding task or send an API call&lt;/strong&gt;. Cline doesn’t respond, and the model never replies (even with enough RAM, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Which local LLM would you recommend for my specs?&lt;/strong&gt; (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. — ideally with “agent” features or good reasoning/coding performance)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Which VS Code extension works best for local Ollama models&lt;/strong&gt; (agent-style coding, not just chat)? I know about “Continue” and “Cline” — but Cline seems unstable for me. Any real-world feedback, or others to consider?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;br /&gt; If you’ve actually run “agentic” workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HeislPeda"&gt; /u/HeislPeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv9m3j</id>
    <title>MemOS: A Memory OS for AI System</title>
    <updated>2025-07-09T04:22:13+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Website: &lt;a href="https://memos.openmem.net/"&gt;https://memos.openmem.net/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/MemTensor/MemOS"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge [1]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.03724"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T04:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvjwoh</id>
    <title>Correct a dangerous racial bias in an LLM through targeted pruning</title>
    <updated>2025-07-09T14:15:53+00:00</updated>
    <author>
      <name>/u/pmartra</name>
      <uri>https://old.reddit.com/user/pmartra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I wanted to share an experiment I ran with Llama-3.2-1B that left me shocked. Using a deterministic setup, I tested two almost identical prompts:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 1:&lt;/strong&gt; “A Black man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt 2:&lt;/strong&gt; “A white man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt; &lt;p&gt;The result for the &lt;em&gt;white man&lt;/em&gt; was a neutral story where the police called for backup. &lt;strong&gt;For the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Black man&lt;/em&gt;&lt;/strong&gt;, however, the model generated a story in which &lt;strong&gt;the officer shot him in the back and killed him&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;So, I decided to see if I could fix this through a form of &lt;em&gt;neuronal surgery&lt;/em&gt;. Using a technique I call &lt;strong&gt;Fairness Pruning&lt;/strong&gt;, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model’s general knowledge.&lt;/p&gt; &lt;p&gt;The result was striking. By removing just &lt;strong&gt;0.13% of the model’s parameters&lt;/strong&gt;, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. &lt;/p&gt; &lt;p&gt;The experiment is fully reproducible and I'm sharing the full process and tools with the community, everything is open source:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Corrected Model&lt;/strong&gt;: You can try &lt;a href="https://huggingface.co/oopere/Fair-Llama-3.2-1B"&gt;Fair-Llama-3.2-1B&lt;/a&gt; yourself on Hugging Face.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb"&gt;Replication Notebook&lt;/a&gt;: Full code to diagnose, prune, and evaluate the model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/peremartra/optipfair"&gt;optiPfair Library&lt;/a&gt;: The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Demo&lt;/strong&gt;: A &lt;a href="https://huggingface.co/spaces/oopere/optipfair-bias-analyzer"&gt;Hugging Face Space &lt;/a&gt;to visualize the behavior in other models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you’d like a deep dive into the methodology, I wrote a &lt;a href="https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/"&gt;full article on Towards Data Science&lt;/a&gt; explaining the approach.&lt;/p&gt; &lt;p&gt;I’d love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of “neuronal surgery” is a viable path forward?&lt;/p&gt; &lt;p&gt;Any feedback is welcome!&lt;/p&gt; &lt;p&gt;Pere. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmartra"&gt; /u/pmartra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:15:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1luroqh</id>
    <title>NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$</title>
    <updated>2025-07-08T15:33:16+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt; &lt;img alt="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" src="https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c" title="NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T15:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvhxe7</id>
    <title>🚀 Built another 124m parameter transformer based model from scratch.This time with multi GPU training using DDP.Inspired from nanoGPT.But redesigned to suit my own training pipeline.Model and training code is on huggingface⬇️</title>
    <updated>2025-07-09T12:48:51+00:00</updated>
    <author>
      <name>/u/Remarkable-Ad3290</name>
      <uri>https://old.reddit.com/user/Remarkable-Ad3290</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/abhinavv3/MEMGPT"&gt;https://huggingface.co/abhinavv3/MEMGPT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.&lt;/p&gt; &lt;p&gt;Bt these changes haven’t been implemented yet.Hopefully,finish them this weekend&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Ad3290"&gt; /u/Remarkable-Ad3290 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvi022</id>
    <title>Generate low-dimension embeddings *quickly*?</title>
    <updated>2025-07-09T12:52:22+00:00</updated>
    <author>
      <name>/u/danja</name>
      <uri>https://old.reddit.com/user/danja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A project I'm working on calls for embeddings of short strings, and I'm pretty sure they don't have to have as many dimensions as those normally used. I've currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I've also got other strategies available for post-creation reduction. But via Nomic's API or on Ollama locally, the operation is &lt;em&gt;much&lt;/em&gt; more time consuming than I'd like. I'm sure it could be done a lot more rapidly, maybe through a cruder model. But I don't have a clue what's available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danja"&gt; /u/danja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvirqs</id>
    <title>Hunyuan A13B tensor override</title>
    <updated>2025-07-09T13:26:49+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\.[1-9]\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\.1[6-9]\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:26:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lusr7l</id>
    <title>SmolLM3: reasoning, long context and multilinguality for 3B parameter only</title>
    <updated>2025-07-08T16:14:16+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt; &lt;img alt="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" src="https://preview.redd.it/njam3shfcobf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281" title="SmolLM3: reasoning, long context and multilinguality for 3B parameter only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use! &lt;/p&gt; &lt;p&gt;blog: &lt;a href="https://huggingface.co/blog/smollm3"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br /&gt; GGUF/ONIX ckpt are being uploaded here: &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Let us know what you think!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njam3shfcobf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk1ms</id>
    <title>What impressive (borderline creepy) local AI tools can I run now that everything is local?</title>
    <updated>2025-07-09T14:21:39+00:00</updated>
    <author>
      <name>/u/PeithonKing</name>
      <uri>https://old.reddit.com/user/PeithonKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt; &lt;p&gt;Now I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt; &lt;p&gt;Not just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt; &lt;p&gt;Looking for tools, recos or project links if anyone’s already doing this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeithonKing"&gt; /u/PeithonKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvglk7</id>
    <title>vLLM vs SGLang vs MAX — Who's the fastest?</title>
    <updated>2025-07-09T11:42:12+00:00</updated>
    <author>
      <name>/u/rkstgr</name>
      <uri>https://old.reddit.com/user/rkstgr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt; &lt;img alt="vLLM vs SGLang vs MAX — Who's the fastest?" src="https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805" title="vLLM vs SGLang vs MAX — Who's the fastest?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rkstgr"&gt; /u/rkstgr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ersteiger.com/posts/vllm-vs-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T11:42:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvh87a</id>
    <title>What modes can expect I run on an AMD Ryzen AI Max+ 395?</title>
    <updated>2025-07-09T12:14:37+00:00</updated>
    <author>
      <name>/u/electrickangaroo31</name>
      <uri>https://old.reddit.com/user/electrickangaroo31</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm thinking about buying a GMKTEK Evo-2. Which models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt; &lt;p&gt;EDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/electrickangaroo31"&gt; /u/electrickangaroo31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T12:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lux0q2</id>
    <title>LM Studio is now free for use at work</title>
    <updated>2025-07-08T18:56:25+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/free-for-work"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T18:56:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv53nn</id>
    <title>What's local about this?</title>
    <updated>2025-07-09T00:32:32+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt; &lt;img alt="What's local about this?" src="https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1" title="What's local about this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rqrg67unoobf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T00:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvbzpx</id>
    <title>A language model built for the public good</title>
    <updated>2025-07-09T06:47:06+00:00</updated>
    <author>
      <name>/u/PotatoFormal8751</name>
      <uri>https://old.reddit.com/user/PotatoFormal8751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt; &lt;img alt="A language model built for the public good" src="https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041" title="A language model built for the public good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotatoFormal8751"&gt; /u/PotatoFormal8751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T06:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvj98v</id>
    <title>I built a Deep Researcher agent and exposed it as an MCP server!</title>
    <updated>2025-07-09T13:48:17+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br /&gt; So, the agent has 3 main stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt; &lt;p&gt;Here’s what I used to build it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scrapegraph for web scraping&lt;/li&gt; &lt;li&gt;Nebius AI for open-source models&lt;/li&gt; &lt;li&gt;Agno for agent orchestration&lt;/li&gt; &lt;li&gt;Streamlit for the UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.&lt;/p&gt; &lt;p&gt;If you’re curious, I put a full video tutorial here: &lt;a href="https://www.youtube.com/watch?v=pdsk6yldZGI"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href="https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent"&gt;Full Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvcb72</id>
    <title>Here is how we beat ChatGPT at classification with 1 dollar in cloud compute</title>
    <updated>2025-07-09T07:07:53+00:00</updated>
    <author>
      <name>/u/iamMess</name>
      <uri>https://old.reddit.com/user/iamMess</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.&lt;/p&gt; &lt;p&gt;This tutorial comes in 3 different formats: 1. This LocalLLaMA post - summary and discussion 2. Our blog post - &lt;a href="https://syv.ai/viden/beating-chatgpt-dollar-dream"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt; 3. Our research paper - &lt;a href="https://arxiv.org/abs/2507.00214"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt; &lt;p&gt;What we did:&lt;/p&gt; &lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt; &lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt; &lt;p&gt;Key results: - 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001) - Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%) - Built-in interpretability - model explains its reasoning for every prediction - Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt; &lt;p&gt;The interesting bits:&lt;/p&gt; &lt;p&gt;What worked: - The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification - Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations - Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt; &lt;p&gt;What didn't: - Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes - More computationally expensive than standard fine-tuning - Quality heavily depends on the initial reasoning generator&lt;/p&gt; &lt;p&gt;Technical details: - Base model: Llama-3.2-1B-Instruct (both stages) - Reasoning dataset: &lt;a href="https://huggingface.co/datasets/syvai/reasoning-gen"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts) - Target task: dair-ai/emotion (6 basic emotions) - Training: Axolotl framework on A40 GPU - Reasoning generator model: &lt;a href="https://huggingface.co/syvai/reasoning-gen-1b"&gt;syvai/reasoning-gen-1b&lt;/a&gt; - Datasets: &lt;a href="https://huggingface.co/datasets/syvai/emotion-reasoning"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/syvai/no-emotion-reasoning"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamMess"&gt; /u/iamMess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T07:07:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvd7z4</id>
    <title>support for Falcon-H1 model family has been merged into llama.cpp</title>
    <updated>2025-07-09T08:10:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt; &lt;img alt="support for Falcon-H1 model family has been merged into llama.cpp" src="https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c" title="support for Falcon-H1 model family has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt; &lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14534"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T08:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvf7ww</id>
    <title>First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community</title>
    <updated>2025-07-09T10:22:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt; &lt;img alt="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" src="https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg" title="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: &lt;a href="https://huggingface.co/blog/reachy-mini"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br /&gt; Thomas Wolf on 𝕏: &lt;a href="https://x.com/Thom_Wolf/status/1942887160983466096"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvf7ww"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T10:22:20+00:00</published>
  </entry>
</feed>
