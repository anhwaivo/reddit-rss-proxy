<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-29T05:23:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1icd5fc</id>
    <title>Don't forget Deepseek 2.5 (and Unsloth)</title>
    <updated>2025-01-28T21:40:02+00:00</updated>
    <author>
      <name>/u/SunTrainAi</name>
      <uri>https://old.reddit.com/user/SunTrainAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Back in November i got the first Deepseek 2.5 running on my Epyc Rome 256GB machine without my 3090. It's MoE just like 3 and R1 but with less parameters (236B, 21B active). I get like 4 Tk/sec with Iq4xs.&lt;/p&gt; &lt;p&gt;Deepseek released an update for 2.5 in december. This is 12. on lmarena and on par with Sonnet.&lt;/p&gt; &lt;p&gt;If &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; could do his magic on this one, many more people could benefit from Deepseek.&lt;/p&gt; &lt;p&gt;IQ2_XXS would be around 64GB but MoE ðŸ¤©&lt;/p&gt; &lt;p&gt;That'd be awesome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunTrainAi"&gt; /u/SunTrainAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icd5fc/dont_forget_deepseek_25_and_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icd5fc/dont_forget_deepseek_25_and_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icd5fc/dont_forget_deepseek_25_and_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T21:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1icef3a</id>
    <title>Hosted deepseek-r1-distill-qwen-32b</title>
    <updated>2025-01-28T22:33:45+00:00</updated>
    <author>
      <name>/u/punkpeye</name>
      <uri>https://old.reddit.com/user/punkpeye</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing that I made &lt;code&gt;deepseek-r1-distill-qwen-32b&lt;/code&gt; available as a hosted endpoint.&lt;/p&gt; &lt;p&gt;&lt;a href="https://glama.ai/models/deepseek-r1-distill-qwen-32b"&gt;https://glama.ai/models/deepseek-r1-distill-qwen-32b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I couldn't find it with other providers. Maybe others will find it useful too.&lt;/p&gt; &lt;p&gt;As far as I can tell based on the &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#4-evaluation-results"&gt;benchmarks&lt;/a&gt;, for codings tasks at least, this model outperforms &lt;code&gt;DeepSeek-R1-Distill-Llama-70B&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/punkpeye"&gt; /u/punkpeye &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icef3a/hosted_deepseekr1distillqwen32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icef3a/hosted_deepseekr1distillqwen32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icef3a/hosted_deepseekr1distillqwen32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ice609</id>
    <title>In July 2024 prior to the r1 release, DeepSeek's Liang Wenfeng was asked whether AI progress would slow down because Scaling Laws aren't delivering. He said AGI could come in "2, 5, or 10 years" and "Weâ€™re relatively optimistic. Our industry as a whole seems to be meeting expectations."</title>
    <updated>2025-01-28T22:22:56+00:00</updated>
    <author>
      <name>/u/aihorsieshoe</name>
      <uri>https://old.reddit.com/user/aihorsieshoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ice609/in_july_2024_prior_to_the_r1_release_deepseeks/"&gt; &lt;img alt="In July 2024 prior to the r1 release, DeepSeek's Liang Wenfeng was asked whether AI progress would slow down because Scaling Laws aren't delivering. He said AGI could come in &amp;quot;2, 5, or 10 years&amp;quot; and &amp;quot;Weâ€™re relatively optimistic. Our industry as a whole seems to be meeting expectations.&amp;quot;" src="https://external-preview.redd.it/bMO1G9G1E0WVUvY8HPHjA38LQYfVyGj34gWdldzh6SI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b2e09b4515afc003a8783ebe18bd63bc7310b32" title="In July 2024 prior to the r1 release, DeepSeek's Liang Wenfeng was asked whether AI progress would slow down because Scaling Laws aren't delivering. He said AGI could come in &amp;quot;2, 5, or 10 years&amp;quot; and &amp;quot;Weâ€™re relatively optimistic. Our industry as a whole seems to be meeting expectations.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aihorsieshoe"&gt; /u/aihorsieshoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ice609/in_july_2024_prior_to_the_r1_release_deepseeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ice609/in_july_2024_prior_to_the_r1_release_deepseeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:22:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibyn2s</id>
    <title>DeepSeek R1 Overthinker: force r1 models to think for as long as you wish</title>
    <updated>2025-01-28T10:30:37+00:00</updated>
    <author>
      <name>/u/anzorq</name>
      <uri>https://old.reddit.com/user/anzorq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt; &lt;img alt="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" src="https://external-preview.redd.it/aWd2cXYyazZwcGZlMQ-Y_nspVqRuENfEqKSBWaLfxAxl82wv6S6Ho3TY9Ea9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12d7d26a204d221746bdbcdbb7c79801dff00b47" title="DeepSeek R1 Overthinker: force r1 models to think for as long as you wish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anzorq"&gt; /u/anzorq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3df8o2k6ppfe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibyn2s/deepseek_r1_overthinker_force_r1_models_to_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T10:30:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibk9us</id>
    <title>Meta is reportedly scrambling multiple â€˜war roomsâ€™ of engineers to figure out how DeepSeekâ€™s AI is beating everyone else at a fraction of the price</title>
    <updated>2025-01-27T21:13:50+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt; &lt;img alt="Meta is reportedly scrambling multiple â€˜war roomsâ€™ of engineers to figure out how DeepSeekâ€™s AI is beating everyone else at a fraction of the price" src="https://external-preview.redd.it/Brnl3ltRvrwiYwAXRD8-9ZQzXA_EE-2JvCrM0Zi5k8U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1046aa83b70828043ace549a5075989da27f1ff4" title="Meta is reportedly scrambling multiple â€˜war roomsâ€™ of engineers to figure out how DeepSeekâ€™s AI is beating everyone else at a fraction of the price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the article: &amp;quot;Of the four war rooms Meta has created to respond to DeepSeekâ€™s potential breakthrough, two teams will try to decipher how High-Flyer lowered the cost of training and running DeepSeek with the goal of using those tactics for Llama, the outlet reported citing one anonymous Meta employee. &lt;/p&gt; &lt;p&gt;Among the remaining two teams, one will try to find out which data DeepSeek used to train its model, and the other will consider how Llama can restructure its models based on attributes of the DeepSeek models, The Information reported.&amp;quot;&lt;/p&gt; &lt;p&gt;I am actually excited by this. If Meta can figure it out, it means Llama 4 or 4.x will be substantially better. Hopefully we'll get a 70B dense model that's on part with DeepSeek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fortune.com/2025/01/27/mark-zuckerberg-meta-llama-assembling-war-rooms-engineers-deepseek-ai-china/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T21:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1icdbej</id>
    <title>RWKV-7 "Goose" ðŸª¿ 1.5B release</title>
    <updated>2025-01-28T21:47:07+00:00</updated>
    <author>
      <name>/u/EmbarrassedBiscotti9</name>
      <uri>https://old.reddit.com/user/EmbarrassedBiscotti9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icdbej/rwkv7_goose_15b_release/"&gt; &lt;img alt="RWKV-7 &amp;quot;Goose&amp;quot; ðŸª¿ 1.5B release" src="https://external-preview.redd.it/LVSONFUnHP0aVif7PvDrqZvU9jQJE2ovO8Viyl41lQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07192aca52fbd82e64156d130dc00e311c1e4c45" title="RWKV-7 &amp;quot;Goose&amp;quot; ðŸª¿ 1.5B release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmbarrassedBiscotti9"&gt; /u/EmbarrassedBiscotti9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/HlpnkmG.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icdbej/rwkv7_goose_15b_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icdbej/rwkv7_goose_15b_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T21:47:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic29lq</id>
    <title>Unsloth made dynamic R1 quants - can be run on as little as 80gb of RAM</title>
    <updated>2025-01-28T14:04:50+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is super cool: &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key points: - they didnâ€™t naively quantized everything - some layers needed more bits to overcome issues - they have a range of quants from 1.58bit to 2.51bit which shrink the model to 131gb-212gb - they say the smallest can be run with as little as 80gb RAM (but full model in RAM or VRAM obviously faster) - GGUFs provided and work on current llama.cpp versions (no update needed)&lt;/p&gt; &lt;p&gt;Might be real option for local R1!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic29lq/unsloth_made_dynamic_r1_quants_can_be_run_on_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T14:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1icc5hq</id>
    <title>DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed</title>
    <updated>2025-01-28T20:59:13+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"&gt; &lt;img alt="DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed" src="https://external-preview.redd.it/T3dwFxBeTh14I0gRGwTgWSW_ZOjtsyWdThn4yOyUB0o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e486673640592fed5e6e7beea3c8482611b687" title="DeepSeek R1 671B running on 2 M2 Ultras faster than reading speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/awnihannun/status/1881412271236346233"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icc5hq/deepseek_r1_671b_running_on_2_m2_ultras_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iccv8d</id>
    <title>I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default.</title>
    <updated>2025-01-28T21:28:22+00:00</updated>
    <author>
      <name>/u/SuperChewbacca</name>
      <uri>https://old.reddit.com/user/SuperChewbacca</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"&gt; &lt;img alt="I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default." src="https://external-preview.redd.it/EGtt3zD3WMnG_Ch64RtvgmZIFRybIuqxtoVkKUqyuuc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6bc53781a062c204f0755b3f538b3cccb6f4906" title="I created MiraConverse, an Open Source project for voice chat with with any AI Model, including local. It keeps context, and has a user selectable trigger keyword (Mira) by default." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperChewbacca"&gt; /u/SuperChewbacca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/n9oD7IPIWVI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iccv8d/i_created_miraconverse_an_open_source_project_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T21:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibej82</id>
    <title>OpenAI employeeâ€™s reaction to Deepseek</title>
    <updated>2025-01-27T17:23:12+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt; &lt;img alt="OpenAI employeeâ€™s reaction to Deepseek" src="https://preview.redd.it/ij7ubrn3mkfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db93fc1e3aea11120926d14eefcc127a43118a66" title="OpenAI employeeâ€™s reaction to Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ij7ubrn3mkfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibej82/openai_employees_reaction_to_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T17:23:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8kpf</id>
    <title>What if releasing R1 is a 4D chess move by a Quant firm?</title>
    <updated>2025-01-28T18:34:23+00:00</updated>
    <author>
      <name>/u/Dull_Art6802</name>
      <uri>https://old.reddit.com/user/Dull_Art6802</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, Quant firms are know to analyze every detail possible in order to predict the future stock prices. I am talking using satellites to observe how many cars come of from Tesla factories, divorce rates, container ship movements, how many times you flush your toilet etc ridiculous details. &lt;/p&gt; &lt;p&gt;Now DeepSeek is owned by such a Quant firm and I find it impossible to believe that they at least did not have some idea what R1's release could cause on the market, so what if before releasing R1 they bought a lot of put options on NVDA and then by releasing the model they crashed Nvidia stock netting in a couple billion USD.&lt;/p&gt; &lt;p&gt;These people might be 4 parallel dimensions ahead of us lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull_Art6802"&gt; /u/Dull_Art6802 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8kpf/what_if_releasing_r1_is_a_4d_chess_move_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic9wi6</id>
    <title>Block released a new open source AI agent called Goose. It can do more than coding for engineers ðŸ‘€</title>
    <updated>2025-01-28T19:27:42+00:00</updated>
    <author>
      <name>/u/emreloperr</name>
      <uri>https://old.reddit.com/user/emreloperr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt; &lt;img alt="Block released a new open source AI agent called Goose. It can do more than coding for engineers ðŸ‘€" src="https://external-preview.redd.it/7ekVDB698j31gkjO7h0OU3gtvIyHbMD1tb8PRnYlUV8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a32c6085097dca9063e7efe673aa54d3a944b2c2" title="Block released a new open source AI agent called Goose. It can do more than coding for engineers ðŸ‘€" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emreloperr"&gt; /u/emreloperr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://block.github.io/goose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic9wi6/block_released_a_new_open_source_ai_agent_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T19:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibzmef</id>
    <title>New bomb dropped from asian researchers: YuE: Open Music Foundation Models for Full-Song Generation</title>
    <updated>2025-01-28T11:37:51+00:00</updated>
    <author>
      <name>/u/wayl</name>
      <uri>https://old.reddit.com/user/wayl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only few days ago a &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; user was going to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ia40om/would_give_up_a_kidney_for_a_local_audio_model/"&gt;give away a kidney&lt;/a&gt; for this.&lt;/p&gt; &lt;p&gt;YuE is an open-source project by HKUST tackling the challenge of generating full-length songs from lyrics (lyrics2song). Unlike existing models limited to short clips, YuE can produce 5-minute songs with coherent vocals and accompaniment. Key innovations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A semantically enhanced audio tokenizer for efficient training.&lt;/li&gt; &lt;li&gt;Dual-token technique for synced vocal-instrumental modeling.&lt;/li&gt; &lt;li&gt;Lyrics-chain-of-thoughts for progressive song generation.&lt;/li&gt; &lt;li&gt;Support for diverse genres, languages, and advanced vocal techniques (e.g., scatting, death growl).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the &lt;a href="https://github.com/multimodal-art-projection/YuE"&gt;GitHub repo&lt;/a&gt; for demos and model checkpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wayl"&gt; /u/wayl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibzmef/new_bomb_dropped_from_asian_researchers_yue_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T11:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic03lx</id>
    <title>DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C</title>
    <updated>2025-01-28T12:08:07+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt; &lt;img alt="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" src="https://b.thumbs.redditmedia.com/Squ2MR8UElQKTlUEoWhmDAJq100Xox0Tn99gOS2k4QM.jpg" title="DeepSeek is running inference on the new home Chinese chips made by Huawei, the 910C" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Alexander Doria on X: &lt;em&gt;I feel this should be a much bigger story: DeepSeek has trained on Nvidia H800 but is running inference on the new home Chinese chips made by Huawei, the 910C.&lt;/em&gt;: &lt;a href="https://x.com/Dorialexander/status/1884167945280278857"&gt;https://x.com/Dorialexander/status/1884167945280278857&lt;/a&gt;&lt;br /&gt; Original source: Zephyr: &lt;em&gt;HUAWEI&lt;/em&gt;: &lt;a href="https://x.com/angelusm0rt1s/status/1884154694123298904"&gt;https://x.com/angelusm0rt1s/status/1884154694123298904&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d"&gt;https://preview.redd.it/sfzjno0q6qfe1.jpg?width=506&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ae0c800b9fffea55bc7861f583160795e935c07d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Partial translation:&lt;br /&gt; &lt;em&gt;In Huawei Cloud&lt;/em&gt;&lt;br /&gt; &lt;em&gt;ModelArts Studio (MaaS) Model-as-a-Service Platform&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Ascend-Adapted New Model is Here!&lt;/em&gt;&lt;br /&gt; &lt;em&gt;DeepSeek-R1-Distill&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Qwen-14B, Qwen-32B, and Llama-8B have been launched.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;More models coming soon.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic03lx/deepseek_is_running_inference_on_the_new_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T12:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichohj</id>
    <title>DeepSeek API: Every Request Is A Timeout :(</title>
    <updated>2025-01-29T01:00:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt; &lt;img alt="DeepSeek API: Every Request Is A Timeout :(" src="https://preview.redd.it/wpmv3ibe0ufe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=449243d847b49583ee6497956bd77db9ec221af7" title="DeepSeek API: Every Request Is A Timeout :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpmv3ibe0ufe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T01:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic4czy</id>
    <title>Qwen2.5-Max</title>
    <updated>2025-01-28T15:41:07+00:00</updated>
    <author>
      <name>/u/Either-Job-341</name>
      <uri>https://old.reddit.com/user/Either-Job-341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another chinese model release, lol. They say it's on par with DeepSeek V3.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen2.5-Max-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Either-Job-341"&gt; /u/Either-Job-341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic4czy/qwen25max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic3k3b</id>
    <title>No censorship when running Deepseek locally.</title>
    <updated>2025-01-28T15:05:21+00:00</updated>
    <author>
      <name>/u/ISNT_A_ROBOT</name>
      <uri>https://old.reddit.com/user/ISNT_A_ROBOT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt; &lt;img alt="No censorship when running Deepseek locally." src="https://preview.redd.it/95fhiv1e2rfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c23d5ab8c71a12862078dfeeb51d4785b2a9bb58" title="No censorship when running Deepseek locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ISNT_A_ROBOT"&gt; /u/ISNT_A_ROBOT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95fhiv1e2rfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8cjf</id>
    <title>$6,000 computer to run Deepseek R1 670B Q8 locally at 6-8 tokens/sec</title>
    <updated>2025-01-28T18:25:13+00:00</updated>
    <author>
      <name>/u/MoltenBoron</name>
      <uri>https://old.reddit.com/user/MoltenBoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw this on X/Twitter: Tower PC with 2 AMD EPYC CPUs and 24 x 32GB DDR5-RDIMM. No GPUs. 400 W power consumption.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/carrigmat/status/1884244369907278106"&gt;https://x.com/carrigmat/status/1884244369907278106&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative link (no login):&lt;/p&gt; &lt;p&gt;&lt;a href="https://threadreaderapp.com/thread/1884244369907278106.html"&gt;https://threadreaderapp.com/thread/1884244369907278106.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoltenBoron"&gt; /u/MoltenBoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic61zb</id>
    <title>"Sir, China just released another model"</title>
    <updated>2025-01-28T16:52:39+00:00</updated>
    <author>
      <name>/u/danilofs</name>
      <uri>https://old.reddit.com/user/danilofs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt; &lt;img alt="&amp;quot;Sir, China just released another model&amp;quot;" src="https://a.thumbs.redditmedia.com/hlgnuSLD8D8wJmgHtUGXn38QzQ7a2xkFvpX65Gj6yM0.jpg" title="&amp;quot;Sir, China just released another model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The burst of DeepSeek V3 has attracted attention from the whole AI community to large-scale MoE models. Concurrently, they have built Qwen2.5-Max, a large MoE LLM pretrained on massive data and post-trained with curated SFT and RLHF recipes. It achieves competitive performance against the top-tier models, and outcompetes DeepSeek V3 in benchmarks like Arena Hard, LiveBench, LiveCodeBench, GPQA-Diamond. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255"&gt;https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danilofs"&gt; /u/danilofs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T16:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7hts</id>
    <title>Everyone and their mother knows about DeepSeek</title>
    <updated>2025-01-28T17:50:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone I interact talks about deepseek now. How it's scary, how it's better than Chatgpt, how it's open-source...&lt;/p&gt; &lt;p&gt;But the fact is, 99.9% of these people (including myself) have no way to run 670b model (which actually is the model in hype) in manner that benefit from open-source. I mean just using their front end is no different than using chatGPT. And chatGPT and cluade have, free versions, which evidently are better!&lt;/p&gt; &lt;p&gt;Heck, I hear news reporters talking about how great it is because it works freakishly well and it is an open-source. But in reality, its just open weight, no one have yet to replicate what they did. &lt;/p&gt; &lt;p&gt;But why all the hype? Don't you feel this is too much? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T17:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1icjg39</id>
    <title>Some evidence of DeepSeek being attacked by DDoS has been released!</title>
    <updated>2025-01-29T02:27:21+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt; &lt;img alt="Some evidence of DeepSeek being attacked by DDoS has been released!" src="https://b.thumbs.redditmedia.com/Lszyle7jiuM1pNHqVGd7qcP6bBSMrdPUgCiUZt1sZRY.jpg" title="Some evidence of DeepSeek being attacked by DDoS has been released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jppuddrxfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=156acff91c3d3055b63a4ba7cc0e90ac54db4216"&gt;In the first phase, on January 3, 4, 6, 7, and 13, there were suspected HTTP proxy attacks.During this period, Xlab could see a large number of proxy requests to link DeepSeek through proxies, which were likely HTTP proxy attacks.In the second phase, on January 20, 22-26, the attack method changed to SSDP and NTP reflection amplification.During this period, the main attack methods detected by XLab were SSDP and NTP reflection amplification, and a small number of HTTP proxy attacks. Usually, the defense of SSDP and NTP reflection amplification attacks is simple and easy to clean up.In the third phase, on January 27 and 28, the number of attacks increased sharply, and the means changed to application layer attacks.Starting from the 27th, the main attack method discovered by XLab changed to HTTP proxy attacks. Attacking such application layer attacks simulates normal user behavior, which is significantly more difficult to defend than classic SSDP and NTP reflection amplification attacks, so it is more effective.XLab also found that the peak of the attack on January 28 occurred between 03:00-04:00 Beijing time (UTC+8), which corresponds to 14:00-15:00 Eastern Standard Time (UTC-5) in North America. This time window selection shows that the attack has border characteristics, and it does not rule out the purpose of targeted attacks on overseas service providers.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sbm25zfyfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b70b8ecd569c5f7c95f8f9ca2a802b8871c1f0c6"&gt;this DDoS attack was accompanied by a large number of brute force attacks. All the brute force attack IPs came from the United States. XLab's data can identify that half of these IPs are VPN exits, and it is speculated that this may be caused by DeepSeek's overseas restrictions on mobile phone users.03DeepSeek responded promptly and minimized the impactFaced with the sudden escalation of large-scale DDoS attacks late at night on the 27th and 28th, DeepSeek responded and handled it immediately. Based on the passivedns data of the big network, XLab saw that DeepSeek switched IP at 00:58 on the morning of the 28th when the attacker launched an effective and destructive HTTP proxy attack. This switching time is consistent with Deepseek's own announcement time in the screenshot above, which should be for better security defense. This also further proves XLab's own judgment on this DDoS attack.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Starting at 03:00 on January 28, the DDoS attack was accompanied by a large number of brute force attacks. All brute force attack IPs come from the United States.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721"&gt;https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721&lt;/a&gt; (only Chinese text)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T02:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichk40</id>
    <title>So much DeepSeek fear mongering</title>
    <updated>2025-01-29T00:54:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt; &lt;img alt="So much DeepSeek fear mongering" src="https://preview.redd.it/zfykihriztfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e955a87992919d24e54df67893449f4eab310d" title="So much DeepSeek fear mongering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are so many people, who have no idea what they're talking about dominating the stage about deep seek?&lt;/p&gt; &lt;p&gt;Stuff like this. WTF &lt;a href="https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq"&gt;https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfykihriztfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T00:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1icaq2z</id>
    <title>DeepSeek's AI breakthrough bypasses Nvidia's industry-standard CUDA, uses assembly-like PTX programming instead</title>
    <updated>2025-01-28T20:00:18+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This level of optimization is nuts but would definitely allow them to eek out more performance at a lower cost. &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek made quite a splash in the AI industry by training its Mixture-of-Experts (MoE) language model with 671 billion parameters &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions"&gt;using a cluster featuring 2,048 Nvidia H800 GPUs in about two months&lt;/a&gt;, showing 10X higher efficiency than AI industry leaders like Meta. The breakthrough was achieved by implementing tons of fine-grained optimizations and usage of assembly-like PTX (Parallel Thread Execution) programming instead of Nvidia's CUDA, according to an analysis from Mirae Asset Securities Korea cited by &lt;a href="https://x.com/Jukanlosreve/status/1883304958432624881"&gt;u/Jukanlosreve&lt;/a&gt;. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1icer8t</id>
    <title>Will Deepseek soon be banned in the US?</title>
    <updated>2025-01-28T22:48:18+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt; &lt;img alt="Will Deepseek soon be banned in the US?" src="https://preview.redd.it/5gpitg40dtfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=785ab6a8af1daeae906fcf4071ac93f79583ffb0" title="Will Deepseek soon be banned in the US?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5gpitg40dtfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:48:18+00:00</published>
  </entry>
</feed>
