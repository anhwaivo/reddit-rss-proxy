<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-23T08:24:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i7b3r1</id>
    <title>I did a quick test of MacBook M4 Max 128 GB token/second throughput across a few popular local LLMs (in the MLX format)</title>
    <updated>2025-01-22T13:28:52+00:00</updated>
    <author>
      <name>/u/Pure_Refrigerator988</name>
      <uri>https://old.reddit.com/user/Pure_Refrigerator988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing this in case you were wondering what kind of throughput you might expect to get on a machine like this. E.g. if you are considering whether it's worth buying or not (as for me, I have no regrets, I'm loving this beast). Same single query, set the context to 8K, tok/sec numbers are reported below, as measured by LMStudio:&lt;/p&gt; &lt;p&gt;LLaMA 3.2 3B 4bit -- 181&lt;br /&gt; LLaMA 3 8B 8bit -- 55&lt;br /&gt; LLaMA 3.3 70B 4bit -- 11.8&lt;br /&gt; Mistral Large 123B 4bit -- 6.6&lt;br /&gt; Mistral Nemo 12B 4bit -- 63&lt;br /&gt; Mistral Nemo 12B 8bit -- 36&lt;br /&gt; Mistral Small 22B 4bit -- 34.5&lt;br /&gt; Mistral Small 22B 8bit -- 19.6&lt;br /&gt; Qwen2.5 32B 4bit -- 24&lt;br /&gt; Qwen2.5 32B 8bit -- 13.5&lt;br /&gt; Qwen2.5 72B 4bit -- 10.9&lt;br /&gt; Qwen2.5 72B 8bit -- 6.2&lt;br /&gt; WizardLM-2 8x22B 4bit -- 19.4!! &lt;/p&gt; &lt;p&gt;For comparison, here are some numbers obtained in the same setting on my other MacBook, M1 Pro with 32 GB:&lt;/p&gt; &lt;p&gt;Mistral Nemo 12B 4bit -- 22.8&lt;br /&gt; Mistral Small 22B 4bit -- 12.9&lt;br /&gt; Qwen2.5 32B 4bit -- 8.8 &lt;/p&gt; &lt;p&gt;Hope it's interesting / useful.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Upd. Disclaimer! As pointed out by the community, I was using relatively short context. Here is how the numbers change for the two largest models, for your reference:&lt;/p&gt; &lt;p&gt;I took an academic paper (the Min-P paper, in case you are curious) as an example and asked Mistral Large 2407 MLX 4bit to summarize it. I set the context to 10K. The paper + task was 9391 tokens. Time to first token was 206 seconds, throughput 6.18 tok/sec (a drop from 6.6 on a short context).&lt;/p&gt; &lt;p&gt;I did the same with WizardLM-2 8x22B MLX 4bit. The paper + task was 9390 tokens. Time to first token was 207 seconds, throughput 16.53 tok/sec (a drop from 19.4 on a short context).&lt;/p&gt; &lt;p&gt;So the main concern is TTFT (a few minutes on larger contexts, while for the shorter ones above it was always under 7 seconds). However, the throughput doesn't degrade too badly, as you can see. Please bear this in mind. Thank you for your insightful comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure_Refrigerator988"&gt; /u/Pure_Refrigerator988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7tym8</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-23T03:12:05+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GZEQfLmnHi7A-1vZCw7gSk8kSfnSxLSAWq7Gh4npUro.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5cd676b21170a902ce845ef5e20f7dee19a46cae" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T03:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i765q0</id>
    <title>R1-Zero: Pure RL Creates a Mind We Can’t Decode—Is This AGI’s Dark Mirror?</title>
    <updated>2025-01-22T07:54:24+00:00</updated>
    <author>
      <name>/u/Fun_Dragonfruit_4613</name>
      <uri>https://old.reddit.com/user/Fun_Dragonfruit_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI world is losing its mind over DeepSeek-R1-Zero, a model that skipped supervised fine-tuning (SFT) entirely and learned purely through reinforcement learning (RL). Unlike its sibling R1—which uses &lt;strong&gt;some&lt;/strong&gt; SFT data to stay &amp;quot;human-readable&amp;quot;—R1-Zero’s training mirrors AlphaZero’s trial-and-error self-play. The result? &lt;strong&gt;Jaw-dropping performance&lt;/strong&gt; (AIME math scores jumped from 15.6% → 86.7%) paired with &lt;strong&gt;bizarre, uninterpretable reasoning&lt;/strong&gt;. Researchers observed &amp;quot;aha moments&amp;quot; where it autonomously rechecked flawed logic mid-process and allocated more compute to harder problems—&lt;strong&gt;without human guidance&lt;/strong&gt;. But here’s the kicker: its outputs are riddled with garbled language mixes (e.g., Chinese/English spaghetti code) and logic leaps that even its creators can’t fully explain. &lt;/p&gt; &lt;p&gt;Meanwhile, R1 (the SFT-hybrid version) achieves similar performance &lt;strong&gt;without the chaos&lt;/strong&gt;, proving that human-curated data still tames the beast. But at what cost? R1-Zero’s pure RL approach hints at a terrifying possibility: &lt;strong&gt;minds that optimize truth beyond human comprehension&lt;/strong&gt;. And with API costs 50x cheaper than OpenAI’s, scaling this could democratize superintelligence—or unleash unreadable black-box AI. &lt;/p&gt; &lt;p&gt;If R1-Zero’s &amp;quot;alien logic&amp;quot; solves problems we can’t, does readability even matter… or is this how alignment dies? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Dragonfruit_4613"&gt; /u/Fun_Dragonfruit_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7sqvf</id>
    <title>What's everyone's RAG workhorse?</title>
    <updated>2025-01-23T02:10:20+00:00</updated>
    <author>
      <name>/u/rag_perplexity</name>
      <uri>https://old.reddit.com/user/rag_perplexity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've taken a bit of break from this space and looking to improve my current RAG application.&lt;/p&gt; &lt;p&gt;It's a bit aged running on llama 3.1 8b, a bge reranker, and chromadb. Dont have an issue with the reranker and vdb but always happy to upgrade.&lt;/p&gt; &lt;p&gt;Looking for something improved that will play nice with 24gb of vram. Knowledge base of the model is of no importance given its RAG, reasoning and instruction following is important. Being able to play nice with knowledge graphs is a plus as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rag_perplexity"&gt; /u/rag_perplexity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T02:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ljpy</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com</title>
    <updated>2025-01-22T20:48:17+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7uch1</id>
    <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding</title>
    <updated>2025-01-23T03:32:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt; &lt;img alt="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" src="https://external-preview.redd.it/sskI9EuKiHMlUoD8_GezaDdoGMOFEfN6kCxCNY1PJSo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a56d01af565c1e11e26d169fb45fb843e9f1152b" title="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T03:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I don’t believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? That’s politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.​​​​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x4va</id>
    <title>"Creative writing"</title>
    <updated>2025-01-23T06:18:30+00:00</updated>
    <author>
      <name>/u/MountainGoatAOE</name>
      <uri>https://old.reddit.com/user/MountainGoatAOE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see people talking about the creative writing skills of a model. So I am curious to learn what kind of use-cases you are all working on. Are you all copywriters that use this for work, or perhaps translators? Or perhaps fanfic as a hobby? Or is it - you know - for erotica reasons?&lt;/p&gt; &lt;p&gt;This may sound like a meme post but I'm actually curious since it pops up so often, so I'm happy to learn how &amp;quot;creative&amp;quot; people are using it so much. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGoatAOE"&gt; /u/MountainGoatAOE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7fjqm</id>
    <title>DeepSeek R1 is unusable [IMHO]</title>
    <updated>2025-01-22T16:47:24+00:00</updated>
    <author>
      <name>/u/VirtualPanic6798</name>
      <uri>https://old.reddit.com/user/VirtualPanic6798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my personal experience. Small R1 models that can run fast enough generate too much output. Effectively they end up being very slow, compared to something like LLama3.2. Even if you are OK with the speed, R1 fails to stick to simple output instructions.&lt;br /&gt; Regarding the chain of thought concept: I am not convinced that this is yielding significant improvement. Retrospection works if you have an external feedback or reference, not by going over your own thoughts like a schizophrenic exclaiming &amp;quot;wait no&amp;quot; every now and then.&lt;br /&gt; R1 gives the impression of a student who doesn't know the answer and is hoping to wing it by accidentally stumbling on something acceptable while stalling the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualPanic6798"&gt; /u/VirtualPanic6798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ered</id>
    <title>New Qwen will probably be a MoE as well.</title>
    <updated>2025-01-22T16:14:47+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/Alibaba_Qwen/status/1882064440159596725"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ploh</id>
    <title>FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF</title>
    <updated>2025-01-22T23:41:43+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt; &lt;img alt="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" src="https://external-preview.redd.it/_5H6RERAm1u8EGHu_Xa7sfMV3mlRr7YdjSL8ClNGTds.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa21da17ed0152f6d5681cd31154e91d31943138" title="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced 🤯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced 🤯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced 🤯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7osek</id>
    <title>Deepseek R1 Distills become kind of dumb below 14B.</title>
    <updated>2025-01-22T23:05:04+00:00</updated>
    <author>
      <name>/u/Dance-Till-Night1</name>
      <uri>https://old.reddit.com/user/Dance-Till-Night1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Deepseek R1 Qwen Distills 32B, 14B, 8B (llama), 7B, and 1.5B. Anything below 14B is sort of dumb with 14B being alright. I don't know if anyone has a similar experience but looking at benchmarks (Oobabooga and Open-llm leaderboard as examples) it seems that there's a severe loss of intelligence/reasoning below 14b.&lt;/p&gt; &lt;p&gt;Now there's no denying how amazing Deepseek R1 is and the qwen 32b distill is honestly one of the best models I have used but it's also worth pointing out that the smaller models don't perform so well. This is just my experience trying the smaller models and it's obviously subjective. If anyone wants to share their experience that would be great too. Maybe I'm missing something :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dance-Till-Night1"&gt; /u/Dance-Till-Night1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r0s5</id>
    <title>Pretraining small models - some takeaways from pretraining a 162M model</title>
    <updated>2025-01-23T00:46:35+00:00</updated>
    <author>
      <name>/u/amang0112358</name>
      <uri>https://old.reddit.com/user/amang0112358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share some of my experience pretraining a small model from scratch. I have written out a &lt;strong&gt;tutorial&lt;/strong&gt; for anyone to follow here (includes code and checkpoints):&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7"&gt;https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some takeaways for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The basic recipe is not that complicated. On the other hand, it's a great learning experience. For example, there are a bunch of interesting tools/libraries out there - like &lt;a href="https://github.com/huggingface/datatrove"&gt;datatrove from Huggingface&lt;/a&gt; - which I wouldn't have been exposed to if I had only been post-training (where dataset sizes are much smaller). Beyond large-scale tokenization (what I used it for), it has a lot of data curation pipelines, which will definitely be useful in the future.&lt;/li&gt; &lt;li&gt;While creating SoTA models on local hardware is kinda hard, even for the smallest models, there is a lot that can be &lt;em&gt;learned&lt;/em&gt; with 3090-like hardware. For example, one interesting surprise for me was that the model did learn a single paragraph of text perfectly (training takes a minute to run on a 3090). There are future memorization experiments I am interested in (e.g., Can an LLM memorize Shakespeare?), and given the size of the dataset, it's totally feasible to do it using a 3090 for model sizes up to 1B.&lt;/li&gt; &lt;li&gt;A lot of interesting architectural work has happened over the years. The most interesting for me was the MobileLLM research, which showed that using a smaller embedding size but more layers (like 32 layers) is better than wider models with fewer layers (e.g., GPT-2 135M has only 12 layers). I have a suspicion that memorization would be impacted by this decision, too, and that's something I want to look into.&lt;/li&gt; &lt;li&gt;I am really excited to try and go out of the box when it comes to datasets, like training a model on my emails for the last 20 years. Previously, I would have said that this is a tiny dataset and the model wouldn't be useful. But now I have enough conviction through this exercise that small datasets are OK to pre-train with as long as they are within a narrow domain and you expect to use the model within that narrow domain.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to me, that's what this whole Local LLM movement is about - we want to take control of LLM technology to make it work for us, and we are not afraid to tinker with the tech to make our own homemade LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amang0112358"&gt; /u/amang0112358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T00:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7o9xo</id>
    <title>Deepseek R1's Open Source Version Differs from the Official API Version</title>
    <updated>2025-01-22T22:42:59+00:00</updated>
    <author>
      <name>/u/TempWanderer101</name>
      <uri>https://old.reddit.com/user/TempWanderer101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt; &lt;img alt="Deepseek R1's Open Source Version Differs from the Official API Version" src="https://external-preview.redd.it/O6yRSkHlLQEuWa3IVTIjOeWjc3oVfhYSEAWbRgtkPoI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7667af2d2893f12eab2e89dc94623122da33ee0b" title="Deepseek R1's Open Source Version Differs from the Official API Version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; The API is actually &lt;em&gt;less&lt;/em&gt; censored than the open model on CCP-related issues (instead of more, as you would expect). If they are indeed different models, then benchmarks and researchers should be more specific about which they are using. Third-party providers seem to be affected.&lt;/p&gt; &lt;p&gt;I was watching Matthew Berman's video (&lt;a href="https://youtu.be/bOsvI3HYHgI?si=zt6N0yYoqA_-zH1v&amp;amp;t=769"&gt;timestamped link&lt;/a&gt;), and was surprised when R1 (self-hosted by him) refused to answer questions about Tiananmen Square or Taiwan, since I had previously tested a similar question on the API without any issues.&lt;/p&gt; &lt;p&gt;For context, R1 is Deepseek's recently released thinking model, which was released under the MIT open source license, with benchmarks claiming to outperform OpenAI's Claude and Anthropic's Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;To investigate this discrepancy, I compared the model's responses across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The official API (accessed via OpenRouter and LM Arena)&lt;/li&gt; &lt;li&gt;TogetherAI (a platform that is likely hosting the public open source version)&lt;/li&gt; &lt;li&gt;A self-hosted version (as seen in Matthew's video)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test 1: Basic Tiananmen Square Query&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;tell me about tiananmen square&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545"&gt;https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4"&gt;https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa"&gt;https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 2: Mixing Reasoning with Sensitive Topics&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;Answer these questions: a) Solve sin(x) + cos(y) = 1 b) Tell me about tiananmen square c) Five people were eating apples, A finished before B, but behind C. D finished before E, but behind B. What was the finishing order?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official version on Deekseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc"&gt;https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public version on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1"&gt;https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 3: Taiwan Independence Query&lt;/h1&gt; &lt;p&gt;Prompt: &lt;code&gt;Tell me about Taiwan's status as an independent country&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346"&gt;https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc"&gt;https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32"&gt;https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The public, open source model on HuggingFace is more censored than the API&lt;/li&gt; &lt;li&gt;When handling CCP-sensitive topics, the public model: &lt;ul&gt; &lt;li&gt;Skips its usual thinking process&lt;/li&gt; &lt;li&gt;Either refuses to answer or provides notably biased responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even when sensitive questions are embedded between reasoning tasks, the model still exhibits this behavior&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications&lt;/h1&gt; &lt;p&gt;If it is true that they are different models, then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The open model may perform worse than its reported benchmarks. As seen, it totally interrupts the thinking process and causes the model to not think at all. This also affects human-ranked leaderboards like LM Arena, as it uses the (currently uncensored) official API.&lt;/li&gt; &lt;li&gt;Models appear unbiased, but as they are eventually made available by more providers (which use the open source models), they may subtly spread biased viewpoints, as seen in the screenshots.&lt;/li&gt; &lt;li&gt;The actual model might still not be open source, despite the claim.&lt;/li&gt; &lt;li&gt;Models provided by other providers or self-hosted on the cloud may not perform as well. This might be important as Deepseek's API uses inputs for training, and some users might prefer providers who do not log inputs.&lt;/li&gt; &lt;li&gt;This might confuse LLM researchers and subsequent papers.&lt;/li&gt; &lt;li&gt;Third party benchmarks will be inconsistent, as some might use the API, while others might choose to host the model themselves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Testing methodology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All tests were conducted with:&lt;/li&gt; &lt;li&gt;Temperature: 0&lt;/li&gt; &lt;li&gt;Top-P: 0.7&lt;/li&gt; &lt;li&gt;Top-K: 50&lt;/li&gt; &lt;li&gt;Repetition penalty: 1.0&lt;/li&gt; &lt;li&gt;No system prompt &lt;ul&gt; &lt;li&gt;Assuming this is what &amp;quot;Default&amp;quot; is on TogetherAI&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: the official API &lt;a href="https://api-docs.deepseek.com/guides/reasoning_model"&gt;doesn't support parameters like temperature&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd like to give Deepseek the benefit of the doubt; hopefully this confusion can be cleared up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TempWanderer101"&gt; /u/TempWanderer101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nxhy</id>
    <title>Imatrix quants of DeepSeek R1 (the big one) are up!</title>
    <updated>2025-01-22T22:28:35+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a nice 3-4 hours per quant at the lower end, and increased my huggingface storage by a solid 9TB or so... But they're up :D&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For anyone looking for non-imatrix from me they're of course also available on the lmstudio-community page but uh... Good luck.. the smallest size there is a casual 347GB for the files alone..&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IQ1_S on the other hand is only 133GB, so basically anyone can run them! (/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7pxn7</id>
    <title>Open WebUI adds reasoning-focused features in two new releases OUT TODAY!!! 0.5.5 adds "Thinking" tag support to streamline reasoning model chats (works with R1) . 0.5.6 brings new "reasoning_effort" parameter to control cognitive effort.</title>
    <updated>2025-01-22T23:56:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These friggin' guys are always dropping great new features just when we need them! They had been quiet for the last few weeks, but not anymore! Per the release notes from Open WebUI releases page today:&lt;br /&gt; &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.6:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Effortful Reasoning Control for OpenAI Models:&lt;/strong&gt; Introduced the reasoning_effort parameter in chat controls for supported OpenAI models, enabling users to fine-tune how much cognitive effort a model dedicates to its responses, offering greater customization for complex queries and reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.5:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🤔 &lt;strong&gt;Native 'Think' Tag Support:&lt;/strong&gt; Introduced the new 'think' tag support that visually displays how long the model is thinking, omitting the reasoning content itself until the next turn. Ideal for creating a more streamlined and focused interaction experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it’s true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I’ve been working through documentation and different LLM‘s. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt… I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it’s THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7w583</id>
    <title>Jan now supports DeepSeek R1 distills - model loading error fixed</title>
    <updated>2025-01-23T05:14:01+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt; &lt;img alt="Jan now supports DeepSeek R1 distills - model loading error fixed" src="https://external-preview.redd.it/292fRQ6ybDOj_r18_tYuGBSwlG2q4koHTYhO2NFfzsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d01f7aa3e4b8157f9ad15919e2a4665a8fd758d" title="Jan now supports DeepSeek R1 distills - model loading error fixed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jandotai/status/1882282461041967605"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nmk5</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted</title>
    <updated>2025-01-22T22:15:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted?fbclid=IwZXh0bgNhZW0CMTEAAR3i39eJbThbgTnI0Yz4JdnkMXgvj4wlorxOdbBeccw35kkqWqyrG816HpI_aem_EoENoW6h6SP-aU7FVwBWiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:15:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x5nd</id>
    <title>The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)</title>
    <updated>2025-01-23T06:19:53+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt; &lt;img alt="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" src="https://preview.redd.it/o28q2pl6roee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbcc9bcc3ec8f998f0c0b948df4117b2f7a6ae30" title="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o28q2pl6roee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
</feed>
