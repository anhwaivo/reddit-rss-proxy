<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-26T01:33:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jjl45h</id>
    <title>Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism</title>
    <updated>2025-03-25T14:48:25+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"&gt; &lt;img alt="Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism" src="https://a.thumbs.redditmedia.com/_IlnldFF_EGoXSBruEJQcoCcpBzX8koRU3W3RBstwu4.jpg" title="Compared performance of vLLM vs SGLang on 2 Nvidia GPUs - SGLang crushes it with Data Parallelism" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up a head-to-head benchmark of vLLM and SGLang on a 2x Nvidia GPU setup, and the results were pretty telling.&lt;/p&gt; &lt;p&gt;Running SGLang with data parallelism (--dp 2) yielded ~150% more requests and tokens generated compared to vLLM using tensor parallelism (--tensor-parallel-size 2). Not entirely surprising, given the architectural differences between data and tensor parallelism, but nice to see it quantified.&lt;/p&gt; &lt;p&gt;SGLang:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ============ Serving Benchmark Result ============ Successful requests: 10000 Benchmark duration (s): 640.00 Total input tokens: 10240000 Total generated tokens: 1255483 Request throughput (req/s): 15.63 Output token throughput (tok/s): 1961.70 Total Token throughput (tok/s): 17961.80 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;vLLM:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ============ Serving Benchmark Result ============ Successful requests: 10000 Benchmark duration (s): 1628.80 Total input tokens: 10240000 Total generated tokens: 1254908 Request throughput (req/s): 6.14 Output token throughput (tok/s): 770.45 Total Token throughput (tok/s): 7057.28 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;For anyone curious or wanting to reproduce: I’ve documented the full setup and benchmark steps for both stacks. Everything is codified with Ansible for fast, reproducible testing: • SGLang: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-SGLANG.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-SGLANG.md&lt;/a&gt; • vLLM: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-VLLM.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts or see if others have similar results across different models or GPU configs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjl45h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjqsa0</id>
    <title>Any insights into Sesame AI's technical moat?</title>
    <updated>2025-03-25T18:41:38+00:00</updated>
    <author>
      <name>/u/Pleasant_Syllabub591</name>
      <uri>https://old.reddit.com/user/Pleasant_Syllabub591</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried building for fun a similar pipeline with Google Streaming STT API --&amp;gt; Streaming LLM --&amp;gt; Streaming ElevenLabs TTS (I want to replace it with CSM-1B)&lt;/p&gt; &lt;p&gt;However, the latency is still far from matching the performance of Sesame Labs AI's demo. Does anyone have any suggestions for improving the latency?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant_Syllabub591"&gt; /u/Pleasant_Syllabub591 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqsa0/any_insights_into_sesame_ais_technical_moat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqsa0/any_insights_into_sesame_ais_technical_moat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqsa0/any_insights_into_sesame_ais_technical_moat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjdv9n</id>
    <title>DeepSeek-V3-0324 HF Model Card Updated With Benchmarks</title>
    <updated>2025-03-25T07:28:57+00:00</updated>
    <author>
      <name>/u/Few_Butterfly_4834</name>
      <uri>https://old.reddit.com/user/Few_Butterfly_4834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/blob/main/README.md"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Butterfly_4834"&gt; /u/Few_Butterfly_4834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjdv9n/deepseekv30324_hf_model_card_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T07:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjxq7r</id>
    <title>Gemini Coder - support for 2.5 Pro with AI Studio has landed!</title>
    <updated>2025-03-25T23:28:44+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"&gt; &lt;img alt="Gemini Coder - support for 2.5 Pro with AI Studio has landed!" src="https://external-preview.redd.it/bxJRNnlXq5IDDRT6Vz7ceiBs6PGN-YNIchXF0QIlGl0.jpg?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b7fdd819da65276924239d95b5dd385b92ae20c" title="Gemini Coder - support for 2.5 Pro with AI Studio has landed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjxq7r/gemini_coder_support_for_25_pro_with_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T23:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjalaj</id>
    <title>Deepseek-v3-0324 on Aider</title>
    <updated>2025-03-25T03:47:28+00:00</updated>
    <author>
      <name>/u/Harrycognito</name>
      <uri>https://old.reddit.com/user/Harrycognito</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"&gt; &lt;img alt="Deepseek-v3-0324 on Aider" src="https://preview.redd.it/ssol9q8ecrqe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a68d0ca221d1deace8a0c0efcbb4bc1ad3d0a5" title="Deepseek-v3-0324 on Aider" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Harrycognito"&gt; /u/Harrycognito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ssol9q8ecrqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjalaj/deepseekv30324_on_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T03:47:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjqnmq</id>
    <title>Amoral Gemma3 v2 (more uncensored this time)</title>
    <updated>2025-03-25T18:36:31+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/soob3123/amoral-gemma3-12B-v2"&gt;https://huggingface.co/soob3123/amoral-gemma3-12B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Big thanks to the community for testing the initial amoral-gemma3 release! Based on your feedback, I'm excited to share version 2 with significantly fewer refusals in pure assistant mode (no system prompts).&lt;/p&gt; &lt;p&gt;Thanks to mradermacher for the quants!&lt;br /&gt; Quants: &lt;a href="https://huggingface.co/mradermacher/amoral-gemma3-12B-v2-GGUF"&gt;mradermacher/amoral-gemma3-12B-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your test results - particularly interested in refusal rate comparisons with v1. Please share any interesting edge cases you find!&lt;/p&gt; &lt;p&gt;Note: 4B and 27B are coming soon! just wanted to test it out with 12B first!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqnmq/amoral_gemma3_v2_more_uncensored_this_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjlk7c</id>
    <title>Build Your Own AI Memory – Tutorial For Dummies</title>
    <updated>2025-03-25T15:07:50+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I just published a quick, beginner friendly tutorial showing how to build an AI memory system from scratch. It walks through:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Short-term vs. long-term memory&lt;/li&gt; &lt;li&gt;How to store and retrieve older chats&lt;/li&gt; &lt;li&gt;A minimal implementation with a simple self-loop you can test yourself&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No fancy jargon or complex abstractions—just a friendly explanation with sample code using &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;PocketFlow&lt;/a&gt;, a 100-line framework. If you’ve ever wondered how a chatbot remembers details, check it out!&lt;/p&gt; &lt;p&gt;&lt;a href="https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch"&gt;https://zacharyhuang.substack.com/p/build-ai-agent-memory-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjlk7c/build_your_own_ai_memory_tutorial_for_dummies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T15:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjhtln</id>
    <title>mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant.</title>
    <updated>2025-03-25T12:09:17+00:00</updated>
    <author>
      <name>/u/NighthawkXL</name>
      <uri>https://old.reddit.com/user/NighthawkXL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"&gt; &lt;img alt="mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant." src="https://external-preview.redd.it/P03UNyUnfLZkVgRKVkoK6RfK1RFxjBeZhspoe9bSuRw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10722e11edbe2a033b67017ad1d4380c7abb1ab2" title="mOrpheus: Using Whisper STT + Orpheus TTS + Gemma 3 using LM Studio to create a virtual assistant." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NighthawkXL"&gt; /u/NighthawkXL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Nighthawk42/mOrpheus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjhtln/morpheus_using_whisper_stt_orpheus_tts_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T12:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjblbt</id>
    <title>Implications for local LLM scene if Trump does a full Nvidia ban in China</title>
    <updated>2025-03-25T04:46:12+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Edit: Getting downvoted. If you'd like to have interesting discussions here, upvote this post. Otherwise, I will delete this post soon and post it somewhere else.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I think this post should belong here because it's very much related to local LLMs. At this point, Chinese LLMs are by far, the biggest contributors to open source LLMs.&lt;/p&gt; &lt;p&gt;DeepSeek and Qwen, and other Chinese models are getting too good despite not having the latest Nvidia hardware. They have to use gimped Nvidia hopper GPUs with limited bandwidth. Or they're using lesser AI chips from Huawei that wasn't made using the latest TSMC node. Chinese companies have been banned from using TSMC N5, N3, and N2 nodes since late 2024. &lt;/p&gt; &lt;p&gt;I'm certain that Sam Altman, Elon, Bezos, Google founders, Zuckerberg are all lobbying Trump to do a fun Nvidia ban in China. Every single one of them showed up at Trump's inauguration and donated to his fund. This likely means not even gimped Nvidia GPUs can be sold in China. &lt;/p&gt; &lt;p&gt;US big tech companies can't get a high ROI if free/low cost Chinese LLMs are killing their profit margins.&lt;/p&gt; &lt;p&gt;When Deepseek R1 destroyed Nvidia's stock price, it wasn't because people thought the efficiency would lead to less Nvidia demand. No, it'd increase Nvidia demand. Instead, I believe Wall Street was worried that tech bros would lobby Trump to do a fun Nvidia ban in China. Tech bros have way more influence on Trump than Nvidia.&lt;/p&gt; &lt;p&gt;A full ban on Nvidia in China would benefit US tech bros in a few ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Slow down competition from China. Blackwell US models vs gimped Hopper Chinese models in late 2025.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Easier and faster access to Nvidia's GPUs for US companies. I estimate that 30% of Nvidia's GPU sales end up in China.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Lower Nvidia GPU prices all around because of the reduced demand.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvzk0</id>
    <title>:|</title>
    <updated>2025-03-25T22:13:49+00:00</updated>
    <author>
      <name>/u/muxxington</name>
      <uri>https://old.reddit.com/user/muxxington</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"&gt; &lt;img alt=":|" src="https://preview.redd.it/6c6z4mxrtwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53b44b9a4b4608007a917982e63cd139f9da2c" title=":|" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muxxington"&gt; /u/muxxington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6c6z4mxrtwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvzk0/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:13:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjis2u</id>
    <title>Recent models really make me think attention is all we need</title>
    <updated>2025-03-25T13:00:19+00:00</updated>
    <author>
      <name>/u/ludosudowudo</name>
      <uri>https://old.reddit.com/user/ludosudowudo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new sonnet 3.7 and Deepseek v3 are really a step up reasoning wise from older models. A lot of people at first also agreed there seemed to be no walls left for reasoning when the inference time reinforcement learning paradigm shift happened a couple of months ago with O1. That's until very recently, when they saw how a Claude 3.7 Agent playing pokemon really childishly struggles with the game. Since then I feel like people are switching again to the opinion that a new breakthrough or architectural solution is needed to solve the better memory and context problem.&lt;/p&gt; &lt;p&gt;However, the more time I spent thinking about it, the more it feels like this context/memory problem is also a solvable problem with reinforcement learning. The problem of memory and context is not the lack of memory, these models have a huge amount of context window. It seems to be a problem related to the management of memory and context. And as we can see with the simple framework the agent playing the game is currently using to manage memory, it seems validating and summarizing context helps. In essence, the problem of memory management and orchestration seems to be climbable with reinforcement learning.&lt;/p&gt; &lt;p&gt;My prediction is that reinforcement learning on memory/context management will cause models to climb their search algorithm to spend more tokens on higher-order context management. Just like with the Deepseek &amp;quot;aha&amp;quot; moment and the &amp;lt;think&amp;gt; tokens, I predict that with reinforcement learning on agentic tasks fairly quickly a &amp;quot;reassess&amp;quot; moment will emerge and a &amp;lt;recontextualize&amp;gt; token will naturally follow. This higher-order context management, just like reasoning, is bound to already be present in the huge amount of pretraining data, and probably can be unlocked with a small reinforcement learning run with the right dataset.&lt;/p&gt; &lt;p&gt;I really think attention, scale and reinforcement learning is all we need to get to human level agent performance.&lt;/p&gt; &lt;p&gt;edit: As to what is valuable data for this kind of ability training, my guess is that the most valuable problems for this kind of climbing will be long simple hierarchical tasks where a lot of diverse subtasks each with a lot of memory/context need to be continuously juggled over long thinking process. The subtasks also need temporal dependencies between them. In essence subtask A can only be solvable up to a certain point x, after which subtask B can only be solvable to a certain point y, after which subtask A is solvable again to point z, etc. Without these temporal dependencies in the problems subtasks, reinforcement learning will optimize probably to fully solving subtask A instead of recontextualizing to subtask B during its long thinking stage.&lt;/p&gt; &lt;p&gt;edit2: A farfetched possible example is the class of problems that are better solvable with breath first search instead of depth first search.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ludosudowudo"&gt; /u/ludosudowudo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjis2u/recent_models_really_make_me_think_attention_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jj6i4m</id>
    <title>Deepseek v3</title>
    <updated>2025-03-25T00:19:31+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt; &lt;img alt="Deepseek v3" src="https://preview.redd.it/xaic503gbqqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=616bfd3de239ef7db7a2416bc9be3a95051f9c0b" title="Deepseek v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xaic503gbqqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jj6i4m/deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T00:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjl49l</id>
    <title>Qwen?! 👀</title>
    <updated>2025-03-25T14:48:34+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt; &lt;img alt="Qwen?! 👀" src="https://external-preview.redd.it/pSzn5luA5bEL814Ul3JN__zTnOX5m5uc7UJYJ7zNQ_k.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa05eb3479835a6a18730e50d61faa6a62ffce2b" title="Qwen?! 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gcu4thlvluqe1.png?width=1168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bc2bbb1a4b8d74ca2d65572c25e1ed2ae19b4db"&gt;Is it what I think it is?!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was posted as a reply shortly after Qwen2.5-VL-32B-Instruct's announcement&lt;br /&gt; &lt;a href="https://x.com/JustinLin610/status/1904231553183744020"&gt;https://x.com/JustinLin610/status/1904231553183744020&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjl49l/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T14:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjuu78</id>
    <title>New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context</title>
    <updated>2025-03-25T21:25:54+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt; &lt;img alt="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" src="https://preview.redd.it/ks0djm85lwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c523a2ac3957a50567391d0e0d6a09816702e7" title="New DeepSeek V3 (significant improvement) and Gemini 2.5 Pro (SOTA) Tested in long context" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ks0djm85lwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjuu78/new_deepseek_v3_significant_improvement_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jji2da</id>
    <title>DeepSeek-V3-0324 GGUF - Unsloth</title>
    <updated>2025-03-25T12:22:51+00:00</updated>
    <author>
      <name>/u/Co0k1eGal3xy</name>
      <uri>https://old.reddit.com/user/Co0k1eGal3xy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available Formats so far;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UD-IQ1_S (140.2 GB)&lt;/li&gt; &lt;li&gt;UD-IQ1_M (155.0 GB)&lt;/li&gt; &lt;li&gt;UD-IQ2_XXS (196.2 GB)&lt;/li&gt; &lt;li&gt;UD-Q2_K_XL (226.6 GB)&lt;/li&gt; &lt;li&gt;Q2_K (244.0 GB)&lt;/li&gt; &lt;li&gt;Q3_K_M (319.2 GB)&lt;/li&gt; &lt;li&gt;UD-Q3_K_XL (320.7 GB)&lt;/li&gt; &lt;li&gt;Q4_K_M (404.3 GB)&lt;/li&gt; &lt;li&gt;UD-Q4_K_XL (404.9 GB)&lt;/li&gt; &lt;li&gt;Q5_K_M (475.4 GB)&lt;/li&gt; &lt;li&gt;Q6_K (550.5 GB)&lt;/li&gt; &lt;li&gt;Q8_0 (712.9 GB)&lt;/li&gt; &lt;li&gt;BF16 (1765.3 GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jji2da/comment/mjnbeh9/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Hey thanks for posting! We haven't finished uploading the rest but currently we're in the process of testing them.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jji2da/comment/mjnbeh9"&gt;You can wait for our official announcement or use the 2, 3 and 4-bit dynamic quants now&lt;/a&gt; - u/&lt;a href="https://www.reddit.com/user/yoracale/"&gt;yoracale&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Co0k1eGal3xy"&gt; /u/Co0k1eGal3xy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jji2da/deepseekv30324_gguf_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T12:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjusya</id>
    <title>Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands</title>
    <updated>2025-03-25T21:24:35+00:00</updated>
    <author>
      <name>/u/AmbitiousSeaweed101</name>
      <uri>https://old.reddit.com/user/AmbitiousSeaweed101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt; &lt;img alt="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" src="https://preview.redd.it/laea7v40lwqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258ada96674ff89666373dd77c431418f63438cc" title="Deepseek V3 0324 got 38.8% SWE-Bench Verified w/ OpenHands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmbitiousSeaweed101"&gt; /u/AmbitiousSeaweed101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/laea7v40lwqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjusya/deepseek_v3_0324_got_388_swebench_verified_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjpsfp</id>
    <title>Google's new Gemini 2.5 beats all other thinking model as per their claims in their article . What are your views on this?</title>
    <updated>2025-03-25T18:01:43+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking"&gt;https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjpsfp/googles_new_gemini_25_beats_all_other_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjqa9a</id>
    <title>AMD Is Reportedly Bringing Strix Halo To Desktop; CEO Lisa Su Confirms In An Interview.</title>
    <updated>2025-03-25T18:21:29+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/"&gt;https://wccftech.com/amd-is-reportedly-bringing-strix-halo-to-desktop/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is so awesome. You will be able to have up to 96Gb dedicated to Vram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjqa9a/amd_is_reportedly_bringing_strix_halo_to_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T18:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjjv8k</id>
    <title>DeepSeek official communication on X: DeepSeek-V3-0324 is out now!</title>
    <updated>2025-03-25T13:53:11+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt; &lt;img alt="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" src="https://b.thumbs.redditmedia.com/__aOAn3RMb1pB4WQ7nZaRtP8KJ2vjbYZROoq35jWyoc.jpg" title="DeepSeek official communication on X: DeepSeek-V3-0324 is out now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jjjv8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjjv8k/deepseek_official_communication_on_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T13:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjv68r</id>
    <title>Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)</title>
    <updated>2025-03-25T21:39:40+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt; &lt;img alt="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" src="https://preview.redd.it/vnkynyqrnwqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b966146c75053316ab0b7e51083981dd658f31bd" title="Aider - A new Gemini pro 2.5 just ate sonnet 3.7 thinking like a snack ;-)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnkynyqrnwqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjv68r/aider_a_new_gemini_pro_25_just_ate_sonnet_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T21:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgje5</id>
    <title>We got competition</title>
    <updated>2025-03-25T10:50:03+00:00</updated>
    <author>
      <name>/u/BlueeWaater</name>
      <uri>https://old.reddit.com/user/BlueeWaater</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt; &lt;img alt="We got competition" src="https://preview.redd.it/bamkfj1yftqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c43810bb7e5d8ea7891aeebc79e47a801d562c8" title="We got competition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueeWaater"&gt; /u/BlueeWaater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bamkfj1yftqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgje5/we_got_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjsiiw</id>
    <title>Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!</title>
    <updated>2025-03-25T19:51:51+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt; &lt;img alt="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" src="https://external-preview.redd.it/N283c2VudGQ0d3FlMV2EuLTbyq8GSEyVM5EFne5QcU-eiwqTnkibTrWsMAGW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=749ce5ef623cae53e2677afd031ff951b8a489ce" title="Mario game made by new a Gemini pro 2.5 in couple minutes - best version I ever saw. Even great physics!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/955pvmtd4wqe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjsiiw/mario_game_made_by_new_a_gemini_pro_25_in_couple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T19:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjgi8y</id>
    <title>Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys.</title>
    <updated>2025-03-25T10:47:48+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt; &lt;img alt="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." src="https://preview.redd.it/4hh6ys9gftqe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01ab49fd276d31a93696fb2a9a9f51d5ad35c7" title="Deepseek V3 0324 is now the best non-reasoning model (across both open and closed source) according to Artificial Analisys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hh6ys9gftqe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjgi8y/deepseek_v3_0324_is_now_the_best_nonreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T10:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjvo4e</id>
    <title>we are just 3 months into 2025</title>
    <updated>2025-03-25T22:00:40+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt; &lt;img alt="we are just 3 months into 2025" src="https://b.thumbs.redditmedia.com/UkF78GvO1l_Iu4ZikUoUTSJvE6F25Fvn1d1yTgP75FU.jpg" title="we are just 3 months into 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81"&gt;https://preview.redd.it/sijszr0lrwqe1.png?width=832&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e3073891e4d9e2650e53ef7f9aa6cd393d23c81&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjvo4e/we_are_just_3_months_into_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T22:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jjorwd</id>
    <title>I think we’re going to need a bigger bank account.</title>
    <updated>2025-03-25T17:20:34+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt; &lt;img alt="I think we’re going to need a bigger bank account." src="https://preview.redd.it/zr3syf8mdvqe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27187b0a5f34d831c3e26fc2978cc6ab6324cf35" title="I think we’re going to need a bigger bank account." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zr3syf8mdvqe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jjorwd/i_think_were_going_to_need_a_bigger_bank_account/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-25T17:20:34+00:00</published>
  </entry>
</feed>
