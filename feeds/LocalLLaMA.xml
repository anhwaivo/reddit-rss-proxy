<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-24T18:07:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k6swa7</id>
    <title>Experiences with open deep research and local LLMs</title>
    <updated>2025-04-24T13:58:16+00:00</updated>
    <author>
      <name>/u/edmcman</name>
      <uri>https://old.reddit.com/user/edmcman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had good results with open deep research implementations using local LLMs?&lt;/p&gt; &lt;p&gt;I am aware of at least several open deep research implementations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/langchain-ai/local-deep-researcher"&gt;https://github.com/langchain-ai/local-deep-researcher&lt;/a&gt; This is the only one I am aware of that seems to have been tested on local LLMs at all. My experience has been hit or miss, with some queries unexpectedly returning an empty string as the running summary using deepseek-r1:8b.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/langchain-ai/open_deep_research"&gt;https://github.com/langchain-ai/open_deep_research&lt;/a&gt; Yes, this seems to be a different but very similar project from langchain. It does not seem to be intended for local LLMs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research"&gt;https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research&lt;/a&gt; I also haven't tried this, but smolagents seems like it is mostly geared towards commercial LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edmcman"&gt; /u/edmcman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6swa7/experiences_with_open_deep_research_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6swa7/experiences_with_open_deep_research_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6swa7/experiences_with_open_deep_research_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T13:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6tf5n</id>
    <title>Does GLM have vision?</title>
    <updated>2025-04-24T14:20:06+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed on the GitHub page they claim GLM is multimodal, but couldn't find anything on its vision capabilities &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6tf5n/does_glm_have_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6tf5n/does_glm_have_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6tf5n/does_glm_have_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T14:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6q54d</id>
    <title>Best small model</title>
    <updated>2025-04-24T11:45:19+00:00</updated>
    <author>
      <name>/u/Jshap623</name>
      <uri>https://old.reddit.com/user/Jshap623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A bit dated, looking to run small models on 6GB VRAM laptop. Best UI still text gen-UI? Qwen good way to go? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jshap623"&gt; /u/Jshap623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6q54d/best_small_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6q54d/best_small_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6q54d/best_small_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T11:45:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k5wdw0</id>
    <title>HP wants to put a local LLM in your printers</title>
    <updated>2025-04-23T11:05:18+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt; &lt;img alt="HP wants to put a local LLM in your printers" src="https://preview.redd.it/9wawej40hkwe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f75beba5aa65b4f7a42767d2301f3c23268219c3" title="HP wants to put a local LLM in your printers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wawej40hkwe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k5wdw0/hp_wants_to_put_a_local_llm_in_your_printers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T11:05:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6mx40</id>
    <title>Looking for better alternatives to Ollama - need faster model updates and easier tool usage</title>
    <updated>2025-04-24T08:10:06+00:00</updated>
    <author>
      <name>/u/netixc1</name>
      <uri>https://old.reddit.com/user/netixc1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Ollama because it's super straightforward - just check the model list on their site, find one with tool support, download it, and you're good to go. But I'm getting frustrated with how slow they are at adding support for new models like Llama 4 and other recent releases.&lt;/p&gt; &lt;p&gt;What alternatives to Ollama would you recommend that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can run in Docker&lt;/li&gt; &lt;li&gt;Add support for new models more quickly&lt;/li&gt; &lt;li&gt;Have built-in tool/function calling support without needing to hunt for templates&lt;/li&gt; &lt;li&gt;Are relatively easy to set up (similar to Ollama's simplicity)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm looking for something that gives me access to newer models faster while still maintaining the convenience factor. Any suggestions would be appreciated!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: I'm specifically looking for self-hosted options that I can run locally, not cloud services.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/netixc1"&gt; /u/netixc1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mx40/looking_for_better_alternatives_to_ollama_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T08:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6mvoi</id>
    <title>Serving new models with vLLM with efficient quantization</title>
    <updated>2025-04-24T08:07:19+00:00</updated>
    <author>
      <name>/u/Swedgetarian</name>
      <uri>https://old.reddit.com/user/Swedgetarian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'd love to hear from vLLM users what you guys' playbooks for serving recently supported models are. &lt;/p&gt; &lt;p&gt;I'm running the &lt;a href="https://hub.docker.com/r/vllm/vllm-openai/tags"&gt;vLLM openai compatiable docker container&lt;/a&gt; on an inferencing server.&lt;/p&gt; &lt;p&gt;Up until now, i've taken the easy path of using pre-quantized AWQ checkpoints from the huggingface hub. But this often excludes a lot of recent models. Conversely, GUUFs are readily available pretty much on day 1. I'm left with a few options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Quantize the target model to AWQ myself either in the vllm container or in a separate env then inject it into the container&lt;/li&gt; &lt;li&gt;Try the experimental GGUF support in vLLM (would love to hear people's experiences with this)&lt;/li&gt; &lt;li&gt;Experiment with the &lt;a href="https://docs.vllm.ai/en/stable/features/quantization/index.html"&gt;other supported quantization formats&lt;/a&gt; like BnB when such checkpoints are available on HF hub.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;There's also the new unsloth dynamic 4-bit quants that sound to be very good bang-for-buck in VRAM. They seem to be based on BnB with new features. Has anyone managed to get models in this format in vLLM working?&lt;/p&gt; &lt;p&gt;Thanks for any inputs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swedgetarian"&gt; /u/Swedgetarian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mvoi/serving_new_models_with_vllm_with_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mvoi/serving_new_models_with_vllm_with_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mvoi/serving_new_models_with_vllm_with_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T08:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6k1pq</id>
    <title>What OS do you use?</title>
    <updated>2025-04-24T04:56:13+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m doing some research for my local inference engine project. I’ll follow up with more polls. Thanks for participating!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1k6k1pq"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6k1pq/what_os_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6jslj</id>
    <title>LLM content on YT becoming repetitive</title>
    <updated>2025-04-24T04:40:57+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been following the discussion and content around LLMs very closely from the beginning of the AI craze on youtube and am subscribed to most LLM related channels. While in the beginning and well throughout most of the last one or two years there was a ton of new content every day, covering all aspects. Content felt very diverse. From RAG to inference, to evals and frameworks like Dspy, chunking strategies and ingestion pipelines, fine tuning libraries like unsloth and agentic frameworks like crewAI and autogen. Or of course the AI IDEs like cursor and windsurf and things like liteLLM need to be mentioned as well, and there's many more which don't come to mind right now.&lt;/p&gt; &lt;p&gt;Fast forward to today and the channels are still around, but they seem to cover only specific topics like MCP and then all at once. Clearly, once something new has been talked about you can't keep bringing it up. But at the same time I have a hard time believing that even in those established projects there's nothing new to talk about.&lt;/p&gt; &lt;p&gt;There would be so much room to speak about the awesome stuff you could do with all these tools, but to me it seems content creators have fallen into a routine. Do you share the same impression? What are channels you are watching that keep bringing innovative and inspiring content still at this stage of where the space has gotten to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6jslj/llm_content_on_yt_becoming_repetitive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6yeqg</id>
    <title>Currently what is the best text to voice model to read articles / ebooks while using 8gb vram?</title>
    <updated>2025-04-24T17:42:16+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for good model that can turn ebooks / article into voice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6yeqg/currently_what_is_the_best_text_to_voice_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6yeqg/currently_what_is_the_best_text_to_voice_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6yeqg/currently_what_is_the_best_text_to_voice_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6vg1e</id>
    <title>quiz yourself with llamatest</title>
    <updated>2025-04-24T15:43:36+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made this to help myself study.&lt;/p&gt; &lt;p&gt;Type in a topic, or paste in text, and llamatest will generate questions and answers.&lt;/p&gt; &lt;p&gt;It tends to get a little wordy in the answers, but I am working on better prompting.&lt;/p&gt; &lt;p&gt;Edit: prompr is better, answers are shorter so it generates faster&lt;/p&gt; &lt;p&gt;just a single html page, requires a running llama-server from llamacpp&lt;/p&gt; &lt;p&gt;I find it useful, hope you do too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openconstruct/llamatest"&gt;https://github.com/openconstruct/llamatest&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6vg1e/quiz_yourself_with_llamatest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T15:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6hah2</id>
    <title>SmolBoi: watercooled 3x RTX 3090 FE &amp; EPYC 7642 in O11D (with build pics)</title>
    <updated>2025-04-24T02:25:43+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt; &lt;img alt="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" src="https://a.thumbs.redditmedia.com/wDcGZwo_b01L-EApmvhJ6AR3vHGWlV4tE6C_nRqL234.jpg" title="SmolBoi: watercooled 3x RTX 3090 FE &amp;amp; EPYC 7642 in O11D (with build pics)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;The initial idea for build started with a single RTX 3090 FE I bought about a year and a half ago, right after the crypto crash. Over the next few months, I bought two more 3090 FEs. &lt;/p&gt; &lt;p&gt;From the beginning, my criteria for this build were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Buy components based on good deals I find in local classifieds, ebay, or tech forums.&lt;/li&gt; &lt;li&gt;Everything that can be bought 2nd hand, shall be bought 2nd hand.&lt;/li&gt; &lt;li&gt;I already had a Lian Li O11D case (not XL, not Evo), so everything shall fit there.&lt;/li&gt; &lt;li&gt;Watercooled to keep noise and temps low despite the size.&lt;/li&gt; &lt;li&gt;ATX motherboard to give myself a bit more space inside the case.&lt;/li&gt; &lt;li&gt;Xeon Scalable or Epyc: I want plenty PCIe lanes, U.2 for storage, lots of RAM, plenty of bandwidth, and I want it cheap.&lt;/li&gt; &lt;li&gt;U.2 SSDs because they're cheaper and more reliable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took a couple more months to source all components, but in the end, here is what ended in this rig, along with purchase price:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro H12SSL-i: 300€.&lt;/li&gt; &lt;li&gt;AMD EPYC 7642: 220€ (bought a few of those together)&lt;/li&gt; &lt;li&gt;512GB 8x64GB Samsung DDR4-2666 ECCRDIMM: 350€&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE: 1550€&lt;/li&gt; &lt;li&gt;2x Samsung PM1735 1.6TB U.2 Gen 4 SSD: 125€&lt;/li&gt; &lt;li&gt;256GB M.2 Gen 3 NVME: 15€&lt;/li&gt; &lt;li&gt;4x Bykski waterblocks: 60€/block&lt;/li&gt; &lt;li&gt;Bykski waterblock GPU bridge: 24€&lt;/li&gt; &lt;li&gt;Alphacool Eisblock XPX Pro 1U: 65€&lt;/li&gt; &lt;li&gt;EVGA 1600W PSU: 100€&lt;/li&gt; &lt;li&gt;3x RTX 3090 FE 21-pin power adapter cable: 45€&lt;/li&gt; &lt;li&gt;3x PCIe Gen 4 x16 risers: 70€&lt;/li&gt; &lt;li&gt;EK 360mm 45mm + 2x alphacool 360mm 30mm: 100€&lt;/li&gt; &lt;li&gt;EK Quantum Kinetic 120mm reservoir: 35€&lt;/li&gt; &lt;li&gt;Xylem D5 pump: 35€&lt;/li&gt; &lt;li&gt;10x Arctic P12 Max: 70€ (9 used)&lt;/li&gt; &lt;li&gt;Arctic P8 Max: 5€&lt;/li&gt; &lt;li&gt;tons of fittings from Aliexpress: 50-70€&lt;/li&gt; &lt;li&gt;Lian Li X11 upright GPU mount: 15€&lt;/li&gt; &lt;li&gt;Anti-sagging GPU brace: 8€&lt;/li&gt; &lt;li&gt;5M fishtank 10x13mm PVC tube: 10€&lt;/li&gt; &lt;li&gt;Custom Aluminum plate for upright GPU mount: 45€&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total: ~3400€&lt;/p&gt; &lt;p&gt;I'm excluding the Mellanox ConnextX-3 56gb infiniband. It's not technically needed, and it was like 13€.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, it's a pretty tight fit. Took a lot of planning and redesign to make everything fit in.&lt;/p&gt; &lt;p&gt;My initial plan was to just plug the watercooled cards into the motherboard witha triple bridge (Bykski sells those, and they'll even make you a custom bridge if you ask nicely, which is why I went for their blocks). Unbeknown to me, the FE cards I went with because they're shorter (I thought easier fit) are also quite a bit taller than reference cards. This made it impossible to fit the cards in the case, as even low profile fitting adapter (the piece that converts the ports on the block to G1/4 fittings) was too high to fit in my case. I explored other case options that could fit three 360mm radiators but couldn't find any that would also have enough height for the blocks.&lt;/p&gt; &lt;p&gt;This height issue necessitated a radical rethinking of how I'd fit the GPUs. I started playing with one GPU with the block attached inside the case to see how I could fit them, and the idea of dangling two from the top of the case was born. I knew Lian Li sold the upright GPU mount, but that was for the EVO. I didn't want to buy the EVO because that would mean reducing the top radiator to 240mm, and I wanted that to be 45mm to do the heavy lifting of removing most heat. &lt;/p&gt; &lt;p&gt;I used my rudimentary OpenSCAD skills to design a plate that would screw to a 120mm fan and provide mounting holes for the upright GPU bracket. With that, I could hang two GPUs. I used JLCPCB to make 2 of them. With two out of the way, finding a place for the 3rd GPU was much easier. The 2nd plate ended having the perfect hole spacing for mounting the PCIe riser connector, providing a base for the 3rd GPU. An anti-sagging GPU brace provided the last bit of support needed to keep the 3rd GPU safe.&lt;/p&gt; &lt;p&gt;As you can see in the pictures, the aluminum (2mm 7075) plate is bent. This was because the case was left on it's side with the two GPUs dangling for well over a month. It was supposed to a few hours, but health issues stopped the build abruptly. The motherboard also died on me (common issue with H12SSL, cost 50€ to fix at Supermicro, including shipping. Motherboard price includes repair cost), which delayed things further. The pictures are from reassembling after I got it back.&lt;/p&gt; &lt;p&gt;The loop (from coldest side) out of the bottom radiator, into the two GPUs, on to the the 3rd GPU, then pump, into the CPU, onwards to the top radiator, leading to the side radiator, and back to the bottom radiator. Temps on the GPUs peak ~51C so far. Though the board's BMC monitors GPU temps directly (I didn't know it could), having the warmest water go to the CPU means the fans will ramp up even if there's no CPU load. The pump PWM is not connected, keeping it at max rpm on purpose for high circulation. Cooling is provided by distilled water with a few drops of Iodine. Been running that on my quad P40 rig for months now without issue.&lt;/p&gt; &lt;p&gt;At idle, the rig is very quiet. Fans idle at 1-1.1k rpm. Haven't checked RPM under load.&lt;/p&gt; &lt;p&gt;Model storage is provided by the two Gen4 PM1735s in RAID0 configuration. Haven't benchmarked them yet, but I saw 13GB/s on nvtop while loading Qwen 32B and Nemotron 49B. The GPUs report Gen4 X16 in nvtop, but I haven't checked for errors. I am blowen by the speed with which models load from disk, even when I tested with --no-mmap.&lt;/p&gt; &lt;p&gt;DeepSeek V3 is still downloading...&lt;/p&gt; &lt;p&gt;And now, for some LLM inference numbers using llama.cpp (b5172). I filled the loop yesterday and got Ubuntu installed today, so I haven't gotten to try vLLM yet. GPU power is the default 350W. Apart from Gemma 3 QAT, all models are Q8.&lt;/p&gt; &lt;h3&gt;Mistral-Small-3.1-24B-Instruct-2503 with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -md /models/Mistral-Small-3.1-DRAFT-0.5B.Q8_0.gguf -fa -sm row --no-mmap -ngl 99 -ngld 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --device-draft CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.35&lt;/td&gt; &lt;td&gt;1044&lt;/td&gt; &lt;td&gt;30.92&lt;/td&gt; &lt;td&gt;34347.16&lt;/td&gt; &lt;td&gt;1154&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.29055 ( 446 accepted / 1535 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Mistral-Small-3.1-24B no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf -fa -sm row --no-mmap -ngl 99 --port 9009 -c 65536 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA2,CUDA1 --tensor-split 0,1,1 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.06&lt;/td&gt; &lt;td&gt;992&lt;/td&gt; &lt;td&gt;30.41&lt;/td&gt; &lt;td&gt;33205.86&lt;/td&gt; &lt;td&gt;1102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B with Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -md /models/gemma-3-1b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 -ngld 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --draft-max 16 --draft-min 5 --draft-p-min 0.5 --device CUDA0,CUDA1 --device-draft CUDA0 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;151.36&lt;/td&gt; &lt;td&gt;1806&lt;/td&gt; &lt;td&gt;14.87&lt;/td&gt; &lt;td&gt;122161.81&lt;/td&gt; &lt;td&gt;1913&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;draft acceptance rate = 0.23570 ( 787 accepted / 3339 generated)&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27b no-Draft&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-Q8_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row --no-mmap -ngl 99 --port 9005 -c 20000 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;152.85&lt;/td&gt; &lt;td&gt;1957&lt;/td&gt; &lt;td&gt;20.96&lt;/td&gt; &lt;td&gt;94078.01&lt;/td&gt; &lt;td&gt;2064&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;QwQ-32B.Q8&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/QwQ-32B.Q8_0.gguf --temp 0.6 --top-k 40 --repeat-penalty 1.1 --min-p 0.0 --dry-multiplier 0.5 -fa -sm row --no-mmap -ngl 99 --port 9008 -c 80000 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;132.51&lt;/td&gt; &lt;td&gt;2313&lt;/td&gt; &lt;td&gt;19.50&lt;/td&gt; &lt;td&gt;119326.49&lt;/td&gt; &lt;td&gt;2406&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Gemma-3-27B QAT Q4&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m llama-server -m /models/gemma-3-27b-it-q4_0.gguf -fa --temp 1.0 --top-k 64 --min-p 0.0 --top-p 0.95 -sm row -ngl 99 -c 65536 --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0 --tensor-split 1,0,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9004 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1042.04&lt;/td&gt; &lt;td&gt;2411&lt;/td&gt; &lt;td&gt;36.13&lt;/td&gt; &lt;td&gt;2673.49&lt;/td&gt; &lt;td&gt;2424&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;634.28&lt;/td&gt; &lt;td&gt;14505&lt;/td&gt; &lt;td&gt;24.58&lt;/td&gt; &lt;td&gt;385537.97&lt;/td&gt; &lt;td&gt;23418&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Qwen2.5-Coder-32B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf --top-k 20 -fa --top-p 0.9 --min-p 0.1 --temp 0.7 --repeat-penalty 1.05 -sm row -ngl 99 -c 65535 --samplers &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,CUDA1 --tensor-split 1,1,0 --slots --metrics --numa distribute -t 40 --no-warmup --no-mmap --port 9005 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;187.50&lt;/td&gt; &lt;td&gt;11709&lt;/td&gt; &lt;td&gt;15.48&lt;/td&gt; &lt;td&gt;558661.10&lt;/td&gt; &lt;td&gt;19390&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Llama-3_3-Nemotron-Super-49B&lt;/h3&gt; &lt;p&gt;&lt;code&gt;bash /models/llama.cpp/llama-server -m /models/Llama-3_3-Nemotron-Super-49B/nvidia_Llama-3_3-Nemotron-Super-49B-v1-Q8_0-00001-of-00002.gguf -fa -sm row -ngl 99 -c 32768 --device CUDA0,CUDA1,CUDA2 --tensor-split 1,1,1 --slots --metrics --numa distribute -t 40 --no-mmap --port 9001 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;prompt eval tk/s&lt;/th&gt; &lt;th&gt;prompt tokens&lt;/th&gt; &lt;th&gt;eval tk/s&lt;/th&gt; &lt;th&gt;total time&lt;/th&gt; &lt;th&gt;total tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;120.56&lt;/td&gt; &lt;td&gt;1164&lt;/td&gt; &lt;td&gt;17.21&lt;/td&gt; &lt;td&gt;68414.89&lt;/td&gt; &lt;td&gt;1259&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;70.11&lt;/td&gt; &lt;td&gt;11644&lt;/td&gt; &lt;td&gt;14.58&lt;/td&gt; &lt;td&gt;274099.28&lt;/td&gt; &lt;td&gt;13219&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k6hah2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6hah2/smolboi_watercooled_3x_rtx_3090_fe_epyc_7642_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T02:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6uk5n</id>
    <title>Updates for FreeOllama, also updates for the FreeLeak series</title>
    <updated>2025-04-24T15:07:12+00:00</updated>
    <author>
      <name>/u/zxbsmk</name>
      <uri>https://old.reddit.com/user/zxbsmk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"&gt; &lt;img alt="Updates for FreeOllama, also updates for the FreeLeak series" src="https://b.thumbs.redditmedia.com/N1F6rYLa3r9ZX9ajJtUn3aDRKGgHWwg3fHudkiINtCk.jpg" title="Updates for FreeOllama, also updates for the FreeLeak series" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously, we discovered that some Ollama servers were pass-protected. To address this, we enhanced our server scanner to confirm the actual availability of all accessible servers. Additionally, we developed FreeChat as a quick verification tool for this purpose.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.freeleakhub.com/"&gt;https://chat.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.freeleakhub.com/"&gt;https://ollama.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.freeleakhub.com/"&gt;https://www.freeleakhub.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxbsmk"&gt; /u/zxbsmk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.freeollama.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6uk5n/updates_for_freeollama_also_updates_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T15:07:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6o4wj</id>
    <title>MCP, an easy explanation</title>
    <updated>2025-04-24T09:39:16+00:00</updated>
    <author>
      <name>/u/SimplifyExtension</name>
      <uri>https://old.reddit.com/user/SimplifyExtension</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I tried looking up what an MCP is, I could only find tweets like “omg how do people not know what MCP is?!?”&lt;/p&gt; &lt;p&gt;So, in the spirit of not gatekeeping, here’s my understanding:&lt;/p&gt; &lt;p&gt;MCP stands for Model Context Protocol. The purpose of this protocol is to define a standardized and flexible way for people to build AI agents with.&lt;/p&gt; &lt;p&gt;MCP has two main parts:&lt;/p&gt; &lt;p&gt;The MCP Server &amp;amp; The MCP Client&lt;/p&gt; &lt;p&gt;The MCP Server is just a normal API that does whatever it is you want to do. The MCP client is just an LLM that knows your MCP server very well and can execute requests.&lt;/p&gt; &lt;p&gt;Let’s say you want to build an AI agent that gets data insights using natural language. &lt;/p&gt; &lt;p&gt;With MCP, your MCP server exposes different capabilities as endpoints… maybe /users to access user information and /transactions to get sales data.&lt;/p&gt; &lt;p&gt;Now, imagine a user asks the AI agent: &amp;quot;What was our total revenue last month?&amp;quot;&lt;/p&gt; &lt;p&gt;The LLM from the MCP client receives this natural language request. Based on its understanding of the available endpoints on your MCP server, it determines that &amp;quot;total revenue&amp;quot; relates to &amp;quot;transactions.&amp;quot; &lt;/p&gt; &lt;p&gt;It then decides to call the /transactions endpoint on your MCP server to get the necessary data to answer the user's question. &lt;/p&gt; &lt;p&gt;If the user asked &amp;quot;How many new users did we get?&amp;quot;, the LLM would instead decide to call the /users endpoint.&lt;/p&gt; &lt;p&gt;Let me know if I got that right or if you have any questions!&lt;/p&gt; &lt;p&gt;I’ve been learning more about agent protocols and post my takeaways on X @joshycodes. Happy to talk more if anyone’s curious!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SimplifyExtension"&gt; /u/SimplifyExtension &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6o4wj/mcp_an_easy_explanation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6p20z</id>
    <title>4x64 DDR5 - 256GB consumer grade build for LLMs?</title>
    <updated>2025-04-24T10:41:44+00:00</updated>
    <author>
      <name>/u/scammer69</name>
      <uri>https://old.reddit.com/user/scammer69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have recently discovered that there are 64GB single sticks of DDR5 available - unregistered, unbuffered, no ECC, so the should in theory be compatible with our consumer grade gaming PCs.&lt;/p&gt; &lt;p&gt;I believe thats fairly new, I haven't seen 64GB single sticks just few months ago&lt;/p&gt; &lt;p&gt;Both AMD 7950x specs and most motherboards (with 4 DDR slots) only list 128GB as their max supported memory - I know for a fact that its possible to go above this, as there are some Ryzen 7950X dedicated servers with 192GB (4x48GB) available.&lt;/p&gt; &lt;p&gt;Has anyone tried to run a LLM on something like this? Its only two memory channels, so bandwidth would be pretty bad compared to enterprise grade builds with more channels, but still interesting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/scammer69"&gt; /u/scammer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T10:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6ably</id>
    <title>Bartowski just updated his glm-4-32B quants. working in lmstudio soon?</title>
    <updated>2025-04-23T21:02:39+00:00</updated>
    <author>
      <name>/u/ieatrox</name>
      <uri>https://old.reddit.com/user/ieatrox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt; &lt;img alt="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="Bartowski just updated his glm-4-32B quants. working in lmstudio soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ieatrox"&gt; /u/ieatrox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6ably/bartowski_just_updated_his_glm432b_quants_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-23T21:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6nuo3</id>
    <title>GLM-4-32B Missile Command</title>
    <updated>2025-04-24T09:19:05+00:00</updated>
    <author>
      <name>/u/Jarlsvanoid</name>
      <uri>https://old.reddit.com/user/Jarlsvanoid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intenté decirle a GLM-4-32B que creara un par de juegos para mí, Missile Command y un juego de Dungeons.&lt;br /&gt; No funciona muy bien con los cuantos de Bartowski, pero sí con los de Matteogeniaccio; No sé si hace alguna diferencia.&lt;/p&gt; &lt;p&gt;EDIT: Using openwebui with ollama 0.6.6 ctx length 8192.&lt;/p&gt; &lt;p&gt;- GLM-4-32B-0414-F16-Q6_K.gguf Matteogeniaccio&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/dkaL7vh3/"&gt; https://jsfiddle.net/dkaL7vh3/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/mc57rf8o/"&gt;https://jsfiddle.net/mc57rf8o/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- GLM-4-32B-0414-F16-Q4_KM.gguf Matteogeniaccio (very good!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/wv9dmhbr/"&gt;https://jsfiddle.net/wv9dmhbr/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Bartowski Q6_K&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/5r1hztyx/"&gt; https://jsfiddle.net/5r1hztyx/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/1bf7jpc5/"&gt;https://jsfiddle.net/1bf7jpc5/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/x7932dtj/"&gt;https://jsfiddle.net/x7932dtj/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/5osg98ca/"&gt;https://jsfiddle.net/5osg98ca/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Con varias pruebas, siempre con una sola instrucción (Hazme un juego de comandos de misiles usando html, css y javascript), el quant de Matteogeniaccio siempre acierta.&lt;/p&gt; &lt;p&gt;- Maziacs style game - GLM-4-32B-0414-F16-Q6_K.gguf Matteogeniaccio:&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/894huomn/"&gt;https://jsfiddle.net/894huomn/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Another example with this quant and a ver simiple prompt: ahora hazme un juego tipo Maziacs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://jsfiddle.net/0o96krej/"&gt;https://jsfiddle.net/0o96krej/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jarlsvanoid"&gt; /u/Jarlsvanoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nuo3/glm432b_missile_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6x0qm</id>
    <title>What is the hardest math your AI can do?</title>
    <updated>2025-04-24T16:46:23+00:00</updated>
    <author>
      <name>/u/OrthogonalToHumanity</name>
      <uri>https://old.reddit.com/user/OrthogonalToHumanity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build an AI for doing math problems only using my local setup.I'm curious to know what results other people have gotten. I've looked online and it seems that the most recent news for a corporate setup was Google solving some geometry problems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrthogonalToHumanity"&gt; /u/OrthogonalToHumanity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6x0qm/what_is_the_hardest_math_your_ai_can_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T16:46:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6n9t6</id>
    <title>o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &gt; premium at this point!ℹ️</title>
    <updated>2025-04-24T08:35:52+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"&gt; &lt;img alt="o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &amp;gt; premium at this point!ℹ️" src="https://b.thumbs.redditmedia.com/bYXYRidx__uLyc78DDX0tlXyl_xsI1tDJDdUiEL4TDA.jpg" title="o4-mini ranks less than DeepSeek V3 | o3 ranks inferior to Gemini 2.5 | freemium &amp;gt; premium at this point!ℹ️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k6n9t6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6n9t6/o4mini_ranks_less_than_deepseek_v3_o3_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T08:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xczy</id>
    <title>Deepcogito Cogito v1 preview 14B Quantized Benchmark</title>
    <updated>2025-04-24T17:00:04+00:00</updated>
    <author>
      <name>/u/fakezeta</name>
      <uri>https://old.reddit.com/user/fakezeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm GPU poor (3060TI with 8GB VRAM) and started using the 14B Deepcogito model based on Qwen 2.5 after seeing their post.&lt;/p&gt; &lt;p&gt;Best Quantization I can use with a decent speed is Q5K_S with a a generation speed varying from 5-10tk/s depending on the context.&lt;/p&gt; &lt;p&gt;From daily usage it seems great: great at instruction following, good text understanding, very good in multi language, not SOTA at coding but it is not my primary use case.&lt;/p&gt; &lt;p&gt;So I wanted to assess how the quant affected the performance and run a subset (9 hour of test) of MMLU-PRO (20%) to have an idea:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO (no reasoning)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;overall&lt;/th&gt; &lt;th align="left"&gt;biology&lt;/th&gt; &lt;th align="left"&gt;business&lt;/th&gt; &lt;th align="left"&gt;chemistry&lt;/th&gt; &lt;th align="left"&gt;computer science&lt;/th&gt; &lt;th align="left"&gt;economics&lt;/th&gt; &lt;th align="left"&gt;engineering&lt;/th&gt; &lt;th align="left"&gt;health&lt;/th&gt; &lt;th align="left"&gt;history&lt;/th&gt; &lt;th align="left"&gt;law&lt;/th&gt; &lt;th align="left"&gt;math&lt;/th&gt; &lt;th align="left"&gt;philosophy&lt;/th&gt; &lt;th align="left"&gt;physics&lt;/th&gt; &lt;th align="left"&gt;psychology&lt;/th&gt; &lt;th align="left"&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;69.32&lt;/td&gt; &lt;td align="left"&gt;81.12&lt;/td&gt; &lt;td align="left"&gt;71.97&lt;/td&gt; &lt;td align="left"&gt;68.14&lt;/td&gt; &lt;td align="left"&gt;74.39&lt;/td&gt; &lt;td align="left"&gt;82.14&lt;/td&gt; &lt;td align="left"&gt;56.48&lt;/td&gt; &lt;td align="left"&gt;71.17&lt;/td&gt; &lt;td align="left"&gt;67.11&lt;/td&gt; &lt;td align="left"&gt;54.09&lt;/td&gt; &lt;td align="left"&gt;78.89&lt;/td&gt; &lt;td align="left"&gt;69.70&lt;/td&gt; &lt;td align="left"&gt;62.16&lt;/td&gt; &lt;td align="left"&gt;79.87&lt;/td&gt; &lt;td align="left"&gt;63.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;An overall of 69.32 is in line with the 70.91 claimed in Deepcogito blog post.&lt;/p&gt; &lt;p&gt;Then I wanted to check the difference between Reasoning and No Reasoning and I choose GPQA diamond for this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPQA no reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.41919191919191917 Refusal fraction: 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;GPQA reasoning&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Accuracy: 0.54 Refusal fraction: 0,020202020202 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The refusal fraction where due to thinking process entering in a loop generating the same sentence over and over again.&lt;/p&gt; &lt;p&gt;This are incredible results considering that according to &lt;a href="https://epoch.ai/data/ai-benchmarking-dashboard"&gt;https://epoch.ai/data/ai-benchmarking-dashboard&lt;/a&gt; and to &lt;a href="https://qwenlm.github.io/blog/qwen2.5-llm/"&gt;https://qwenlm.github.io/blog/qwen2.5-llm/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-14B ==&amp;gt; 0.447 Qwen 2.5 14B ==&amp;gt; 0.328 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both at full precision.&lt;/p&gt; &lt;p&gt;These are numbers in par with a couple of higher class LLMs and also the Reasoning mode is quite usable and usually not generating a lot of tokens for thinking.&lt;/p&gt; &lt;p&gt;I definitely recommend this model in favour of Gemma3 or Mistral Small for us GPU poors and I would really love to see how the 32B version perform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakezeta"&gt; /u/fakezeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xczy/deepcogito_cogito_v1_preview_14b_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6pplv</id>
    <title>GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length</title>
    <updated>2025-04-24T11:20:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt; &lt;img alt="GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length" src="https://external-preview.redd.it/3NYpVgamx1NXpydfb32BxQDBSawDgIlUbaanFyS12QE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e09f35ea9f5809bb0108aaeb81cfcd9b214c0a72" title="GLM-4-32B Q5_K_S can fit in 24GB cards with decent context length" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;30K context, Q8 KV Cache, all layers in GPU, no offload, ollama 0.6.6&lt;/p&gt; &lt;p&gt;The &amp;quot;context efficiency&amp;quot; of this model is significantly better than that of Qwen2.5-32B. I can only get 8k context for Qwen when using the 32B-Q5_K_S gguf.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ix21gs9fnrwe1.png?width=1423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=223f520b5bca53f0c5a171c1fbc03739ace47877"&gt;https://preview.redd.it/ix21gs9fnrwe1.png?width=1423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=223f520b5bca53f0c5a171c1fbc03739ace47877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/blob/main/THUDM_GLM-4-32B-0414-Q5_K_S.gguf"&gt;https://huggingface.co/bartowski/THUDM_GLM-4-32B-0414-GGUF/blob/main/THUDM_GLM-4-32B-0414-Q5_K_S.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;set OLLAMA_FLASH_ATTENTION=1 &amp;amp;&amp;amp; set OLLAMA_KV_CACHE_TYPE=q8_0 &amp;amp;&amp;amp; ollama serve&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6pplv/glm432b_q5_k_s_can_fit_in_24gb_cards_with_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T11:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6je2v</id>
    <title>Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model</title>
    <updated>2025-04-24T04:16:54+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt; &lt;img alt="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" src="https://external-preview.redd.it/RTiZ46sO11nfXyNlVM8vyr9cqgUVM4y93u2zm8v-5Bg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df7083b743c70efd512caf939d946bd65171d252" title="Skywork-R1V2-38B - New SOTA open-source multimodal reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Skywork/Skywork-R1V2-38B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6je2v/skyworkr1v238b_new_sota_opensource_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T04:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xiy1</id>
    <title>RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x</title>
    <updated>2025-04-24T17:06:34+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt; &lt;img alt="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" src="https://external-preview.redd.it/eiCf8y8ncWiZV_kRkOqMb9U44-ptjGTnaw-ROU8qPKM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b1fc08d775d6fe0c3886c63639965284e34fe2" title="RTX 5090 LLM Benchmarks - outperforming the A100 by 2.6x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Our testing revealed that despite having less VRAM than both the A100 (80GB) and RTX 6000 Ada (48GB), the RTX 5090 with its 32GB of memory consistently delivered superior performance across all token lengths and batch sizes.&lt;/p&gt; &lt;p&gt;To put the pricing in perspective, the 5090 costs $0.89/hr in Secure Cloud, compared to the $0.77/hr for the RTX 6000 Ada, and $1.64/hr for the A100. But aside from the standpoint of VRAM (the 5090 has the least, at 32GB) it handily outperforms both of them. If you are serving a model on an A100 though you could simply rent a 2x 5090 pod for about the same price and likely get double the token throughput - so for LLMs, at least, it appears there is a new sheriff in town.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.runpod.io/rtx-5090-llm-benchmarks-for-ai-is-it-the-best-gpu-for-ml/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xiy1/rtx_5090_llm_benchmarks_outperforming_the_a100_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6nrl1</id>
    <title>I benchmarked the Gemma 3 27b QAT models</title>
    <updated>2025-04-24T09:12:34+00:00</updated>
    <author>
      <name>/u/jaxchang</name>
      <uri>https://old.reddit.com/user/jaxchang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to know what models performed the best, and it seemed like nobody had actual numbers for this information... so I ran the numbers myself. &lt;/p&gt; &lt;p&gt;I am running on llama.cpp v1.27.1 for the GGUFs, and LM Studio MLX v0.13.2 for the MLX model. &lt;/p&gt; &lt;p&gt;At first, I tried calculating perplexity. However, the PPL numbers kept on yielding really weird values from the PTB/wiki.test.raw corpus. The QAT models would generate numbers higher than the original BF16, and Bartowski's quant scored higher than the original QAT from google. I think the model is overfitting there, so it's not really a good metric. &lt;/p&gt; &lt;p&gt;So I decided to just use GPQA-main instead. It's more a more biased benchmark in terms of topic, but I suspect that actually doesn't matter too much. We're comparing different quants of the same model, not different finetunes/models. In the latter case, we might expect different finetunes/models to maybe perform better at say math but worse at coding/writing, have more biology questions in the training data set vs physics, or other biased performance skew etc. However, quantization is not so fine-grained; it simply truncates the lowest value bits for each parameter, so quality reduction/noise introduced should be more generalizable. &lt;/p&gt; &lt;p&gt;Here are the GPQA-main scores for the quants I tested: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model name&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;mlx-community/gemma-3-27b-it-qat-4bit&lt;/td&gt; &lt;td&gt;0.333&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/td&gt; &lt;td&gt;0.346&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bartowski/google_gemma-3-27b-it-qat-GGUF (Q4_0)&lt;/td&gt; &lt;td&gt;0.352&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;unsloth/gemma-3-27b-it (via Openrouter api Chutes)&lt;/td&gt; &lt;td&gt;0.371&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Unquantized Gemma 3 27b (via Huggingface api)&lt;/td&gt; &lt;td&gt;0.375&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note that it takes 2-3 hours to run this benchmark per model for me, so it's not exactly a quick test.&lt;/p&gt; &lt;p&gt;Seems like the &lt;strong&gt;Bartowski QAT Q4_0 is the probably the best choice&lt;/strong&gt; if you want to run Gemma 3 QAT locally. It also seems to be 1-2tok/sec faster than the MLX model for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaxchang"&gt; /u/jaxchang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6nrl1/i_benchmarked_the_gemma_3_27b_qat_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T09:12:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6xqt2</id>
    <title>Summaries of the creative writing quality of Llama 4 Maverick, DeepSeek R1, DeepSeek V3-0324, Qwen QwQ, Gemma 3, and Microsoft Phi-4, based on 18,000 grades and comments for each</title>
    <updated>2025-04-24T17:15:17+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xqt2/summaries_of_the_creative_writing_quality_of/"&gt; &lt;img alt="Summaries of the creative writing quality of Llama 4 Maverick, DeepSeek R1, DeepSeek V3-0324, Qwen QwQ, Gemma 3, and Microsoft Phi-4, based on 18,000 grades and comments for each" src="https://external-preview.redd.it/PbWQtQP3VCVQRKmZsUvVD5jujXfoLHDMnY-JSPdr4pQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84b60f56eb912080e78c3c7baee89d36331d02f3" title="Summaries of the creative writing quality of Llama 4 Maverick, DeepSeek R1, DeepSeek V3-0324, Qwen QwQ, Gemma 3, and Microsoft Phi-4, based on 18,000 grades and comments for each" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From &lt;a href="https://github.com/lechmazur/writing/"&gt;LLM Creative Story-Writing Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/15xrva4cktwe1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a79ee58cbaa18c43daff4b527fbe56799220836"&gt;https://preview.redd.it/15xrva4cktwe1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a79ee58cbaa18c43daff4b527fbe56799220836&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Llama 4 Maverick (score: 6.35)&lt;/h1&gt; &lt;h1&gt;1. Overall Evaluation of Llama 4 Maverick’s Performance&lt;/h1&gt; &lt;p&gt;Across six writing tasks, &lt;strong&gt;Llama 4 Maverick&lt;/strong&gt; demonstrates notable technical competence and surface-level creativity, but is consistently undermined by deeply rooted narrative and stylistic shortcomings. Its primary strength lies in the generation of visually imaginative settings, consistent tonal control, and the ability to weave together prompt-required elements in a superficially coherent manner. The model’s output exhibits punctual use of metaphor, frequent poetic flourishes, and occasional sparks of inventive imagery or motif.&lt;/p&gt; &lt;p&gt;However, major weaknesses are pervasive and damaging to literary quality:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lack of Depth and Specificity&lt;/strong&gt;: Characters remain archetypal and undeveloped, their motivations and transformations told rather than convincingly dramatized. Emotional journeys are declared through summary, not built through scenes, and little psychological consistency or growth is observed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plot Inertia and Mechanical Structure&lt;/strong&gt;: Story events are stitched together by logic of prompt rather than by organic causality. Obstacles and conflicts are minimal or generic, with resolutions often feeling rushed, forced, or unearned. Narrative arcs follow predictable templates, rarely subverting expectations or delivering genuine surprise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Surface-Level Worldbuilding&lt;/strong&gt;: While settings are visually rich, they are typically props for the premise rather than engines driving character or plot. Multisensory immersion is rare, as is any sense that the world’s internal logic matters or is shaped by the story’s events.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stylistic Overwriting and Abstraction&lt;/strong&gt;: Maverick persistently confuses abstraction and ornament with depth, resorting to purple prose, heavy-handed metaphors, and platitudinous conclusions that substitute for earned emotional payoff. The prose is technically “writerly” but often rings hollow or generic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Artificial Integration of Required Elements&lt;/strong&gt;: Especially under tight word constraints, the model treats prompts as checklists, inserting tokens in ways that serve requirement rather than narrative necessity, hampering organic storytelling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deficiency in Conflict and Stakes&lt;/strong&gt;: Internal and external stakes are routine, vague, or absent. Rarely do characters face difficult choices or credible adversity; narrative change is asserted rather than constructed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Summary Judgment:&lt;/strong&gt; Llama 4 Maverick produces fiction that is competent on the surface but hollow at its core. Its inability to dramatize, to risk specificity, and to unite character, plot, and setting into mutually reinforcing engines makes its stories read as exercises or atmospheric sketches rather than lived, memorable fiction. The work is rarely alive to surprise, ambiguity, or narrative rigor. For all the creative window-dressing, the essential machinery of dynamic storytelling remains missing.&lt;/p&gt; &lt;h1&gt;DeepSeek R1 (score: 8.34)&lt;/h1&gt; &lt;h1&gt;1. Overall Evaluation: Strengths &amp;amp; Weaknesses&lt;/h1&gt; &lt;p&gt;DeepSeek R1 displays impressive literary competence, marked by vivid sensory detail, structural discipline, inventive world-building, and the ability to maintain cohesive, compressed narratives under tight constraints. The model excels at integrating mandated story elements, presenting clear arcs (even in microfiction), and weaving metaphor and symbolism into its prose. Voice consistency and originality—particularly in metaphor and conceptual blend—set this model apart from more formulaic LLMs.&lt;/p&gt; &lt;p&gt;However, these technical strengths often become excesses. The model leans on dense, ornate language—metaphor and symbolism risk crossing from evocative to overwrought, diluting clarity and narrative propulsion. While the settings and imagery are frequently lush and inventive, genuine psychological depth, character messiness, and narrative surprise are lacking. Too often, characters are archetypes or vessels for theme, their transformation either rushed, asserted, or falling back on familiar genre beats. Emotional and philosophical ambit sometimes outpace narrative payoff, with endings that can be abrupt, ambiguous, or more poetic than satisfying.&lt;/p&gt; &lt;p&gt;Dialogue and supporting roles are underdeveloped; side characters tend to serve plot mechanics rather than organic interaction or voice. Thematic resonance is attempted through weighty abstraction, but the most successful stories ground meaning in concrete stakes and lived, embodied consequence.&lt;/p&gt; &lt;p&gt;In sum: &lt;strong&gt;DeepSeek R1 is an accomplished stylist and structuralist, whose inventiveness and control over microfiction is clear—but who too often mistakes linguistic flourish for authentic storytelling. The next leap demands a willingness to risk imperfection: less reliance on prescribed metaphor, more unpredictable humanity; less narrative convenience, more earned, organic transformation.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;DeepSeek V3-0324 (score: 7.78)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Overall Evaluation: DeepSeek V3-0324 Across Tasks (Q1–Q6)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek V3-0324 demonstrates solid baseline competence at literary microtasks, showing consistent strengths in structural clarity, evocative atmospheric detail, and the integration of symbolic motifs. Across genres and prompt constraints, the model reliably produces stories with clear beginnings, middles, and ends, knitting together assigned elements or tropes with mechanical efficiency. Its ability to conjure immersive settings, particularly via sensory language and metaphor, stands out as a persistent strength—descriptions are often vivid, with imaginative worldbuilding and a penchant for liminal or symbolic locales.&lt;/p&gt; &lt;p&gt;Narrative cohesion and deliberate brevity are frequently praised, as is the avoidance of egregious AI “tells” like incoherent plot jumps. Occasionally, the model manifests moments of genuine resonance, threading physical object or environment seamlessly with character emotion and theme.&lt;/p&gt; &lt;p&gt;However, an equally persistent set of weaknesses undermines the literary impact. Emotional arcs and character transformations are generally formulaic, proceeding along predictable lines with tidy, unearned resolutions and minimal risk or friction. The model frequently tells rather than shows, especially around epiphanies, conflict, and internal change, leading to an abundance of abstract or expository statements that crowd out subtext and psychological depth.&lt;/p&gt; &lt;p&gt;Symbolic motifs and metaphors, while initially striking, become a crutch—either forced or repetitive, with over-explained significance that erodes nuance. Dialogue is typically utilitarian and rarely idiosyncratic or memorable. Too often, assigned story elements or required objects feel artificially inserted rather than organically essential; the constraint is managed, not transcended. Stories default to atmospheric set-dressing or ornate prose, but this sometimes veers into purple or generic territory, with style overtaking clear narrative stakes or authentic emotion.&lt;/p&gt; &lt;p&gt;In sum: DeepSeek V3-0324 is a capable literary generalist. It excels at prompt satisfaction, atmospheric writing, and surface cohesion, but lacks the risk, subversiveness, and organic emotional complexity that elevates microfiction from competent to truly memorable. Its work is reliably “complete” and sometimes striking, but too rarely lingers, surprises, or fully earns its insight.&lt;/p&gt; &lt;h1&gt;Qwen QwQ-32B 16K (score: 8.07)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Overall Evaluation of Qwen QwQ-32B 16K Across Six Writing Tasks (Q1–Q6):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen QwQ-32B 16K demonstrates a notable level of consistency and technical proficiency across varied fiction writing tasks. The model excels at basic storytelling competence: it unfailingly provides clear character motivations, structured plot arcs, vivid sensory details, and cohesively integrates prompts and assigned elements—even under tight word constraints. Its command of atmospheric language and symbolic imagery stands out, frequently producing lush, poetic passages and stories that leave readers with a sense of lingering resonance or philosophical closure.&lt;/p&gt; &lt;p&gt;However, this technical fluency often comes at the cost of emotional immediacy, originality, and genuine literary risk. The model habitually “checks the boxes” for motivation, transformation, and theme, but the results feel mechanically competent rather than lived or surprising. Emotional arcs and character changes are typically announced or summarized, rather than dramatized; backstories and stakes are routinely present but rarely idiosyncratic, and dialogue is functional more than distinctive. Settings are immersive, but can veer into genre-derived tropes, serving as skilled pastiche rather than authentic worlds.&lt;/p&gt; &lt;p&gt;The thematic ambition is evident: stories regularly grapple with memory, loss, tradition, identity, and transformation. Yet, the model’s penchant for abstraction, symbolism, and tightly-woven theme sometimes yields opacity, didacticism, or a lack of visceral impact. Endings are often neat, poetic, and “lingering,” but seldom unsettle or cathartically satisfy—the narrative risk and narrative messiness of great fiction are largely absent.&lt;/p&gt; &lt;p&gt;In summary, Qwen QwQ-32B 16K is a master of the “artificially artful”—technically even-handed, symbolically rich, and atmospherically adept. Still, it often feels like a virtuoso performer of literary scales, not an improviser: it rarely surprises, bruises, or stuns, instead delivering careful, competent fiction that evokes admiration, not awe. Its greatest barrier to true literary excellence lies in its relentless safety, formula adherence, and preference for tidy thought over authentic emotional rupture.&lt;/p&gt; &lt;h1&gt;Gemma 3 27B (score: 8.04)&lt;/h1&gt; &lt;h1&gt;1. Concise Overall Evaluation of Gemma 3 27B across Q1–Q6&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Gemma 3 27B demonstrates a high level of literary craft, especially in its ability to generate structurally coherent, thematically cohesive, and “literary” short fiction that integrates given elements with notable smoothness. Across all tasks, the model is praised for its clarity of purpose, consistent narrative arcs, and frequent use of symbolic detail, metaphor, and creative approaches to prompt requirements. When at its best, Gemma can weave disparate elements (e.g., objects, timeframes, attributes) into organic, resonant stories boasting subtle thematic undertones and emotionally satisfying, if understated, resolutions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;However, this proficiency often reveals its algorithmic seams. Recurring weaknesses include a tendency toward surface-level characterization (“traits are labeled, not lived”), conflict and transformation that are told rather than shown, and resolutions that too frequently feel rushed or unearned. The model’s prose, though often polished and poetic, lapses into familiar metaphors, abstract statements, and sometimes over-orchestrated language that prioritizes form over substance. While Gemma reliably achieves “closure” and thematic neatness, it seldom generates the surprise, risk, or psychological messiness that marks unforgettable fiction.&lt;/p&gt; &lt;p&gt;Supporting characters are consistently underdeveloped, serving mainly as devices for protagonist growth or plot necessity. The settings can be vivid and atmospherically charged, but their integration into plot and character motivation sometimes feels decorative or forced. Even when stories are imaginative in premise, originality is often undercut by formulaic structures and familiar emotional arcs.&lt;/p&gt; &lt;p&gt;In sum, Gemma 3 27B is a skilled generator of high-level, publishable vignettes and literary exercises. Its work is rarely bad or generic, usually polished and thoughtful, yet it remains “safe,” tending to echo predictable literary conventions and avoiding the narrative risks required for true artistic distinction. The stories are compellingly crafted, but rarely haunting, urgent, or genuinely novel in either theme or execution.&lt;/p&gt; &lt;h1&gt;Microsoft Phi-4 (score: 6.40)&lt;/h1&gt; &lt;h1&gt;1. Concise Overall Evaluation (≈200–300 words)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Microsoft Phi-4 demonstrates technical competence and mechanical reliability in short literary tasks, but its writing falls short of true artistry or emotional resonance.&lt;/strong&gt; Across all prompts, the model consistently produces stories that are well-structured, grammatically correct, and attentive to required elements. It is particularly adept at thematic framing, deploying symbolic objects or motifs, and establishing a mood or atmosphere.&lt;/p&gt; &lt;p&gt;However, the model’s fundamental weaknesses consistently undermine these strengths. Chief among these is an overwhelming reliance on generalization and abstraction: characters’ traits, motivations, and transformations are &lt;em&gt;told&lt;/em&gt; rather than &lt;em&gt;shown&lt;/em&gt;, typically through summary statements and platitudes rather than dramatized action or dialogue. Settings, while superficially imaginative, serve mostly as decorative backdrops that rarely influence character behavior or narrative progression in meaningful ways. Conflict, stakes, and genuine change are muted or glossed over—resolutions arrive conveniently, emotional shifts happen by narrative fiat, and obstacles either lack bite or are philosophical rather than situational.&lt;/p&gt; &lt;p&gt;Stylistically, Phi-4’s stories frequently deploy “poetic” or ornate language, but this often functions as window-dressing, masking thin plotting and a deficit of concrete detail. The prose quickly becomes repetitive, abstract, and formulaic, betraying the underlying algorithm. Characters lack idiosyncratic voice; their emotional journeys feel preordained and safe, with little evidence of narrative risk, surprise, or messy humanity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In sum, Phi-4’s stories embody competent structure and surface-level creativity, but suffer from hollowness, generic abstraction, and a formulaic, “checkbox” approach to storytelling.&lt;/strong&gt; Until the model can imbue narrative with specific, lived detail and organic dramatic movement, it will remain on the threshold of literary credibility—able to simulate fiction, but rarely to &lt;em&gt;move&lt;/em&gt; the reader.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: forgot to add Mistral Large 2&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Mistral Large 2 (score: 7.00)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Overall Evaluation of Mistral Large 2 Across Writing Tasks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Across diverse fiction-writing tasks (Q1–Q6), Mistral Large 2 demonstrates reliable competence in basic narrative construction, technical fluency, and prompt adherence, but consistently falls short of literary distinction. Its primary strengths are surface-level: it &lt;strong&gt;generally produces plausible stories with clear beginnings, middles, and ends&lt;/strong&gt;, evident thematic consistency, occasional imaginative premises, and a sound grasp of symbolic and metaphorical tools. The model’s prose is seldom confusing and, at times, achieves flashes of inventiveness—particularly in setting details or conceptual hooks.&lt;/p&gt; &lt;p&gt;However, critical weaknesses are both broad and deep. &lt;strong&gt;Characterization is formulaic and emotionally shallow:&lt;/strong&gt; personalities are quickly sketched via explicit traits that rarely manifest in action or authentic internal conflict. Most emotional arcs are &amp;quot;told, not shown,&amp;quot; and character transformations feel abrupt, mechanical, or unearned. Plots tend toward predictable, linear resolutions, often depending on coincidence or deus ex machina instead of organic causality or real struggle. Symbolism, metaphor, and literary devices are attempted eagerly, but with a heavy hand and little subtlety, resulting in a didactic or programmatic feel that stifles ambiguity and complexity.&lt;/p&gt; &lt;p&gt;World-building, while occasionally vivid, leans heavily on genre tropes and generic backdrops. Sensory description defaults to visual cues, leaving whole sensory registers underutilized. Voices lack distinction, and style shifts between flat clarity and over-ornate “purple prose,” seldom displaying a genuinely original or memorable touch.&lt;/p&gt; &lt;p&gt;Structurally, the model’s brief stories almost always “check the boxes” of prompt elements, but this constraint reveals a larger issue: stories read as artificial exercises rather than organic artistic expression. In emotional, psychological, and thematic domains, the writing is cautious, opting for safe and familiar transformations over risk, ambiguity, or the messiness of truly lived experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt;: Mistral Large 2 excels at reliably producing well-structured, linguistically competent, and thematically “on task” fiction. But across all six tasks, it remains stuck at the level of technically acceptable imitation—failing to surprise, move, or haunt the reader. Ambition is evident, but mastery of “show, don’t tell,” organic integration, narrative risk, and authentic emotional impact is still mostly out of reach.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hjgvbnbdktwe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e13c5001d0cd295fbf8ba4ce4a86c627f350c66"&gt;https://preview.redd.it/hjgvbnbdktwe1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e13c5001d0cd295fbf8ba4ce4a86c627f350c66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xqt2/summaries_of_the_creative_writing_quality_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xqt2/summaries_of_the_creative_writing_quality_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6xqt2/summaries_of_the_creative_writing_quality_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T17:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k6mols</id>
    <title>Details on OpenAI's upcoming 'open' AI model</title>
    <updated>2025-04-24T07:53:04+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"&gt; &lt;img alt="Details on OpenAI's upcoming 'open' AI model" src="https://external-preview.redd.it/jBLPKrE-sNiDaxe0zsX1DO2Ghuda8KNpR6LvSh4IYoc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=462c8e493352f840f1bcce92fb0555e2b79db252" title="Details on OpenAI's upcoming 'open' AI model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- In very early stages, targeting an early summer launch&lt;/p&gt; &lt;p&gt;- Will be a reasoning model, aiming to be the top open reasoning model when it launches&lt;/p&gt; &lt;p&gt;- Exploring a highly permissive license, perhaps unlike Llama and Gemma&lt;/p&gt; &lt;p&gt;- Text in text out, reasoning can be tuned on and off&lt;/p&gt; &lt;p&gt;- Runs on &amp;quot;high-end consumer hardware&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/04/23/openai-seeks-to-make-its-upcoming-open-ai-model-best-in-class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k6mols/details_on_openais_upcoming_open_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-24T07:53:04+00:00</published>
  </entry>
</feed>
