<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-21T16:51:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lgcbyh</id>
    <title>Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound.</title>
    <updated>2025-06-20T19:09:42+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt; &lt;img alt="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." src="https://a.thumbs.redditmedia.com/smxOsICItFcgpgZ6jwpSvygFZFitUy4PBiwrObgw-D4.jpg" title="Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys. I'm reposting as the old post got removed by some reason.&lt;/p&gt; &lt;p&gt;Now it is time to compare LLMs, where these GPUs shine the most.&lt;/p&gt; &lt;p&gt;hardware-software config:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM DDR5 6000Mhz CL30&lt;/li&gt; &lt;li&gt;MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Fedora 41 (Linux), Kernel 6.19&lt;/li&gt; &lt;li&gt;Torch 2.7.1+cu128&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each card was tuned to try to get the highest clock possible, highest VRAM bandwidth and less power consumption.&lt;/p&gt; &lt;p&gt;The benchmark was run on ikllamacpp, as&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-sweep-bench -m '/GUFs/gemma-3-27b-it-Q4_K_M.gguf' -ngl 999 -c 8192 -fa -ub 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The tuning was made on each card, and none was power limited (basically all with the slider maxed for PL)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 5090: &lt;ul&gt; &lt;li&gt;Max clock: 3010 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 1000&lt;/li&gt; &lt;li&gt;Basically an undervolt plus overclock near the 0.9V point (Linux doesn't let you see voltages)&lt;/li&gt; &lt;li&gt;VRAM overclock: +3000Mhz (34 Gbps effective, so about 2.1 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 4090: &lt;ul&gt; &lt;li&gt;Max clock: 2865 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an undervolt+OC about the 0.91V point.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1650Mhz (22.65 Gbps effective, so about 1.15 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX 3090: &lt;ul&gt; &lt;li&gt;Max clock: 1905 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 180&lt;/li&gt; &lt;li&gt;This is confirmed, from windows, an UV + OC of 1905Mhz at 0.9V.&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (so about 1.08 TB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;RTX A6000: &lt;ul&gt; &lt;li&gt;Max clock: 1740 Mhz&lt;/li&gt; &lt;li&gt;Clock offset: 150&lt;/li&gt; &lt;li&gt;This is an UV + OC of about 0.8V&lt;/li&gt; &lt;li&gt;VRAM Overclock: +1000Mhz (about 870 GB/s bandwidth)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For reference: PP (pre processing) is mostly compute bound, and TG (text generation) is bandwidth bound.&lt;/p&gt; &lt;p&gt;I have posted the raw performance metrics on pastebin, as it is a bit hard to make it readable here on reddit, on &lt;a href="https://pastebin.com/g3vjU6jY"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance Summary (N_KV = 0)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;TG Speed (t/s)&lt;/th&gt; &lt;th align="left"&gt;Power (W)&lt;/th&gt; &lt;th align="left"&gt;PP t/s/W&lt;/th&gt; &lt;th align="left"&gt;TG t/s/W&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;4,641.54&lt;/td&gt; &lt;td align="left"&gt;76.78&lt;/td&gt; &lt;td align="left"&gt;425&lt;/td&gt; &lt;td align="left"&gt;10.92&lt;/td&gt; &lt;td align="left"&gt;0.181&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;3,625.95&lt;/td&gt; &lt;td align="left"&gt;54.38&lt;/td&gt; &lt;td align="left"&gt;375&lt;/td&gt; &lt;td align="left"&gt;9.67&lt;/td&gt; &lt;td align="left"&gt;0.145&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1,538.49&lt;/td&gt; &lt;td align="left"&gt;44.78&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;td align="left"&gt;4.27&lt;/td&gt; &lt;td align="left"&gt;0.124&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1,578.69&lt;/td&gt; &lt;td align="left"&gt;38.60&lt;/td&gt; &lt;td align="left"&gt;280&lt;/td&gt; &lt;td align="left"&gt;5.64&lt;/td&gt; &lt;td align="left"&gt;0.138&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Relative Performance (vs RTX 3090 baseline)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Speed&lt;/th&gt; &lt;th align="left"&gt;TG Speed&lt;/th&gt; &lt;th align="left"&gt;PP Efficiency&lt;/th&gt; &lt;th align="left"&gt;TG Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;3.02x&lt;/td&gt; &lt;td align="left"&gt;1.71x&lt;/td&gt; &lt;td align="left"&gt;2.56x&lt;/td&gt; &lt;td align="left"&gt;1.46x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;2.36x&lt;/td&gt; &lt;td align="left"&gt;1.21x&lt;/td&gt; &lt;td align="left"&gt;2.26x&lt;/td&gt; &lt;td align="left"&gt;1.17x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;td align="left"&gt;1.00x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;1.03x&lt;/td&gt; &lt;td align="left"&gt;0.86x&lt;/td&gt; &lt;td align="left"&gt;1.32x&lt;/td&gt; &lt;td align="left"&gt;1.11x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Degradation with Context (N_KV)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;PP Drop (0→6144)&lt;/th&gt; &lt;th align="left"&gt;TG Drop (0→6144)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 5090&lt;/td&gt; &lt;td align="left"&gt;-15.7%&lt;/td&gt; &lt;td align="left"&gt;-13.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 4090&lt;/td&gt; &lt;td align="left"&gt;-16.3%&lt;/td&gt; &lt;td align="left"&gt;-14.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3090&lt;/td&gt; &lt;td align="left"&gt;-12.7%&lt;/td&gt; &lt;td align="left"&gt;-14.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;-14.1%&lt;/td&gt; &lt;td align="left"&gt;-14.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And some images!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a"&gt;https://preview.redd.it/0immnis9s48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=589766f32331a2f5eaa43f0612bcde80352e432a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949"&gt;https://preview.redd.it/nzrpmf7as48f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08fa432df4dbb6f5358a8a3eb3e11e71014c1949&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6"&gt;https://preview.redd.it/t1qpg2kny48f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad8e1a5d0ffa75069f85b52e003f01e57df1b0d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T19:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg80cq</id>
    <title>New Mistral Small 3.2</title>
    <updated>2025-06-20T16:14:11+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open weights: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/MistralAI/status/1936093325116781016/photo/1"&gt;https://x.com/MistralAI/status/1936093325116781016/photo/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgv0y9</id>
    <title>Help me build a good TTS + LLM + STT stack</title>
    <updated>2025-06-21T12:07:25+00:00</updated>
    <author>
      <name>/u/sync_co</name>
      <uri>https://old.reddit.com/user/sync_co</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I am currently in the lookout for a good conversational AI system I can run. I want to use it conversational AI and be able to handle some complex prompts. Essentially I would like to try and build a alternative to retell or VAPI voice AI systems but using some of the newer voice systems &amp;amp; in my own cloud for privacy.&lt;/p&gt; &lt;p&gt;Can anyone help me with directions on how best to implement this?&lt;/p&gt; &lt;p&gt;So far I have tried -&lt;br /&gt; LiveKit for the telephony&lt;br /&gt; Cerebras for the LLM&lt;br /&gt; Orpheus for the STT&lt;br /&gt; Whisper as the TTS (tried Whisperx, Faster-Whisper, v3 on baseten. All batshit slow)&lt;br /&gt; Deepgram (very fast but not very accurate)&lt;br /&gt; Existing voice to voice models (ultravox etc. not attached to any smart LLM)&lt;/p&gt; &lt;p&gt;I would ideally like to have a response of full voice to voice to be under 600ms. I think this is possible because Orpheus TTFB is quite fast (sub 150ms) and the cerebras LLMs are also very high throughput but getting around 300ms TTFB (could also have network latency) but using whisper is very slow. Deepgram still has alot of transcription errors&lt;/p&gt; &lt;p&gt;Can anyone recommend a stack and a system that can work sub 600ms voice to voice? Details including hosting options would be ideal.&lt;/p&gt; &lt;p&gt;my dream is seasame's platform but they have released a garbage open source 1b while their 8b shines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sync_co"&gt; /u/sync_co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgv0y9/help_me_build_a_good_tts_llm_stt_stack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgv0y9/help_me_build_a_good_tts_llm_stt_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgv0y9/help_me_build_a_good_tts_llm_stt_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T12:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgrxkc</id>
    <title>UAE to appoint their National AI system as ministers' council advisory member</title>
    <updated>2025-06-21T08:45:18+00:00</updated>
    <author>
      <name>/u/tabspaces</name>
      <uri>https://old.reddit.com/user/tabspaces</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabspaces"&gt; /u/tabspaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/posts/mohammedbinrashid_%D8%A7%D9%84%D8%A5%D8%AE%D9%88%D8%A9-%D9%88%D8%A7%D9%84%D8%A3%D8%AE%D9%88%D8%A7%D8%AA-%D8%A8%D8%B9%D8%AF-%D8%A7%D9%84%D8%AA%D8%B4%D8%A7%D9%88%D8%B1-%D9%85%D8%B9-%D8%A3%D8%AE%D9%8A-%D8%B1%D8%A6%D9%8A%D8%B3-activity-7341867717781614592-NH8k?utm_source=share&amp;amp;utm_medium=member_android&amp;amp;rcm=ACoAAA_qTHABhZU1hYm_lxYQw_ApFsOUKzigti8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrxkc/uae_to_appoint_their_national_ai_system_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgrxkc/uae_to_appoint_their_national_ai_system_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T08:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgzd58</id>
    <title>Build DeepSeek-R1-Distill-Qwen-7B from Scratch</title>
    <updated>2025-06-21T15:36:20+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a big fan of Sebastian Raschka's earlier work on LLMs from scratch. He recently switched from Llama to Qwen (a switch I recently made too thanks to someone in this subreddit) and wrote a Jupyter notebook implementing Qwen3 from scratch.&lt;/p&gt; &lt;p&gt;Highly recommend this resource as a learning project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgzd58/build_deepseekr1distillqwen7b_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgzd58/build_deepseekr1distillqwen7b_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T15:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0div</id>
    <title>Ollama alternatives</title>
    <updated>2025-06-21T16:20:32+00:00</updated>
    <author>
      <name>/u/Maleficent_Payment44</name>
      <uri>https://old.reddit.com/user/Maleficent_Payment44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Linux Ubuntu server with 192GB ram and a geoforce rtx 4090 GPU. I've been creating some python apps lately using ollama and langchain with models like gemma3:27b.&lt;/p&gt; &lt;p&gt;I know ollama and langchain are both not the most cutting edge tools. I am pretty good in programming and configuration so could probably move on to better options.&lt;/p&gt; &lt;p&gt;Interested in rag and data related projects using statistics and machine learning. Have built some pretty cool stuff with plotly, streamlit and duckdb.&lt;/p&gt; &lt;p&gt;Just started really getting hands on with local LLMs. For those that are further along and graduated from ollama etc. Do you have any suggestions on things that I should consider to maximize accuracy and speed. Either in terms of frameworks, models or LLM clients?&lt;/p&gt; &lt;p&gt;I plan to test qwen3 and llama4 models, but gemma3 is pretty decent. I would like to do more with models that aupport tooling, which gemma3 does not. I installed devstral for that reason.&lt;/p&gt; &lt;p&gt;Even though I mentioned a lot about models, my question is broader than that. I am more interested on others thoughts around ollama and langchain, which I know can be slow or bloated, but that is where I started, and not necessarily where I want to end up.&lt;/p&gt; &lt;p&gt;Thank you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent_Payment44"&gt; /u/Maleficent_Payment44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0div/ollama_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0div/ollama_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0div/ollama_alternatives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T16:20:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lg7vuc</id>
    <title>mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face</title>
    <updated>2025-06-20T16:09:13+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt; &lt;img alt="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" src="https://external-preview.redd.it/3DBqKqgOLKDMFbcrOD5Qa-3M1IIegLfhMX6TTbsgXeU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6eecbfa2b523b92f82faf94cb6ab334696d320" title="mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T16:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgs0d3</id>
    <title>RIGEL: An open-source hybrid AI assistant/framework</title>
    <updated>2025-06-21T08:50:46+00:00</updated>
    <author>
      <name>/u/__z3r0_0n3__</name>
      <uri>https://old.reddit.com/user/__z3r0_0n3__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"&gt; &lt;img alt="RIGEL: An open-source hybrid AI assistant/framework" src="https://external-preview.redd.it/YLvcO6GZN90fnKUxGABmFCgN1xACgLSeDvnM0Igr0UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c355b0db52a56986ec016efb4f52e3e845b3734a" title="RIGEL: An open-source hybrid AI assistant/framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Hey all,&lt;/h3&gt; &lt;p&gt;We're building an open-source project at Zerone Labs called RIGEL — a hybrid AI system that acts as both:&lt;/p&gt; &lt;p&gt;a multi-agent assistant, and&lt;/p&gt; &lt;p&gt;a modular control plane for tools and system-level operations.&lt;/p&gt; &lt;p&gt;It's not a typical desktop assistant — instead, it's designed to work as an AI backend for apps, services, or users who want more intelligent interfaces and automation.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-LLM support (local: Ollama / LLaMA.cpp, remote: Groq, etc.)&lt;/li&gt; &lt;li&gt;Tool-calling via a built-in MCP layer (run commands, access files, monitor systems)&lt;/li&gt; &lt;li&gt;D-Bus API integration (Linux) for embedding AI in other apps&lt;/li&gt; &lt;li&gt;Speech (Whisper STT, Piper TTS) optional but local&lt;/li&gt; &lt;li&gt;Memory and partial RAG support (ChromaDB)&lt;/li&gt; &lt;li&gt;Designed for local-first setups, but cloud-extensible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s currently in developer beta. Still rough in places, but usable and actively growing.&lt;/p&gt; &lt;p&gt;We’d appreciate feedback, issues, or thoughts — especially from people building their own agents, platform AIs, or AI-driven control systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__z3r0_0n3__"&gt; /u/__z3r0_0n3__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zerone-Laboratories/RIGEL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgs0d3/rigel_an_opensource_hybrid_ai_assistantframework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T08:50:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgkhdk</id>
    <title>A100 80GB can't serve 10 concurrent users - what am I doing wrong?</title>
    <updated>2025-06-21T01:18:55+00:00</updated>
    <author>
      <name>/u/Creative_Yoghurt25</name>
      <uri>https://old.reddit.com/user/Creative_Yoghurt25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Qwen2.5-14B-AWQ on A100 80GB for voice calls.&lt;/p&gt; &lt;p&gt;People say RTX 4090 serves 10+ users fine. My A100 with 80GB VRAM can't even handle 10 concurrent requests without terrible TTFT (30+ seconds).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current vLLM config:&lt;/strong&gt; &lt;code&gt;yaml --model Qwen/Qwen2.5-14B-Instruct-AWQ --quantization awq_marlin --gpu-memory-utilization 0.95 --max-model-len 12288 --max-num-batched-tokens 4096 --max-num-seqs 64 --enable-chunked-prefill --enable-prefix-caching --block-size 32 --preemption-mode recompute --enforce-eager &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Configs I've tried:&lt;/strong&gt; - &lt;code&gt;max-num-seqs&lt;/code&gt;: 4, 32, 64, 256, 1024 - &lt;code&gt;max-num-batched-tokens&lt;/code&gt;: 2048, 4096, 8192, 16384, 32768 - &lt;code&gt;gpu-memory-utilization&lt;/code&gt;: 0.7, 0.85, 0.9, 0.95 - &lt;code&gt;max-model-len&lt;/code&gt;: 2048 (too small), 4096, 8192, 12288 - Removed limits entirely - still terrible&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; Input is ~6K tokens (big system prompt + conversation history). Output is only ~100 tokens. User messages are small but system prompt is large.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GuideLLM benchmark results:&lt;/strong&gt; - 1 user: 36ms TTFT ✅&lt;br /&gt; - 25 req/s target: Only got 5.34 req/s actual, 30+ second TTFT - Throughput test: 3.4 req/s max, 17+ second TTFT - 10+ concurrent: 30+ second TTFT ❌&lt;/p&gt; &lt;p&gt;Also considering Triton but haven't tried yet.&lt;/p&gt; &lt;p&gt;Need to maintain &amp;lt;500ms TTFT for at least 30 concurrent users. What vLLM config should I use? Is 14B just too big for this workload?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative_Yoghurt25"&gt; /u/Creative_Yoghurt25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgkhdk/a100_80gb_cant_serve_10_concurrent_users_what_am/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T01:18:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lghrf9</id>
    <title>If your tools and parameters aren’t too complex, even Qwen1.5 0.5B can handle tool calling with a simple DSL and finetuning.</title>
    <updated>2025-06-20T23:04:41+00:00</updated>
    <author>
      <name>/u/umtksa</name>
      <uri>https://old.reddit.com/user/umtksa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Update: I tried Qwen3-0.6B and its better at converting natural language Turkish math problems to math formulas and handling complex sentences&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I designed a super minimal syntax like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;TOOL: param1, param2, param3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then fine-tuned Qwen 1.5 0.5B for just &lt;strong&gt;5 epochs&lt;/strong&gt;, and now it can reliably call &lt;strong&gt;all 11 tools&lt;/strong&gt; in my dataset without any issues.&lt;/p&gt; &lt;p&gt;I'm working in Turkish, and before this, I could only get accurate tool calls using much larger models like &lt;strong&gt;Gemma3:12B&lt;/strong&gt;. But this little model now handles it surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; – If your tool names and parameters are relatively simple like mine, just invent a small DSL and fine-tune a base model. Even &lt;strong&gt;Google Colab’s free tier&lt;/strong&gt; is enough.&lt;/p&gt; &lt;p&gt;here is my own dataset that I use to fine tune&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/umtksa/tools"&gt;https://huggingface.co/datasets/umtksa/tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here is the finetune script I use on my macbook pro m2 &lt;a href="https://gist.github.com/umtksa/912050d7c76c4aff182f4e922432bf94"&gt;https://gist.github.com/umtksa/912050d7c76c4aff182f4e922432bf94&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here is the Modelfile to use finetuned model with ollama&lt;br /&gt; &lt;a href="https://gist.github.com/umtksa/4071e6ff8e31b557a2b650babadcc3d0"&gt;https://gist.github.com/umtksa/4071e6ff8e31b557a2b650babadcc3d0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*added train script link and ollama Modelfile link for Qwen3-0.6B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umtksa"&gt; /u/umtksa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lghrf9/if_your_tools_and_parameters_arent_too_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T23:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgtcrb</id>
    <title>Open source tool to fix LLM-generated JSON</title>
    <updated>2025-06-21T10:24:16+00:00</updated>
    <author>
      <name>/u/arthurtakeda</name>
      <uri>https://old.reddit.com/user/arthurtakeda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! Ever since I started using LLMs to generate JSON for my side projects I occasionally get an error and when looking at the logs it’s usually because of some parsing errors.&lt;/p&gt; &lt;p&gt;I’ve built a tool to fix the most common errors I came across: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Markdown Block Extraction: Extracts JSON from ```json code blocks and inline code&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Trailing Content Removal: Removes explanatory text after valid JSON structures&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Quote Fixing: Fixes unescaped quotes inside JSON strings&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Missing Comma Detection: Adds missing commas between array elements and object properties&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s just pure typescript so it’s very lightweight, hope it’s useful!! Any feedbacks are welcome, thinking of building a Python equivalent soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/aotakeda/ai-json-fixer"&gt;https://github.com/aotakeda/ai-json-fixer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arthurtakeda"&gt; /u/arthurtakeda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgtcrb/open_source_tool_to_fix_llmgenerated_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T10:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgnri0</id>
    <title>What are some AI tools (free or paid) that genuinely helped you get more done — especially the underrated ones not many talk about?</title>
    <updated>2025-06-21T04:17:18+00:00</updated>
    <author>
      <name>/u/Melted_gun</name>
      <uri>https://old.reddit.com/user/Melted_gun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not looking for the obvious ones like ChatGPT or Midjourney — more curious about those lesser-known tools that actually made a difference in your workflow, mindset, or daily routine.&lt;/p&gt; &lt;p&gt;Could be anything — writing, coding, research, time-blocking, design, personal journaling, habit tracking, whatever.&lt;/p&gt; &lt;p&gt;Just trying to find tools that might not be in my radar but could quietly improve things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Melted_gun"&gt; /u/Melted_gun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgnri0/what_are_some_ai_tools_free_or_paid_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T04:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgyv8a</id>
    <title>Steering LLM outputs</title>
    <updated>2025-06-21T15:14:23+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"&gt; &lt;img alt="Steering LLM outputs" src="https://external-preview.redd.it/NmN0cDU5bnZwYThmMcWg2kr7Oe9IfY8fGfsf43KXN8n2ZXafTDS0jzzrXQ6i.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7b47ea3b5a867f0a76afffe31ce20c58bbf78a4" title="Steering LLM outputs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Optimising LLM proxy runs workflow that mixes instructions from multiple anchor prompts based on their weights&lt;/li&gt; &lt;li&gt;Weights are controlled via specially crafted artifact. The artifact connects back to the workflow over websockets and is able of sending/receiving data.&lt;/li&gt; &lt;li&gt;The artifact can pause or slow down the generation as well for better control.&lt;/li&gt; &lt;li&gt;Runs completely outside the inference engine, at OpenAI-compatible API level&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/promx.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run it?&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standalone - &lt;code&gt;docker pull&lt;/code&gt; &lt;a href="http://ghcr.io/av/harbor-boost:latest"&gt;&lt;code&gt;ghcr.io/av/harbor-boost:latest&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/av/harbor/wiki/5.2.-Harbor-Boost#standalone-usage"&gt;configuration reference&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Also see &lt;a href="https://github.com/av/boost-starter"&gt;example starter repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;with &lt;a href="https://github.com/av/harbor"&gt;Harbor&lt;/a&gt; - &lt;code&gt;harbor up boost&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0351w9ovpa8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgyv8a/steering_llm_outputs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T15:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgy4wa</id>
    <title>Build Qwen3 from Scratch</title>
    <updated>2025-06-21T14:41:27+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a big fan of Sebastian Raschka's earlier work on LLMs from scratch. He recently switched from Llama to Qwen (a switch I recently made too thanks to someone in this subreddit) and wrote a Jupyter notebook implementing Qwen3 from scratch.&lt;/p&gt; &lt;p&gt;Highly recommend this resource as a learning project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy4wa/build_qwen3_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgx222</id>
    <title>Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension</title>
    <updated>2025-06-21T13:51:53+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"&gt; &lt;img alt="Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension" src="https://preview.redd.it/o9sgqppkca8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3738a800b971da66fb76ce1bfd12e92a05565ae" title="Minimax-M1 is competitive with Gemini 2.5 Pro 05-06 on Fiction.liveBench Long Context Comprehension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o9sgqppkca8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgx222/minimaxm1_is_competitive_with_gemini_25_pro_0506/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsykj</id>
    <title>AbsenceBench: LLMs can't tell what's missing</title>
    <updated>2025-06-21T09:58:18+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt; &lt;img alt="AbsenceBench: LLMs can't tell what's missing" src="https://b.thumbs.redditmedia.com/PjMEdZcgsAhmRkC952iAQojRviDlyPY_z4tXX2TYqCE.jpg" title="AbsenceBench: LLMs can't tell what's missing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://arxiv.org/pdf/2506.11440"&gt;AbsenceBench paper&lt;/a&gt; establishes a test that's basically Needle In A Haystack (NIAH) in reverse. &lt;a href="https://github.com/harvey-fin/absence-bench"&gt;Code here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The idea is that models score 100% on NIAH tests, thus perfectly identify added tokens that stand out - which is not equal to perfectly reasoning over longer context though - and try that in reverse, with added hints.&lt;/p&gt; &lt;p&gt;They gave the model poetry, number sequences and GitHub PRs, &lt;em&gt;together with&lt;/em&gt; a modified version with removed words or lines, and then asked the model to identify what's missing. A simple program can figure this out with 100% accurracy. The LLMs can't.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285"&gt;https://preview.redd.it/rzlyybfr598f1.png?width=2154&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcadbf591cdd0de119850a164f3ad1488efa3285&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using around 8k thinking tokens improved the score by 8% on average. Those 8k thinking tokens are quite longer than the average input - just 5k, with almost all tests being shorter than 12k. Thus, this isn't an issue of long context handling, although results get worse with longer context. For some reason the results also got worse when testing with shorter omissions.&lt;/p&gt; &lt;p&gt;The hypothesis is that the attention mechanism can only attend to tokens that exist. Omissions have no tokens, thus there are no tokens to put attention on. They tested this by adding placeholders, which boosted the scores by 20% to 50%.&lt;/p&gt; &lt;p&gt;The NIAH test just tested finding literal matches. Models that didn't score close to 100% were also bad at long context understanding. Yet as we've seen with NoLiMa and fiction.liveBench, getting 100% NIAH score doesn't equal good long context &lt;em&gt;understanding&lt;/em&gt;. This paper only tests literal omissions and not semantic omissions, like incomplete evidence for a conclusion. Thus, like NIAH a model scoring 100% here won't automatically guarantee good long context understanding.&lt;/p&gt; &lt;p&gt;Bonus: They also shared the average reasoning tokens per model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874"&gt;https://preview.redd.it/6b6gzd2w698f1.png?width=1053&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c62b0fe40613886510bd91922032278ec146a874&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsykj/absencebench_llms_cant_tell_whats_missing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:58:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgg7a1</id>
    <title>Google releases MagentaRT for real time music generation</title>
    <updated>2025-06-20T21:54:20+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Omar from the Gemma team here, to talk about MagentaRT, our new music generation model. It's real-time, with a permissive license, and just has 800 million parameters.&lt;/p&gt; &lt;p&gt;You can find a video demo right here &lt;a href="https://www.youtube.com/watch?v=Ae1Kz2zmh9M"&gt;https://www.youtube.com/watch?v=Ae1Kz2zmh9M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A blog post at &lt;a href="https://magenta.withgoogle.com/magenta-realtime"&gt;https://magenta.withgoogle.com/magenta-realtime&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GitHub repo &lt;a href="https://github.com/magenta/magenta-realtime"&gt;https://github.com/magenta/magenta-realtime&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And our repository #1000 on Hugging Face: &lt;a href="https://huggingface.co/google/magenta-realtime"&gt;https://huggingface.co/google/magenta-realtime&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgg7a1/google_releases_magentart_for_real_time_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-20T21:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgy12q</id>
    <title>moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face</title>
    <updated>2025-06-21T14:36:31+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"&gt; &lt;img alt="moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face" src="https://external-preview.redd.it/nn6Om0LrvY9dh6qkhvLPezIS-aJdRaC0O6BpYJYgA5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c180a6be6bee520890e9e2bcc9303148c925f48" title="moonshotai/Kimi-VL-A3B-Thinking-2506 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgy12q/moonshotaikimivla3bthinking2506_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgxjw2</id>
    <title>Self Adapting LLMs - legit?</title>
    <updated>2025-06-21T14:14:34+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"&gt; &lt;img alt="Self Adapting LLMs - legit?" src="https://preview.redd.it/rlhp01gfca8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad7294f3dda4aacfc84c69129907508eae493c63" title="Self Adapting LLMs - legit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across the new MIT paper &lt;em&gt;Self-Adapting Language Models&lt;/em&gt; (Zweiger et al., June 2025).&lt;br /&gt; The core idea is wild:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The LLM produces a &lt;strong&gt;self-edit&lt;/strong&gt;—a chunk of text that can (a) rewrite / augment the input data, (b) pick hyper-parameters, or (c) call external tools for data augmentation or gradient updates.&lt;/li&gt; &lt;li&gt;Those self-edits are fed straight back into supervised finetuning (or RL), so the model &lt;em&gt;persistently&lt;/em&gt; updates its own weights.&lt;/li&gt; &lt;li&gt;They train the model to &lt;em&gt;judge its own edits&lt;/em&gt; with a downstream reward signal, so it keeps iterating until performance improves.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Essentially the model becomes both &lt;strong&gt;student and curriculum designer&lt;/strong&gt;, continuously generating the exactly-what-it-needs data to get better.&lt;/p&gt; &lt;p&gt;My (much humbler) attempt &amp;amp; pain points&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For a tweet-classification project I had GPT-4 &lt;strong&gt;select&lt;/strong&gt; real tweets and &lt;strong&gt;synthesize&lt;/strong&gt; new ones to expand the finetuning set.&lt;/li&gt; &lt;li&gt;Quality was decent, but (1) &lt;strong&gt;insanely expensive&lt;/strong&gt;, and (2) performance &lt;strong&gt;regressed&lt;/strong&gt; vs. a baseline where I manually hand-picked examples.&lt;/li&gt; &lt;li&gt;I only did straight SFT; didn’t try RL-style feedback (wasn’t aware of anything cleaner than full-blown PPO/DPO at the time).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I wrong to think that this will not hold in main use cases? Why not just try GRPO RL for the use cases that the user wants? I am honestly a bit confused, can someone explain or discuss on what am I missing here? How can a model know what it needs other than a much bigger model giving it feedback on every iteration? Has RL worked on other stuff than text before in this context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rlhp01gfca8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgxjw2/self_adapting_llms_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T14:14:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lh0noy</id>
    <title>Autopaste MFAs from Gmail using LLaMA</title>
    <updated>2025-06-21T16:32:58+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by Apple's &amp;quot;insert code from SMS&amp;quot; feature, made a tool to speed up the process of inserting incoming email MFAs: &lt;a href="https://github.com/yahorbarkouski/auto-mfa"&gt;https://github.com/yahorbarkouski/auto-mfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lh0noy/autopaste_mfas_from_gmail_using_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T16:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgsxyw</id>
    <title>Unsloth Dynamic GGUF Quants For Mistral 3.2</title>
    <updated>2025-06-21T09:57:06+00:00</updated>
    <author>
      <name>/u/No-Refrigerator-1672</name>
      <uri>https://old.reddit.com/user/No-Refrigerator-1672</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt; &lt;img alt="Unsloth Dynamic GGUF Quants For Mistral 3.2" src="https://external-preview.redd.it/CrtSkHQg7FYlqUCKyAhEr6h8Hgeh7uXu4dg2iLzQFtI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=206fbbf02fe74bed130c7c80f847013da0053f61" title="Unsloth Dynamic GGUF Quants For Mistral 3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Refrigerator-1672"&gt; /u/No-Refrigerator-1672 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgsxyw/unsloth_dynamic_gguf_quants_for_mistral_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T09:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgwcfb</id>
    <title>Semantically search and ask your Gmail using local LLaMA</title>
    <updated>2025-06-21T13:16:44+00:00</updated>
    <author>
      <name>/u/samewakefulinsomnia</name>
      <uri>https://old.reddit.com/user/samewakefulinsomnia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt; &lt;img alt="Semantically search and ask your Gmail using local LLaMA" src="https://external-preview.redd.it/u5mOM2CU1WqdBPMvqlJEeyyQe4ITGIoML7OHXi1ZUCs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4550f23557f54ee6f8ab8804140ab8bb8007bf02" title="Semantically search and ask your Gmail using local LLaMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got fed up with Apple Mail’s clunky search and built my own tool: a lightweight, local-LLM-first CLI that lets you semantically search and ask questions about your Gmail inbox:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/vs2cz0f66a8f1.gif"&gt;https://i.redd.it/vs2cz0f66a8f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Grab it here: &lt;a href="https://github.com/yahorbarkouski/semantic-mail"&gt;https://github.com/yahorbarkouski/semantic-mail&lt;/a&gt;&lt;/p&gt; &lt;p&gt;any feedback/contributions are very much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samewakefulinsomnia"&gt; /u/samewakefulinsomnia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwcfb/semantically_search_and_ask_your_gmail_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgvl40</id>
    <title>After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch</title>
    <updated>2025-06-21T12:38:20+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"&gt; &lt;img alt="After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch" src="https://external-preview.redd.it/1Tc0yB3lwHCV4Qo8QzQZtbMZw5Hyi2St2rr1CDzMJaE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e72c893ef4fea62f81300f8447709b6c1be403c" title="After trying to buy Ilya Sutskever's $32B AI startup, Meta looks to hire its CEO | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What hapening to zuck? after scale ai , now Safe Superintelligence&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/06/20/after-trying-to-buy-ilya-sutskevers-32b-ai-startup-meta-looks-to-hire-its-ceo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgvl40/after_trying_to_buy_ilya_sutskevers_32b_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T12:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lglhll</id>
    <title>Mistral's "minor update"</title>
    <updated>2025-06-21T02:12:10+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt; &lt;img alt="Mistral's &amp;quot;minor update&amp;quot;" src="https://preview.redd.it/rb70qb16v68f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7248b214307a876a51003f595eeeb9564be8245" title="Mistral's &amp;quot;minor update&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rb70qb16v68f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lglhll/mistrals_minor_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T02:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lgwsdr</id>
    <title>DeepSeek Guys Open-Source nano-vLLM</title>
    <updated>2025-06-21T13:38:49+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek guys just open-sourced &lt;a href="https://github.com/GeeeekExplorer/nano-vllm"&gt;nano-vLLM&lt;/a&gt;. It’s a lightweight vLLM implementation built from scratch.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;🚀 &lt;strong&gt;Fast offline inference&lt;/strong&gt; - Comparable inference speeds to vLLM&lt;/li&gt; &lt;li&gt;📖 &lt;strong&gt;Readable codebase&lt;/strong&gt; - Clean implementation in ~ 1,200 lines of Python code&lt;/li&gt; &lt;li&gt;⚡ &lt;strong&gt;Optimization Suite&lt;/strong&gt; - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lgwsdr/deepseek_guys_opensource_nanovllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-21T13:38:49+00:00</published>
  </entry>
</feed>
