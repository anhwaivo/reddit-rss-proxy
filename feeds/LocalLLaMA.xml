<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-30T04:08:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mce934</id>
    <title>My Honest Take on Recently Popular Open Models (A Realistic Assessment)</title>
    <updated>2025-07-29T15:19:43+00:00</updated>
    <author>
      <name>/u/Ok_Technology_3421</name>
      <uri>https://old.reddit.com/user/Ok_Technology_3421</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's great to see open models continuing to advance. I believe most people in this community would agree that there's often a significant gap between benchmark scores and real-world performance. With that in mind, I've put together some candid thoughts on several open models from an end-user's perspective.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-4.5&lt;/strong&gt;: I find it exceptionally good for everyday use. There's a clear distinction from previous LLMs that would excessively praise users or show off with markdown tables. I noticed some quirks in its reasoning similar to Deepseek R1, but nothing problematic. Personally, I recommend using it through &lt;a href="http://chat.z.ai"&gt;chat.z.ai&lt;/a&gt;, which offers an excellent UI/UX experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;: I found it to perform excellently at both coding tasks and creative work. However, it's noticeably slow with prominent rate limiting even when accessed through Openrouter. The fact that its app and website only support Chinese is a significant downside for international users.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 Coder&lt;/strong&gt;: While I've heard it benchmarks better than Kimi K2, my actual experience was quite disappointing. It warrants further testing, though it does offer a larger context window than Kimi K2, which is commendable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Instruct 2507&lt;/strong&gt;: I also get the sense that its benchmarks are inflated, but it's actually quite decent. It has a noticeably &amp;quot;LLM-like&amp;quot; quality to its responses, which might make it less ideal for creative endeavors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Thinking 2507&lt;/strong&gt;: Its large thinking budget is advantageous, but this can backfire, sometimes resulting in excessively long response times. For now, I find Deepseek R1-0528 more practical to use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deepseek R1-0528&lt;/strong&gt;: This one needs no introduction - it proves to be quite versatile, high-performing, and user-friendly. Among Openrouter's free models, it offers the most stable inference, and the API provides excellent value for money (the official API has discounted periods that can save you up to 70%).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Technology_3421"&gt; /u/Ok_Technology_3421 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T15:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc5oh2</id>
    <title>This year’s best open-source models and most cost-effective models</title>
    <updated>2025-07-29T08:09:33+00:00</updated>
    <author>
      <name>/u/Apart-River475</name>
      <uri>https://old.reddit.com/user/Apart-River475</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt; &lt;img alt="This year’s best open-source models and most cost-effective models" src="https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg" title="This year’s best open-source models and most cost-effective models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br /&gt; The &lt;strong&gt;GLM-4.5&lt;/strong&gt; series models are foundation models designed for intelligent agents. GLM-4.5 has &lt;strong&gt;355&lt;/strong&gt; billion total parameters with &lt;strong&gt;32&lt;/strong&gt; billion active parameters, while GLM-4.5-Air adopts a more compact design with &lt;strong&gt;106&lt;/strong&gt; billion total parameters and &lt;strong&gt;12&lt;/strong&gt; billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"&gt;Bench performance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://z.ai/blog/glm-4.5"&gt;blog&lt;/a&gt;｜&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;huggingface&lt;/a&gt;｜ &lt;a href="https://github.com/zai-org/GLM-4.5"&gt;github&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-River475"&gt; /u/Apart-River475 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:09:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcqrwh</id>
    <title>RL Library for Multi-Trainable-Agents</title>
    <updated>2025-07-29T23:18:35+00:00</updated>
    <author>
      <name>/u/rd211x</name>
      <uri>https://old.reddit.com/user/rd211x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently released my experimental library &lt;em&gt;Actors.&lt;/em&gt; Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the &lt;strong&gt;GPU poor&lt;/strong&gt; and &lt;strong&gt;middle class&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/RD211/actors"&gt;https://github.com/RD211/actors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - &lt;strong&gt;Multi-Trainable-Agents&lt;/strong&gt;: You can do things like adversarial, collaborative or simulation-like environments.&lt;br /&gt; - &lt;strong&gt;Multi-Environments&lt;/strong&gt;: Lets you make very complex environments and makes it easy to combine them together.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM Efficiency&lt;/strong&gt;, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:&lt;br /&gt; - Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).&lt;br /&gt; - Streamed weight updates to vLLM that do not make a spike in memory usage.&lt;br /&gt; - A small triton kernel for reference Log-probs calculations.&lt;br /&gt; - in-memory LoRA updates to vLLM.&lt;/p&gt; &lt;p&gt;The library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Algorithms,&lt;/strong&gt; we currently have &lt;strong&gt;GSPO&lt;/strong&gt; and &lt;strong&gt;GRPO&lt;/strong&gt; both with &lt;strong&gt;Liger-Kernel&lt;/strong&gt; implementations but you can probably get DAPO and some others by just adjusting some of the settings.&lt;/p&gt; &lt;p&gt;Feedback and issues are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rd211x"&gt; /u/rd211x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T23:18:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8evq</id>
    <title>Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train 😅</title>
    <updated>2025-07-29T11:02:25+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt; &lt;img alt="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train 😅" src="https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg" title="Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train 😅" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! 😅&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt; &lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt; &lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt; &lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)&lt;/li&gt; &lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt; &lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt; &lt;li&gt;~£30-50k needed for full training run of 1000 epochs (I could only afford testing 😅)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt: &lt;ul&gt; &lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt; &lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt; &lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;⭐️ &lt;a href="https://github.com/Danau5tin/terminal-bench-rl"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Danau5tin/tbench-agentic-data-pipeline"&gt;⭐️ Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href="https://github.com/rllm-org/rllm"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href="https://www.tbench.ai/"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mc8evq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcw1sl</id>
    <title>GLM 4.5 Air Tool Calling Issues In LM Studio</title>
    <updated>2025-07-30T03:27:43+00:00</updated>
    <author>
      <name>/u/Sharpastic</name>
      <uri>https://old.reddit.com/user/Sharpastic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, is anyone else having issues with GLM 4.5 Air not properly formatting its tool calls in LM Studio? This is an example from my most recent chat:&lt;/p&gt; &lt;p&gt;&amp;lt;tool\_call&amp;gt;browser_navigate&lt;br /&gt; &amp;lt;arg\_key&amp;gt;url&amp;lt;/arg\_key&amp;gt;&lt;br /&gt; &amp;lt;arg\_value&amp;gt;&lt;a href="https://www.example.com"&gt;https://www.example.com&lt;/a&gt;&amp;lt;/arg\_value&amp;gt;&lt;br /&gt; &amp;lt;/tool\_call&amp;gt;&lt;/p&gt; &lt;p&gt;It seems to be formatting it in XML, where I believe LM Studio uses Json. Does anyone have an idea on how to fix this, or should I just wait until an official patch/update to the system prompt comes out?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sharpastic"&gt; /u/Sharpastic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcw1sl/glm_45_air_tool_calling_issues_in_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcw1sl/glm_45_air_tool_calling_issues_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcw1sl/glm_45_air_tool_calling_issues_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T03:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mbvf2z</id>
    <title>its getting comical</title>
    <updated>2025-07-28T23:09:30+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt; &lt;img alt="its getting comical" src="https://preview.redd.it/txsukljc5pff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d" title="its getting comical" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txsukljc5pff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-28T23:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcjz8j</id>
    <title>One year’s benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models</title>
    <updated>2025-07-29T18:50:58+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"&gt; &lt;img alt="One year’s benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models" src="https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc" title="One year’s benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year’s hindsight. Of course benchmarks aren’t everything. They aren’t nothing either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T18:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc6fbp</id>
    <title>GLM 4.5 support is landing in llama.cpp</title>
    <updated>2025-07-29T08:59:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt; &lt;img alt="GLM 4.5 support is landing in llama.cpp" src="https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd" title="GLM 4.5 support is landing in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T08:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcnq7r</id>
    <title>AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio</title>
    <updated>2025-07-29T21:12:54+00:00</updated>
    <author>
      <name>/u/ZZZCodeLyokoZZZ</name>
      <uri>https://old.reddit.com/user/ZZZCodeLyokoZZZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"&gt; &lt;img alt="AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio" src="https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1407a6de51b1efe682d3aec309cbbdadb1b1d910" title="AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too ~15 tk/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZZZCodeLyokoZZZ"&gt; /u/ZZZCodeLyokoZZZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:12:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mce9tt</id>
    <title>zai-org/GLM-4.5 · We Have Gemini At Home</title>
    <updated>2025-07-29T15:20:33+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"&gt; &lt;img alt="zai-org/GLM-4.5 · We Have Gemini At Home" src="https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=403fb88f398e6c5fe36b4f1c95408e0675027e55" title="zai-org/GLM-4.5 · We Have Gemini At Home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tested for same, is it trained on gemini outputs ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5/discussions/1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T15:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mchj7h</id>
    <title>AFM 4.5B</title>
    <updated>2025-07-29T17:20:54+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"&gt; &lt;img alt="AFM 4.5B" src="https://preview.redd.it/c7yvmvdgkuff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a68967cc776ececd5151071c32eb068a2fd1ddad" title="AFM 4.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting small model, hadn't seen it before.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-GGUF"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c7yvmvdgkuff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mchj7h/afm_45b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcl15k</id>
    <title>Qwen 1.7B tool calling across Android on Pixel 9 and S22</title>
    <updated>2025-07-29T19:30:26+00:00</updated>
    <author>
      <name>/u/Economy-Mud-6626</name>
      <uri>https://old.reddit.com/user/Economy-Mud-6626</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"&gt; &lt;img alt="Qwen 1.7B tool calling across Android on Pixel 9 and S22" src="https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0090c885e3fd475871f7cc3149139b5e73e39a86" title="Qwen 1.7B tool calling across Android on Pixel 9 and S22" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How about running a local agent on a smartphone? Here's how I did it. &lt;/p&gt; &lt;p&gt;I stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via &lt;code&gt;uint16_t&lt;/code&gt;), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! &lt;/p&gt; &lt;h1&gt;Tool Calling Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-step conversation support&lt;/strong&gt; with automatic tool execution&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON-based tool calling&lt;/strong&gt; with &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; XML tags&lt;/li&gt; &lt;li&gt;&lt;strong&gt;test tools&lt;/strong&gt;: weather, math calculator, time, location&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Used &lt;a href="https://github.com/mlc-ai/tokenizers-cpp"&gt;tokenizer-cpp&lt;/a&gt; from MLC&lt;/h1&gt; &lt;p&gt;which binds rust &lt;a href="https://github.com/huggingface/tokenizers"&gt;huggingface/tokenizers&lt;/a&gt; giving full support for android/iOS.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// - dist/tokenizer.json void HuggingFaceTokenizerExample() { auto blob = LoadBytesFromFile(&amp;quot;dist/tokenizer.json&amp;quot;); auto tok = Tokenizer::FromBlobJSON(blob); std::string prompt = &amp;quot;What is the capital of Canada?&amp;quot;; std::vector&amp;lt;int&amp;gt; ids = tok-&amp;gt;Encode(prompt); std::string decoded_prompt = tok-&amp;gt;Decode(ids); } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Push LLM streams into Kotlin Flows&lt;/h1&gt; &lt;pre&gt;&lt;code&gt; suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&amp;gt;Unit) : String? { val res = NimbleNet.runMethod( &amp;quot;prompt_for_tool_calling&amp;quot;, inputs = hashMapOf( &amp;quot;prompt&amp;quot; to NimbleNetTensor(input, DATATYPE.STRING, null), &amp;quot;output_stream_callback&amp;quot; to createNimbleNetTensorFromForeignFunction(callback) ), ) assert(res.status) { &amp;quot;NimbleNet.runMethod('prompt_for_tool_calling') failed with status: ${res.status}&amp;quot; } return res.payload?.get(&amp;quot;results&amp;quot;)?.data as String? } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the code soon merging in Delite AI (&lt;a href="https://github.com/NimbleEdge/deliteAI/pull/165"&gt;https://github.com/NimbleEdge/deliteAI/pull/165&lt;/a&gt;)&lt;br /&gt; Or try in the assistant app (&lt;a href="https://github.com/NimbleEdge/assistant"&gt;https://github.com/NimbleEdge/assistant&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Mud-6626"&gt; /u/Economy-Mud-6626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3wcxuotf7vff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T19:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcvc46</id>
    <title>GLM-4.5 Air on 64gb Mac with MLX</title>
    <updated>2025-07-30T02:52:11+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Simon Willison says “Ivan Fioravanti built this 44GB 3bit quantized version for MLX, specifically sized so people with 64GB machines could have a chance of running it. I tried it out... and it works extremely well.”&lt;/p&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email"&gt;https://open.substack.com/pub/simonw/p/my-25-year-old-laptop-can-write-space?r=bmuv&amp;amp;utm_campaign=post&amp;amp;utm_medium=email&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve run the model with LMStudio on a 64gb M1 Max Studio. LMStudio initially would not run the model, providing a popup to that effect. The popup also allowed me to adjust the guardrails. I had to turn them off entirely to run the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcvc46/glm45_air_on_64gb_mac_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T02:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcrx23</id>
    <title>PSA: The new Threadripper PROs (9000 WX) are still CCD-Memory Bandwidth bottlenecked</title>
    <updated>2025-07-30T00:10:03+00:00</updated>
    <author>
      <name>/u/henfiber</name>
      <uri>https://old.reddit.com/user/henfiber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That's not the case.&lt;/p&gt; &lt;p&gt;The issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.&lt;/p&gt; &lt;p&gt;Check the &amp;quot;Latest baselines&amp;quot; section in a processor's page at &lt;a href="http://cpubenchmark.net"&gt;cpubenchmark.net&lt;/a&gt; with links to individual results where the &amp;quot;Memory Threaded&amp;quot; result is listed under &amp;quot;Memory Mark&amp;quot;:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Memory BW&lt;/th&gt; &lt;th align="left"&gt;Reference&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;amp;id=6803"&gt;AMD Threadripper PRO 9955WX&lt;/a&gt; (16-cores)&lt;/td&gt; &lt;td align="left"&gt;~115 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=509905130667"&gt;BL5099051 - Jul 20 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;2x CCD&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;amp;id=6804"&gt;AMD Threadripper PRO 9965WX&lt;/a&gt; (24-cores)&lt;/td&gt; &lt;td align="left"&gt;~272 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=279748548819"&gt;BL2797485 - Jul 29 2025&lt;/a&gt; (other baselines start from 250GB/s)&lt;/td&gt; &lt;td align="left"&gt;4x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;amp;id=6799"&gt;AMD Threadripper PRO 9975WX&lt;/a&gt; (32-cores)&lt;/td&gt; &lt;td align="left"&gt;~272 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=279782022829"&gt;BL2797820 - Jul 29 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;4x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;amp;id=6807"&gt;AMD Threadripper PRO 9985WX&lt;/a&gt; (64-cores)&lt;/td&gt; &lt;td align="left"&gt;~367 GB/s&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.passmark.com/baselines/V11/display.php?id=509913021820"&gt;BL5099130 - Jul 21 2025&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8x CCDs&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Therefore:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. &lt;a href="https://www.passmark.com/baselines/V10/display.php?id=226455755507"&gt;7R43 with 191 GB/s&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).&lt;/li&gt; &lt;li&gt;the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For comparison, check the excellent related threads by &lt;a href="/u/fairydreaming"&gt;u/fairydreaming&lt;/a&gt; for the previous gen Threadrippers and EPYC Genoa/Turin:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/"&gt;Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/"&gt;Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/"&gt;STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henfiber"&gt; /u/henfiber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T00:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcp7dp</id>
    <title>GLM-4.5 on fiction.livebench</title>
    <updated>2025-07-29T22:11:50+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"&gt; &lt;img alt="GLM-4.5 on fiction.livebench" src="https://preview.redd.it/aey1fr0e0wff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=164ac21dfb78027d97359b16dae6fc48436681ec" title="GLM-4.5 on fiction.livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aey1fr0e0wff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T22:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mc8tks</id>
    <title>I just tried GLM 4.5</title>
    <updated>2025-07-29T11:24:54+00:00</updated>
    <author>
      <name>/u/AI-On-A-Dime</name>
      <uri>https://old.reddit.com/user/AI-On-A-Dime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt; &lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt; &lt;p&gt;Here’s the link to the results: &lt;a href="https://chat.z.ai/space/r05c76960ff0-ppt"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Here’s the initial prompt:&lt;/p&gt; &lt;p&gt;”Create a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.”&lt;/p&gt; &lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt; &lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt; &lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-On-A-Dime"&gt; /u/AI-On-A-Dime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T11:24:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcfuka</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-29T16:19:15+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new qwen moe!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcee42</id>
    <title>My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX</title>
    <updated>2025-07-29T15:25:02+00:00</updated>
    <author>
      <name>/u/ChiliPepperHott</name>
      <uri>https://old.reddit.com/user/ChiliPepperHott</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"&gt; &lt;img alt="My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX" src="https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5a9e25e0e831120dffb4dbb77fc7392c4ccb49" title="My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiliPepperHott"&gt; /u/ChiliPepperHott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://simonwillison.net/2025/Jul/29/space-invaders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T15:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcji8s</id>
    <title>Qwen3-30b-3ab-2507 is a beast for MCP usage!</title>
    <updated>2025-07-29T18:33:19+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt; &lt;img alt="Qwen3-30b-3ab-2507 is a beast for MCP usage!" src="https://b.thumbs.redditmedia.com/Ku7pPJKnjoNSXHTx41JonGncgMhMPCUf8ZnqoDSjoGY.jpg" title="Qwen3-30b-3ab-2507 is a beast for MCP usage!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;C'est la première fois qu'un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n'est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !&lt;/p&gt; &lt;p&gt;For those who want my MCP flow, here’s the Pastebin:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/WNPrcjLS"&gt;https://pastebin.com/WNPrcjLS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268"&gt;https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T18:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcg4qt</id>
    <title>🚀 Qwen3-30B-A3B Small Update</title>
    <updated>2025-07-29T16:29:59+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt; &lt;img alt="🚀 Qwen3-30B-A3B Small Update" src="https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d" title="🚀 Qwen3-30B-A3B Small Update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt; &lt;p&gt;✨ Key Enhancements:&lt;/p&gt; &lt;p&gt;✅ Enhanced reasoning, coding, and math skills&lt;/p&gt; &lt;p&gt;✅ Broader multilingual knowledge&lt;/p&gt; &lt;p&gt;✅ Improved long-context understanding (up to 256K tokens)&lt;/p&gt; &lt;p&gt;✅ Better alignment with user intent and open-ended tasks&lt;/p&gt; &lt;p&gt;✅ No more &amp;lt;think&amp;gt; blocks — now operating exclusively in non-thinking mode&lt;/p&gt; &lt;p&gt;🔧 With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Chat: &lt;a href="https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nd904g7gbuff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mco449</id>
    <title>Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT</title>
    <updated>2025-07-29T21:28:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt; &lt;img alt="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" src="https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d458366fefeec47c4d65d3844419bf9e79783f21" title="Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7xpye5hurvff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:28:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcfmd2</id>
    <title>Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face</title>
    <updated>2025-07-29T16:11:03+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T16:11:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcr64f</id>
    <title>4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.</title>
    <updated>2025-07-29T23:36:00+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt; &lt;img alt="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." src="https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg" title="4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-X-4B-0729"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt; &lt;p&gt;As per the last post -&amp;gt;&lt;br /&gt; Specifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt; &lt;p&gt;We're looking for some beta testers for some new models and open source projects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mcr64f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T23:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mcoce7</id>
    <title>AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs</title>
    <updated>2025-07-29T21:37:02+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt; &lt;img alt="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" src="https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0" title="AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T21:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mci7uu</id>
    <title>Newest Qwen made me cry. It's not perfect, but I still love it.</title>
    <updated>2025-07-29T17:45:49+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt; &lt;img alt="Newest Qwen made me cry. It's not perfect, but I still love it." src="https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=431c53f32897af3a4225062d97bdc95913f53ec0" title="Newest Qwen made me cry. It's not perfect, but I still love it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gnkbnxzlouff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-29T17:45:49+00:00</published>
  </entry>
</feed>
