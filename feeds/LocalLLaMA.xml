<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-27T12:10:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jkv955</id>
    <title>Local Workstations</title>
    <updated>2025-03-27T03:52:30+00:00</updated>
    <author>
      <name>/u/Personal-Attitude872</name>
      <uri>https://old.reddit.com/user/Personal-Attitude872</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been planning out a workstation for a little bit now and I’ve run into some questions I think are better answered by those with experience. My proposed build is as follows:&lt;/p&gt; &lt;p&gt;CPU: AMD Threadripper 7965WX&lt;/p&gt; &lt;p&gt;GPU: 1x 4090 + 2-3x 3090 (undervolted to ~200w)&lt;/p&gt; &lt;p&gt;MoBo: Asus Pro WS WRX90E-SAGE&lt;/p&gt; &lt;p&gt;RAM: 512gb DDR5&lt;/p&gt; &lt;p&gt;This would give me 72gb of VRAM and 512gb of system memory to fallback on. &lt;/p&gt; &lt;p&gt;Ideally I want to be able to run Qwen 2.5-coder 32b and a smaller model for inline copilot completions. From what I read Qwen can be ran at the 16bit quant comfortably at 64gb so I’d be able to load this into VRAM (i assume) however that would be about it. I can’t go over a 2000w power consumption so there’s not much room for expansion either. &lt;/p&gt; &lt;p&gt;I then ran into the M3 ultra mac studio at 512gb. This machine seems perfect and the results on even larger models is insane. However, I’m a linux user at heart and switching to a mac just doesn’t sit right with me.&lt;/p&gt; &lt;p&gt;So what should I do? Is the mac a no-brainer? Is there other options I don’t know about for local builds?&lt;/p&gt; &lt;p&gt;I’m a beginner in this space, only running smaller models on my 4060 but I’d love some input from you guys or some resources to further educate myself. Any response is appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Personal-Attitude872"&gt; /u/Personal-Attitude872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkv955/local_workstations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkv955/local_workstations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkv955/local_workstations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T03:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke5e5</id>
    <title>V3.1 on livebench</title>
    <updated>2025-03-26T15:14:31+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"&gt; &lt;img alt="V3.1 on livebench" src="https://preview.redd.it/u6go7pw0w1re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e8ce1c39f759069e5c705eb541c28ff5d91aee6" title="V3.1 on livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u6go7pw0w1re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5e5/v31_on_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl0dd4</id>
    <title>Are there any Benchmark/Models that focuses on RAG capabilities?</title>
    <updated>2025-03-27T09:59:32+00:00</updated>
    <author>
      <name>/u/nojukuramu</name>
      <uri>https://old.reddit.com/user/nojukuramu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know that all high performing models are great at this but most of them are very large models. Im thinking of Small Models that could be trained to respond based on retrieved informations. It Doesn't have to be intelligent. Being able to use the lrovided information is enough.&lt;/p&gt; &lt;p&gt;Some of the small models aren't trained solely for that but they can be somewhat good with some level of error rates. Would be nice to know if there are some Benchmarking that does this??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nojukuramu"&gt; /u/nojukuramu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl0dd4/are_there_any_benchmarkmodels_that_focuses_on_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl0dd4/are_there_any_benchmarkmodels_that_focuses_on_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl0dd4/are_there_any_benchmarkmodels_that_focuses_on_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T09:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkcd5l</id>
    <title>😲 DeepSeek-V3-4bit &gt;20tk/s, &lt;200w on M3 Ultra 512GB, MLX</title>
    <updated>2025-03-26T13:56:26+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This might be the best and most user-friendly way to run DeepSeek-V3 on consumer hardware, possibly the most affordable too.&lt;/p&gt; &lt;p&gt;It sounds like you can finally run a GPT-4o level model locally at home, possibly with even better quality.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/"&gt;https://venturebeat.com/ai/deepseek-v3-now-runs-at-20-tokens-per-second-on-mac-studio-and-thats-a-nightmare-for-openai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;I'm not sure if there's difference between v3 and r1, but here's a result with 13k context from &lt;a href="/u/ifioravanti"&gt;/u/ifioravanti&lt;/a&gt; with DeepSeek R1 671B 4bit using MLX.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Prompt: 13140 tokens, 59.562 tokens-per-sec - Generation: 720 tokens, 6.385 tokens-per-sec - Peak memory: 491.054 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j9vjf1/deepseek_r1_671b_q4_m3_ultra_512gb_with_mlx/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That's about 3.5 minutes of prompt processing 13k tokens. Your subsequent chat will go faster with prompt caching. Obviously it depends on your usage and speed tolerance, but 6.385tk/s is not too bad IMO.&lt;/p&gt; &lt;p&gt;You can purchase it on a monthly plan, with $1,531.10 upfront payment, test it for 14 days, and get a refund if you're not happy. lol&lt;/p&gt; &lt;p&gt;In 2020, if someone had said that within five years, a $10k computer could look at a simple text instruction and generate fully runnable code for a basic arcade game in just minutes at home, no one would have believed it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkcd5l/deepseekv34bit_20tks_200w_on_m3_ultra_512gb_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkyxz7</id>
    <title>Request from HuggingFace to release KBLaM models and datasets</title>
    <updated>2025-03-27T08:06:41+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"&gt; &lt;img alt="Request from HuggingFace to release KBLaM models and datasets" src="https://external-preview.redd.it/JzhIs4JD0Slf_53LQELGJrm4Ih8GgOW9SYQYMoU0mvg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23e45ffaa30e9b8e0f8f288f53876176cdf5d996" title="Request from HuggingFace to release KBLaM models and datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/KBLaM/issues/25"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkyxz7/request_from_huggingface_to_release_kblam_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jk7cka</id>
    <title>Plenty 3090 FE's for sale in the Netherlands</title>
    <updated>2025-03-26T08:56:31+00:00</updated>
    <author>
      <name>/u/jwestra</name>
      <uri>https://old.reddit.com/user/jwestra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt; &lt;img alt="Plenty 3090 FE's for sale in the Netherlands" src="https://preview.redd.it/3bxpnick00re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7295d7a24ea4033f0e4d96d4cdbf0b662770a23a" title="Plenty 3090 FE's for sale in the Netherlands" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwestra"&gt; /u/jwestra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bxpnick00re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jk7cka/plenty_3090_fes_for_sale_in_the_netherlands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T08:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkxevv</id>
    <title>What's the background for the current image generating improvements?</title>
    <updated>2025-03-27T06:11:17+00:00</updated>
    <author>
      <name>/u/Jentano</name>
      <uri>https://old.reddit.com/user/Jentano</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI image generation seems to improve a lot across the board. &lt;/p&gt; &lt;p&gt;The new GPT4o image generation is very good, although it has a lot of blocking compliance rules like not wanting to modify real fotos.&lt;/p&gt; &lt;p&gt;But others also seem to be progressing a lot in image accuracy, image-text precision amd prompt following.&lt;/p&gt; &lt;p&gt;Were there any paper breakthroughs or is this mostly better training, perhaps text insertion and more correction loops?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jentano"&gt; /u/Jentano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxevv/whats_the_background_for_the_current_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T06:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzgpt</id>
    <title>Open source AI model for image modification</title>
    <updated>2025-03-27T08:48:46+00:00</updated>
    <author>
      <name>/u/pikmin04</name>
      <uri>https://old.reddit.com/user/pikmin04</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I'm sure some of you have seen the new trend of converting images to Ghibli style.&lt;/p&gt; &lt;p&gt;I'd like to dabble with it, but obviously without giving my own images to OpenAI.&lt;/p&gt; &lt;p&gt;Is there a model that I could run locally able to do this kind of work ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pikmin04"&gt; /u/pikmin04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzgpt/open_source_ai_model_for_image_modification/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzgpt/open_source_ai_model_for_image_modification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzgpt/open_source_ai_model_for_image_modification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkl8ul</id>
    <title>Megastructure made by new gemini 2.5 Pro one shot</title>
    <updated>2025-03-26T20:05:32+00:00</updated>
    <author>
      <name>/u/KillyOnTerra</name>
      <uri>https://old.reddit.com/user/KillyOnTerra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"&gt; &lt;img alt="Megastructure made by new gemini 2.5 Pro one shot" src="https://external-preview.redd.it/Y3BoMjhvYnliM3JlMXCwHJYcCFS2at4U0624bh9xAyv9yXDKiCS8t1hTdo0t.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e0f520cb053129adb6695eed4de9b9adaf60706" title="Megastructure made by new gemini 2.5 Pro one shot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see alot of people testing ai with 2D games but I wanted to see how it handles 3D. &lt;/p&gt; &lt;p&gt;Prompt: make an enormous megastructure in unity using c# make it complex and interesting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KillyOnTerra"&gt; /u/KillyOnTerra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ej6ukleyb3re1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkl8ul/megastructure_made_by_new_gemini_25_pro_one_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T20:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl2bxr</id>
    <title>Are we due a new qwen model today?</title>
    <updated>2025-03-27T12:05:34+00:00</updated>
    <author>
      <name>/u/Perfect_Technology73</name>
      <uri>https://old.reddit.com/user/Perfect_Technology73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or have we had all the new models already?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Technology73"&gt; /u/Perfect_Technology73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl2bxr/are_we_due_a_new_qwen_model_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T12:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkotcs</id>
    <title>Delving deep into Llama.cpp and exploiting Llama.cpp's Heap Maze, from Heap-Overflow to Remote-Code Execution.</title>
    <updated>2025-03-26T22:36:54+00:00</updated>
    <author>
      <name>/u/FitItem2633</name>
      <uri>https://old.reddit.com/user/FitItem2633</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://retr0.blog/blog/llama-rpc-rce"&gt;https://retr0.blog/blog/llama-rpc-rce&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitItem2633"&gt; /u/FitItem2633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkotcs/delving_deep_into_llamacpp_and_exploiting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T22:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl04dt</id>
    <title>Running Qwen 2.5 Omni 7B Voice Locally</title>
    <updated>2025-03-27T09:40:16+00:00</updated>
    <author>
      <name>/u/latestagecapitalist</name>
      <uri>https://old.reddit.com/user/latestagecapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone know how or when this will be possible?&lt;/p&gt; &lt;p&gt;Also where to track any team who is working on it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/latestagecapitalist"&gt; /u/latestagecapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl04dt/running_qwen_25_omni_7b_voice_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl04dt/running_qwen_25_omni_7b_voice_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl04dt/running_qwen_25_omni_7b_voice_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T09:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl1do8</id>
    <title>Advice on host system for RTX PRO 6000</title>
    <updated>2025-03-27T11:08:21+00:00</updated>
    <author>
      <name>/u/didroe</name>
      <uri>https://old.reddit.com/user/didroe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm considering buying an RTX PRO 6000 when they're released, and I'm looking for some advice about the rest of the system to build around it.&lt;/p&gt; &lt;p&gt;My current thought is to buy a high end consumer CPU (Ryzen 7/9) and 64gb DDR5 (dual channel).&lt;/p&gt; &lt;p&gt;Is there any value in other options? Some of the options I've considered and my (ignorant!) thoughts on them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen AI Max+ 395 (eg. Framework PC) - Added compute might be good, memory bandwidth seems limited and also wouldn't have full x16 PCIe for the GPU.&lt;/li&gt; &lt;li&gt;Threadripper/EPYC - Expensive for ones that have 8/12 channel memory support. Compute not that great for LLM?&lt;/li&gt; &lt;li&gt;Mac - non-starter as GPU not supported. Maybe not worth it even if it was, as compute doesn't seem that great&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I want a decent experience in t/s. Am I best just focusing on models that would run on the GPU? Or is there value in pairing it with a beefier host system?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/didroe"&gt; /u/didroe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1do8/advice_on_host_system_for_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1do8/advice_on_host_system_for_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1do8/advice_on_host_system_for_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T11:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkbh4f</id>
    <title>Google releases TxGemma, open models for therapeutic applications</title>
    <updated>2025-03-26T13:13:38+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt; &lt;img alt="Google releases TxGemma, open models for therapeutic applications" src="https://external-preview.redd.it/w4gdQx4Lq4f5EYpFmMZH_AWSFA12WrreFd38e_AppFM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4444c30b578119b10e027ae74a8e53550f6800b" title="Google releases TxGemma, open models for therapeutic applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We're excited to share TxGemma! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemma 2-based model for multiple therapeutic tasks &lt;ul&gt; &lt;li&gt;Classification (will molecule cross blood-brain barrier)&lt;/li&gt; &lt;li&gt;Regression (drug's binding affinity)&lt;/li&gt; &lt;li&gt;Generation (given product of some reaction, generate reactant set)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;2B, 9B, and 27B, with 27B being SOTA for many tasks, including versus single-task models&lt;/li&gt; &lt;li&gt;Chat version for general reasoning, to answer questions and engage in discussions&lt;/li&gt; &lt;li&gt;Fine-tunable with transformers, with an example notebook&lt;/li&gt; &lt;li&gt;Agentic-Tx for agentic systems, powered with Gemini, and using TxGemma as a tool&lt;/li&gt; &lt;li&gt;Models on HF: &lt;a href="https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87"&gt;https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/?linkId=13647386"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkbh4f/google_releases_txgemma_open_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T13:13:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkkqs3</id>
    <title>Free Search: Making Search Free 4 All</title>
    <updated>2025-03-26T19:45:09+00:00</updated>
    <author>
      <name>/u/Far-Celebration-470</name>
      <uri>https://old.reddit.com/user/Far-Celebration-470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;👋 Hi all! &lt;/p&gt; &lt;p&gt;For any AI agent, internet search 🔎 is an important tool. However, with APIs like Tavily and Exa, it becomes really difficult to keep up with the cost. In some cases, these Internet APIs cost more than the LLM. &lt;/p&gt; &lt;p&gt;To solve, this, I am making a playwright wrapper API on top of publicly available searXNG instances. This will enable agent applications to fetch internet results for free. &lt;/p&gt; &lt;p&gt;Currently, I have set up a basic GitHub repo, and I will continue developing advanced search features, such as image search 🖼️ &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/HanzlaJavaid/Free-Search/tree/main"&gt;https://github.com/HanzlaJavaid/Free-Search/tree/main&lt;/a&gt; &lt;/p&gt; &lt;p&gt;🚀 Try the deployed version: &lt;a href="https://freesearch.replit.app/docs"&gt;https://freesearch.replit.app/docs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you find this useful, consider starring ⭐️ the GitHub repository to support further development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Celebration-470"&gt; /u/Far-Celebration-470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkkqs3/free_search_making_search_free_4_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T19:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkwe7u</id>
    <title>How does gpt4o image generator works? and there's gemini flash too, what techinique do they use?</title>
    <updated>2025-03-27T05:01:09+00:00</updated>
    <author>
      <name>/u/aman167k</name>
      <uri>https://old.reddit.com/user/aman167k</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i want to replicate this for domain specific tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aman167k"&gt; /u/aman167k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkwe7u/how_does_gpt4o_image_generator_works_and_theres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T05:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkvv1m</id>
    <title>What wrong with Gemma 3?</title>
    <updated>2025-03-27T04:27:41+00:00</updated>
    <author>
      <name>/u/Neffor</name>
      <uri>https://old.reddit.com/user/Neffor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got the impression that Gemma 3 was held captive or detained in a basement, perhaps? The model is excellent and very accurate, but if anything, it constantly belittles itself and apologizes. Unlike the second version, which was truly friendly, the third version is creepy because it behaves like a frightened servant, not an assistant-colleague.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neffor"&gt; /u/Neffor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkvv1m/what_wrong_with_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T04:27:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgv2f</id>
    <title>Qwen releases Qwen/Qwen2.5-Omni-7B</title>
    <updated>2025-03-26T17:07:13+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt; &lt;img alt="Qwen releases Qwen/Qwen2.5-Omni-7B" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen releases Qwen/Qwen2.5-Omni-7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgv2f/qwen_releases_qwenqwen25omni7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jke5wg</id>
    <title>M3 Ultra Mac Studio 512GB prompt and write speeds for Deepseek V3 671b gguf q4_K_M, for those curious</title>
    <updated>2025-03-26T15:15:08+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone curious, here's the gguf numbers for Deepseek V3 q4_K_M (the older V3, not the newest one from this week). I loaded it up last night and tested some prompts:&lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf without Flash Attention&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:8102/16384, Amt:902/4000, Init:0.04s, Process:792.65s (9.05T/s), Generate:146.21s (6.17T/s), Total:938.86s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note above: normally I run in debugmode to get the ms per token, but forgot to enable it this time. Comes out to &lt;strong&gt;about 110ms per token for prompt processing&lt;/strong&gt;, and &lt;strong&gt;about 162ms per token for prompt response&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;M3 Ultra Mac Studio 512GB Deepseek V3 671b q4_K_M gguf with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:7847/16384, Amt:647/4000, Init:0.04s, Process:793.14s (110.2ms/T = 9.08T/s), Generate:103.81s (160.5ms/T = 6.23T/s), Total:896.95s (0.72T/s) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In comparison, here is Llama 3.3 70b q8 with Flash Attention On&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CtxLimit:6293/16384, Amt:222/800, Init:0.07s, Process:41.22s (8.2ms/T = 121.79T/s), Generate:35.71s (160.8ms/T = 6.22T/s), Total:76.92s (2.89T/s &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jke5wg/m3_ultra_mac_studio_512gb_prompt_and_write_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T15:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzpwf</id>
    <title>Models that can actually be used on a 3060</title>
    <updated>2025-03-27T09:08:31+00:00</updated>
    <author>
      <name>/u/negiconfit</name>
      <uri>https://old.reddit.com/user/negiconfit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are some models you folks are using on a 3060 graphics card and what problem does it solve for you.&lt;/p&gt; &lt;p&gt;It has to be something you actually are using and not about whether it is capable of running it cuz there’s many models that can run but not practicable use because it just hallucinates like crazy &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/negiconfit"&gt; /u/negiconfit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzpwf/models_that_can_actually_be_used_on_a_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T09:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkd8ik</id>
    <title>Notes on Deepseek v3 0324: Finally, the Sonnet 3.5 at home!</title>
    <updated>2025-03-26T14:35:21+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe we finally have the Claude 3.5 Sonnet at home. &lt;/p&gt; &lt;p&gt;With a release that was very Deepseek-like, the Whale bros released an updated Deepseek v3 with a significant boost in reasoning abilities. &lt;/p&gt; &lt;p&gt;This time, it's a proper MIT license, unlike the original model with a custom license, a 641GB, 685b model. With a knowledge cut-off date of July'24.&lt;br /&gt; But the significant difference is a massive boost in reasoning abilities. It's a base model, but the responses are similar to how a CoT model will think. And I believe RL with GRPO has a lot to do with it.&lt;/p&gt; &lt;p&gt;The OG model matched GPT-4o, and with this upgrade, it's on par with Claude 3.5 Sonnet; though you still may find Claude to be better at some edge cases, the gap is negligible.&lt;/p&gt; &lt;p&gt;To know how good it is compared to Claude Sonnets, I ran a few prompts, &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Deepseek v3 0324 understands &lt;strong&gt;user intention better&lt;/strong&gt; than before; I'd say it's better than Claude 3.7 Sonnet base and thinking. 3.5 is still better at this (perhaps the best)&lt;/li&gt; &lt;li&gt;Again, in raw quality &lt;strong&gt;code generation&lt;/strong&gt;, it is better than 3.7, on par with 3.5, and sometimes better.&lt;/li&gt; &lt;li&gt;Great at &lt;strong&gt;reasoning&lt;/strong&gt;, much better than any and all non-reasoning models available right now.&lt;/li&gt; &lt;li&gt;Better at the &lt;strong&gt;instruction following&lt;/strong&gt; than 3,7 Sonnet but below 3.5 Sonnet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For raw capability in real-world tasks, 3.5 &amp;gt;= v3 &amp;gt; 3.7&lt;/p&gt; &lt;p&gt;For a complete analysis and commentary, check out this blog post: &lt;a href="https://composio.dev/blog/deepseek-v3-0324-the-sonnet-3-5-at-home/"&gt;Deepseek v3 0324: The Sonnet 3.5 at home&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's crazy that there's no similar hype as the OG release for such a massive upgrade. They missed naming it v3.5, or else it would've wiped another bunch of billions from the market. It might be the time Deepseek hires good marketing folks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I’d love to hear about your experience with the new DeepSeek-V3 (0324). How do you like it, and how would you compare it to Claude 3.5 Sonnet?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkd8ik/notes_on_deepseek_v3_0324_finally_the_sonnet_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T14:35:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkxhcu</id>
    <title>Gemini 2.5 Pro Dropping Balls</title>
    <updated>2025-03-27T06:16:11+00:00</updated>
    <author>
      <name>/u/Few_Ask683</name>
      <uri>https://old.reddit.com/user/Few_Ask683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"&gt; &lt;img alt="Gemini 2.5 Pro Dropping Balls" src="https://external-preview.redd.it/ZzkyZjhta3FjNnJlMVzwUJIkOD2Hxv0jtWenSPiKkDRwVwE01itA-s_OLvqI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd90e107d4028ec7a2d280854a842f53ce9b0600" title="Gemini 2.5 Pro Dropping Balls" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Ask683"&gt; /u/Few_Ask683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7e5dflkqc6re1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkxhcu/gemini_25_pro_dropping_balls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T06:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkgvxn</id>
    <title>Qwen 2.5 Omni 7B is out</title>
    <updated>2025-03-26T17:08:12+00:00</updated>
    <author>
      <name>/u/Lowkey_LokiSN</name>
      <uri>https://old.reddit.com/user/Lowkey_LokiSN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt; &lt;img alt="Qwen 2.5 Omni 7B is out" src="https://external-preview.redd.it/8SmAxGhIQPYbKQ360sskPqKhJl5vPSWEfB2CyOiyRq8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be40495e2b1d57173ebf46c043544693d2bbcf52" title="Qwen 2.5 Omni 7B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172"&gt;https://preview.redd.it/huvknotdh2re1.png?width=1182&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=866c6cca9237016d2756c3b36b573cb2e3a92172&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF link: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;https://huggingface.co/Qwen/Qwen2.5-Omni-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Tweet seems to have been deleted so attached image&lt;br /&gt; Edit #2: Reposted tweet: &lt;a href="https://x.com/Alibaba_Qwen/status/1904944923159445914"&gt;https://x.com/Alibaba_Qwen/status/1904944923159445914&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowkey_LokiSN"&gt; /u/Lowkey_LokiSN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkgvxn/qwen_25_omni_7b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T17:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkix1t</id>
    <title>China may effectively ban at least some Nvidia GPUs. What will Nvidia do with all those GPUs if they can't sell them in China?</title>
    <updated>2025-03-26T18:30:23+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has made cut down versions of Nvidia GPUs for China that duck under the US export restrictions to China. But it looks like China may effectively ban those Nvidia GPUs in China because they are so power hungry. They violate China's green laws. That's a pretty big market for Nvidia. What will Nvidia do with all those GPUs if they can't sell the in China?&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513"&gt;https://www.investopedia.com/beijing-enforcement-of-energy-rules-could-hit-nvidia-china-business-report-says-11703513&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkix1t/china_may_effectively_ban_at_least_some_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-26T18:30:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzjve</id>
    <title>Microsoft develop a more efficient way to add knowledge into LLMs</title>
    <updated>2025-03-27T08:55:51+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt; &lt;img alt="Microsoft develop a more efficient way to add knowledge into LLMs" src="https://external-preview.redd.it/aCGhAR6FEKRX-h5rqecZAFckea8B8CJ4kaRGE3aJoC0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ed759c95a14ac48041ed2121cc23df6c9a4808d" title="Microsoft develop a more efficient way to add knowledge into LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:55:51+00:00</published>
  </entry>
</feed>
